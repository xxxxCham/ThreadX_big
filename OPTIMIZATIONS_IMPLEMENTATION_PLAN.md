# ðŸš€ OPTIMISATIONS THREADX - PLAN D'IMPLÃ‰MENTATION COMPLET

**Date**: 31 Octobre 2025
**Objectif**: Utiliser 100% puissance calcul (CPU 90%+, RAM 80%+, GPU1 80%+, GPU2 pleine utilisation)

---

## âœ… OPTIMISATIONS DÃ‰JÃ€ IMPLÃ‰MENTÃ‰ES

### 1. **Estimation Temps RÃ©el** â±ï¸
**Fichier**: `src/threadx/optimization/engine.py`

**ImplÃ©mentÃ©**:
- âœ… MÃ©thode `_update_progress_estimation()` - Calcul vitesse traitement
- âœ… Historique glissant (10 derniers points)
- âœ… Estimation basÃ©e sur `scÃ©narios/seconde`
- âœ… Format temps lisible (Xh Ym Zs)
- âœ… Affichage dans `_log_progress()`:
  ```
  ðŸ“Š ProgrÃ¨s: 1500/10000 (15.0%) |
  â±ï¸  Ã‰coulÃ©: 5m 30s |
  â³ Restant: 31m 15s |
  âš¡ Vitesse: 4.53 combos/s
  ```

**Variables ajoutÃ©es**:
```python
self.completed_scenarios = 0
self.last_progress_time = None
self.progress_history = []  # (timestamp, completed_count)
self.estimated_time_remaining = None
self.avg_scenario_time = None
```

---

### 2. **Preset Manuel 30 Workers** ðŸ”§
**Fichier**: `src/threadx/optimization/presets/execution_presets.toml` (nouveau)

**ImplÃ©mentÃ©**:
- âœ… Fichier TOML avec presets workers
- âœ… Preset `workers.manuel_30`:
  ```toml
  [workers.manuel_30]
  max_workers = 30
  batch_size = 2000
  gpu_utilization_target = 0.85
  cpu_utilization_target = 0.90
  ram_utilization_target = 0.80
  ```
- âœ… Preset combinÃ© `combined.manuel_30_full_power`:
  ```toml
  [combined.manuel_30_full_power]
  max_workers = 30
  batch_size = 2000
  gpu_target = 0.85
  cpu_target = 0.90
  ram_target = 0.80
  estimated_speedup = "5-10x vs dÃ©faut"
  ```

**Fichier**: `src/threadx/optimization/presets/ranges.py`

**ImplÃ©mentÃ©**:
- âœ… Fonction `load_execution_presets()` - Charge config TOML
- âœ… Fonction `get_execution_preset(preset_name)` - RÃ©cupÃ¨re preset par nom
- âœ… Export dans `__all__`

**Usage**:
```python
from threadx.optimization.presets import get_execution_preset

preset = get_execution_preset("manuel_30")
runner = SweepRunner(max_workers=preset["max_workers"])
```

---

## ðŸ”¨ OPTIMISATIONS Ã€ IMPLÃ‰MENTER (Phase 2)

### 3. **Graphique RÃ©sultats Backtests** ðŸ“Š
**Fichier Ã  crÃ©er**: `src/threadx/visualization/backtest_charts.py`

**FonctionnalitÃ©s requises**:
```python
def generate_backtest_chart(
    results_df: pd.DataFrame,
    ohlcv_data: pd.DataFrame,
    best_combo: Dict,
    symbol: str,
    timeframe: str,
    output_path: str = "backtest_results.html"
):
    """
    GÃ©nÃ¨re graphique interactif des rÃ©sultats backtest.

    Affiche:
    - Bougies japonaises (OHLCV) sur pÃ©riode complÃ¨te
    - Signaux ENTRÃ‰E (flÃ¨ches vertes â–²)
    - Signaux SORTIE (flÃ¨ches rouges â–¼)
    - Ã‰quitÃ© curve (overlay ou subplot)
    - Indicateurs utilisÃ©s (Bollinger Bands, ATR, etc.)

    Librairie recommandÃ©e: Plotly (interactif HTML)
    """
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots

    # Subplot 1: Prix + Signaux
    # Subplot 2: Ã‰quitÃ©
    # Subplot 3: Indicateurs (optionnel)

    fig = make_subplots(
        rows=3, cols=1,
        shared_xaxes=True,
        row_heights=[0.6, 0.2, 0.2],
        subplot_titles=('Prix & Signaux', 'Ã‰quitÃ©', 'Indicateurs')
    )

    # Bougies
    fig.add_trace(go.Candlestick(
        x=ohlcv_data.index,
        open=ohlcv_data['open'],
        high=ohlcv_data['high'],
        low=ohlcv_data['low'],
        close=ohlcv_data['close'],
        name='Prix'
    ), row=1, col=1)

    # EntrÃ©es (rÃ©cupÃ©rer depuis trades_history)
    entries = ...  # Extraire timestamps et prix d'entrÃ©e
    fig.add_trace(go.Scatter(
        x=entries.index,
        y=entries['price'],
        mode='markers',
        marker=dict(symbol='triangle-up', size=12, color='green'),
        name='EntrÃ©es'
    ), row=1, col=1)

    # Sorties
    exits = ...  # Extraire timestamps et prix de sortie
    fig.add_trace(go.Scatter(
        x=exits.index,
        y=exits['price'],
        mode='markers',
        marker=dict(symbol='triangle-down', size=12, color='red'),
        name='Sorties'
    ), row=1, col=1)

    # Ã‰quitÃ© curve
    fig.add_trace(go.Scatter(
        x=equity_curve.index,
        y=equity_curve['equity'],
        mode='lines',
        name='Ã‰quitÃ©',
        line=dict(color='blue', width=2)
    ), row=2, col=1)

    # Indicateurs (ex: Bollinger Bands)
    if 'bb_upper' in indicators:
        fig.add_trace(go.Scatter(
            x=indicators.index,
            y=indicators['bb_upper'],
            mode='lines',
            name='BB Upper',
            line=dict(color='gray', dash='dash')
        ), row=1, col=1)

    # Sauvegarder
    fig.write_html(output_path)
    logger.info(f"Graphique sauvegardÃ©: {output_path}")
```

**IntÃ©gration**:
- Appeler aprÃ¨s `run_grid()` ou `run_monte_carlo()`
- Extraire meilleur combo depuis `results_df`
- Charger donnÃ©es OHLCV utilisÃ©es
- RÃ©cupÃ©rer `trades_history` depuis stratÃ©gie

---

### 4. **Optimisation Puissance Calcul** ðŸ”¥

#### A. **Augmenter Batch Size Dynamique**
**Fichier**: `src/threadx/optimization/engine.py`

**Changements**:
```python
# AVANT (ligne 478):
batch_size = 1000

# APRÃˆS:
if self.max_workers >= 30:
    batch_size = 2000  # Pour preset manuel_30
elif self.max_workers >= 16:
    batch_size = 1500
else:
    batch_size = 1000
```

**Impact**: Moins de overhead soumission futures, GPU mieux saturÃ©

---

#### B. **Optimisation GPU Load Balancing**
**Fichier**: `src/threadx/gpu/multi_gpu.py`

**ProblÃ¨me actuel**:
- GPU1 (RTX 5090) utilise seulement 2.5GB / 16GB = 15%
- GPU2 (RTX 2060) peu utilisÃ©
- Balance par dÃ©faut: 75% / 25%

**Solution**:

```python
# Dans MultiGPUManager.__init__()

# AVANT:
self.device_balance = {
    "5090": 0.75,
    "2060": 0.25
}

# APRÃˆS (plus agressif):
self.device_balance = {
    "5090": 0.80,  # Augmenter Ã  80%
    "2060": 0.20
}

# ET activer auto-balance dynamique:
if auto_optimize:
    optimal_ratios = self.profile_auto_balance(
        sample_size=100_000,
        warmup=3,
        runs=5
    )
    self.set_balance(optimal_ratios)
```

**Fichier**: `src/threadx/indicators/gpu_integration.py`

**Changements**:
```python
# RÃ©duire seuil GPU (ligne ~65):
# AVANT:
self.min_samples_for_gpu = 1000

# APRÃˆS:
self.min_samples_for_gpu = 500  # Utiliser GPU plus tÃ´t
```

---

#### C. **Augmenter Chunk Size DonnÃ©es**
**Fichier**: `src/threadx/gpu/multi_gpu.py`

**Dans `_split_workload()`**:
```python
# Augmenter taille minimale chunks pour mieux saturer VRAM

MIN_CHUNK_SIZE_GPU = 50_000  # Au lieu de 10_000

# Validation chunk size
for chunk in chunks:
    if chunk.expected_size < MIN_CHUNK_SIZE_GPU and device != 'cpu':
        logger.warning(
            f"Chunk trop petit ({chunk.expected_size}) "
            f"pour GPU {device}, risque sous-utilisation VRAM"
        )
```

---

#### D. **ParallÃ©lisme Indicateurs Bank**
**Fichier**: `src/threadx/indicators/bank.py`

**Augmenter workers batch**:
```python
# Ligne ~400 dans batch_ensure_indicators()

# AVANT:
max_workers = 4

# APRÃˆS:
import os
max_workers = os.cpu_count() or 8  # Utiliser tous les cores CPU
```

---

#### E. **Prefetch DonnÃ©es GPU**
**Fichier**: `src/threadx/gpu/multi_gpu.py`

**Dans `_compute_chunk()`**:
```python
# Ajouter pinned memory pour transferts asynchrones

import cupy as cp

# Allouer pinned memory pool
if not hasattr(cp.cuda, '_pinned_pool'):
    cp.cuda.set_pinned_memory_allocator(
        cp.cuda.PinnedMemoryPool().malloc
    )

# Dans transfert GPU:
with cp.cuda.Stream(non_blocking=True):
    device_data = cp.asarray(chunk_data, order='C')  # Contiguous
```

**Impact**: Overlap transferts CPUâ†”GPU avec compute

---

#### F. **Augmenter Workers Dynamiquement**
**Fichier**: `src/threadx/optimization/engine.py`

**Dans `__init__()`**:
```python
# Si preset manuel_30 dÃ©tectÃ©:
if max_workers >= 30:
    # DÃ©sactiver ajustement adaptatif workers
    self._adaptive_workers = False
    logger.info("Mode haute performance: 30 workers fixes")

    # Augmenter batch soumission
    self._submission_batch_size = 3000

    # Pre-warm GPU
    if self.use_multigpu and self.gpu_manager:
        logger.info("Pre-warming GPUs...")
        self.gpu_manager.profile_auto_balance(
            sample_size=50_000,
            warmup=5  # Plus de warmup pour stabilitÃ©
        )
```

---

#### G. **Monitoring Utilisation Ressources**
**Fichier nouveau**: `src/threadx/utils/resource_monitor.py`

```python
"""
Monitoring utilisation ressources en temps rÃ©el.
"""

import psutil
import time
from typing import Dict, Optional
import cupy as cp

def get_resource_usage() -> Dict[str, float]:
    """
    RÃ©cupÃ¨re utilisation actuelle CPU/RAM/GPU.

    Returns:
        {
            'cpu_percent': 45.2,
            'ram_percent': 62.1,
            'ram_used_gb': 15.3,
            'gpu0_percent': 25.8,
            'gpu0_vram_used_gb': 2.5,
            'gpu1_percent': 8.3,
            'gpu1_vram_used_gb': 0.8
        }
    """
    stats = {}

    # CPU
    stats['cpu_percent'] = psutil.cpu_percent(interval=0.1)

    # RAM
    mem = psutil.virtual_memory()
    stats['ram_percent'] = mem.percent
    stats['ram_used_gb'] = mem.used / (1024**3)

    # GPUs
    try:
        for i in range(cp.cuda.runtime.getDeviceCount()):
            with cp.cuda.Device(i):
                mem_info = cp.cuda.runtime.memGetInfo()
                used = (mem_info[1] - mem_info[0]) / (1024**3)
                total = mem_info[1] / (1024**3)

                stats[f'gpu{i}_percent'] = (used / total) * 100
                stats[f'gpu{i}_vram_used_gb'] = used
                stats[f'gpu{i}_vram_total_gb'] = total
    except:
        pass

    return stats

def log_resource_usage(logger):
    """Log pÃ©riodique des ressources."""
    stats = get_resource_usage()
    logger.info(
        f"ðŸ’» CPU: {stats.get('cpu_percent', 0):.1f}% | "
        f"ðŸ§  RAM: {stats.get('ram_percent', 0):.1f}% "
        f"({stats.get('ram_used_gb', 0):.1f} GB) | "
        f"ðŸŽ® GPU0: {stats.get('gpu0_percent', 0):.1f}% "
        f"({stats.get('gpu0_vram_used_gb', 0):.1f} GB) | "
        f"ðŸŽ® GPU1: {stats.get('gpu1_percent', 0):.1f}% "
        f"({stats.get('gpu1_vram_used_gb', 0):.1f} GB)"
    )
```

**IntÃ©gration dans `engine.py`**:
```python
from threadx.utils.resource_monitor import log_resource_usage

# Dans boucle exÃ©cution (tous les 500 combos):
if completed_count[0] % 500 == 0:
    self._log_progress()
    log_resource_usage(self.logger)  # Monitoring ressources
```

---

## ðŸ“Š RÃ‰SUMÃ‰ DES GAINS ATTENDUS

| Optimisation | Impact CPU | Impact RAM | Impact GPU1 | Impact GPU2 | Speedup |
|-------------|-----------|-----------|------------|------------|---------|
| **30 workers** | +70% â†’ 90% | +50% â†’ 80% | +30% â†’ 60% | +20% â†’ 40% | 3-4x |
| **Batch 2000** | - | - | +20% â†’ 80% | +20% â†’ 60% | 1.5x |
| **Chunk sizeâ†‘** | - | - | +15% | +15% | 1.2x |
| **Prefetch async** | - | - | +10% | +10% | 1.3x |
| **Total combinÃ©** | **90%+** | **80%+** | **85%+** | **70%+** | **~8-10x** |

---

## ðŸš€ CHECKLIST D'IMPLÃ‰MENTATION

### Phase 1 (DÃ©jÃ  fait) âœ…
- [x] Estimation temps rÃ©el (engine.py)
- [x] Preset manuel_30 TOML (execution_presets.toml)
- [x] Fonctions load/get execution preset (ranges.py)
- [x] Batch sizing dynamique (engine.py: 30+â†’2000, 16+â†’1500)
- [x] GPU threshold rÃ©duit (gpu_integration.py: 1000â†’500)

### Phase 2 (DÃ©jÃ  fait) âœ…
- [x] CrÃ©er `resource_monitor.py` avec get_resource_usage(), log_resource_usage(), get_utilization_score()
- [x] IntÃ©grer monitoring dans _log_progress() (tous les 500 combos)
- [x] Warning si score d'utilisation < 50%
- [x] CrÃ©er `backtest_charts.py` avec generate_backtest_chart()
- [x] ImplÃ©menter graphiques Plotly (candlesticks + BB + entrÃ©es/sorties + Ã©quitÃ©)
- [x] CrÃ©er `visualization/__init__.py`

### Phase 3 (ImplÃ©mentÃ© maintenant) âœ… ðŸ†•
- [x] **Workers IndicatorBank â†’ cpu_count()** (bank.py: max_workers = None auto)
- [x] **MIN_CHUNK_SIZE_GPU = 50000** (multi_gpu.py: constante + validation)
- [x] **Auto-balance GPU startup** (engine.py: profile_auto_balance au __init__)
- [x] **ETA ajustÃ© durÃ©e plage** (engine.py: facteur correction Ã—duration_days/30)
- [ ] IntÃ©grer gÃ©nÃ©ration graphiques dans UI (aprÃ¨s run_grid/monte_carlo)
- [ ] ImplÃ©menter pinned memory async transfers (CuPy allocator)
- [ ] IntÃ©grer appel dans UI aprÃ¨s run
- [ ] Ajouter bouton "Voir Graphique" Streamlit

---

## ðŸ”§ COMMANDES D'APPLICATION RAPIDE

```bash
# 1. Appliquer preset manuel_30
# Dans votre code UI/CLI:
from threadx.optimization.presets import get_execution_preset
preset = get_execution_preset("manuel_30")
runner = SweepRunner(max_workers=preset["max_workers"])

# 2. Forcer auto-balance GPU
gpu_manager = get_default_manager()
optimal = gpu_manager.profile_auto_balance(sample_size=100_000, warmup=5, runs=5)
gpu_manager.set_balance(optimal)

# 3. Monitoring ressources
from threadx.utils.resource_monitor import log_resource_usage
log_resource_usage(logger)  # Appeler pÃ©riodiquement
```

---

**Conclusion**: Optimisations 1-2 implÃ©mentÃ©es. Optimisations 3-4 nÃ©cessitent modifications additionnelles dÃ©crites ci-dessus.
