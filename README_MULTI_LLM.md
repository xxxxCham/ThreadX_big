# ü§ñ Multi-LLM Optimizer - Guide d'Ex√©cution

Syst√®me d'optimisation automatique de strat√©gies de trading utilisant 2 agents LLM collaboratifs.

---

## üìã Vue d'Ensemble

Ce POC d√©montre un syst√®me multi-LLM capable d'analyser des r√©sultats de backtests et de proposer automatiquement des am√©liorations de param√®tres.

**Architecture**:
- **Analyst Agent** (deepseek-r1:70b): Analyse quantitative, identification de patterns
- **Strategist Agent** (gpt-oss:20b): G√©n√©ration cr√©ative de propositions
- **BacktestEngine** (GPU): Validation automatique des propositions

**Workflow**:
```
Sweep GPU (24 configs)
    ‚Üì
Analyst ‚Üí Patterns + Recommandations
    ‚Üì
Strategist ‚Üí 3 Propositions cr√©atives
    ‚Üì
Tests GPU ‚Üí Validation performances
    ‚Üì
Rapport + Visualisation
```

**Temps d'ex√©cution**: ~2-5 minutes total (selon GPU + vitesse LLM)

---

## ‚öôÔ∏è Pr√©requis

### 1. Ollama + Mod√®les LLM

Installer Ollama:
```bash
# Windows: T√©l√©charger depuis https://ollama.ai
# Linux/Mac:
curl -fsSL https://ollama.ai/install.sh | sh
```

Lancer serveur Ollama:
```bash
ollama serve
```

T√©l√©charger mod√®les (dans un autre terminal):
```bash
ollama pull deepseek-r1:70b   # ~40GB - Analyse quantitative
ollama pull gpt-oss:20b       # ~12GB - Propositions cr√©atives
```

V√©rifier mod√®les disponibles:
```bash
ollama list
```

### 2. Environnement Python

ThreadX requiert Python 3.12+ avec GPU support (optionnel mais recommand√©).

Activer environnement:
```powershell
# PowerShell
.\activate_threadx.ps1
```

V√©rifier packages:
```bash
pip list | grep -E "ollama|scipy|numpy|pandas"
```

### 3. GPU (Optionnel)

Le POC fonctionne sur CPU mais **5-10x plus rapide sur GPU**.

V√©rifier GPU disponible:
```python
import cupy as cp
print(cp.cuda.Device(0).compute_capability)  # Doit afficher version CUDA
```

Si erreur: ThreadX utilisera NumPy (CPU) automatiquement.

---

## üöÄ Ex√©cution Rapide

### Option 1: Notebook Jupyter (Recommand√©)

```bash
# Lancer Jupyter
jupyter notebook notebooks/multi_llm_optimizer.ipynb
```

Ex√©cuter cellules **dans l'ordre**:
1. **Section 1**: Configuration environnement
2. **Section 2**: D√©finition param√®tres
3. **Section 3**: Validation donn√©es
4. **Section 4**: Sweep initial (24 configs, ~30s GPU)
5. **Section 5**: Analyse Analyst (~30-60s)
6. **Section 6**: Propositions Strategist (~20-40s)
7. **Section 7**: Tests automatiques (~10s)
8. **Section 8**: Visualisation + Rapport

**Temps total**: 2-5 minutes

### Option 2: Script Python

```python
# TODO: Cr√©er version script standalone
# python scripts/run_multi_llm_poc.py --strategy MA_Crossover --n-proposals 3
```

---

## üìä R√©sultats Attendus

### Outputs du Notebook

1. **Tableau Sweep Initial** (Section 4):
   ```
   Top 3 Sharpe:
      short_period  long_period  sharpe_ratio  max_drawdown
   0            15           30         1.823        -0.156
   1            10           50         1.742        -0.189
   2            20           30         1.698        -0.142
   ```

2. **Analyse Analyst** (Section 5):
   ```
   PATTERNS IDENTIFI√âS:
   1. short_period < 15 dans 4/5 top configs
   2. long_period entre 30-50 optimal
   3. use_ema=False l√©g√®rement sup√©rieur
   
   RECOMMANDATIONS:
   1. Tester short_period=12 avec long_period=35
   2. Explorer zone short_period < 10
   3. R√©duire long_period pour limiter lag
   ```

3. **Propositions Strategist** (Section 6):
   ```
   PROPOSITION 1: Conservative
      short_period: 10 ‚Üí 12
      long_period: 30 ‚Üí 35
      Rationale: R√©duit drawdown observ√©...
   
   PROPOSITION 2: Aggressive
      short_period: 10 ‚Üí 8
      long_period: 30 ‚Üí 40
      Rationale: Exploite pattern short < 10...
   
   PROPOSITION 3: Exploratoire
      short_period: 10 ‚Üí 18
      long_period: 30 ‚Üí 25
      Rationale: Teste zone peu explor√©e...
   ```

4. **Comparaison R√©sultats** (Section 8):
   ```
   MEILLEURE CONFIG: Aggressive
      Sharpe: 1.912 (+0.089)
      Return: 42.3% (+8.1%)
      Drawdown: -14.2%
      
      üí° Am√©lioration Sharpe: +4.9%
   ```

5. **Visualisation**: `multi_llm_comparison.png` (3 graphiques bars)

---

## üîß Configuration Avanc√©e

### Modifier Strat√©gie

```python
# Dans Section 2 du notebook
STRATEGY_NAME = "Bollinger_Breakout"  # Au lieu de MA_Crossover

BASELINE_PARAMS = {
    "period": 20,
    "num_std": 2.0,
    # ...
}

PARAM_SPECS = {
    "period": {"min": 10, "max": 50, "step": 5, "type": int},
    # ...
}
```

### Ajuster Sweep

```python
# Plus de configs pour meilleure analyse
SWEEP_CONFIG = {
    "short_period": [5, 8, 10, 12, 15, 20],  # 6 valeurs
    "long_period": [25, 30, 40, 50, 70],     # 5 valeurs
    "use_ema": [False, True],                # 2 valeurs
}
# Total: 6 * 5 * 2 = 60 configs (~1 min GPU)
```

### Changer Mod√®les LLM

```python
# Analyst plus rapide (moins pr√©cis)
analyst = Analyst(model="qwen3-vl:30b", debug=False)

# Strategist plus cr√©atif
strategist = Strategist(model="gpt-oss:120b", debug=False)
```

Mod√®les disponibles ThreadX:
- `deepseek-r1:70b` (70B params, analyse profonde)
- `gpt-oss:120b` (120B params, tr√®s cr√©atif, lent)
- `gpt-oss:20b` (20B params, bon compromis)
- `gemma3:27b` (27B params, rapide)
- `qwen3-vl:30b` (30B params, multimodal)

---

## üêõ Troubleshooting

### Erreur: "Connection refused to Ollama"

**Cause**: Ollama serveur pas lanc√©.

**Solution**:
```bash
# Terminal 1: Lancer serveur
ollama serve

# Terminal 2: V√©rifier status
curl http://localhost:11434/api/tags
```

### Erreur: "Model not found: deepseek-r1:70b"

**Cause**: Mod√®le pas t√©l√©charg√©.

**Solution**:
```bash
ollama pull deepseek-r1:70b
ollama pull gpt-oss:20b
```

### Erreur: "CUDA out of memory"

**Cause**: GPU m√©moire insuffisante pour mod√®le 70B.

**Solution**:
```python
# Utiliser mod√®le plus petit
analyst = Analyst(model="gemma3:27b")  # Au lieu de deepseek-r1:70b
```

Ou activer quantization Ollama (r√©duit VRAM):
```bash
ollama run deepseek-r1:70b --quantize q4_0  # 4-bit quantization
```

### Sweep tr√®s lent (>5 min)

**Cause**: CPU uniquement (pas de GPU).

**Solution**:
- R√©duire configs de sweep (12 au lieu de 24)
- Installer CuPy pour GPU: `pip install cupy-cuda12x`
- V√©rifier GPU dispo: `nvidia-smi`

### LLM timeout apr√®s 60s

**Cause**: Mod√®le trop lent ou serveur surcharg√©.

**Solution**:
```python
# Augmenter timeout
analyst = Analyst(model="deepseek-r1:70b", timeout=120.0)  # 2 min au lieu de 1 min
```

---

## üìà Prochaines √âtapes

### Niveau 2: Semi-Automatique (2-3 semaines)

1. **Orchestrateur de boucle**:
   - It√©rations automatiques (N rounds)
   - Meilleure config devient nouvelle baseline
   - Arr√™t si Sharpe converge

2. **Historique des runs**:
   - SQLite database pour tracer propositions
   - Analyse meta-learning (quels patterns fonctionnent)

3. **UI Streamlit**:
   - Dashboard visualisation temps r√©el
   - Contr√¥le manuel (pause/resume/abort)

### Niveau 3: Production Compl√®te (6-8 semaines)

1. **D√©bat multi-agents**:
   - 3+ agents (Analyst, Strategist, Risk Manager)
   - Rounds de discussion (pro/con chaque proposition)
   - Vote consensuel

2. **Adaptive Sweep**:
   - LLM g√©n√®re les ranges de sweep
   - Bayesian optimization guid√©e par LLM

3. **Walk-Forward Validation**:
   - Tests out-of-sample automatiques
   - D√©tection overfitting
   - Robustness scoring

---

## üìö Ressources

- **Documentation ThreadX**: `COMPLETE_CODEBASE_SURVEY.md`
- **Architecture Multi-LLM**: `ARCHITECTURE_MULTI_LLM.md`
- **Use Cases d√©taill√©s**: `POC_MULTI_LLM_AGENT.md`
- **Code Agents**: `src/threadx/llm/agents/`

---

## ü§ù Support

**Probl√®mes courants**: Voir section Troubleshooting ci-dessus

**Questions/Bugs**: Ouvrir issue GitHub ou consulter docs ThreadX

**Performances LLM**: V√©rifier `ollama logs` pour diagnostics

---

**Version**: 1.0.0 (POC Option A)  
**Derni√®re MAJ**: 2025-01-XX  
**Auteur**: ThreadX Team
