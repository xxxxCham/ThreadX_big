========================================
FICHIER: LE_CODE_analyse_tierce5.txt
========================================
Voici quelques pistes d'optimisation :

Optimisation de la boucle de backtest Numba (bb_atr.py)

La fonction _backtest_loop_numba est le point le plus chaud (35.4%).

VÃ©rifiez que tous les appels Ã  cette fonction utilisent des types de donnÃ©es compatibles avec Numba (types numÃ©riques, pas de pandas DataFrame Ã  l'intÃ©rieur de la boucle).

Essayez de simplifier la logique Ã  l'intÃ©rieur de la boucle, rÃ©duisez les conditionnelles et les calculs redondants.

Utilisez nopython=True si ce n'est pas dÃ©jÃ  le cas (mais le dÃ©corateur actuel a nogil=True et boundscheck=False ce qui est bon).

VÃ©rifiez que la fonction est bien compilÃ©e en mode fastmath (dÃ©jÃ  activÃ©).

Optimisation de la gÃ©nÃ©ration de signaux (engine.py)

La mÃ©thode generate_signals prend 13.9% du temps.

Essayez de vectoriser les calculs de signaux autant que possible, en utilisant des opÃ©rations sur des tableaux NumPy plutÃ´t que des boucles.

Si des indicateurs sont recalculÃ©s plusieurs fois avec les mÃªmes paramÃ¨tres, envisagez de les mettre en cache.

Optimisation des indicateurs (bank.py)

La mÃ©thode ensure_indicators prend 7.2% du temps, mais a un taux de succÃ¨s de cache de 100%. C'est donc surtout le temps de vÃ©rification du cache et de chargement.

VÃ©rifiez que le cache est efficace (stockage en mÃ©moire, format binaire, etc.).

Si les indicateurs sont calculÃ©s sur des donnÃ©es volumineuses, envisagez de les prÃ©calculer et de les stocker sur disque (dÃ©jÃ  fait en Parquet, mais peut-Ãªtre optimiser la lecture).

Optimisation de l'exÃ©cution parallÃ¨le

Le systÃ¨me utilise ProcessPoolExecutor et ThreadPoolExecutor. VÃ©rifiez que le nombre de workers est optimal (ni trop, ni trop peu).

La fonction _calculate_optimal_workers (lignes 141-191) semble dÃ©jÃ  calculer le nombre optimal de workers. VÃ©rifiez qu'elle fonctionne correctement.

Pour le Grid Search, si le nombre de combinaisons est trÃ¨s grand, envisagez de diviser le travail en plusieurs lots (chunking) pour Ã©viter la surcharge de mÃ©moire.

Optimisation de la mÃ©moire

Lors de l'exÃ©cution de nombreux backtests, la mÃ©moire peut devenir un problÃ¨me. Utilisez des donnÃ©es de type float32 si la prÃ©cision le permet, pour rÃ©duire la mÃ©moire.

LibÃ©rez explicitement la mÃ©moire des objets volumineux (comme les DataFrames) dÃ¨s qu'ils ne sont plus nÃ©cessaires.

Optimisation des donnÃ©es d'entrÃ©e

Utilisez des formats de donnÃ©es efficaces (Parquet est dÃ©jÃ  bien, mais vÃ©rifiez la compression et les types de colonnes).

Lors de la lecture des fichiers Parquet, utilisez le filtrage par colonnes pour ne charger que les colonnes nÃ©cessaires.

Optimisation du cache des indicateurs

Le cache des indicateurs est dÃ©jÃ  efficace (100% hit rate), mais on peut envisager de le prÃ©chauffer (precompute) pour les paramÃ¨tres les plus courants.

Optimisation du code Numba

Assurez-vous que les fonctions appelÃ©es Ã  l'intÃ©rieur de la boucle Numba sont Ã©galement compilÃ©es avec Numba (ou sont des fonctions Numba natives).

Ã‰vitez d'utiliser des appels Ã  des fonctions Python pures dans la boucle Numba.

Optimisation du garbage collector

DÃ©sactivez le garbage collector pendant l'exÃ©cution de la boucle critique, puis rÃ©activez-le.

Utilisation du GPU

Le code utilise dÃ©jÃ  CuPy pour certaines opÃ©rations. VÃ©rifiez si les parties critiques (comme le calcul des indicateurs) peuvent Ãªtre accÃ©lÃ©rÃ©es par le GPU.

Attention: le transfert de donnÃ©es entre CPU et GPU a un coÃ»t. Il faut que les calculs soient suffisamment lourds pour justifier ce transfert.

RÃ©duction du nombre de combinaisons

Pour le Grid Search, si l'espace des paramÃ¨tres est trop grand, envisagez des mÃ©thodes d'optimisation plus intelligentes (comme le Bayesian Optimization) pour explorer l'espace plus efficacement.

Optimisation de la reconstruction des trades (bb_atr.py, lignes 843-900)

Cette partie est active et reprÃ©sente une part importante du temps. Essayez de simplifier la logique de reconstruction des trades.

PrÃ©chauffage Numba (engine.py, lignes 366-436)

La fonction _warmup_numba_jit est utilisÃ©e pour prÃ©chauffer le compilateur Numba. Assurez-vous qu'elle est efficace et qu'elle couvre les cas les plus courants.

Parallelisation des indicateurs

Si les indicateurs sont indÃ©pendants, calculez-les en parallÃ¨le (dÃ©jÃ  fait en partie avec le ThreadPoolExecutor).

Utilisation de Dask pour la parallÃ©lisation

Si le nombre de combinaisons est trÃ¨s grand, envisagez d'utiliser Dask pour une parallÃ©lisation plus efficace et une meilleure gestion de la mÃ©moire.

Profiling plus fin

Utilisez un profiler plus fin (comme line_profiler ou vprof) pour identifier les lignes spÃ©cifiques qui prennent le plus de temps dans les fonctions critiques.

Compilation Ahead-of-Time (AOT) avec Numba

Pour les fonctions Numba critiques, envisagez une compilation AOT pour Ã©viter le temps de compilation Ã  l'exÃ©cution.

Optimisation des structures de donnÃ©es

Utilisez des tableaux NumPy contigus en mÃ©moire pour de meilleures performances avec Numba.

RÃ©duction de la prÃ©cision

Si la prÃ©cision n'est pas critique, utilisez float32 au lieu de float64 pour rÃ©duire la mÃ©moire et accÃ©lÃ©rer les calculs.

Ã‰viter les copies inutiles

VÃ©rifiez que le code ne fait pas de copies inutiles de donnÃ©es, surtout dans les boucles critiques.

En rÃ©sumÃ©, concentrez-vous d'abord sur la boucle Numba de backtest et la gÃ©nÃ©ration de signaux. Ensuite, optimisez la parallÃ©lisation et la mÃ©moire. Enfin, envisagez des mÃ©thodes d'optimisation plus intelligentes pour rÃ©duire le nombre de combinaisons Ã  tester.

En analysant votre systÃ¨me de calcul, voici plusieurs pistes d'optimisation structurÃ©es par prioritÃ© :

ðŸ”´ OPTIMISATIONS CRITIQUES (Impact immÃ©diat)
1. Optimisation Numba - Point chaud 35.4%
python
# Dans bb_atr.py - Optimiser la boucle principale
@njit(fastmath=True, cache=True, boundscheck=False, nogil=True, parallel=True)  # â† AJOUTER parallel=True
def _backtest_loop_numba_optimized(close, signals, ...):
    # PrÃ©-allouer les arrays avec numpy.empty()
    positions = np.empty_like(close, dtype=np.float64)
    pnls = np.empty_like(close, dtype=np.float64)
    
    # Vectoriser les calculs rÃ©pÃ©titifs
    for i in prange(len(close)):  # â† Utiliser prange pour parallÃ©lisation
        if i > 0:
            # Ã‰viter les recalculs de valeurs constantes
            price_change = close[i] - close[i-1]
            # Logique optimisÃ©e...
2. Cache stratÃ©gique des indicateurs
python
# Dans bank.py - ImplÃ©menter un cache multi-niveaux
class MultiLevelIndicatorCache:
    def __init__(self):
        self.memory_cache = {}  # L1: MÃ©moire (LRU)
        self.disk_cache_path = "cache/indicators/"  # L2: Disk (Parquet)
        self.hit_rates = {}
    
    def get_indicators_batch(self, symbol_tf_pairs):
        """Chargement batch pour Ã©viter les I/O sÃ©quentiels"""
        # Group by symbol/timeframe pour minimiser les accÃ¨s disque
        pass
ðŸŸ¡ OPTIMISATIONS STRUCTURELLES
3. Parallelisation intelligente du Grid Search
python
# Dans engine.py - Optimiser _execute_combinations()
def _execute_combinations_optimized(self, combinations):
    # Dynamic batching basÃ© sur la complexitÃ© des paramÃ¨tres
    batch_size = self._calculate_adaptive_batch_size(combinations)
    
    # Group by similar parameter patterns pour rÃ©utiliser le cache
    grouped_combos = self._group_similar_combinations(combinations)
    
    with ProcessPoolExecutor(max_workers=self.optimal_workers) as executor:
        # Soumettre par batches avec timeout intelligent
        futures = []
        for batch in self._create_balanced_batches(grouped_combos, batch_size):
            future = executor.submit(self._evaluate_batch, batch)
            futures.append(future)
        
        # Collecter rÃ©sultats avec gestion d'erreurs
        return self._collect_results_with_fallback(futures)
4. MÃ©canisme de Warmup JIT amÃ©liorÃ©
python
def _warmup_numba_jit_optimized(self):
    """Warmup intelligent avec donnÃ©es synthÃ©tiques reprÃ©sentatives"""
    # CrÃ©er des datasets de warmup de diffÃ©rentes tailles
    warmup_sizes = [1000, 5000, 10000]  # Couvrir les cas d'usage
    
    for size in warmup_sizes:
        synthetic_data = self._generate_representative_data(size)
        
        # PrÃ©-compiler toutes les fonctions Numba critiques
        self._precompile_critical_functions(synthetic_data)
    
    # Warmup GPU simultanÃ©ment si disponible
    if self.use_gpu:
        self._warmup_gpu_kernels()
5. Optimisation mÃ©moire des trades
python
# Dans bb_atr.py - Lignes 843-900
def _reconstruct_trades_optimized(self, signals, prices, timestamps):
    """Version optimisÃ©e de la reconstruction des trades"""
    # Utiliser des arrays structurÃ©s numpy au lieu de listes de dicts
    trade_dtype = np.dtype([
        ('entry_time', 'datetime64[ns]'),
        ('exit_time', 'datetime64[ns]'), 
        ('pnl', 'float64'),
        ('size', 'float64'),
        ('entry_price', 'float64'),
        ('exit_price', 'float64')
    ])
    
    trades = np.empty(1000, dtype=trade_dtype)  # PrÃ©-allocation
    # Logique vectorisÃ©e...
ðŸŸ¢ OPTIMISATIONS AVANCÃ‰ES
6. Adaptive Worker Allocation
python
def _calculate_optimal_workers_adaptive(self, combinations):
    """Allocation dynamique basÃ©e sur la complexitÃ©"""
    complexity_score = self._estimate_computation_complexity(combinations)
    available_ram = psutil.virtual_memory().available / (1024 ** 3)  # GB
    
    if complexity_score > 1000 and available_ram > 32:
        return min(60, os.cpu_count() * 4)  # Agressif pour les grosses grilles
    elif complexity_score > 100:
        return min(30, os.cpu_count() * 2)  # Standard
    else:
        return min(8, os.cpu_count())  # Conservateur
7. Pipeline de donnÃ©es streamÃ©
python
class StreamingDataPipeline:
    def __init__(self):
        self.batch_size = 10000
        self.preload_queue = Queue(maxsize=3)
    
    def stream_ohlcv_data(self, symbol, timeframe):
        """Streaming au lieu de chargement complet"""
        # Yield des batches avec prÃ©-chargement anticipatif
        pass
8. Optimisations Monte-Carlo spÃ©cifiques
python
def run_monte_carlo_optimized(self, n_scenarios, params):
    """Monte-Carlo avec Ã©chantillonnage adaptatif"""
    # Early stopping basÃ© sur la convergence
    best_score = -np.inf
    patience = 100
    
    for i in range(n_scenarios):
        scenario = self._sample_informed_params(params, best_score)
        score = self._evaluate_scenario(scenario)
        
        if score > best_score:
            best_score = score
            patience = 100  # Reset patience
        else:
            patience -= 1
            
        if patience == 0 and i > n_scenarios // 2:
            break  # ArrÃªt anticipÃ© si stagnation
    
    return best_score
ðŸ“Š METRICS & MONITORING
9. SystÃ¨me de profiling en temps rÃ©el
python
class OptimizationProfiler:
    def __init__(self):
        self.metrics = {
            'numba_time': [], 
            'indicator_time': [],
            'memory_usage': [],
            'cache_hit_rate': []
        }
    
    def suggest_optimizations(self):
        """Recommandations automatiques basÃ©es sur les metrics"""
        if self.metrics['cache_hit_rate'][-1] < 0.8:
            return "Augmenter la taille du cache L1"
        if self.metrics['numba_time'][-1] > 0.4:  # 40% du temps
            return "Optimiser la boucle Numba avec parallel=True"
ðŸŽ¯ PLAN D'ACTION PRIORISÃ‰
Semaine 1: ImplÃ©menter les optimisations Numba (35.4% de gain potentiel)

Semaine 2: Optimiser le cache multi-niveaux et le batch loading

Semaine 3: Parallelisation adaptive et dynamic batching

Semaine 4: Streaming pipeline et early stopping Monte-Carlo

Gain estimÃ©: 40-60% de rÃ©duction du temps d'exÃ©cution sur les grosses grilles.





========================================
FICHIER: LE_CODE_analyse_tierce6.txt
========================================
### Key Optimization Opportunities

Optimizing a backtesting and optimization system like ThreadX involves balancing computational efficiency, memory usage, and scalability, especially for grid search and Monte Carlo simulations. Research suggests that bottlenecks in similar Python-based quant finance tools often stem from loops, data handling, and parallelization overheads, with potential speedups of 2-5x through targeted refactoring. It seems likely that focusing on Numba and GPU integration could yield the highest immediate gains, though results may vary based on hardware and dataset size.

- **Numba Loop Refactoring**: Evidence leans toward eliminating Python objects in hot paths, potentially reducing CPU time by 20-35% in the core backtest loop.
- **Shared Resources for Indicators**: Sharing precomputed indicators across workers appears effective for cutting redundant computations, with studies on multiprocessing showing up to 50% memory savings.
- **Parallelization and Batching**: Chunking scenarios and centralizing I/O could minimize overhead, as parallel executors in Python often introduce scheduling costs without it.
- **GPU Acceleration**: For large-scale simulations, offloading to GPU might accelerate vectorized operations, though transfer costs require careful management.
- **Adaptive Sampling in Monte Carlo**: Intelligent sampling around promising parameters may reduce total evaluations needed, balancing exploration and exploitation.

#### Immediate Quick Wins (1-2 Days)
Start with low-effort changes like enabling Numba's parallel mode and pre-allocating arrays in `_backtest_loop_numba` (bb_atr.py). This could address the 35.4% bottleneck without major rewrites. Add a "fast_eval" mode in BacktestEngine to return only scalar metrics, skipping full DataFrames during sweeps.

#### Structural Improvements (3-5 Days)
Implement shared memory for indicators via multiprocessing.shared_memory to avoid recalculations per worker. Use batching in ProcessPoolExecutor to group scenarios, reducing submit overheadâ€”aim for chunks of 32-64 based on typical combo counts.

#### Advanced Enhancements (1-2 Weeks)
Explore GPU pipelines with CuPy for signal generation and trade simulation. For Monte Carlo, adopt adaptive sampling to focus on high-performing regions, potentially halving iterations while maintaining result quality.

Supporting resources: For Numba best practices, see Numba documentation (numba.pydata.org). GPU tips from CuPy docs (cupy.dev).

---

### Comprehensive Survey of Optimization Strategies for ThreadX Backtesting System

This survey provides an in-depth exploration of optimization opportunities for the ThreadX framework, a Python-based system for backtesting trading strategies with grid search (sweep) and Monte Carlo optimizations. Drawing from third-party analyses and established best practices in computational finance, we examine bottlenecks, propose refactoring strategies, and outline implementation paths. The goal is to enhance performance in high-compute scenarios, such as evaluating thousands of parameter combinations on large OHLCV datasets. While the system already incorporates strong elements like Numba JIT compilation, caching, and multi-GPU detection, analyses indicate significant room for improvement in CPU-bound loops, memory sharing, I/O efficiency, and adaptive algorithms. We'll structure this around key componentsâ€”Numba core, parallel execution, GPU integration, Monte Carlo specifics, and monitoringâ€”while incorporating code examples, potential gains, and caveats. Note that all suggestions assume a setup with Python 3.12+, Numba, CuPy, and access to multi-core CPUs or GPUs; testing on representative datasets is essential to quantify benefits.

#### 1. Core Numba Loop Optimization: Addressing the Primary Bottleneck
The `_backtest_loop_numba` function in bb_atr.py emerges as the hottest spot, consuming approximately 35.4% of CPU time across both grid and Monte Carlo modes. This loop handles trade simulation, signal application, and PnL calculations, making it central to every scenario evaluation. Analyses consistently highlight regressions from Python object reconstructions (e.g., lines 843-900), which undermine Numba's nopython mode efficiency. Best practices from Numba's documentation emphasize pure array-based operations to leverage just-in-time compilation and vectorization.

**Key Refactoring Approach: Two-Pass Pure Numba Design**  
Shift to a two-pass structure: (1) a fully Numba-compiled pass for raw computations using pre-allocated NumPy arrays, and (2) a post-processing Python step for DataFrame construction. This eliminates dynamic allocations like list.append() inside the JIT, which Numba handles poorly. Pre-allocate arrays to the maximum possible size (e.g., len(prices)//2 for trades) to avoid resizing.

Example Code Snippet (from analysis suggestions):
```python
from numba import njit, prange
import numpy as np

@njit(fastmath=True, cache=True, parallel=True)
def _backtest_loop_numba_pure(prices, signals, ...):
    max_trades = len(prices) // 2
    entry_idx = np.empty(max_trades, dtype=np.int32)
    exit_idx = np.empty(max_trades, dtype=np.int32)
    pnl = np.empty(max_trades, dtype=np.float64)
    trade_count = 0
    for i in prange(len(prices)):
        if entry_condition:  # Vectorized logic where possible
            entry_idx[trade_count] = i
            # ... compute exit and pnl ...
            trade_count += 1
    return entry_idx[:trade_count], exit_idx[:trade_count], pnl[:trade_count]

# Post-process in Python (engine.py)
def reconstruct_trades_dataframe(entry_idx, exit_idx, pnl, timestamps):
    return pd.DataFrame({'entry_time': timestamps[entry_idx], ...})
```

**Vectorization of Stops and Profits**:  
Further optimize by vectorizing stop-loss/take-profit checks using NumPy operations, which Numba can compile efficiently. This is particularly useful for GPU offloading later.

Example:
```python
@njit(fastmath=True)
def apply_stops_vectorized(prices, entry_prices, stop_pct, tp_pct):
    stop_prices = entry_prices * (1 - stop_pct)
    tp_prices = entry_prices * (1 + tp_pct)
    exit_signals = np.where((prices <= stop_prices) | (prices >= tp_prices), 1, 0)
    return exit_signals
```

**Expected Impact and Caveats**:  
- Gains: 20-40% reduction in per-scenario time, scaling linearly with combination count (e.g., 10,000 scenarios could see hours shaved off).
- Caveats: Parallel=True requires stateless logic; test for race conditions. Use float32 where precision allows to halve memory.

**Table 1: Numba Optimization Prioritization**
| Priority | Change | Estimated Gain | Effort Level | Dependencies |
|----------|--------|----------------|--------------|--------------|
| High | Remove Python lists/dicts; use arrays | 25-35% CPU | Medium | Numba 0.58+ |
| Medium | Add parallel=True with prange | 10-20% on multi-core | Low | Independent loops |
| Low | AOT compilation | 5-10% startup | High | Production builds |

#### 2. Memory and Data Sharing: Eliminating Redundant Computations
Indicator calculations via IndicatorBank (bank.py) account for ~7.2% of time, despite a 100% cache hit rate. The issue arises in multiprocessing: each worker may recompute or reload indicators independently. Shared memory solutions, as recommended in Python's multiprocessing docs, can centralize precomputation.

**Shared Memory Implementation for Indicators**:  
Precompute all indicators (RSI, Bollinger, ATR) once in the parent process and share via shared_memory.SharedMemory. Workers access read-only views, reducing per-worker overhead from ~7% to <1%.

Example Code (integrated into engine.py):
```python
from multiprocessing import shared_memory
import numpy as np

class SharedIndicatorBank:
    def __init__(self):
        self.shm_dict = {}

    def precompute_all_indicators(self, ohlcv_df):
        indicators = {'rsi': compute_rsi(ohlcv_df), 'atr': compute_atr(ohlcv_df), ...}
        for name, array in indicators.items():
            shm = shared_memory.SharedMemory(create=True, size=array.nbytes, name=f"indicator_{name}")
            shm_array = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf)
            shm_array[:] = array[:]
            self.shm_dict[name] = (shm.name, array.shape, array.dtype)
        return self.shm_dict

# In run_sweep or run_monte_carlo
shared_bank = SharedIndicatorBank()
shm_meta = shared_bank.precompute_all_indicators(ohlcv_df)
with ProcessPoolExecutor(max_workers=30, initializer=init_worker, initargs=(shm_meta,)) as executor:
    ...
```

**Data Normalization and Preloading**:  
Normalize OHLCV data upstream (data/normalize.py) and store in optimized Parquet with columnar compression. Use memmap for large datasets to avoid full loads.

**Impact**: Up to 50% memory reduction in multi-worker setups; faster startup as workers skip computations.

#### 3. Parallel Execution and I/O Enhancements: Scaling Workloads
ProcessPoolExecutor in engine.py handles parallel scenario evaluation, but per-task overhead grows with fine-grained submissions (e.g., one per combo). Analyses suggest chunking and centralized I/O to mitigate this.

**Scenario Chunking**:  
Group combinations into batches (e.g., 64 per worker) to reduce executor submits from 10,000 to ~156.

Example:
```python
def chunk_scenarios(scenarios, chunk_size=64):
    for i in range(0, len(scenarios), chunk_size):
        yield scenarios[i:i+chunk_size]

with ProcessPoolExecutor(max_workers=30) as executor:
    futures = [executor.submit(process_chunk, chunk, shm_meta) for chunk in chunk_scenarios(all_scenarios)]
```

**Centralized Batch I/O**:  
Use a dedicated thread for writing checkpoints to Parquet, batching results to avoid file locks.

Example with Queue:
```python
from queue import Queue
from threading import Thread

class CheckpointWriter:
    def __init__(self, output_path, batch_size=100):
        self.queue = Queue()
        self.writer_thread = Thread(target=self._write_loop, daemon=True)
        self.writer_thread.start()

    def _write_loop(self):
        batch = []
        while True:
            result = self.queue.get()
            if result is None: break
            batch.append(result)
            if len(batch) >= self.batch_size:
                pd.DataFrame(batch).to_parquet(output_path, append=True, compression='snappy')
                batch = []
```

**Fast Eval Mode**:  
In BacktestEngine, add a mode returning only dicts with metrics (sharpe, return, dd), skipping equity/trades DataFrames during optimization.

**Table 2: Parallelization Metrics Comparison**
| Metric | Current Approach | Optimized (Chunking + Shared Mem) | Gain |
|--------|------------------|-----------------------------------|------|
| Tasks Submitted | 10,000 (per combo) | 156 (chunks of 64) | 98% reduction in overhead |
| Memory per Worker | High (individual caches) | Low (shared views) | 40-60% savings |
| I/O Contention | High (per-worker appends) | Low (batched) | 70% faster writes |

#### 4. GPU Integration: Full Pipeline Acceleration
While GPU detection exists (gpu/multi_gpu.py), much logic remains CPU-bound. Port signal generation and trade simulation to CuPy for vectorized batches.

**Batch GPU Backtesting**:
Process scenario batches on GPU with custom kernels.

Example:
```python
import cupy as cp

class GPUBacktestEngine:
    @staticmethod
    def backtest_batch_gpu(scenarios_batch, prices_gpu, indicators_gpu):
        # ... CuPy arrays and kernel execution ...
        return cp.asnumpy(pnls_gpu)  # Minimal transfer

# Multi-GPU distribution
n_gpus = cp.cuda.runtime.getDeviceCount()
scenarios_per_gpu = np.array_split(scenarios, n_gpus)
with ThreadPoolExecutor(max_workers=n_gpus) as executor:
    results = executor.map(process_on_gpu, range(n_gpus), scenarios_per_gpu)
```

**Impact**: 2-5x speedup for large batches; caution on data transfer costs for small datasets.

#### 5. Monte Carlo-Specific Adaptations: Intelligent Sampling
Standard Monte Carlo (monte_carlo_optimizer.py) uses uniform sampling, but adaptive methods focus on promising regions.

**Adaptive Optimizer**:
Start with broad exploration, then Gaussian perturbations around top performers.

Example:
```python
class AdaptiveMonteCarloOptimizer:
    def run_adaptive(self, n_iterations=5):
        scenarios = self._sample_uniform(1000)
        results = self._evaluate_batch(scenarios)
        for _ in range(n_iterations):
            top_20 = self._get_top_percentile(results, 0.2)
            new_scenarios = self._sample_gaussian_around_best(top_20, 500, sigma=0.1)
            results.extend(self._evaluate_batch(new_scenarios))
```

**Early Stopping**: Halt if scores stagnate (patience counter).

**Impact**: Reduce evaluations by 30-50% while improving convergence.

#### 6. Monitoring, Profiling, and Warmup Enhancements
- **Real-Time Profiler**: Track metrics like Numba time and cache hits; suggest auto-optimizations (e.g., increase cache if hit rate <80%).
- **Warmup JIT**: Use synthetic data of varying sizes in initializer to cover cases.
- **Adaptive Workers**: Dynamically adjust based on complexity (psutil for RAM/CPU).

**Phased Action Plan**:
- Phase 1 (Quick Wins): Numba refactor, fast_eval mode (40-50% gain).
- Phase 2 (Structural): Shared memory, chunking (30-40% additional).
- Phase 3 (Advanced): GPU pipeline, adaptive MC (2-5x with hardware).

This survey expands on the direct recommendations, incorporating detailed implementations and quantitative estimates grounded in Python ecosystem benchmarks. Thorough profiling (e.g., with line_profiler) post-implementation is advised to validate gains.

#### Key Citations
- [Numba Documentation](https://numba.pydata.org/numba-doc/latest/user/parallel.html)
- [CuPy Best Practices](https://docs.cupy.dev/en/stable/user_guide/performance.html)
- [Python Multiprocessing Guide](https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes)
- [Adaptive Optimization in Quant Finance](https://arxiv.org/pdf/2310.03302.pdf) (via browse for Bayesian parallels)



========================================
FICHIER: LE_CODE_analyse_tierce7.txt
========================================
1. SÃ©parer clairement "gÃ©nÃ©ration de scÃ©narios" et "exÃ©cution"

ProblÃ¨me
Aujourdâ€™hui run_sweep() et run_monte_carlo() crÃ©ent chacun leurs scÃ©narios (generate_param_grid() pour le Sweep, generate_monte_carlo() pour le Monte Carlo), puis passent Ã§a Ã  _execute_combinations() dans le moteur. 

LE_CODE_analyse_tierce6


Donc lâ€™intention est dÃ©jÃ  bonne: lâ€™exÃ©cution est unifiÃ©e.

AmÃ©lioration concrÃ¨te
Rendre la gÃ©nÃ©ration complÃ¨tement plug-in, style:

GÃ©nÃ©rateur de scÃ©narios (nâ€™importe lequel) â†’ List[Dict[str, Any]].

Moteur dâ€™exÃ©cution â†’ ne connaÃ®t pas la source des scÃ©narios.

MÃ©canisme de reporting identique (mÃ©triques, logs, checkpoints).

Pourquoi

Tu supprimes tout code conditionnel if scenario_type == "grid"/"monte_carlo" au niveau CLI et orchestration.

Tu prÃ©pares lâ€™arrivÃ©e dâ€™autres modes (Bayesian / Adaptive Monte Carlo) sans toucher le moteur.

Ce qui change

Extraire generate_param_grid() et generate_monte_carlo() dans une interface commune (ex: ScenarioGenerator).

run.py devient juste un routeur trivial vers la bonne implÃ©mentation au lieu dâ€™avoir de la logique propre Ã  chaque mode. 

LE_CODE_analyse_tierce6

Impact
LisibilitÃ©. Maintenance. Pas un gain perf direct.
Valeur: oui.
PrioritÃ©: moyenne.

2. RÃ©duire la charge CPU par scÃ©nario

ProblÃ¨me
Le temps total dâ€™un sweep (grid ou Monte Carlo) est dominÃ© par deux choses:

Boucle de backtest Numba _backtest_loop_numba (~35.4 % du temps total). 

LE_CODE_analyse_tierce6

GÃ©nÃ©ration des signaux et gestion des trades (generate_signals() ~13.9 %, backtest_full() ~35.4 %). 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te A
CrÃ©er un mode "Ã©valuation rapide" qui ne reconstruit pas tout (DataFrame trades complet, equity curve complÃ¨te) pendant le sweep.
Tu calcules uniquement les mÃ©triques scalaires utiles Ã  la dÃ©cision (PNL total, Sharpe, max drawdown, etc.) et tu sauves le dÃ©tail complet seulement pour le top N. 

LE_CODE_analyse_tierce6

Effet

Moins de sÃ©ries pandas crÃ©Ã©es

Moins dâ€™allocations mÃ©moire

Moins dâ€™I/O dans les checkpoints

Impact attendu
RÃ©duction nette du coÃ»t par combinaison. Gain direct sur Sweep et Monte Carlo.
Valeur: Ã©levÃ©e.
PrioritÃ©: trÃ¨s haute.
Ceci est Ã  faire.

AmÃ©lioration concrÃ¨te B
Refactor de _backtest_loop_numba en mode 100 % arrays Numba, sans reconstruction dâ€™objets trades dans la partie JIT. On exÃ©cute la simulation purement numÃ©rique dans Numba puis on reconstruit le DataFrame trades aprÃ¨s coup cÃ´tÃ© Python. Cette refonte retire les structures Python dans le chemin chaud. Les estimations raisonnables sont 20 Ã  35 % de CPU en moins par scÃ©nario. 

LE_CODE_analyse_tierce6

Impact attendu
Câ€™est le meilleur levier perf pur.
Valeur: trÃ¨s Ã©levÃ©e.
PrioritÃ©: critique.

Remarque
Tu as notÃ© une rÃ©gression de -2.2 % liÃ©e Ã  la reconstruction des trades (lignes 843-900 de la boucle Numba actuelle). 

LE_CODE_analyse_tierce6


Rollbacker cette partie ou la sortir hors JIT rÃ¨gle Ã§a sans risque.

3. Mutualiser les donnÃ©es dâ€™entrÃ©e entre workers

ProblÃ¨me
Chaque worker recalculerait ou chargerait ses indicateurs (IndicatorBank.ensure_indicators() ~7.2 % du temps mÃªme avec 100 % de cache hit). 

LE_CODE_analyse_tierce6


Sous multiprocessing classique, chaque process a sa copie mÃ©moire.

AmÃ©lioration concrÃ¨te
PrÃ©-calculer tous les indicateurs nÃ©cessaires une seule fois dans le process parent puis exposer ces arrays en mÃ©moire partagÃ©e en lecture seule aux workers.
Exemple: multiprocessing.shared_memory ou Ã©quivalent local. 

LE_CODE_analyse_tierce6

Effet

DÃ©marrage worker plus rapide

Moins de RAM totale utilisÃ©e

Moins de pression GC

Impact attendu
Fort si tu testes beaucoup de scÃ©narios sur les mÃªmes donnÃ©es OHLCV.
Faible si tu fais juste quelques scÃ©narios ponctuels.
Valeur: Ã©levÃ©e pour les sweeps massifs.
PrioritÃ©: haute si tu lances 1 000+ combinaisons. Sinon pas urgent.

4. Batching cÃ´tÃ© exÃ©cuteur

ProblÃ¨me
Actuellement chaque combinaison de paramÃ¨tres part comme une tÃ¢che sÃ©parÃ©e dans ProcessPoolExecutor (30 workers). Cette granularitÃ© crÃ©e du coÃ»t dâ€™ordonnancement et de lock disque pour les checkpoints Parquet. 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te
Envoyer les scÃ©narios par lot (ex taille 32 ou 64) Ã  chaque worker.
Un worker traite un batch, renvoie un tableau de rÃ©sultats.
Un seul thread dÃ©diÃ© cÃ´tÃ© main process Ã©crit pÃ©riodiquement ces rÃ©sultats dans le fichier Parquet checkpoint, au lieu de laisser chaque worker toucher le disque.

Effet

Beaucoup moins de contention disque

Beaucoup moins dâ€™overhead submit/future

TraÃ§abilitÃ© plus simple

Impact attendu
Gains de stabilitÃ© et de dÃ©bit global, surtout quand tu passes Ã  plusieurs milliers de combinaisons Grid.
Valeur: Ã©levÃ©e si tu fais du volume.
PrioritÃ©: moyenne-haute.

Remarque
Pas besoin de changer la logique trading interne pour faire Ã§a. Tu touches juste lâ€™orchestration.

5. Monte Carlo: Ã©chantillonnage adaptatif

ProblÃ¨me
Le Monte Carlo actuel tire des combinaisons uniformÃ©ment avec un seed reproductible via np.random.RandomState(seed) dans monte_carlo_optimizer. Il explore mais nâ€™exploite pas les zones prometteuses. 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te
AprÃ¨s une premiÃ¨re passe, ne plus Ã©chantillonner partout.
Prendre les meilleurs rÃ©sultats (par exemple top 20 %) et gÃ©nÃ©rer de nouveaux tirages centrÃ©s autour de ces zones (perturbation gaussienne locale).
ArrÃªter si la perf stagne.

Effet

Moins dâ€™itÃ©rations totales pour converger

Plus ciblÃ©

MÃªme infra _execute_combinations(), donc pas de dette technique nouvelle

Impact attendu
30 Ã  50 % de runs en moins pour obtenir une zone optimale plausible. 

LE_CODE_analyse_tierce6


Valeur: Ã©levÃ©e si objectif = trouver vite un bon set de paramÃ¨tres.
PrioritÃ©: moyenne.

Remarque
Aucun intÃ©rÃªt pour un sweep Grid exhaustif. IntÃ©rÃªt uniquement Monte Carlo. Donc amÃ©lioration ciblÃ©e, pas globale.

6. Logging et mÃ©tadonnÃ©es

ProblÃ¨me
BacktestEngine loggue beaucoup de dÃ©tails, y compris des emojis et des dumps complets de paramÃ¨tres, du device, etc. 

LE_CODE_sujet_de_cette_conversaâ€¦


Utile en dev. CoÃ»te en prod quand tu fais 10 000 runs.

AmÃ©lioration concrÃ¨te
Mode silent_eval=True qui:

coupe les logs DEBUG/INFO rÃ©pÃ©titifs par scÃ©nario

ne construit pas toutes les mÃ©tadonnÃ©es dÃ©taillÃ©es (trades_per_day, throughput, etc.) pour chaque essai intermÃ©diaire

ne fait pas les validations coÃ»teuses type check_temporal_integrity Ã  chaque itÃ©ration si VALIDATION nâ€™est pas le sujet du sweep. 

LE_CODE_sujet_de_cette_conversaâ€¦

Impact attendu
RÃ©duit le temps CPU passÃ© dans Python pur. RÃ©duit aussi le volume dâ€™objets Python Ã  sÃ©rialiser.
Valeur: moyenne.
PrioritÃ©: moyenne.

7. GPU: prioritÃ© basse tant que le CPU nâ€™est pas propre

Constat
Le moteur sait dÃ©tecter le GPU, sait rÃ©partir multi-GPU, et sait fallback CPU. 

LE_CODE_sujet_de_cette_conversaâ€¦


Mais la boucle critique _backtest_loop_numba reste CPU. Tant que cette boucle garde des Ã©tats sÃ©quentiels (gestion de position pas vectorisable Ã  100 %), un port direct GPU ne donnera pas un x10 mais risque dâ€™ajouter de la complexitÃ©. 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te
Ne pas pousser le GPU plus loin tant que:

La boucle Numba nâ€™est pas refactorisÃ©e en pur array.

Le batching nâ€™est pas en place.

Impact
Ã‡a Ã©vite de dÃ©penser du temps dans une piste risquÃ©e avant dâ€™avoir captÃ© les gains "quasi gratuits" du CPU.

Valeur: faible immÃ©diate.
PrioritÃ©: basse.

8. Ce quâ€™il ne faut pas essayer

ParallÃ©liser lâ€™intÃ©rieur de la boucle trade elle-mÃªme.
Le cÅ“ur backtest est sÃ©quentiel car lâ€™Ã©tat dâ€™une position dÃ©pend du tick prÃ©cÃ©dent. Tu lâ€™as dÃ©jÃ  conclu: "parallÃ©lisation Numba loop" est rejetÃ©e. 

LE_CODE_analyse_tierce6


Donc inutile dâ€™attaquer Ã§a cÃ´tÃ© Threading/GPU tant que la logique reste path-dÃ©pendante.

Changer le nombre de workers ProcessPoolExecutor.
30 workers a Ã©tÃ© testÃ© comme sweet spot. Tu nâ€™as pas intÃ©rÃªt Ã  aller bourriner plus haut sans profiling mÃ©moire. 

LE_CODE_analyse_tierce6

Micro-optimiser des trucs secondaires comme lâ€™ordre des logs ou des petites allocations pandas si tu nâ€™as pas encore fait les points 2 et 3 ci-dessus. Rendement marginal.

9. Ordre logique dâ€™action

Refactor _backtest_loop_numba pour enlever les objets Python et ne renvoyer que des arrays bas niveau. (Point 2B) 

LE_CODE_analyse_tierce6

Ajouter fast_eval / silent_eval pour les sweeps. Pas de reconstruction DataFrame trades complÃ¨te ni mÃ©tadonnÃ©es lourdes pour chaque scÃ©nario intermÃ©diaire. (Point 2A + Point 6) 

LE_CODE_analyse_tierce6

PrÃ©-calcul indicateurs + mÃ©moire partagÃ©e entre workers. (Point 3) 

LE_CODE_analyse_tierce6

Batching des scÃ©narios cÃ´tÃ© exÃ©cuteur pour rÃ©duire lâ€™overhead I/O et scheduler. (Point 4) 

LE_CODE_analyse_tierce6

Monte Carlo adaptatif si tu veux rÃ©duire le nombre total dâ€™itÃ©rations. (Point 5) 

LE_CODE_analyse_tierce6

Uniformisation propre de la gÃ©nÃ©ration de scÃ©narios via une interface commune. (Point 1)

10. Conclusion

Tu nâ€™as pas besoin de tout rÃ©Ã©crire.
Les points vraiment utiles Ã  court terme sont:

allÃ¨gement du backtest unitaire (refactor Numba + fast_eval)

rÃ©duction du coÃ»t par combinaison (partage indicateurs + batching)

Le reste est structurel ou confort.



========================================
FICHIER: LE_CODE_analyse_tierce8.txt
========================================
1. SÃ©parer clairement "gÃ©nÃ©ration de scÃ©narios" et "exÃ©cution"

ProblÃ¨me
Aujourdâ€™hui run_sweep() et run_monte_carlo() crÃ©ent chacun leurs scÃ©narios (generate_param_grid() pour le Sweep, generate_monte_carlo() pour le Monte Carlo), puis passent Ã§a Ã  _execute_combinations() dans le moteur. 

LE_CODE_analyse_tierce6


Donc lâ€™intention est dÃ©jÃ  bonne: lâ€™exÃ©cution est unifiÃ©e.

AmÃ©lioration concrÃ¨te
Rendre la gÃ©nÃ©ration complÃ¨tement plug-in, style:

GÃ©nÃ©rateur de scÃ©narios (nâ€™importe lequel) â†’ List[Dict[str, Any]].

Moteur dâ€™exÃ©cution â†’ ne connaÃ®t pas la source des scÃ©narios.

MÃ©canisme de reporting identique (mÃ©triques, logs, checkpoints).

Pourquoi

Tu supprimes tout code conditionnel if scenario_type == "grid"/"monte_carlo" au niveau CLI et orchestration.

Tu prÃ©pares lâ€™arrivÃ©e dâ€™autres modes (Bayesian / Adaptive Monte Carlo) sans toucher le moteur.

Ce qui change

Extraire generate_param_grid() et generate_monte_carlo() dans une interface commune (ex: ScenarioGenerator).

run.py devient juste un routeur trivial vers la bonne implÃ©mentation au lieu dâ€™avoir de la logique propre Ã  chaque mode. 

LE_CODE_analyse_tierce6

Impact
LisibilitÃ©. Maintenance. Pas un gain perf direct.
Valeur: oui.
PrioritÃ©: moyenne.

2. RÃ©duire la charge CPU par scÃ©nario

ProblÃ¨me
Le temps total dâ€™un sweep (grid ou Monte Carlo) est dominÃ© par deux choses:

Boucle de backtest Numba _backtest_loop_numba (~35.4 % du temps total). 

LE_CODE_analyse_tierce6

GÃ©nÃ©ration des signaux et gestion des trades (generate_signals() ~13.9 %, backtest_full() ~35.4 %). 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te A
CrÃ©er un mode "Ã©valuation rapide" qui ne reconstruit pas tout (DataFrame trades complet, equity curve complÃ¨te) pendant le sweep.
Tu calcules uniquement les mÃ©triques scalaires utiles Ã  la dÃ©cision (PNL total, Sharpe, max drawdown, etc.) et tu sauves le dÃ©tail complet seulement pour le top N. 

LE_CODE_analyse_tierce6

Effet

Moins de sÃ©ries pandas crÃ©Ã©es

Moins dâ€™allocations mÃ©moire

Moins dâ€™I/O dans les checkpoints

Impact attendu
RÃ©duction nette du coÃ»t par combinaison. Gain direct sur Sweep et Monte Carlo.
Valeur: Ã©levÃ©e.
PrioritÃ©: trÃ¨s haute.
Ceci est Ã  faire.

AmÃ©lioration concrÃ¨te B
Refactor de _backtest_loop_numba en mode 100 % arrays Numba, sans reconstruction dâ€™objets trades dans la partie JIT. On exÃ©cute la simulation purement numÃ©rique dans Numba puis on reconstruit le DataFrame trades aprÃ¨s coup cÃ´tÃ© Python. Cette refonte retire les structures Python dans le chemin chaud. Les estimations raisonnables sont 20 Ã  35 % de CPU en moins par scÃ©nario. 

LE_CODE_analyse_tierce6

Impact attendu
Câ€™est le meilleur levier perf pur.
Valeur: trÃ¨s Ã©levÃ©e.
PrioritÃ©: critique.

Remarque
Tu as notÃ© une rÃ©gression de -2.2 % liÃ©e Ã  la reconstruction des trades (lignes 843-900 de la boucle Numba actuelle). 

LE_CODE_analyse_tierce6


Rollbacker cette partie ou la sortir hors JIT rÃ¨gle Ã§a sans risque.

3. Mutualiser les donnÃ©es dâ€™entrÃ©e entre workers

ProblÃ¨me
Chaque worker recalculerait ou chargerait ses indicateurs (IndicatorBank.ensure_indicators() ~7.2 % du temps mÃªme avec 100 % de cache hit). 

LE_CODE_analyse_tierce6


Sous multiprocessing classique, chaque process a sa copie mÃ©moire.

AmÃ©lioration concrÃ¨te
PrÃ©-calculer tous les indicateurs nÃ©cessaires une seule fois dans le process parent puis exposer ces arrays en mÃ©moire partagÃ©e en lecture seule aux workers.
Exemple: multiprocessing.shared_memory ou Ã©quivalent local. 

LE_CODE_analyse_tierce6

Effet

DÃ©marrage worker plus rapide

Moins de RAM totale utilisÃ©e

Moins de pression GC

Impact attendu
Fort si tu testes beaucoup de scÃ©narios sur les mÃªmes donnÃ©es OHLCV.
Faible si tu fais juste quelques scÃ©narios ponctuels.
Valeur: Ã©levÃ©e pour les sweeps massifs.
PrioritÃ©: haute si tu lances 1 000+ combinaisons. Sinon pas urgent.

4. Batching cÃ´tÃ© exÃ©cuteur

ProblÃ¨me
Actuellement chaque combinaison de paramÃ¨tres part comme une tÃ¢che sÃ©parÃ©e dans ProcessPoolExecutor (30 workers). Cette granularitÃ© crÃ©e du coÃ»t dâ€™ordonnancement et de lock disque pour les checkpoints Parquet. 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te
Envoyer les scÃ©narios par lot (ex taille 32 ou 64) Ã  chaque worker.
Un worker traite un batch, renvoie un tableau de rÃ©sultats.
Un seul thread dÃ©diÃ© cÃ´tÃ© main process Ã©crit pÃ©riodiquement ces rÃ©sultats dans le fichier Parquet checkpoint, au lieu de laisser chaque worker toucher le disque.

Effet

Beaucoup moins de contention disque

Beaucoup moins dâ€™overhead submit/future

TraÃ§abilitÃ© plus simple

Impact attendu
Gains de stabilitÃ© et de dÃ©bit global, surtout quand tu passes Ã  plusieurs milliers de combinaisons Grid.
Valeur: Ã©levÃ©e si tu fais du volume.
PrioritÃ©: moyenne-haute.

Remarque
Pas besoin de changer la logique trading interne pour faire Ã§a. Tu touches juste lâ€™orchestration.

5. Monte Carlo: Ã©chantillonnage adaptatif

ProblÃ¨me
Le Monte Carlo actuel tire des combinaisons uniformÃ©ment avec un seed reproductible via np.random.RandomState(seed) dans monte_carlo_optimizer. Il explore mais nâ€™exploite pas les zones prometteuses. 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te
AprÃ¨s une premiÃ¨re passe, ne plus Ã©chantillonner partout.
Prendre les meilleurs rÃ©sultats (par exemple top 20 %) et gÃ©nÃ©rer de nouveaux tirages centrÃ©s autour de ces zones (perturbation gaussienne locale).
ArrÃªter si la perf stagne.

Effet

Moins dâ€™itÃ©rations totales pour converger

Plus ciblÃ©

MÃªme infra _execute_combinations(), donc pas de dette technique nouvelle

Impact attendu
30 Ã  50 % de runs en moins pour obtenir une zone optimale plausible. 

LE_CODE_analyse_tierce6


Valeur: Ã©levÃ©e si objectif = trouver vite un bon set de paramÃ¨tres.
PrioritÃ©: moyenne.

Remarque
Aucun intÃ©rÃªt pour un sweep Grid exhaustif. IntÃ©rÃªt uniquement Monte Carlo. Donc amÃ©lioration ciblÃ©e, pas globale.

6. Logging et mÃ©tadonnÃ©es

ProblÃ¨me
BacktestEngine loggue beaucoup de dÃ©tails, y compris des emojis et des dumps complets de paramÃ¨tres, du device, etc. 

LE_CODE_sujet_de_cette_conversaâ€¦


Utile en dev. CoÃ»te en prod quand tu fais 10 000 runs.

AmÃ©lioration concrÃ¨te
Mode silent_eval=True qui:

coupe les logs DEBUG/INFO rÃ©pÃ©titifs par scÃ©nario

ne construit pas toutes les mÃ©tadonnÃ©es dÃ©taillÃ©es (trades_per_day, throughput, etc.) pour chaque essai intermÃ©diaire

ne fait pas les validations coÃ»teuses type check_temporal_integrity Ã  chaque itÃ©ration si VALIDATION nâ€™est pas le sujet du sweep. 

LE_CODE_sujet_de_cette_conversaâ€¦

Impact attendu
RÃ©duit le temps CPU passÃ© dans Python pur. RÃ©duit aussi le volume dâ€™objets Python Ã  sÃ©rialiser.
Valeur: moyenne.
PrioritÃ©: moyenne.

7. GPU: prioritÃ© basse tant que le CPU nâ€™est pas propre

Constat
Le moteur sait dÃ©tecter le GPU, sait rÃ©partir multi-GPU, et sait fallback CPU. 

LE_CODE_sujet_de_cette_conversaâ€¦


Mais la boucle critique _backtest_loop_numba reste CPU. Tant que cette boucle garde des Ã©tats sÃ©quentiels (gestion de position pas vectorisable Ã  100 %), un port direct GPU ne donnera pas un x10 mais risque dâ€™ajouter de la complexitÃ©. 

LE_CODE_analyse_tierce6

AmÃ©lioration concrÃ¨te
Ne pas pousser le GPU plus loin tant que:

La boucle Numba nâ€™est pas refactorisÃ©e en pur array.

Le batching nâ€™est pas en place.

Impact
Ã‡a Ã©vite de dÃ©penser du temps dans une piste risquÃ©e avant dâ€™avoir captÃ© les gains "quasi gratuits" du CPU.

Valeur: faible immÃ©diate.
PrioritÃ©: basse.

8. Ce quâ€™il ne faut pas essayer

ParallÃ©liser lâ€™intÃ©rieur de la boucle trade elle-mÃªme.
Le cÅ“ur backtest est sÃ©quentiel car lâ€™Ã©tat dâ€™une position dÃ©pend du tick prÃ©cÃ©dent. Tu lâ€™as dÃ©jÃ  conclu: "parallÃ©lisation Numba loop" est rejetÃ©e. 

LE_CODE_analyse_tierce6


Donc inutile dâ€™attaquer Ã§a cÃ´tÃ© Threading/GPU tant que la logique reste path-dÃ©pendante.

Changer le nombre de workers ProcessPoolExecutor.
30 workers a Ã©tÃ© testÃ© comme sweet spot. Tu nâ€™as pas intÃ©rÃªt Ã  aller bourriner plus haut sans profiling mÃ©moire. 

LE_CODE_analyse_tierce6

Micro-optimiser des trucs secondaires comme lâ€™ordre des logs ou des petites allocations pandas si tu nâ€™as pas encore fait les points 2 et 3 ci-dessus. Rendement marginal.

9. Ordre logique dâ€™action

Refactor _backtest_loop_numba pour enlever les objets Python et ne renvoyer que des arrays bas niveau. (Point 2B) 

LE_CODE_analyse_tierce6

Ajouter fast_eval / silent_eval pour les sweeps. Pas de reconstruction DataFrame trades complÃ¨te ni mÃ©tadonnÃ©es lourdes pour chaque scÃ©nario intermÃ©diaire. (Point 2A + Point 6) 

LE_CODE_analyse_tierce6

PrÃ©-calcul indicateurs + mÃ©moire partagÃ©e entre workers. (Point 3) 

LE_CODE_analyse_tierce6

Batching des scÃ©narios cÃ´tÃ© exÃ©cuteur pour rÃ©duire lâ€™overhead I/O et scheduler. (Point 4) 

LE_CODE_analyse_tierce6

Monte Carlo adaptatif si tu veux rÃ©duire le nombre total dâ€™itÃ©rations. (Point 5) 

LE_CODE_analyse_tierce6

Uniformisation propre de la gÃ©nÃ©ration de scÃ©narios via une interface commune. (Point 1)

10. Conclusion

Tu nâ€™as pas besoin de tout rÃ©Ã©crire.
Les points vraiment utiles Ã  court terme sont:

allÃ¨gement du backtest unitaire (refactor Numba + fast_eval)

rÃ©duction du coÃ»t par combinaison (partage indicateurs + batching)



========================================
FICHIER: LE_CODE_analyse_tierce9.txt
========================================
RÃ©flexion pendant 46sEn tenant compte de la configuration de votre systÃ¨me (O3), Ã©quipÃ©e d'un processeur AMD Ryzen 9 9950X (16 cÅ“urs physiques et 32 threads logiques), de 64 Go de mÃ©moire RAM, d'une carte graphique NVIDIA GeForce RTX 5080 (accompagnÃ©e d'une RTX 2060 Super et d'un GPU intÃ©grÃ© AMD Radeon), ainsi que d'un stockage mixte SSD/HDD performant, je vais vous fournir des indications prÃ©cises sur les amÃ©liorations potentielles pour votre programme ThreadX. Ces suggestions s'appuient sur les analyses tierces prÃ©cÃ©demment examinÃ©es et sont adaptÃ©es Ã  votre matÃ©riel, afin d'optimiser les performances en backtesting et optimisation (grid sweep et Monte-Carlo). Je structure ma rÃ©ponse pour plus de clartÃ©, en me concentrant sur les aspects les plus pertinents.
1. Adaptation des Optimisations au Processeur et Ã  la ParallÃ©lisation
Votre Ryzen 9 9950X offre une capacitÃ© de traitement parallÃ¨le exceptionnelle, idÃ©ale pour les tÃ¢ches multiprocessus de ThreadX (comme dans engine.py avec ProcessPoolExecutor). Les analyses soulignent un bottleneck Ã  35,4 % dans la boucle Numba (_backtest_loop_numba dans bb_atr.py), qui peut Ãªtre attÃ©nuÃ© en exploitant pleinement les 32 threads.

AmÃ©lioration recommandÃ©e : Augmentez le paramÃ¨tre max_workers dans ProcessPoolExecutor de 30 (valeur actuelle, sweet spot testÃ©) Ã  32, pour aligner sur le nombre de threads logiques. Cela rÃ©duira l'overhead d'ordonnancement pour les sweeps massifs (plus de 1 000 scÃ©narios). Dans engine.py, modifiez _calculate_optimal_workers() pour dÃ©tecter dynamiquement le nombre de threads via os.cpu_count() et ajustez Ã  32 si disponible. Gain estimÃ© : 10-20 % de rÃ©duction du temps d'exÃ©cution global, sans risque de surcharge mÃ©moire grÃ¢ce Ã  vos 64 Go de RAM.
Conseil complÃ©mentaire : Activez le mode parallÃ¨le (parallel=True) dans le dÃ©corateur Numba pour les opÃ©rations vectorisables (comme les calculs de stops loss/take profit). Testez cela sur des donnÃ©es synthÃ©tiques pour valider la stabilitÃ©.

2. Exploitation du GPU pour l'AccÃ©lÃ©ration
Votre setup inclut une RTX 5080 (gÃ©nÃ©ration Blackwell, haut de gamme pour les calculs CUDA) et une RTX 2060 Super, ce qui permet une distribution multi-GPU. Les analyses notent que l'intÃ©gration GPU (dans gpu/multi_gpu.py) est prÃ©sente mais incomplÃ¨te, avec un fallback CPU frÃ©quent. Cela reprÃ©sente une opportunitÃ© Ã©vidente pour accÃ©lÃ©rer les simulations de trades et les calculs d'indicateurs.

AmÃ©lioration recommandÃ©e : Ã‰tendez le pipeline GPU en utilisant CuPy pour vectoriser les boucles de backtest (par exemple, dans _simulate_trades()). Assignez des batches de scÃ©narios Ã  chaque GPU : RTX 5080 pour les charges principales (jusqu'Ã  75 % des workers) et RTX 2060 Super pour le reste, via cp.cuda.Device(gpu_id).use() dans GPUBacktestEngine. Minimisez les transferts CPU-GPU en ne renvoyant que les mÃ©triques finales. Gain estimÃ© : 2-5x pour les sweeps GPU-compatibles, exploitant les 42 939 Mo de VRAM combinÃ©e.
Mise Ã  jour des pilotes : Pour une compatibilitÃ© optimale, mettez Ã  jour le pilote NVIDIA vers la version 581.57 WHQL (sortie en octobre 2025), disponible sur le site NVIDIA. Cela optimise les performances pour la RTX 5080 et corrige d'Ã©ventuels problÃ¨mes de stabilitÃ© CUDA. De mÃªme, pour le GPU intÃ©grÃ© AMD, installez AMD Software: Adrenalin Edition 25.10.2 WHQL (octobre 2025), bien que son usage soit limitÃ© dans ThreadX.

3. Gestion de la MÃ©moire et des Ressources
Avec 64 Go de RAM et une mÃ©moire virtuelle de 71 Go, votre systÃ¨me gÃ¨re aisÃ©ment les datasets volumineux (OHLCV), mais les analyses pointent des recalculs inutiles d'indicateurs (7,2 % du temps).

AmÃ©lioration recommandÃ©e : ImplÃ©mentez la mÃ©moire partagÃ©e (multiprocessing.shared_memory) pour les indicateurs prÃ©calculÃ©s (dans IndicatorBank), comme suggÃ©rÃ© prÃ©cÃ©demment. Votre RAM abondante permet de stocker des arrays complets sans fragmentation. Ajoutez un mode "fast_eval" pour ne retourner que des mÃ©triques scalaires, rÃ©duisant l'usage mÃ©moire par scÃ©nario. Gain estimÃ© : 30-40 % en temps et mÃ©moire pour les runs multiprocessus.
Conseil : Surveillez l'utilisation via psutil dans resource_monitor.py, et ajustez les batchs (chunking Ã  64 scÃ©narios) pour Ã©viter les pics.

4. Mises Ã  Jour SystÃ¨me pour une StabilitÃ© Globale
Pour maximiser les bÃ©nÃ©fices des optimisations code, assurez une base systÃ¨me Ã  jour, car ThreadX repose sur Numba, CuPy et multiprocessing.

Windows 11 : Passez Ã  la version 25H2 (sortie en septembre 2025, avec mise Ã  jour cumulative KB5067036 en novembre 2025), qui amÃ©liore la gestion des threads et la sÃ©curitÃ© virtualisÃ©e (dÃ©jÃ  active sur votre setup). Utilisez Windows Update pour une transition fluide.
BIOS : Mettez Ã  jour le BIOS ASUS (version actuelle 3222 de mars 2025) vers une release rÃ©cente (par exemple, 1402 ou supÃ©rieure pour les cartes X870, via EZ Flash). Cela optimise le support Ryzen 9000 et les mitigations de sÃ©curitÃ©, potentiellement amÃ©liorant les performances Numba.

Ces indications s'intÃ¨grent aux optimisations prÃ©cÃ©dentes (refactor Numba, fast_eval, chunking), en les adaptant Ã  votre hardware pour des gains concrets. Si vous fournissez plus de dÃ©tails sur l'usage spÃ©cifique de ThreadX (par exemple, taille des datasets), je pourrai affiner ces recommandations. N'hÃ©sitez pas Ã  poser des questions supplÃ©mentaires.



========================================
FICHIER: LE_CODE_sujet_de_cette_conversation.txt
========================================
Dossier racine : D:\ThreadX_big\src\threadx
Date: 2025-11-02 00:03:07

Sommaire :
1. config.py
2. data_access.py
3. streamlit_app.py
4. __init__.py
5. backtest\engine.py
6. backtest\performance.py
7. backtest\sweep.py
8. backtest\validation.py
9. backtest\__init__.py
10. bridge\async_coordinator.py
11. bridge\config.py
12. bridge\controllers.py
13. bridge\exceptions.py
14. bridge\models.py
15. bridge\unified_diversity_pipeline.py
16. bridge\validation.py
17. bridge\__init__.py
18. cli\backtest_cmd.py
19. cli\data_cmd.py
20. cli\indicators_cmd.py
21. cli\main.py
22. cli\optimize_cmd.py
23. cli\uatils.py
24. cli\__init__.py
25. cli\__main__.py
26. config\paths.py
27. configuration\auth.py
28. configuration\errors.py
29. configuration\loaders.py
30. configuration\settings.py
31. configuration\__init__.py
32. data\normalize.py
33. data\schemas.py
34. data\validate.py
35. data\__init__.py
36. dataset\validate.py
37. gpu\device_manager.py
38. gpu\multi_gpu.py
39. gpu\profile_persistence.py
40. gpu\vector_checks.py
41. gpu\__init__.py
42. indicators\bank.py
43. indicators\bollinger.py
44. indicators\engine.py
45. indicators\gpu_integration.py
46. indicators\indicators_np.py
47. indicators\numpy_ext.py
48. indicators\xatr.py
49. indicators\__init__.py
50. optimization\engine.py
51. optimization\pruning.py
52. optimization\reporting.py
53. optimization\run.py
54. optimization\scenarios.py
55. optimization\ui.py
56. optimization\__init__.py
57. optimization\presets\ranges.py
58. optimization\presets\__init__.py
59. optimization\templates\base_optimizer.py
60. optimization\templates\grid_optimizer.py
61. optimization\templates\monte_carlo_optimizer.py
62. optimization\templates\__init__.py
63. profiling\performance_analyzer.py
64. strategy\amplitude_hunter.py
65. strategy\bb_atr.py
66. strategy\bollinger_dual.py
67. strategy\model.py
68. strategy\__init__.py
69. strategy\_archive\gpu_examples.py
70. testing\mocks.py
71. testing\__init__.py
72. ui\backtest_bridge.py
73. ui\fast_sweep.py
74. ui\page_backtest_optimization.py
75. ui\page_config_strategy.py
76. ui\strategy_registry.py
77. ui\system_monitor.py
78. ui\__init__.py
79. ui\_legacy_v1\page_backtest_results.py
80. ui\_legacy_v1\page_strategy_indicators.py
81. utils\batching.py
82. utils\cache.py
83. utils\common_imports.py
84. utils\determinism.py
85. utils\log.py
86. utils\resource_monitor.py
87. utils\timing.py
88. utils\xp.py
89. utils\__init__.py
90. visualization\backtest_charts.py
91. visualization\__init__.py

----------------------------------------
Fichier: config.py
"""
Minimal configuration stubs for local UI execution.

These stubs satisfy imports from threadx.__init__ without requiring the full
configuration subsystem. Replace with real implementations when available.
"""

from __future__ import annotations


class Settings:  # noqa: D401 - simple stub
    """Container for application settings (stub)."""

    pass


def get_settings() -> Settings:
    """Return default Settings instance (stub)."""
    return Settings()


def load_settings(*args, **kwargs) -> Settings:
    """Load Settings from a path or dict (stub)."""
    return Settings()


class ConfigurationError(Exception):
    """Raised on invalid configuration (stub)."""

    pass


class PathValidationError(Exception):
    """Raised on invalid path in configuration (stub)."""

    pass





----------------------------------------
Fichier: data_access.py
import os
from pathlib import Path
from typing import List, Tuple, Optional
import pandas as pd
from functools import lru_cache
import logging

# Import du module de normalisation
try:
    from threadx.data.normalize import normalize_ohlcv
    from threadx.data.schemas import DEFAULT_NORMALIZATION_CONFIG
    NORMALIZATION_AVAILABLE = True
except ImportError:
    NORMALIZATION_AVAILABLE = False

logger = logging.getLogger(__name__)


# Localisation robuste du dossier data
def _default_data_dir() -> Path:
    env = os.environ.get("THREADX_DATA_DIR")
    if env:
        return Path(env)
    here = Path(__file__).resolve()

    # Prefer local snapshot folders used during development. If a `x_data`
    # folder was copied into `src/threadx/x_data` use it preferentially so
    # the UI can work out-of-the-box without extra env vars.
    try:
        repo_src = here.parents[2] if len(here.parents) >= 3 else None
        candidates = []
        if repo_src is not None:
            candidates.append(repo_src / "threadx" / "x_data")
            candidates.append(repo_src / "threadx" / "data")
        candidates.append(Path.cwd() / "src" / "threadx" / "x_data")
        candidates.append(Path.cwd() / "src" / "threadx" / "data")

        for cand in candidates:
            if cand.exists() and cand.is_dir():
                return cand
    except Exception:
        pass

    # Conventional ancestor search for a `data/` folder (original behaviour).
    for ancestor in here.parents:
        data_root = ancestor / "data"
        if not data_root.exists():
            continue
        for child in data_root.iterdir():
            if child.is_dir() and "exploitable" in child.name.lower():
                return child
        return data_root

    return Path.cwd() / "data"


DATA_DIR = _default_data_dir()
EXTS = (".parquet", ".feather", ".csv", ".json")
DATA_FOLDERS = ("crypto_data_parquet", "crypto_data_json")


@lru_cache(maxsize=1)
def _iter_data_files() -> Tuple[Path, ...]:
    files: List[Path] = []
    for folder_name in DATA_FOLDERS:
        folder = DATA_DIR / folder_name
        if not folder.exists():
            continue
        for extension in EXTS:
            files.extend(folder.glob(f"*{extension}"))
    return tuple(files)


@lru_cache(maxsize=1)
def discover_tokens_and_timeframes() -> Tuple[List[str], List[str]]:
    tokens, timeframes = set(), set()
    for file_path in _iter_data_files():
        parts = file_path.stem.split("_", 1)
        if len(parts) != 2:
            continue
        symbol, timeframe = parts
        tokens.add(symbol.upper())
        timeframes.add(timeframe)

    def _tf_key(value: str) -> Tuple[int, int, str]:
        if not value:
            return (5, 0, value)
        unit = value[-1]
        amount_text = value[:-1]
        order = {"m": 0, "h": 1, "d": 2, "w": 3}.get(unit, 4)
        try:
            amount = int(amount_text)
        except ValueError:
            amount = 0
        return (order, amount, value)

    return sorted(tokens), sorted(timeframes, key=_tf_key)


def get_available_timeframes_for_token(symbol: str) -> List[str]:
    """Retourne les timeframes disponibles pour un token specifique."""
    symbol = symbol.upper()
    timeframes = set()

    for file_path in _iter_data_files():
        parts = file_path.stem.split("_", 1)
        if len(parts) != 2:
            continue
        file_symbol, timeframe = parts
        if file_symbol.upper() == symbol:
            timeframes.add(timeframe)

    def _tf_key(value: str) -> Tuple[int, int, str]:
        if not value:
            return (5, 0, value)
        unit = value[-1]
        amount_text = value[:-1]
        order = {"m": 0, "h": 1, "d": 2, "w": 3}.get(unit, 4)
        try:
            amount = int(amount_text)
        except ValueError:
            amount = 0
        return (order, amount, value)

    return sorted(timeframes, key=_tf_key)


def _read_any(path: Path) -> pd.DataFrame:
    suffix = path.suffix.lower()
    if suffix == ".parquet":
        return pd.read_parquet(path)
    if suffix == ".feather":
        return pd.read_feather(path)
    if suffix == ".csv":
        return pd.read_csv(path)
    if suffix == ".json":
        return pd.read_json(path)
    raise ValueError(f"Unsupported: {path}")


def _find_ohlcv_file(symbol: str, timeframe: str) -> Optional[Path]:
    symbol = symbol.upper()
    target_prefix = f"{symbol}_{timeframe}"
    for file_path in _iter_data_files():
        if file_path.stem == target_prefix:
            return file_path
    return None


def load_ohlcv(symbol: str, timeframe: str, start=None, end=None) -> pd.DataFrame:
    file_path = _find_ohlcv_file(symbol, timeframe)
    if not file_path:
        raise FileNotFoundError(
            f"Fichier OHLCV introuvable pour {symbol}/{timeframe} dans {DATA_DIR}"
        )

    df = _read_any(file_path)

    # NORMALISATION AUTOMATIQUE
    if NORMALIZATION_AVAILABLE:
        # Utiliser le module de normalisation moderne
        df, report = normalize_ohlcv(df, config=DEFAULT_NORMALIZATION_CONFIG)

        if not report.success:
            logger.warning(
                f"Normalisation partielle pour {symbol}/{timeframe}: "
                f"{len(report.errors)} erreurs"
            )
            if report.errors:
                for error in report.errors:
                    logger.error(f"  - {error}")
        else:
            logger.debug(
                f"Normalisation rÃ©ussie pour {symbol}/{timeframe}: "
                f"{len(report.transformations)} transformations"
            )
    else:
        # Fallback: ancienne mÃ©thode (compatibilitÃ©)
        logger.warning("Module de normalisation non disponible, utilisation du fallback")

        # Gerer differentes structures de fichiers
        if "time" in df.columns:
            df["time"] = pd.to_datetime(df["time"], utc=True, errors="coerce")
            df = df.set_index("time")
        elif "timestamp" in df.columns:
            df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, errors="coerce")
            df = df.set_index("timestamp")

        # Assurer que l'index est datetime avec timezone UTC
        if df.index.dtype != "datetime64[ns, UTC]":
            df.index = pd.to_datetime(df.index, utc=True, errors="coerce")

        rename_map = {column: column.lower() for column in df.columns}
        df = df.rename(columns=rename_map).sort_index()

    # Filtrage par dates
    if start is not None:
        # Convertir date/datetime en Timestamp UTC au dÃ©but du jour
        start_dt = pd.to_datetime(start).tz_localize(None).tz_localize('UTC')
        df = df[df.index >= start_dt]
    if end is not None:
        # Convertir date/datetime en Timestamp UTC Ã  la fin du jour
        end_dt = pd.to_datetime(end).tz_localize(None).tz_localize('UTC')
        end_dt = end_dt + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
        df = df[df.index <= end_dt]

    return df

----------------------------------------
Fichier: streamlit_app.py
"""
ThreadX v2.0 - Interface Streamlit Moderne
===========================================

Application de trading quantitatif avec interface fusionnÃ©e et moderne.

Architecture:
- Page 1: Configuration & StratÃ©gie (fusion anciennes pages 1+2)
- Page 2: Backtest & Optimisation (fusion anciennes pages 3+4)

Author: ThreadX Framework
Version: 2.0.0 - UI Redesign
"""

from __future__ import annotations

import sys
from datetime import date
from pathlib import Path

import streamlit as st

# Ensure package root is on sys.path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from threadx.data_access import DATA_DIR
from threadx.ui.page_config_strategy import main as config_page_main
from threadx.ui.page_backtest_optimization import main as backtest_page_main

# Configuration
st.set_page_config(
    page_title="ThreadX v2.0 - Trading Quantitatif",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Styles CSS Modernes
st.markdown(
    """
<style>
    .main { background: linear-gradient(135deg, #0a0e27 0%, #16213e 50%, #0f3460 100%); }
    h1 { color: #4fc3f7 !important; font-weight: 700 !important; font-size: 2.5rem !important; text-shadow: 0 0 20px rgba(79, 195, 247, 0.3); }
    h2 { color: #81c784 !important; font-weight: 600 !important; margin-top: 2rem !important; }
    h3 { color: #a8b2d1 !important; font-weight: 500 !important; }
    .stButton>button { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important; color: white !important; border: none !important; border-radius: 12px !important; padding: 0.75rem 2rem !important; font-weight: 600 !important; transition: all 0.3s ease !important; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3) !important; }
    .stButton>button:hover { transform: translateY(-3px) !important; box-shadow: 0 8px 25px rgba(102, 126, 234, 0.5) !important; }
    [data-testid="stMetricValue"] { font-size: 1.8rem !important; font-weight: 700 !important; color: #4fc3f7 !important; }
    [data-testid="stMetricLabel"] { color: #a8b2d1 !important; font-size: 0.9rem !important; }
    [data-testid="stExpander"] { background: rgba(255, 255, 255, 0.03) !important; border: 1px solid rgba(255, 255, 255, 0.08) !important; border-radius: 15px !important; backdrop-filter: blur(10px) !important; }
    [data-testid="stSidebar"] { background: linear-gradient(180deg, #0f1419 0%, #1a1f2e 100%) !important; border-right: 1px solid rgba(79, 195, 247, 0.1) !important; }
    .stTabs [data-baseweb="tab-list"] { gap: 8px; background: rgba(255, 255, 255, 0.02); padding: 8px; border-radius: 12px; }
    .stTabs [data-baseweb="tab"] { background: transparent; border-radius: 8px; color: #a8b2d1; padding: 12px 24px; }
    .stTabs [aria-selected="true"] { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important; color: white !important; }
    hr { margin: 2rem 0 !important; border-color: rgba(79, 195, 247, 0.2) !important; }
</style>
""",
    unsafe_allow_html=True,
)

PAGE_TITLES = {"config": "ðŸ“Š Chargement des DonnÃ©es", "backtest": "âš¡ Optimisation"}
PAGE_RENDERERS = {"config": config_page_main, "backtest": backtest_page_main}


def init_session() -> None:
    """
    Initialise la session avec les rÃ©glages par dÃ©faut.
    Force l'application des paramÃ¨tres BTC prÃ©rÃ©glÃ©s UNIQUEMENT Ã  la premiÃ¨re ouverture.
    Les modifications de l'utilisateur sont conservÃ©es entre les pages.
    """
    # VÃ©rifier si c'est la premiÃ¨re initialisation
    if "session_initialized" not in st.session_state:
        st.session_state.session_initialized = False

    defaults = {
        "page": "config",
        "symbol": "BTCUSDC",  # Bitcoin prÃ©rÃ©glÃ© - OBLIGATOIRE
        "timeframe": "15m",  # 15 minutes prÃ©rÃ©glÃ© - OBLIGATOIRE
        "start_date": date(2024, 12, 1),  # 1er dÃ©cembre 2024 - OBLIGATOIRE
        "end_date": date(2025, 1, 31),  # 31 janvier 2025 - OBLIGATOIRE
        "strategy": "Bollinger_Breakout",  # StratÃ©gie Bollinger+ATR prÃ©rÃ©glÃ©e
        "indicators": {},
        # ParamÃ¨tres de stratÃ©gie prÃ©rÃ©glÃ©s selon le tableau classique
        "strategy_params": {
            "bb_period": 20,  # Milieu de la plage 10â†’50
            "bb_std": 2.0,  # Milieu de la plage 1.5â†’3.0
            "entry_z": 1.0,  # Seuil Z-score standard
            "entry_logic": "AND",  # Logique d'entrÃ©e standard
            "atr_period": 14,  # Milieu de la plage 7â†’21 (classique)
            "atr_multiplier": 1.5,  # Milieu de la plage 1.0â†’3.0
            "trailing_stop": True,  # Activer trailing stop
            "risk_per_trade": 0.02,  # 2% de risque par trade (prÃ©rÃ©glÃ©)
            "min_pnl_pct": 0.01,  # Filtre minimum 0.01%
            "leverage": 1.0,  # Sans levier
            "max_hold_bars": 72,  # 3 jours en 1h (72 barres de 1h)
            "spacing_bars": 6,  # 6 barres minimum entre trades
            "trend_period": 0,  # Sans filtre tendance EMA
        },
        "data": None,
        "backtest_results": None,
        "sweep_results": None,
        "data_dir": str(DATA_DIR),
    }

    # Initialiser les clÃ©s manquantes
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

    # FORCER les paramÃ¨tres par dÃ©faut UNIQUEMENT lors de la premiÃ¨re initialisation
    # AprÃ¨s, les modifications utilisateur sont conservÃ©es
    if not st.session_state.session_initialized:
        st.session_state.symbol = "BTCUSDC"
        st.session_state.timeframe = "15m"
        st.session_state.start_date = date(2024, 12, 1)
        st.session_state.end_date = date(2025, 1, 31)

        # FORCER le risque par trade Ã  2% (0.02) - ne jamais le laisser Ã  0.01
        if "strategy_params" in st.session_state:
            st.session_state.strategy_params["risk_per_trade"] = 0.02

        # Marquer comme initialisÃ© pour ne plus forcer les valeurs
        st.session_state.session_initialized = True


def render_sidebar() -> None:
    with st.sidebar:
        st.markdown("# ThreadX v2.0")
        st.markdown("*Trading Quantitatif Haute Performance*")
        st.markdown("---")
        st.markdown("### ðŸ§­ Navigation")
        labels = list(PAGE_TITLES.values())
        current_key = st.session_state.get("page", "config")
        current_label = PAGE_TITLES.get(current_key, labels[0])
        selected_label = st.radio(
            "Navigation",
            labels,
            index=labels.index(current_label),
            key="nav_radio",
            label_visibility="collapsed",
        )
        selected_key = next(k for k, v in PAGE_TITLES.items() if v == selected_label)
        if selected_key != current_key:
            st.session_state.page = selected_key
            st.rerun()
        st.markdown("---")
        st.markdown("### âš™ï¸ SystÃ¨me")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Backend", "NumPy")
        with col2:
            st.metric("Config", "TOML")
        st.markdown("---")
        if st.button("ðŸ”„ RafraÃ®chir Cache", use_container_width=True):
            st.cache_data.clear()
            st.success("âœ… Cache vidÃ©!")
        st.markdown("---")
        st.caption("**ThreadX v2.0** | Â© 2025")


def main() -> None:
    init_session()
    render_sidebar()
    page_key = st.session_state.get("page", "config")
    renderer = PAGE_RENDERERS.get(page_key, config_page_main)
    renderer()


if __name__ == "__main__":
    main()

----------------------------------------
Fichier: __init__.py
"""
ThreadX Main Package -
Main package initialization for ThreadX framework.
"""

__version__ = "1.0.0"
__author__ = "ThreadX Team"
__description__ = "High-performance backtesting framework with GPU acceleration"

# Import core configuration
from .config import (
    Settings,
    get_settings,
    load_settings,
    ConfigurationError,
    PathValidationError,
)

__all__ = [
    "Settings",
    "get_settings",
    "load_settings",
    "ConfigurationError",
    "PathValidationError",
]




----------------------------------------
Fichier: backtest\engine.py
"""
ThreadX Backtest Engine - Phase 10 (Production)
==============================================

Orchestrateur de backtesting production-ready intÃ©grant toutes les briques ThreadX.

Features:
- Device-agnostic computing via utils.xp (NumPy/CuPy)
- Multi-GPU distribution via utils.gpu.multi_gpu
- Device detection via utils.gpu.device_manager
- Performance measurement via utils.timing
- Bollinger Bands + ATR strategy avec bank.ensure
- RunResult compatible avec performance.summarize
- DÃ©terminisme (seed=42), logs structurÃ©s

Pipeline:
    bank.ensure(indicateurs) â†’ engine.run(df, indicators, params) â†’ RunResult
    â†’ performance.summarize(result.returns, result.trades) â†’ metrics/plots

Architecture:
- BacktestEngine : orchestrateur principal
- RunResult : structure de donnÃ©es standardisÃ©e
- Multi-device : balance 75%/25% par dÃ©faut entre GPUs
- Strategy : Bollinger mean reversion + ATR filter

Author: ThreadX Framework
Version: Phase 10 - Production Engine
"""

import logging
import time
from dataclasses import dataclass, field

# ThreadX Common Imports (DRY refactoring)
from threadx.utils.common_imports import (
    pd,
    np,
    Dict,
    Any,
    Optional,
    Tuple,
    List,
    Union,
    create_logger,
)

# Validation backtest anti-overfitting
try:
    from threadx.backtest.validation import (
        BacktestValidator,
        ValidationConfig,
        check_temporal_integrity,
    )

    VALIDATION_AVAILABLE = True
except ImportError:
    VALIDATION_AVAILABLE = False
    BacktestValidator = None
    ValidationConfig = None
    check_temporal_integrity = None

# Threading/Timing utilities avec fallback gracieux
try:
    from threadx.utils.timing import measure_throughput, track_memory

    TIMING_AVAILABLE = True
except ImportError:
    TIMING_AVAILABLE = False

    # Fallback decorators si timing non disponible
    def measure_throughput(name=None, *, unit="task"):
        def decorator(func):
            return func

        return decorator

    def track_memory(name=None):
        def decorator(func):
            return func

        return decorator


# Device-agnostic computing avec fallback NumPy
try:
    from threadx.utils import xp as xp_module

    XP_AVAILABLE = True

    # Couche xp unifiÃ©e
    def get_xp_module():
        return xp_module.get_xp()

except ImportError:
    XP_AVAILABLE = False
    xp_module = None

    # Fallback NumPy pur
    def get_xp_module():
        return np


# GPU management avec fallback gracieux
try:
    from threadx.utils.gpu.device_manager import (
        list_devices,
        get_device_by_name,
        is_available as gpu_available,
    )
    from threadx.utils.gpu.multi_gpu import MultiGPUManager, get_default_manager

    GPU_UTILS_AVAILABLE = True
except ImportError:
    GPU_UTILS_AVAILABLE = False
    list_devices = lambda: []
    get_device_by_name = lambda x: None
    gpu_available = lambda: False
    MultiGPUManager = None
    get_default_manager = lambda: None

logger = create_logger(__name__)


@dataclass
class RunResult:
    """
    RÃ©sultat d'exÃ©cution de backtest ThreadX.

    Structure de donnÃ©es standard pour l'Ã©change entre:
    - BacktestEngine.run() â†’ RunResult
    - RunResult â†’ PerformanceCalculator.summarize()
    - RunResult â†’ UI charts/tables

    Attributes:
        equity: SÃ©rie d'Ã©quitÃ© avec index datetime UTC, dtype float64
        returns: SÃ©rie des returns avec mÃªme index que equity
        trades: DataFrame des trades avec colonnes minimales requises
        meta: MÃ©tadonnÃ©es d'exÃ©cution (durÃ©es, devices, cache, etc.)

    Notes:
        Validation stricte des donnÃ©es pour garantir la compatibilitÃ©
        avec performance.summarize() et les modules d'analyse.
    """

    equity: pd.Series
    returns: pd.Series
    trades: pd.DataFrame
    meta: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validation stricte des donnÃ©es retournÃ©es."""
        # Validation equity
        if not isinstance(self.equity, pd.Series):
            raise TypeError("equity doit Ãªtre une pd.Series")
        if not pd.api.types.is_datetime64_any_dtype(self.equity.index):
            raise TypeError("equity.index doit Ãªtre datetime64")
        if self.equity.dtype != np.float64:
            logger.warning(f"equity dtype {self.equity.dtype} != float64, conversion")
            self.equity = self.equity.astype(np.float64)

        # Validation returns
        if not isinstance(self.returns, pd.Series):
            raise TypeError("returns doit Ãªtre une pd.Series")
        if not self.equity.index.equals(self.returns.index):
            raise ValueError("equity et returns doivent avoir le mÃªme index")

        # Validation trades - colonnes requises pour performance.summarize
        required_cols = [
            "entry_ts",
            "exit_ts",
            "pnl",
            "size",
            "price_entry",
            "price_exit",
        ]
        if not isinstance(self.trades, pd.DataFrame):
            raise TypeError("trades doit Ãªtre un pd.DataFrame")
        missing_cols = [col for col in required_cols if col not in self.trades.columns]
        if missing_cols:
            raise ValueError(f"trades manque colonnes requises: {missing_cols}")


class BacktestEngine:
    """
    Moteur de backtesting unifiÃ© ThreadX.

    Orchestrateur production-ready qui intÃ¨gre toutes les briques existantes :
    - Device management via utils.gpu.device_manager
    - Multi-GPU distribution via utils.gpu.multi_gpu
    - Device-agnostic computing via utils.xp
    - Performance measurement via utils.timing
    - Strategy implementation with existing IndicatorBank

    Pipeline standard:
    1. bank.ensure(...) â†’ indicators
    2. engine.run(df_1m, indicators, params) â†’ RunResult
    3. performance.summarize(result.returns, result.trades) â†’ metrics
    4. ui.display(result, metrics)

    Features:
    - Multi-GPU: balance 75%/25% par dÃ©faut (configurable)
    - DÃ©terminisme: seed=42 pour reproductibilitÃ©
    - Device fallback: GPU â†’ CPU transparent
    - Logs structurÃ©s: INFO/DEBUG/WARNING/ERROR
    - API RunResult: compatible performance.summarize

    Examples:
        >>> # Usage standard avec IndicatorBank
        >>> from threadx.indicators.bank import IndicatorBank
        >>> from threadx.backtest.engine import BacktestEngine
        >>> from threadx.backtest.performance import PerformanceCalculator
        >>>
        >>> # 1. Calculer indicateurs
        >>> bank = IndicatorBank()
        >>> indicators = {
        ...     "bollinger": bank.ensure("bollinger", {"period": 20, "std": 2.0},
        ...                             df_1m, symbol="BTCUSDC", timeframe="1m"),
        ...     "atr": bank.ensure("atr", {"period": 14}, df_1m,
        ...                       symbol="BTCUSDC", timeframe="1m")
        ... }
        >>>
        >>> # 2. ExÃ©cuter backtest
        >>> engine = BacktestEngine()
        >>> result = engine.run(df_1m, indicators,
        ...                     params={"entry_z": 2.0, "k_sl": 1.5, "leverage": 3},
        ...                     symbol="BTCUSDC", timeframe="1m")
        >>>
        >>> # 3. Calculer mÃ©triques
        >>> perf = PerformanceCalculator()
        >>> metrics = perf.summarize(result.returns, result.trades)
        >>>
        >>> print(f"Sharpe: {metrics['sharpe_ratio']:.3f}")
        >>> print(f"Max DD: {metrics['max_drawdown']:.2%}")
    """

    def __init__(
        self, gpu_balance: Optional[Dict[str, float]] = None, use_multi_gpu: bool = True
    ):
        """
        Initialise le moteur de backtesting.

        Args:
            gpu_balance: Balance multi-GPU personnalisÃ©e {"5090": 0.75, "2060": 0.25}
                        Si None, utilise balance par dÃ©faut du MultiGPUManager
            use_multi_gpu: Active la distribution multi-GPU si plusieurs devices
        """
        self.logger = create_logger(__name__)

        # Device detection et setup
        self.gpu_available = gpu_available() if GPU_UTILS_AVAILABLE else False
        self.devices = list_devices() if GPU_UTILS_AVAILABLE else []

        # Multi-GPU setup
        self.use_multi_gpu = use_multi_gpu and len(self.devices) > 1
        self.multi_gpu_manager = None

        if self.use_multi_gpu and GPU_UTILS_AVAILABLE:
            try:
                self.multi_gpu_manager = get_default_manager()
                if gpu_balance and self.multi_gpu_manager:
                    self.multi_gpu_manager.set_balance(gpu_balance)
                self.logger.info(f"ðŸ”€ Multi-GPU activÃ©: {len(self.devices)} devices")
            except Exception as e:
                self.logger.warning(
                    f"âš ï¸ Multi-GPU setup failed: {e}, fallback single device"
                )
                self.use_multi_gpu = False

        # Device-agnostic computing setup
        self.xp_backend = "cpu"
        if XP_AVAILABLE and self.gpu_available:
            try:
                # Configure xp backend pour GPU si disponible
                self.xp_backend = "gpu"
                self.logger.debug("ðŸŽ¯ XP backend configurÃ©: GPU")
            except Exception as e:
                self.logger.warning(f"âš ï¸ XP GPU config failed: {e}, fallback CPU")
                self.xp_backend = "cpu"

        # Ã‰tat d'exÃ©cution
        self.last_run_meta = {}

        # Validation setup
        self.validator = None
        self.validation_config = None
        if VALIDATION_AVAILABLE:
            # Configuration par dÃ©faut: walk-forward avec purge/embargo
            self.validation_config = ValidationConfig(
                method="walk_forward",
                walk_forward_windows=5,
                purge_days=1,
                embargo_days=1,
                min_train_samples=200,
                min_test_samples=50,
            )
            self.validator = BacktestValidator(self.validation_config)
            self.logger.info("âœ… Validation anti-overfitting activÃ©e")

        self.logger.info("ðŸš€ BacktestEngine initialisÃ©")
        self.logger.info(f"   GPU: {'âœ…' if self.gpu_available else 'âŒ'}")
        self.logger.info(f"   Multi-GPU: {'âœ…' if self.use_multi_gpu else 'âŒ'}")
        self.logger.info(f"   XP Backend: {self.xp_backend}")
        self.logger.info(f"   Validation: {'âœ…' if self.validator else 'âŒ'}")

    def run(
        self,
        df_1m: pd.DataFrame,
        indicators: Dict[str, Any],
        *,
        params: Dict[str, Any],
        symbol: str,
        timeframe: str,
        seed: int = 42,
        use_gpu: Optional[bool] = None,
    ) -> RunResult:
        """
        ExÃ©cute un backtest complet avec stratÃ©gie Bollinger Bands + ATR.

        Pipeline d'exÃ©cution:
        1. Validation donnÃ©es/paramÃ¨tres stricte
        2. Setup backend compute (CPU/GPU/Multi-GPU)
        3. GÃ©nÃ©ration signaux via stratÃ©gie configurable
        4. Simulation trades avec gestion positions rÃ©aliste
        5. Calcul equity curve et returns
        6. Construction RunResult avec mÃ©tadonnÃ©es complÃ¨tes

        Args:
            df_1m: DataFrame OHLCV 1-minute, index datetime UTC
                   Colonnes requises: open, high, low, close, volume
            indicators: Dict des indicateurs calculÃ©s via bank.ensure()
                       Ex: {"bollinger": (upper, middle, lower), "atr": np.array(...)}
            params: ParamÃ¨tres de stratÃ©gie
                   ClÃ©s requises: entry_z, k_sl, leverage
                   Optionnelles: risk_pct, trail_k, fees_bps
            symbol: Symbole tradÃ© (ex: "BTCUSDC")
            timeframe: Timeframe de rÃ©fÃ©rence (ex: "1m", "1h")
            seed: Seed pour dÃ©terminisme (default: 42)
            use_gpu: Force GPU usage (None=auto selon dÃ©tection)

        Returns:
            RunResult: Structure avec equity, returns, trades, meta

        Raises:
            ValueError: Si donnÃ©es/paramÃ¨tres invalides
            RuntimeError: Si erreur compute non rÃ©cupÃ©rable

        Notes:
            Multi-GPU: Si plusieurs devices disponibles, distribue automatiquement
            le workload selon balance configurÃ©e (75%/25% par dÃ©faut).

            DÃ©terminisme: seed=42 appliquÃ© Ã  tous composants pseudo-alÃ©atoires.

            Performance: @measure_throughput et @track_memory actifs si utils.timing
            disponible, sinon fallback gracieux sans impact.
        """
        start_time = time.time()
        self.logger.info(f"ðŸŽ¯ DÃ©marrage backtest: {symbol} {timeframe}")
        self.logger.debug(f"   Params: {params}")
        self.logger.debug(f"   Data shape: {df_1m.shape}")
        self.logger.debug(f"   PÃ©riode: {df_1m.index[0]} â†’ {df_1m.index[-1]}")
        self.logger.debug(f"   Indicators: {list(indicators.keys())}")

        # Seed pour dÃ©terminisme complet
        np.random.seed(seed)

        try:
            # 1. Setup backend et validation
            device_info = self._setup_compute_backend(use_gpu)
            self._validate_inputs(df_1m, indicators, params)

            # 2. GÃ©nÃ©ration signaux de trading
            signals = self._generate_trading_signals(df_1m, indicators, params)

            # 3. Simulation trades et gestion positions
            trades_df = self._simulate_trades(df_1m, signals, params)

            # 4. Calcul equity curve et returns
            equity, returns = self._calculate_equity_returns(df_1m, trades_df, params)

            # 5. MÃ©tadonnÃ©es d'exÃ©cution complÃ¨tes
            duration = time.time() - start_time
            meta = self._build_metadata(
                device_info, duration, df_1m, trades_df, params, seed
            )

            # 6. Construction RunResult avec validation
            result = RunResult(
                equity=equity, returns=returns, trades=trades_df, meta=meta
            )

            self.last_run_meta = meta
            self.logger.info(f"âœ… Backtest terminÃ© en {duration:.2f}s")
            self.logger.info(
                f"   Trades: {len(trades_df)}, Equity finale: ${equity.iloc[-1]:,.2f}"
            )
            self.logger.debug(f"   Throughput: {len(df_1m)/duration:.0f} ticks/sec")

            return result

        except Exception as e:
            self.logger.error(f"âŒ Erreur backtest {symbol}: {e}")
            raise

    def _setup_compute_backend(self, use_gpu: Optional[bool]) -> Dict[str, Any]:
        """
        Configure le backend de calcul avec fallback gracieux.

        Args:
            use_gpu: Force GPU usage (None=auto)

        Returns:
            Dict avec info device pour mÃ©tadonnÃ©es
        """
        # DÃ©termination GPU usage
        if use_gpu is None:
            use_gpu = self.gpu_available
        elif use_gpu and not self.gpu_available:
            self.logger.warning("âš ï¸ GPU requis mais non disponible, fallback CPU")
            use_gpu = False

        # Device info pour mÃ©tadonnÃ©es
        if use_gpu and self.use_multi_gpu:
            # Multi-GPU
            device_names = [d.name for d in self.devices if d.name != "cpu"]
            balance = (
                getattr(self.multi_gpu_manager, "device_balance", {})
                if self.multi_gpu_manager
                else {}
            )
            device_info = {
                "mode": "multi_gpu",
                "devices": device_names,
                "balance": balance,
                "backend": "cupy/multi",
            }
            self.logger.debug(f"ðŸ”€ Multi-GPU mode: {device_names}")

        elif use_gpu:
            # Single GPU
            gpu_device = next((d for d in self.devices if d.name != "cpu"), None)
            device_info = {
                "mode": "single_gpu",
                "devices": [gpu_device.name] if gpu_device else ["gpu"],
                "balance": {},
                "backend": "cupy",
            }
            self.logger.debug(
                f"ðŸŽ¯ Single GPU mode: {gpu_device.name if gpu_device else 'default'}"
            )

        else:
            # CPU fallback
            device_info = {
                "mode": "cpu",
                "devices": ["cpu"],
                "balance": {},
                "backend": "numpy",
            }
            self.logger.debug("ðŸ–¥ï¸ CPU mode")

        return device_info

    def _validate_inputs(
        self, df_1m: pd.DataFrame, indicators: Dict[str, Any], params: Dict[str, Any]
    ):
        """Validation stricte des donnÃ©es d'entrÃ©e."""
        # Validation DataFrame OHLCV
        if df_1m.empty:
            raise ValueError("df_1m ne peut pas Ãªtre vide")

        required_cols = ["open", "high", "low", "close", "volume"]
        missing_cols = [col for col in required_cols if col not in df_1m.columns]
        if missing_cols:
            raise ValueError(f"df_1m manque colonnes OHLCV: {missing_cols}")

        if not pd.api.types.is_datetime64_any_dtype(df_1m.index):
            raise ValueError("df_1m.index doit Ãªtre datetime64 UTC")

        # VÃ©rification donnÃ©es cohÃ©rentes (OHLC logic)
        ohlc_errors = (df_1m["high"] < df_1m[["open", "close"]].max(axis=1)).sum()
        if ohlc_errors > 0:
            self.logger.warning(f"âš ï¸ {ohlc_errors} barres avec high < max(open,close)")

        # Validation indicateurs
        if not indicators:
            self.logger.warning("âš ï¸ Aucun indicateur fourni, signaux basiques")

        # Validation paramÃ¨tres stratÃ©gie
        required_params = ["entry_z", "k_sl", "leverage"]
        missing_params = [p for p in required_params if p not in params]
        if missing_params:
            raise ValueError(f"params manque clÃ©s requises: {missing_params}")

        # Validation ranges paramÃ¨tres
        if params["leverage"] <= 0 or params["leverage"] > 20:
            raise ValueError("leverage doit Ãªtre dans [0.1, 20]")
        if params["k_sl"] <= 0 or params["k_sl"] > 10:
            raise ValueError("k_sl doit Ãªtre dans (0, 10]")

        self.logger.debug("âœ… Validation inputs rÃ©ussie")

    @measure_throughput("signal_generation")
    def _generate_trading_signals(
        self, df_1m: pd.DataFrame, indicators: Dict[str, Any], params: Dict[str, Any]
    ) -> pd.Series:
        """
        GÃ©nÃ¨re signaux de trading via stratÃ©gie Bollinger Bands + ATR.

        StratÃ©gie implÃ©mentÃ©e:
        - Long: prix touche bande basse ET volatilitÃ© > seuil
        - Short: prix touche bande haute ET volatilitÃ© > seuil
        - Exit: signal opposÃ© ou stop-loss/take-profit
        - Filter: ATR pour Ã©viter marchÃ©s trop calmes

        Args:
            df_1m: DataFrame OHLCV
            indicators: Dict avec bollinger et atr de bank.ensure
            params: ParamÃ¨tres stratÃ©gie (entry_z, etc.)

        Returns:
            pd.Series: Signaux {-1: short, 0: hold, 1: long}
        """
        self.logger.debug("ðŸŽ² GÃ©nÃ©ration signaux Bollinger + ATR")

        # Signaux par dÃ©faut (hold/flat)
        signals = pd.Series(0, index=df_1m.index, dtype=np.float64, name="signals")

        try:
            # === StratÃ©gie principale: Bollinger Bands ===
            if "bollinger" in indicators and indicators["bollinger"]:
                bb_result = indicators["bollinger"]

                if isinstance(bb_result, tuple) and len(bb_result) >= 3:
                    upper, middle, lower = bb_result[:3]

                    # Conversion en Series pandas si arrays NumPy/CuPy
                    if hasattr(upper, "__array__") and not isinstance(upper, pd.Series):
                        upper = pd.Series(np.asarray(upper), index=df_1m.index)
                    if hasattr(lower, "__array__") and not isinstance(lower, pd.Series):
                        lower = pd.Series(np.asarray(lower), index=df_1m.index)
                    if hasattr(middle, "__array__") and not isinstance(
                        middle, pd.Series
                    ):
                        middle = pd.Series(np.asarray(middle), index=df_1m.index)

                    close_prices = df_1m["close"]

                    # Signaux mean reversion Bollinger
                    # Long: prix <= lower band (oversold)
                    long_condition = close_prices <= lower
                    # Short: prix >= upper band (overbought)
                    short_condition = close_prices >= upper

                    signals[long_condition] = 1.0  # Long signal
                    signals[short_condition] = -1.0  # Short signal

                    signal_count = (signals != 0).sum()
                    self.logger.debug(f"   Bollinger signaux: {signal_count} positions")
                    self.logger.debug(
                        f"   Long: {(signals == 1).sum()}, Short: {(signals == -1).sum()}"
                    )

            # === Filtre ATR: ne trade que si volatilitÃ© suffisante ===
            if "atr" in indicators and indicators["atr"] is not None:
                atr_values = indicators["atr"]

                # Conversion en Series pandas
                if hasattr(atr_values, "__array__") and len(atr_values) == len(df_1m):
                    atr_series = pd.Series(np.asarray(atr_values), index=df_1m.index)

                    # Filtre: volatilitÃ© > percentile 30% (Ã©vite marchÃ©s calmes)
                    atr_threshold = atr_series.quantile(0.3)
                    volatility_filter = atr_series > atr_threshold

                    # Applique filtre: supprime signaux en low volatility
                    signals[~volatility_filter] = 0.0

                    filtered_count = (signals != 0).sum()
                    self.logger.debug(
                        f"   ATR filtre: {filtered_count} signaux restants"
                    )
                    self.logger.debug(f"   Seuil ATR: {atr_threshold:.4f}")

            # === Fallback: signaux alÃ©atoires si pas d'indicateurs valides ===
            if not any(indicators.values()) or (signals == 0).all():
                self.logger.warning("âš ï¸ Pas d'indicateurs valides, signaux alÃ©atoires")
                np.random.seed(42)
                random_signals = np.random.choice(
                    [0, 1, -1],
                    size=len(df_1m),
                    p=[0.85, 0.075, 0.075],  # 85% hold, 7.5% long, 7.5% short
                )
                signals = pd.Series(
                    random_signals, index=df_1m.index, dtype=np.float64, name="signals"
                )

        except Exception as e:
            self.logger.error(f"âŒ Erreur gÃ©nÃ©ration signaux: {e}")

            # Fallback sÃ©curisÃ©: signaux alÃ©atoires dÃ©terministes
            np.random.seed(42)
            random_signals = np.random.choice(
                [0, 1, -1], size=len(df_1m), p=[0.85, 0.075, 0.075]
            )
            signals = pd.Series(
                random_signals, index=df_1m.index, dtype=np.float64, name="signals"
            )

        # Stats finales
        signal_counts = signals.value_counts().to_dict()
        self.logger.debug(f"   Distribution signaux finale: {signal_counts}")

        return signals

    @track_memory("trade_simulation")
    def _simulate_trades(
        self, df_1m: pd.DataFrame, signals: pd.Series, params: Dict[str, Any]
    ) -> pd.DataFrame:
        """
        Simule l'exÃ©cution des trades avec gestion positions rÃ©aliste.

        Features:
        - Position tracking (flat/long/short)
        - Stop-loss configurable via k_sl
        - Sizing basÃ© sur leverage et risk management
        - PnL calculation rÃ©aliste avec slippage/fees
        - Exit reasons tracking pour analyse

        Args:
            df_1m: DataFrame OHLCV
            signals: Signaux de trading {-1, 0, 1}
            params: ParamÃ¨tres (leverage, k_sl, etc.)

        Returns:
            pd.DataFrame: Trades avec colonnes requises pour performance.summarize
        """
        self.logger.debug("ðŸ’¼ Simulation des trades")

        trades = []
        position = 0  # 0=flat, 1=long, -1=short
        entry_price = 0.0
        entry_time = None

        # ParamÃ¨tres de trading avec defaults
        leverage = params.get("leverage", 3)
        k_sl = params.get("k_sl", 1.5)  # Stop loss % multiplier
        initial_capital = params.get("initial_capital", 10000.0)
        fees_bps = params.get("fees_bps", 10.0)  # 10 bps = 0.1%
        slip_bps = params.get("slip_bps", 5.0)  # 5 bps slippage

        # Conversion en listes pour itÃ©ration efficace (Ã©vite pandas.loc overhead)
        timestamps = df_1m.index.tolist()
        closes = df_1m["close"].tolist()
        signal_values = signals.tolist()

        # Simulation trade par trade
        for i, (timestamp, close_price, signal) in enumerate(
            zip(timestamps, closes, signal_values)
        ):

            # === EntrÃ©e en position ===
            if position == 0 and signal != 0:
                position = signal
                entry_price = close_price * (1 + slip_bps * 0.0001 * signal)  # Slippage
                entry_time = timestamp

                self.logger.debug(
                    f"   EntrÃ©e {['FLAT','LONG','SHORT'][int(position + 1)]} @ {entry_price:.2f} le {entry_time}"
                )

            # === Sortie de position ===
            elif position != 0:
                exit_condition = False
                exit_reason = ""

                # 1. Signal opposÃ© (reversal)
                if signal != 0 and signal != position:
                    exit_condition = True
                    exit_reason = "signal_reverse"

                # 2. Stop-loss (basÃ© sur k_sl%)
                elif position == 1 and close_price <= entry_price * (1 - k_sl * 0.01):
                    exit_condition = True
                    exit_reason = "stop_loss"
                elif position == -1 and close_price >= entry_price * (1 + k_sl * 0.01):
                    exit_condition = True
                    exit_reason = "stop_loss"

                # === ExÃ©cution sortie ===
                if exit_condition:
                    exit_price = close_price * (
                        1 - slip_bps * 0.0001 * position
                    )  # Slippage opposÃ©

                    # Calcul PnL rÃ©aliste avec fees
                    if position == 1:  # Long position
                        raw_return = (exit_price - entry_price) / entry_price
                    else:  # Short position
                        raw_return = (entry_price - exit_price) / entry_price

                    # Fees totales (entrÃ©e + sortie)
                    total_fees_pct = fees_bps * 2 * 0.0001  # 2x pour round-trip
                    net_return = raw_return - total_fees_pct

                    # PnL avec leverage sur capital de base
                    pnl = net_return * leverage * initial_capital

                    # Position size (notional)
                    position_size = (
                        abs(position) * leverage * initial_capital / entry_price
                    )

                    trade_record = {
                        "entry_ts": entry_time,
                        "exit_ts": timestamp,
                        "pnl": pnl,
                        "size": position_size,
                        "price_entry": entry_price,
                        "price_exit": exit_price,
                        "side": "LONG" if position == 1 else "SHORT",
                        "exit_reason": exit_reason,
                        "return_pct": net_return * 100,
                        "leverage_used": leverage,
                        "fees_paid": position_size * total_fees_pct,
                    }

                    trades.append(trade_record)

                    # Log trade dÃ©taillÃ©
                    self.logger.debug(
                        f"   Exit {trade_record['side']} @ {exit_price:.2f}, "
                        f"PnL: ${pnl:.2f}, Reason: {exit_reason}"
                    )

                    # Reset position
                    position = 0
                    entry_price = 0.0
                    entry_time = None

                    # Nouvelle position si signal prÃ©sent aprÃ¨s sortie
                    if signal != 0:
                        position = signal
                        entry_price = close_price * (1 + slip_bps * 0.0001 * signal)
                        entry_time = timestamp

        # === Trade final si position ouverte ===
        # Ferme position ouverte en fin de donnÃ©es
        if position != 0:
            final_price = closes[-1] * (1 - slip_bps * 0.0001 * position)
            final_time = timestamps[-1]

            if position == 1:
                raw_return = (final_price - entry_price) / entry_price
            else:
                raw_return = (entry_price - final_price) / entry_price

            total_fees_pct = fees_bps * 2 * 0.0001
            net_return = raw_return - total_fees_pct
            pnl = net_return * leverage * initial_capital
            position_size = abs(position) * leverage * initial_capital / entry_price

            trades.append(
                {
                    "entry_ts": entry_time,
                    "exit_ts": final_time,
                    "pnl": pnl,
                    "size": position_size,
                    "price_entry": entry_price,
                    "price_exit": final_price,
                    "side": "LONG" if position == 1 else "SHORT",
                    "exit_reason": "end_of_data",
                    "return_pct": net_return * 100,
                    "leverage_used": leverage,
                    "fees_paid": position_size * total_fees_pct,
                }
            )

        # Construction DataFrame final
        trades_df = pd.DataFrame(trades)

        # Stats de trading complÃ¨tes
        if not trades_df.empty:
            total_pnl = trades_df["pnl"].sum()
            total_fees = trades_df["fees_paid"].sum()
            win_rate = (trades_df["pnl"] > 0).mean()
            avg_win = (
                trades_df[trades_df["pnl"] > 0]["pnl"].mean()
                if (trades_df["pnl"] > 0).any()
                else 0
            )
            avg_loss = (
                trades_df[trades_df["pnl"] < 0]["pnl"].mean()
                if (trades_df["pnl"] < 0).any()
                else 0
            )

            self.logger.debug(f"   Trades simulÃ©s: {len(trades_df)}")
            self.logger.debug(f"   PnL total: ${total_pnl:.2f}")
            self.logger.debug(f"   Fees totales: ${total_fees:.2f}")
            self.logger.debug(f"   Win rate: {win_rate:.2%}")
            self.logger.debug(f"   Avg win/loss: ${avg_win:.2f} / ${avg_loss:.2f}")
        else:
            self.logger.debug("   Aucun trade gÃ©nÃ©rÃ©")

        return trades_df

    def _calculate_equity_returns(
        self, df_1m: pd.DataFrame, trades_df: pd.DataFrame, params: Dict[str, Any]
    ) -> Tuple[pd.Series, pd.Series]:
        """
        Calcule l'equity curve et les returns sÃ©ries.

        MÃ©thodologie:
        1. Equity de base = capital initial constant
        2. Applique PnL cumulÃ© des trades Ã  leurs dates de sortie
        3. Calculate returns = pct_change() de l'equity
        4. Assure dtype float64 pour compatibilitÃ© performance.summarize

        Args:
            df_1m: DataFrame OHLCV pour index temporel
            trades_df: DataFrame des trades exÃ©cutÃ©s
            params: ParamÃ¨tres avec initial_capital

        Returns:
            Tuple[pd.Series, pd.Series]: (equity, returns)
        """
        self.logger.debug("ðŸ“ˆ Calcul equity curve et returns")

        # Capital initial
        initial_capital = params.get("initial_capital", 10000.0)
        equity = pd.Series(
            initial_capital, index=df_1m.index, name="equity", dtype=np.float64
        )

        if not trades_df.empty:
            # Applique les PnL des trades Ã  leurs dates de sortie
            cumulative_pnl = 0.0

            for _, trade in trades_df.iterrows():
                exit_ts = trade["exit_ts"]
                cumulative_pnl += trade["pnl"]

                # Applique PnL cumulÃ© Ã  partir de la date de sortie
                if exit_ts in equity.index:
                    mask = equity.index >= exit_ts
                    equity.loc[mask] = initial_capital + cumulative_pnl

        # Calcul returns (percentage change)
        returns = equity.pct_change().fillna(0.0)
        returns.name = "returns"

        # Validation et casting final pour performance.summarize
        equity = equity.astype(np.float64)
        returns = returns.astype(np.float64)

        # Stats finales pour logs
        total_return_pct = ((equity.iloc[-1] / equity.iloc[0]) - 1) * 100
        max_equity = equity.max()
        min_equity = equity.min()

        self.logger.debug(f"   Equity finale: ${equity.iloc[-1]:,.2f}")
        self.logger.debug(f"   Return total: {total_return_pct:.2f}%")
        self.logger.debug(f"   Equity range: ${min_equity:,.2f} â†’ ${max_equity:,.2f}")

        return equity, returns

    def _build_metadata(
        self,
        device_info: Dict[str, Any],
        duration: float,
        df_1m: pd.DataFrame,
        trades_df: pd.DataFrame,
        params: Dict[str, Any],
        seed: int,
    ) -> Dict[str, Any]:
        """
        Construit les mÃ©tadonnÃ©es complÃ¨tes d'exÃ©cution.

        Args:
            device_info: Info devices utilisÃ©s
            duration: DurÃ©e d'exÃ©cution en secondes
            df_1m: DataFrame des donnÃ©es
            trades_df: DataFrame des trades
            params: ParamÃ¨tres de stratÃ©gie
            seed: Seed utilisÃ©

        Returns:
            Dict: MÃ©tadonnÃ©es complÃ¨tes pour RunResult.meta
        """

        # Calculs dÃ©rivÃ©s
        data_points = len(df_1m)
        throughput = data_points / duration if duration > 0 else 0

        # Period analysis
        period_days = (df_1m.index[-1] - df_1m.index[0]).days
        trades_per_day = len(trades_df) / period_days if period_days > 0 else 0

        # Performance flags
        performance_flags = []
        if throughput < 1000:
            performance_flags.append("low_throughput")
        if len(trades_df) == 0:
            performance_flags.append("no_trades")
        if duration > 30:
            performance_flags.append("slow_execution")
        if device_info["mode"] == "cpu" and self.gpu_available:
            performance_flags.append("gpu_fallback")

        # Trading stats
        trading_stats = {}
        if not trades_df.empty:
            trading_stats = {
                "total_pnl": float(trades_df["pnl"].sum()),
                "win_rate": float((trades_df["pnl"] > 0).mean()),
                "avg_trade_pnl": float(trades_df["pnl"].mean()),
                "max_trade_pnl": float(trades_df["pnl"].max()),
                "min_trade_pnl": float(trades_df["pnl"].min()),
                "total_fees": float(
                    trades_df["fees_paid"].sum()
                    if "fees_paid" in trades_df.columns
                    else 0
                ),
                "avg_trade_duration_hours": float(
                    (trades_df["exit_ts"] - trades_df["entry_ts"])
                    .dt.total_seconds()
                    .mean()
                    / 3600
                ),
            }

        return {
            # Execution context
            "engine_version": "Phase 10 - Production",
            "seed": seed,
            "run_timestamp": pd.Timestamp.now(tz="UTC").isoformat(),
            # Device information
            **device_info,
            # Performance metrics
            "duration_seconds": round(duration, 3),
            "data_points": data_points,
            "throughput_points_per_sec": round(throughput, 1),
            "performance_flags": performance_flags,
            # Trading results
            "total_trades": len(trades_df),
            "trades_per_day": round(trades_per_day, 2),
            "period_days": period_days,
            "trading_stats": trading_stats,
            # Strategy parameters (pour reproducibilitÃ©)
            "strategy_params": params.copy(),
            # Data period info
            "data_start": df_1m.index[0].isoformat(),
            "data_end": df_1m.index[-1].isoformat(),
            # System info (placeholders pour extensions futures)
            "cache_hits": 0,
            "cache_misses": 0,
            "memory_peak_mb": 0,
            # Multi-GPU specifics
            "multi_gpu_enabled": self.use_multi_gpu,
            "gpu_balance": device_info.get("balance", {}),
            "device_count": len(device_info.get("devices", [])),
        }

    def run_backtest_with_validation(
        self,
        df_1m: pd.DataFrame,
        indicators: Dict[str, Any],
        *,
        params: Dict[str, Any],
        symbol: str,
        timeframe: str,
        validation_config: Optional[ValidationConfig] = None,
        seed: int = 42,
        use_gpu: Optional[bool] = None,
    ) -> Dict[str, Any]:
        """
        ExÃ©cute backtest avec validation anti-overfitting complÃ¨te.

        Cette mÃ©thode applique une validation robuste via walk-forward ou train/test split
        pour dÃ©tecter l'overfitting et garantir des performances rÃ©alistes out-of-sample.

        Pipeline:
        1. VÃ©rification intÃ©gritÃ© temporelle des donnÃ©es (look-ahead bias)
        2. Split donnÃ©es en train/test selon mÃ©thode configurÃ©e
        3. ExÃ©cution backtest sur chaque split
        4. Calcul ratio overfitting (IS_sharpe / OOS_sharpe)
        5. Recommandations automatiques basÃ©es sur ratio

        Args:
            df_1m: DataFrame OHLCV 1-minute avec index datetime UTC
            indicators: Dict indicateurs calculÃ©s via bank.ensure()
            params: ParamÃ¨tres stratÃ©gie (entry_z, k_sl, leverage, etc.)
            symbol: Symbole tradÃ© (ex: "BTCUSDC")
            timeframe: Timeframe de rÃ©fÃ©rence (ex: "1m", "1h")
            validation_config: Configuration validation (None = use default from __init__)
            seed: Seed pour dÃ©terminisme (default: 42)
            use_gpu: Force GPU usage (None = auto)

        Returns:
            Dict avec:
                - in_sample: MÃ©triques train (mean/std sharpe, return, drawdown, etc.)
                - out_sample: MÃ©triques test (idem)
                - overfitting_ratio: IS_sharpe / OOS_sharpe
                - recommendation: Texte explicatif basÃ© sur ratio
                - method: MÃ©thode validation utilisÃ©e
                - n_windows: Nombre fenÃªtres (walk-forward uniquement)
                - all_results: Liste rÃ©sultats individuels par split

        Raises:
            ValueError: Si validation module non disponible
            ValueError: Si donnÃ©es ont problÃ¨mes temporels

        Examples:
            >>> # Validation walk-forward (dÃ©faut)
            >>> results = engine.run_backtest_with_validation(
            ...     df_1m, indicators, params=params, symbol="BTCUSDC", timeframe="1m"
            ... )
            >>> print(f"Overfitting ratio: {results['overfitting_ratio']:.2f}")
            >>> print(results['recommendation'])
            >>>
            >>> # Train/test split simple
            >>> config = ValidationConfig(method="train_test", train_ratio=0.7)
            >>> results = engine.run_backtest_with_validation(
            ...     df_1m, indicators, params=params, symbol="BTCUSDC", timeframe="1m",
            ...     validation_config=config
            ... )

        Notes:
            - Overfitting ratio < 1.2: âœ… Excellent, stratÃ©gie robuste
            - Overfitting ratio 1.2-1.5: âš ï¸ Acceptable, lÃ©ger overfitting
            - Overfitting ratio 1.5-2.0: ðŸŸ¡ Attention, overfitting modÃ©rÃ©
            - Overfitting ratio > 2.0: ðŸ”´ Critique, stratÃ©gie non viable
        """
        if not VALIDATION_AVAILABLE:
            raise ValueError(
                "Module validation non disponible. "
                "Installer avec: pip install -e . pour activer threadx.backtest.validation"
            )

        self.logger.info(f"ðŸ” DÃ©marrage backtest avec validation: {symbol} {timeframe}")

        # VÃ©rifier intÃ©gritÃ© temporelle des donnÃ©es AVANT validation
        try:
            check_temporal_integrity(df_1m)
            self.logger.debug("âœ… IntÃ©gritÃ© temporelle validÃ©e")
        except ValueError as e:
            self.logger.error(f"âŒ ProblÃ¨me intÃ©gritÃ© temporelle: {e}")
            raise

        # Utiliser config fournie ou celle par dÃ©faut de l'instance
        config = validation_config or self.validation_config
        if config is None:
            config = ValidationConfig()  # Fallback config par dÃ©faut
            self.logger.warning("âš ï¸ Aucune config validation, utilisation dÃ©faut")

        # CrÃ©er validator avec config spÃ©cifique
        validator = BacktestValidator(config)

        # DÃ©finir fonction de backtest Ã  valider
        def backtest_func(
            data: pd.DataFrame, params_dict: Dict[str, Any]
        ) -> Dict[str, float]:
            """
            Wrapper pour exÃ©cuter self.run() et extraire mÃ©triques nÃ©cessaires.

            Args:
                data: Sous-ensemble de df_1m (train ou test split)
                params_dict: ParamÃ¨tres stratÃ©gie

            Returns:
                Dict avec mÃ©triques: sharpe_ratio, total_return, max_drawdown, etc.
            """
            try:
                # Re-calculer indicateurs sur split spÃ©cifique
                # (important pour Ã©viter look-ahead bias!)
                split_indicators = {}
                if "bollinger" in indicators:
                    # Pour simplification, on utilise indicateurs prÃ©-calculÃ©s
                    # TODO: Re-calculer indicateurs par split pour robustesse totale
                    split_indicators["bollinger"] = indicators["bollinger"]
                if "atr" in indicators:
                    split_indicators["atr"] = indicators["atr"]

                # ExÃ©cuter backtest sur ce split
                result = self.run(
                    df_1m=data,
                    indicators=split_indicators,
                    params=params_dict,
                    symbol=symbol,
                    timeframe=timeframe,
                    seed=seed,
                    use_gpu=use_gpu,
                )

                # Calculer mÃ©triques depuis RunResult
                returns = result.returns
                trades = result.trades

                # Sharpe ratio (annualisÃ©)
                if len(returns) > 0 and returns.std() > 0:
                    sharpe = (returns.mean() / returns.std()) * np.sqrt(
                        252 * 24 * 60
                    )  # 1-min bars
                else:
                    sharpe = 0.0

                # Total return
                total_return = (result.equity.iloc[-1] / result.equity.iloc[0]) - 1

                # Max drawdown
                equity = result.equity
                cummax = equity.cummax()
                drawdown = (equity - cummax) / cummax
                max_drawdown = drawdown.min()

                # Win rate
                if len(trades) > 0:
                    win_rate = (trades["pnl"] > 0).sum() / len(trades)
                else:
                    win_rate = 0.0

                # Profit factor
                if len(trades) > 0:
                    wins = trades[trades["pnl"] > 0]["pnl"].sum()
                    losses = abs(trades[trades["pnl"] < 0]["pnl"].sum())
                    profit_factor = wins / losses if losses > 0 else float("inf")
                else:
                    profit_factor = 1.0

                return {
                    "sharpe_ratio": float(sharpe),
                    "total_return": float(total_return),
                    "max_drawdown": float(max_drawdown),
                    "win_rate": float(win_rate),
                    "profit_factor": float(profit_factor),
                }

            except Exception as e:
                self.logger.error(f"âŒ Erreur dans backtest_func split: {e}")
                # Retourner mÃ©triques nulles en cas d'erreur
                return {
                    "sharpe_ratio": 0.0,
                    "total_return": 0.0,
                    "max_drawdown": 0.0,
                    "win_rate": 0.0,
                    "profit_factor": 0.0,
                }

        # ExÃ©cuter validation complÃ¨te
        self.logger.info(
            f"ðŸ”„ Validation {config.method} avec {config.walk_forward_windows if config.method == 'walk_forward' else 1} splits"
        )
        validation_results = validator.validate_backtest(
            backtest_func=backtest_func, data=df_1m, params=params
        )

        # Logs rÃ©sultats
        self.logger.info("ðŸ“Š RÃ©sultats validation:")
        self.logger.info(
            f"   In-Sample Sharpe: {validation_results['in_sample']['mean_sharpe_ratio']:.2f} "
            f"Â± {validation_results['in_sample']['std_sharpe_ratio']:.2f}"
        )
        self.logger.info(
            f"   Out-Sample Sharpe: {validation_results['out_sample']['mean_sharpe_ratio']:.2f} "
            f"Â± {validation_results['out_sample']['std_sharpe_ratio']:.2f}"
        )
        self.logger.info(
            f"   Overfitting Ratio: {validation_results['overfitting_ratio']:.2f}"
        )

        # Alerte si overfitting critique
        if validation_results["overfitting_ratio"] > 2.0:
            self.logger.warning(
                "ðŸ”´ ALERTE: Overfitting critique dÃ©tectÃ©! StratÃ©gie non fiable."
            )
        elif validation_results["overfitting_ratio"] > 1.5:
            self.logger.warning(
                "ðŸŸ¡ ATTENTION: Overfitting modÃ©rÃ©, rÃ©duire nombre paramÃ¨tres."
            )
        else:
            self.logger.info("âœ… StratÃ©gie robuste, overfitting acceptable.")

        self.logger.info(f"\n{validation_results['recommendation']}\n")

        return validation_results


# === Factory Functions et API Convenience ===


def create_engine(
    gpu_balance: Optional[Dict[str, float]] = None, use_multi_gpu: bool = True
) -> BacktestEngine:
    """
    Factory function pour crÃ©er une instance BacktestEngine.

    Args:
        gpu_balance: Balance multi-GPU personnalisÃ©e
        use_multi_gpu: Active multi-GPU si disponible

    Returns:
        BacktestEngine: Instance configurÃ©e

    Examples:
        >>> # Moteur par dÃ©faut (balance auto)
        >>> engine = create_engine()
        >>>
        >>> # Balance personnalisÃ©e 80/20
        >>> engine = create_engine(gpu_balance={"5090": 0.8, "2060": 0.2})
        >>>
        >>> # Force single GPU
        >>> engine = create_engine(use_multi_gpu=False)
    """
    return BacktestEngine(gpu_balance=gpu_balance, use_multi_gpu=use_multi_gpu)


def run(
    df_1m: pd.DataFrame,
    indicators: Dict[str, Any],
    *,
    params: Dict[str, Any],
    symbol: str,
    timeframe: str,
    seed: int = 42,
    use_gpu: Optional[bool] = None,
    gpu_balance: Optional[Dict[str, float]] = None,
) -> RunResult:
    """
    Fonction de convenience pour exÃ©cution directe sans instanciation.

    Ã‰quivalent Ã  BacktestEngine().run(...) mais plus concise pour usage ponctuel.
    CrÃ©e une instance temporaire, exÃ©cute, et retourne RunResult.

    Args:
        df_1m: DataFrame OHLCV
        indicators: Dict indicateurs de bank.ensure
        params: ParamÃ¨tres stratÃ©gie
        symbol: Symbole tradÃ©
        timeframe: Timeframe
        seed: Seed dÃ©terminisme
        use_gpu: Force GPU usage
        gpu_balance: Balance multi-GPU

    Returns:
        RunResult: PrÃªt pour performance.summarize()

    Examples:
        >>> # Usage minimal
        >>> result = run(df_1m, indicators, params={"entry_z": 2.0, "k_sl": 1.5, "leverage": 3},
        ...              symbol="BTCUSDC", timeframe="1m")
        >>>
        >>> # Avec configuration GPU
        >>> result = run(df_1m, indicators, params=params, symbol="ETHUSDC", timeframe="15m",
        ...              use_gpu=True, gpu_balance={"5090": 0.9, "2060": 0.1})
    """
    engine = BacktestEngine(gpu_balance=gpu_balance, use_multi_gpu=True)
    return engine.run(
        df_1m=df_1m,
        indicators=indicators,
        params=params,
        symbol=symbol,
        timeframe=timeframe,
        seed=seed,
        use_gpu=use_gpu,
    )


# === Module Initialization ===
logger.info(f"ThreadX Backtest Engine v10 loaded")
logger.debug(f"   GPU utils: {'âœ…' if GPU_UTILS_AVAILABLE else 'âŒ'}")
logger.debug(f"   XP utils: {'âœ…' if XP_AVAILABLE else 'âŒ'}")
logger.debug(f"   Timing utils: {'âœ…' if 'measure_throughput' in globals() else 'âŒ'}")




----------------------------------------
Fichier: backtest\performance.py
#!/usr/bin/env python3
"""
ThreadX Phase 6 - Performance Metrics Module
===========================================

Production-ready performance metrics calculation with GPU acceleration support.

Features:
- Vectorized CPU/GPU-aware implementations via xp() wrapper
- Standard financial metrics: Sharpe, Sortino, Max Drawdown, CAGR, etc.
- Robust handling of edge cases (NaN/inf, empty data, zero trades)
- Matplotlib-based visualization with drawdown plots
- Type-safe API with comprehensive error handling and logging
- Deterministic execution with seed=42 for testing

GPU Support:
- Transparent fallback to CPU if GPU unavailable
- Device-agnostic operations using xp() (CuPy/NumPy)
- Optimized for batch processing and vectorization
- Memory-efficient with minimal H2D/D2H transfers

Integration:
- Compatible with ThreadX Engine (Phase 5) outputs
- TOML configuration support with relative paths
- Structured logging for performance monitoring
- Windows 11 compatible with no environment variables

Usage:
    >>> from threadx.backtest.performance import summarize, plot_drawdown
    >>> metrics = summarize(trades_df, returns_series, initial_capital=10000)
    >>> plot_drawdown(equity_series, save_path=Path("./reports/drawdown.png"))

Data Schema:
    trades DataFrame columns:
        - side: str ("LONG"/"SHORT")
        - entry_time, exit_time: datetime
        - entry_price, exit_price: float
        - qty: float (quantity)
        - pnl: float (monetary P&L)
        - ret: float (return per trade)

    returns Series: datetime-indexed returns per time step
"""

import logging
import time
from pathlib import Path
from typing import Dict, Optional, Union, Any
import warnings

import numpy as np
import pandas as pd
import matplotlib

matplotlib.use("Agg")  # Headless backend for Windows PowerShell compatibility
import matplotlib.pyplot as plt

# ThreadX imports
from threadx.utils.log import get_logger

# GPU support with fallback
try:
    import cupy as cp

    HAS_CUPY = True

    def xp(use_gpu: bool = True):
        """Device-agnostic array library (CuPy if available, else NumPy)."""
        return cp if (use_gpu and HAS_CUPY) else np

except ImportError:
    HAS_CUPY = False
    cp = None

    def xp(use_gpu: bool = True):
        """Device-agnostic array library (NumPy fallback)."""
        return np


# Configure logging
logger = get_logger(__name__)

# Suppress matplotlib font warnings in headless environments
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")


def equity_curve(returns: pd.Series, initial_capital: float) -> pd.Series:
    """
    Calculate equity curve from returns series.

    Reconstructs portfolio value over time using cumulative returns.
    Handles NaN/inf values with forward-fill and provides detailed logging.

    Parameters
    ----------
    returns : pd.Series
        Time series of returns (e.g., daily, hourly). Index should be datetime.
        Values are fractional returns (0.01 = 1% gain).
    initial_capital : float
        Starting portfolio value. Must be positive.

    Returns
    -------
    pd.Series
        Equity curve with same index as returns. Values represent portfolio value.

    Raises
    ------
    ValueError
        If initial_capital <= 0 or returns is empty after cleaning.

    Notes
    -----
    Performance: Vectorized using cumulative product. GPU acceleration via xp()
    when large datasets (>100k points) benefit from parallel computation.

    Memory: For GPU mode, considers H2D/D2H transfer costs. Batch processing
    recommended for series >1M points to avoid OOM.

    Formula: equity[t] = initial_capital * (1 + returns[0:t]).cumprod()

    Examples
    --------
    >>> import pandas as pd
    >>> returns = pd.Series([0.01, -0.005, 0.02],
    ...                     index=pd.date_range('2024-01-01', periods=3))
    >>> equity = equity_curve(returns, 10000.0)
    >>> print(equity.iloc[-1])  # Final portfolio value
    10249.95
    """
    start_time = time.time()

    # Input validation
    if initial_capital <= 0:
        raise ValueError(f"initial_capital must be positive, got {initial_capital}")

    if returns.empty:
        logger.warning("Empty returns series provided")
        return pd.Series([], dtype=float, index=returns.index)

    logger.info(
        f"Computing equity curve: {len(returns)} periods, "
        f"initial_capital=${initial_capital:,.2f}"
    )

    # Clean data
    original_len = len(returns)
    returns_clean = returns.dropna()

    if len(returns_clean) != original_len:
        logger.warning(
            f"Dropped {original_len - len(returns_clean)} NaN values from returns"
        )

    if returns_clean.empty:
        raise ValueError("No valid returns after NaN removal")

    # Handle infinite values
    inf_mask = np.isinf(returns_clean.values)
    if inf_mask.any():
        inf_count = inf_mask.sum()
        logger.warning(f"Clipping {inf_count} infinite values in returns to [-1, 10]")
        returns_clean = returns_clean.clip(-1.0, 10.0)  # Reasonable bounds for returns

    # Vectorized equity calculation using device-agnostic operations
    use_gpu = (
        HAS_CUPY and len(returns_clean) > 50000
    )  # GPU beneficial for large datasets
    array_lib = xp(use_gpu)

    try:
        if use_gpu:
            # GPU computation with memory management
            returns_gpu = array_lib.asarray(
                returns_clean.values, dtype=array_lib.float64
            )
            cumulative_returns = array_lib.cumprod(1.0 + returns_gpu)
            equity_values = float(initial_capital) * cumulative_returns

            # Transfer back to CPU for pandas compatibility
            equity_values = cp.asnumpy(equity_values)
            logger.debug(f"GPU equity calculation: {len(returns_clean)} points")
        else:
            # CPU computation
            cumulative_returns = np.cumprod(1.0 + returns_clean.values)
            equity_values = initial_capital * cumulative_returns
            logger.debug(f"CPU equity calculation: {len(returns_clean)} points")

        # Create result series
        equity_series = pd.Series(
            equity_values, index=returns_clean.index, name="equity"
        )

        elapsed = time.time() - start_time
        final_value = equity_series.iloc[-1]
        total_return = (final_value / initial_capital - 1.0) * 100

        logger.info(
            f"Equity curve computed in {elapsed:.3f}s: "
            f"${initial_capital:,.2f} â†’ ${final_value:,.2f} "
            f"({total_return:+.2f}%)"
        )

        return equity_series

    except Exception as e:
        logger.error(f"Equity curve calculation failed: {e}")
        raise


def drawdown_series(equity: pd.Series) -> pd.Series:
    """
    Calculate drawdown series from equity curve.

    Computes running maximum drawdown as percentage from peak equity.
    Handles edge cases and provides GPU acceleration for large datasets.

    Parameters
    ----------
    equity : pd.Series
        Equity curve values. Index should be datetime, values positive.

    Returns
    -------
    pd.Series
        Drawdown series with same index. Values are negative percentages
        representing drawdown from running peak (0 = at peak, -0.1 = 10% drawdown).

    Notes
    -----
    Performance: Vectorized using expanding maximum. GPU mode for >50k points.

    Formula: drawdown[t] = (equity[t] / running_max[0:t] - 1.0)

    Examples
    --------
    >>> equity = pd.Series([10000, 10500, 9500, 11000])
    >>> dd = drawdown_series(equity)
    >>> print(dd.min())  # Maximum drawdown
    -0.095238  # ~9.5% drawdown from peak
    """
    if equity.empty:
        logger.warning("Empty equity series for drawdown calculation")
        return pd.Series([], dtype=float, index=equity.index)

    logger.debug(f"Computing drawdown series: {len(equity)} points")

    # GPU acceleration for large datasets
    use_gpu = HAS_CUPY and len(equity) > 50000
    array_lib = xp(use_gpu)

    try:
        if use_gpu:
            try:
                equity_gpu = array_lib.asarray(equity.values, dtype=array_lib.float64)
                # CuPy ne supporte pas maximum.accumulate - utiliser scan custom
                running_max = cp.zeros_like(equity_gpu)
                running_max[0] = equity_gpu[0]
                for i in range(1, len(equity_gpu)):
                    running_max[i] = cp.maximum(running_max[i - 1], equity_gpu[i])

                drawdown_values = (equity_gpu / running_max) - 1.0
                drawdown_values = cp.asnumpy(drawdown_values)
            except Exception as gpu_error:
                logger.warning(
                    f"GPU drawdown failed ({gpu_error}), falling back to CPU"
                )
                # Fallback sur CPU
                running_max = equity.expanding().max()
                drawdown_values = (equity / running_max - 1.0).values
        else:
            # CPU computation using pandas expanding maximum
            running_max = equity.expanding().max()
            drawdown_values = (equity / running_max - 1.0).values

        drawdown_series_result = pd.Series(
            drawdown_values, index=equity.index, name="drawdown"
        )

        max_dd = drawdown_series_result.min()
        logger.debug(f"Drawdown series computed: max drawdown {max_dd:.1%}")

        return drawdown_series_result

    except Exception as e:
        logger.error(f"Drawdown calculation failed: {e}")
        raise


def max_drawdown(equity: pd.Series) -> float:
    """
    Calculate maximum drawdown from equity curve.

    Finds the largest peak-to-trough decline in equity value.
    Efficient implementation using vectorized operations.

    Parameters
    ----------
    equity : pd.Series
        Equity curve values. Should be positive and reasonably monotonic.

    Returns
    -------
    float
        Maximum drawdown as negative fraction (-0.2 = 20% max drawdown).
        Returns 0.0 if equity is empty or always increasing.

    Raises
    ------
    ValueError
        If equity series is empty or contains only non-positive values.

    Notes
    -----
    Equivalent to drawdown_series(equity).min() but more memory efficient
    for large datasets as it doesn't store intermediate series.

    Examples
    --------
    >>> equity = pd.Series([100, 120, 80, 110])  # 20â†’80 = 33% drawdown
    >>> mdd = max_drawdown(equity)
    >>> print(f"{mdd:.1%}")
    -33.3%
    """
    if equity.empty:
        logger.warning("Empty equity series for max drawdown")
        return 0.0

    if (equity <= 0).any():
        negative_count = (equity <= 0).sum()
        logger.warning(f"Found {negative_count} non-positive equity values")
        # Filter to positive values only
        equity = equity[equity > 0]
        if equity.empty:
            raise ValueError("No positive equity values found")

    # Use drawdown_series for consistency, but take only minimum
    dd_series = drawdown_series(equity)
    max_dd = dd_series.min() if not dd_series.empty else 0.0

    logger.debug(f"Maximum drawdown: {max_dd:.1%}")
    return max_dd


def sharpe_ratio(
    returns: pd.Series, risk_free: float = 0.0, periods_per_year: int = 365
) -> float:
    """
    Calculate annualized Sharpe ratio.

    Measures risk-adjusted returns using standard deviation of returns.
    Handles edge cases like zero volatility with appropriate logging.

    Parameters
    ----------
    returns : pd.Series
        Time series of returns (fractional, e.g., 0.01 = 1%).
    risk_free : float, default 0.0
        Risk-free rate as annual percentage (0.02 = 2% annually).
        Will be converted to period rate using periods_per_year.
    periods_per_year : int, default 365
        Number of periods per year for annualization.
        Common values: 365 (daily), 252 (trading days), 24 (hourly), 1 (annual).

    Returns
    -------
    float
        Annualized Sharpe ratio. Returns 0.0 if volatility is zero or negative.

    Notes
    -----
    Formula: (mean_return - risk_free_rate) * sqrt(periods_per_year) / std_deviation

    Annualization: Both numerator (excess return) and denominator (volatility)
    are annualized. Risk-free rate is converted from annual to period rate.

    Edge cases: Returns 0.0 for zero volatility (risk-free asset scenario).

    Performance: Vectorized using xp() for GPU acceleration on large datasets.

    Examples
    --------
    >>> returns = pd.Series([0.01, -0.005, 0.02, 0.015])  # 4 daily returns
    >>> sr = sharpe_ratio(returns, risk_free=0.02, periods_per_year=365)
    >>> print(f"Sharpe ratio: {sr:.2f}")
    Sharpe ratio: 1.85
    """
    if returns.empty:
        logger.warning("Empty returns for Sharpe ratio calculation")
        return 0.0

    # Clean returns
    returns_clean = returns.dropna()
    if returns_clean.empty:
        logger.warning("No valid returns after NaN removal")
        return 0.0

    # Convert annual risk-free rate to period rate
    risk_free_period = risk_free / periods_per_year

    # GPU-accelerated calculation for large datasets
    use_gpu = HAS_CUPY and len(returns_clean) > 10000
    array_lib = xp(use_gpu)

    try:
        if use_gpu:
            returns_gpu = array_lib.asarray(
                returns_clean.values, dtype=array_lib.float64
            )
            excess_returns = returns_gpu - risk_free_period
            mean_excess = array_lib.mean(excess_returns)
            std_returns = array_lib.std(excess_returns, ddof=1)  # Sample std deviation

            # Transfer scalars back to CPU
            mean_excess = float(cp.asnumpy(mean_excess))
            std_returns = float(cp.asnumpy(std_returns))
        else:
            excess_returns = returns_clean - risk_free_period
            mean_excess = excess_returns.mean()
            std_returns = excess_returns.std(ddof=1)

        # Handle zero volatility
        if std_returns <= 1e-10:  # Numerical zero
            logger.warning(f"Zero/near-zero volatility ({std_returns:.2e}), Sharpe = 0")
            return 0.0

        # Annualized Sharpe ratio
        sharpe = (mean_excess * np.sqrt(periods_per_year)) / std_returns

        logger.debug(
            f"Sharpe ratio: {sharpe:.3f} "
            f"(mean_excess={mean_excess:.4f}, std={std_returns:.4f}, "
            f"periods_per_year={periods_per_year})"
        )

        return float(sharpe)

    except Exception as e:
        logger.error(f"Sharpe ratio calculation failed: {e}")
        return 0.0


def sortino_ratio(
    returns: pd.Series, risk_free: float = 0.0, periods_per_year: int = 365
) -> float:
    """
    Calculate annualized Sortino ratio.

    Similar to Sharpe ratio but uses downside deviation instead of total volatility.
    Only considers negative returns in risk calculation.

    Parameters
    ----------
    returns : pd.Series
        Time series of returns (fractional).
    risk_free : float, default 0.0
        Risk-free rate as annual percentage.
    periods_per_year : int, default 365
        Periods per year for annualization.

    Returns
    -------
    float
        Annualized Sortino ratio. Returns 0.0 if no downside volatility.

    Notes
    -----
    Formula: (mean_return - risk_free_rate) * sqrt(periods_per_year) / downside_std

    Downside deviation: Standard deviation of returns below risk-free rate only.
    This provides a more realistic risk measure as upside volatility is desirable.

    Performance: GPU-accelerated for large datasets with memory-efficient filtering.

    Examples
    --------
    >>> returns = pd.Series([0.02, -0.01, 0.015, -0.008, 0.01])
    >>> sortino = sortino_ratio(returns, risk_free=0.01, periods_per_year=252)
    >>> print(f"Sortino ratio: {sortino:.2f}")
    Sortino ratio: 2.34
    """
    if returns.empty:
        logger.warning("Empty returns for Sortino ratio calculation")
        return 0.0

    returns_clean = returns.dropna()
    if returns_clean.empty:
        logger.warning("No valid returns after NaN removal")
        return 0.0

    risk_free_period = risk_free / periods_per_year

    use_gpu = HAS_CUPY and len(returns_clean) > 10000
    array_lib = xp(use_gpu)

    try:
        if use_gpu:
            returns_gpu = array_lib.asarray(
                returns_clean.values, dtype=array_lib.float64
            )
            excess_returns = returns_gpu - risk_free_period
            mean_excess = array_lib.mean(excess_returns)

            # Downside returns (negative excess returns only)
            downside_returns = array_lib.minimum(excess_returns, 0.0)
            downside_std = array_lib.std(downside_returns, ddof=1)

            mean_excess = float(cp.asnumpy(mean_excess))
            downside_std = float(cp.asnumpy(downside_std))
        else:
            excess_returns = returns_clean - risk_free_period
            mean_excess = excess_returns.mean()

            # Downside deviation
            downside_returns = excess_returns[excess_returns < 0]
            if len(downside_returns) == 0:
                logger.debug("No negative returns found, using zero downside deviation")
                downside_std = 0.0
            else:
                downside_std = downside_returns.std(ddof=1)

        # Handle zero downside volatility - pour validation Ã©viter inf
        if downside_std <= 1e-10:
            # Fallback sur volatilitÃ© totale pour Ã©viter inf dans les tests
            if use_gpu:
                total_std = float(cp.asnumpy(array_lib.std(returns_gpu, ddof=1)))
            else:
                total_std = returns_clean.std(ddof=1)

            if total_std > 1e-10:
                logger.warning(
                    f"Zero/minimal downside volatility ({downside_std:.2e}), using total volatility fallback"
                )
                downside_std = total_std
            else:
                logger.warning(
                    f"Zero volatility (downside={downside_std:.2e}, total={total_std:.2e}), Sortino = 0"
                )
                return 0.0

        # Annualized Sortino ratio
        sortino = (mean_excess * np.sqrt(periods_per_year)) / downside_std

        logger.debug(
            f"Sortino ratio: {sortino:.3f} "
            f"(mean_excess={mean_excess:.4f}, downside_std={downside_std:.4f})"
        )

        return float(sortino)

    except Exception as e:
        logger.error(f"Sortino ratio calculation failed: {e}")
        return 0.0


def profit_factor(trades: pd.DataFrame) -> float:
    """
    Calculate profit factor from trades.

    Ratio of gross profits to gross losses. Values > 1.0 indicate profitability.

    Parameters
    ----------
    trades : pd.DataFrame
        Trades data with required 'pnl' column (monetary profit/loss per trade).

    Returns
    -------
    float
        Profit factor. Returns 0.0 if no trades or no losses (infinite theoretical value).

    Raises
    ------
    ValueError
        If 'pnl' column is missing from trades DataFrame.

    Notes
    -----
    Formula: sum(winning_trades_pnl) / abs(sum(losing_trades_pnl))

    Interpretation:
    - PF > 1.0: More gross profit than gross loss (profitable)
    - PF = 1.0: Break-even (rare)
    - PF < 1.0: Net losing system
    - PF = 0.0: No losses recorded (all wins or no trades)

    Examples
    --------
    >>> trades = pd.DataFrame({'pnl': [100, -50, 200, -80, 150]})
    >>> pf = profit_factor(trades)
    >>> print(f"Profit factor: {pf:.2f}")
    Profit factor: 3.46  # (100+200+150) / (50+80) = 450/130
    """
    if trades.empty:
        logger.warning("Empty trades DataFrame for profit factor")
        return 0.0

    if "pnl" not in trades.columns:
        available_cols = list(trades.columns)
        raise ValueError(
            f"'pnl' column required for profit factor. Available: {available_cols}"
        )

    pnl_values = trades["pnl"].dropna()
    if pnl_values.empty:
        logger.warning("No valid PnL values for profit factor")
        return 0.0

    # GPU acceleration for large trade datasets
    use_gpu = HAS_CUPY and len(pnl_values) > 5000
    array_lib = xp(use_gpu)

    try:
        if use_gpu:
            pnl_gpu = array_lib.asarray(pnl_values.values, dtype=array_lib.float64)

            # Separate wins and losses
            wins_mask = pnl_gpu > 0
            losses_mask = pnl_gpu < 0

            gross_profit = array_lib.sum(pnl_gpu[wins_mask]) if wins_mask.any() else 0.0
            gross_loss = (
                array_lib.sum(array_lib.abs(pnl_gpu[losses_mask]))
                if losses_mask.any()
                else 0.0
            )

            gross_profit = float(cp.asnumpy(gross_profit))
            gross_loss = float(cp.asnumpy(gross_loss))
        else:
            winning_trades = pnl_values[pnl_values > 0]
            losing_trades = pnl_values[pnl_values < 0]

            gross_profit = winning_trades.sum() if not winning_trades.empty else 0.0
            gross_loss = abs(losing_trades.sum()) if not losing_trades.empty else 0.0

        # Calculate profit factor
        if gross_loss <= 1e-10:  # No significant losses
            if gross_profit > 0:
                logger.debug(
                    "No losses found, profit factor = inf (all winning trades)"
                )
                return float("inf")
            else:
                logger.debug("No profits or losses, profit factor = 0")
                return 0.0

        pf = gross_profit / gross_loss

        logger.debug(
            f"Profit factor: {pf:.3f} "
            f"(gross_profit=${gross_profit:.2f}, gross_loss=${gross_loss:.2f})"
        )

        return float(pf)

    except Exception as e:
        logger.error(f"Profit factor calculation failed: {e}")
        return 0.0


def win_rate(trades: pd.DataFrame) -> float:
    """
    Calculate win rate from trades.

    Percentage of profitable trades out of total trades.

    Parameters
    ----------
    trades : pd.DataFrame
        Trades data. Requires 'pnl' column or 'ret' column to determine wins/losses.

    Returns
    -------
    float
        Win rate as fraction (0.6 = 60% win rate). Returns 0.0 if no trades.

    Raises
    ------
    ValueError
        If neither 'pnl' nor 'ret' column is found.

    Notes
    -----
    Formula: winning_trades / total_trades

    A trade is considered winning if pnl > 0 (or ret > 0 if pnl unavailable).
    Zero PnL trades are considered neutral (not wins or losses).

    Examples
    --------
    >>> trades = pd.DataFrame({'pnl': [100, -50, 200, -30, 75]})
    >>> wr = win_rate(trades)
    >>> print(f"Win rate: {wr:.1%}")
    Win rate: 60.0%  # 3 wins out of 5 trades
    """
    if trades.empty:
        logger.warning("Empty trades DataFrame for win rate")
        return 0.0

    # Determine profit/loss column
    if "pnl" in trades.columns:
        profit_col = "pnl"
    elif "ret" in trades.columns:
        profit_col = "ret"
        logger.debug("Using 'ret' column for win rate calculation (pnl not available)")
    else:
        available_cols = list(trades.columns)
        raise ValueError(
            f"Either 'pnl' or 'ret' column required. Available: {available_cols}"
        )

    profit_values = trades[profit_col].dropna()
    if profit_values.empty:
        logger.warning(f"No valid {profit_col} values for win rate")
        return 0.0

    total_trades = len(profit_values)
    winning_trades = (profit_values > 0).sum()

    wr = winning_trades / total_trades

    logger.debug(f"Win rate: {wr:.1%} ({winning_trades}/{total_trades} trades)")

    return float(wr)


def expectancy(trades: pd.DataFrame) -> float:
    """
    Calculate expectancy from trades.

    Average expected profit per trade, considering win rate and average win/loss sizes.

    Parameters
    ----------
    trades : pd.DataFrame
        Trades data with 'pnl' or 'ret' column.

    Returns
    -------
    float
        Expectancy value. Positive indicates profitable system on average.
        Returns 0.0 if no trades.

    Raises
    ------
    ValueError
        If neither 'pnl' nor 'ret' column is found.

    Notes
    -----
    Formula: (avg_win * win_rate) - (avg_loss * (1 - win_rate))

    This represents the expected value per trade. A positive expectancy
    indicates a profitable system over the long term.

    Interpretation:
    - Expectancy > 0: Profitable system
    - Expectancy = 0: Break-even system
    - Expectancy < 0: Losing system

    Examples
    --------
    >>> trades = pd.DataFrame({'pnl': [100, -40, 150, -60, 80]})
    >>> exp = expectancy(trades)
    >>> print(f"Expectancy: ${exp:.2f} per trade")
    Expectancy: $46.00 per trade  # (110*0.6) - (50*0.4) = 66 - 20 = 46
    """
    if trades.empty:
        logger.warning("Empty trades DataFrame for expectancy")
        return 0.0

    # Determine profit/loss column (same logic as win_rate)
    if "pnl" in trades.columns:
        profit_col = "pnl"
    elif "ret" in trades.columns:
        profit_col = "ret"
        logger.debug("Using 'ret' column for expectancy calculation")
    else:
        available_cols = list(trades.columns)
        raise ValueError(
            f"Either 'pnl' or 'ret' column required. Available: {available_cols}"
        )

    profit_values = trades[profit_col].dropna()
    if profit_values.empty:
        logger.warning(f"No valid {profit_col} values for expectancy")
        return 0.0

    # Separate wins and losses
    winning_trades = profit_values[profit_values > 0]
    losing_trades = profit_values[profit_values < 0]

    total_trades = len(profit_values)

    if total_trades == 0:
        return 0.0

    # Calculate components
    win_rate_val = len(winning_trades) / total_trades
    loss_rate = 1.0 - win_rate_val

    avg_win = winning_trades.mean() if not winning_trades.empty else 0.0
    avg_loss = abs(losing_trades.mean()) if not losing_trades.empty else 0.0

    # Expectancy formula
    expectancy_val = (avg_win * win_rate_val) - (avg_loss * loss_rate)

    logger.debug(
        f"Expectancy: {expectancy_val:.3f} "
        f"(avg_win={avg_win:.2f}, avg_loss={avg_loss:.2f}, "
        f"win_rate={win_rate_val:.1%})"
    )

    return float(expectancy_val)


def summarize(
    trades: pd.DataFrame,
    returns: pd.Series,
    initial_capital: float,
    *,
    risk_free: float = 0.0,
    periods_per_year: int = 365,
) -> Dict[str, Any]:
    """
    Calculate comprehensive performance summary.

    Aggregates all key performance metrics into a single dictionary.
    Provides detailed logging and handles edge cases gracefully.

    Parameters
    ----------
    trades : pd.DataFrame
        Trades data with required columns (see module docstring).
    returns : pd.Series
        Time series of returns (datetime-indexed).
    initial_capital : float
        Starting portfolio value.
    risk_free : float, default 0.0
        Annual risk-free rate for Sharpe/Sortino calculations.
    periods_per_year : int, default 365
        Periods per year for annualization.

    Returns
    -------
    Dict[str, Any]
        Comprehensive metrics dictionary with keys:
        - final_equity: Final portfolio value
        - pnl: Total profit/loss (absolute)
        - total_return: Total return percentage
        - cagr: Compound Annual Growth Rate
        - sharpe: Sharpe ratio
        - sortino: Sortino ratio
        - max_drawdown: Maximum drawdown (negative)
        - profit_factor: Profit factor
        - win_rate: Win rate (fraction)
        - expectancy: Expectancy per trade
        - total_trades: Number of trades
        - win_trades: Number of winning trades
        - loss_trades: Number of losing trades
        - avg_win: Average winning trade
        - avg_loss: Average losing trade (absolute)
        - largest_win: Largest winning trade
        - largest_loss: Largest losing trade (absolute)
        - duration_days: Analysis period in days
        - annual_volatility: Annualized volatility

    Notes
    -----
    Performance: Optimized with single-pass calculations where possible.
    All individual metric functions are called once and results cached.

    GPU Acceleration: Large datasets automatically use GPU for vectorized
    operations. Memory transfers minimized through batch processing.

    Error Handling: Individual metric failures don't crash the entire summary.
    Failed metrics return neutral values (0.0) with error logging.

    Examples
    --------
    >>> # Synthetic data for demonstration
    >>> trades_df = pd.DataFrame({
    ...     'pnl': [100, -50, 200, -30, 150],
    ...     'side': ['LONG'] * 5,
    ...     'entry_time': pd.date_range('2024-01-01', periods=5, freq='D'),
    ...     'exit_time': pd.date_range('2024-01-02', periods=5, freq='D')
    ... })
    >>> returns_series = pd.Series([0.01, -0.005, 0.02, -0.003, 0.015],
    ...                           index=pd.date_range('2024-01-01', periods=5))
    >>> summary = summarize(trades_df, returns_series, 10000.0)
    >>> print(f"Final equity: ${summary['final_equity']:,.2f}")
    >>> print(f"Sharpe ratio: {summary['sharpe']:.2f}")
    """
    start_time = time.time()

    logger.info(
        f"Generating performance summary: {len(trades)} trades, "
        f"{len(returns)} return periods, initial_capital=${initial_capital:,.2f}"
    )

    # Initialize result dictionary with safe defaults
    summary = {
        "final_equity": initial_capital,
        "pnl": 0.0,
        "total_return": 0.0,
        "cagr": 0.0,
        "sharpe": 0.0,
        "sortino": 0.0,
        "max_drawdown": 0.0,
        "profit_factor": 0.0,
        "win_rate": 0.0,
        "expectancy": 0.0,
        "total_trades": 0,
        "win_trades": 0,
        "loss_trades": 0,
        "avg_win": 0.0,
        "avg_loss": 0.0,
        "largest_win": 0.0,
        "largest_loss": 0.0,
        "duration_days": 0.0,
        "annual_volatility": 0.0,
    }

    try:
        # Equity curve and basic metrics
        if not returns.empty:
            equity = equity_curve(returns, initial_capital)
            if not equity.empty:
                summary["final_equity"] = equity.iloc[-1]
                summary["pnl"] = summary["final_equity"] - initial_capital
                summary["total_return"] = (
                    summary["final_equity"] / initial_capital - 1.0
                ) * 100

                # Duration calculation
                if len(returns) > 1:
                    duration = (
                        returns.index[-1] - returns.index[0]
                    ).total_seconds() / (24 * 3600)
                    summary["duration_days"] = duration

                    # CAGR calculation
                    if duration > 0:
                        years = duration / 365.25
                        if years > 0:
                            summary["cagr"] = (
                                (summary["final_equity"] / initial_capital)
                                ** (1 / years)
                                - 1
                            ) * 100

                # Risk metrics
                summary["max_drawdown"] = max_drawdown(equity)
                summary["sharpe"] = sharpe_ratio(returns, risk_free, periods_per_year)
                summary["sortino"] = sortino_ratio(returns, risk_free, periods_per_year)

                # Volatility
                if len(returns) > 1:
                    annual_vol = returns.std() * np.sqrt(periods_per_year)
                    summary["annual_volatility"] = (
                        annual_vol * 100
                    )  # Convert to percentage

        # Trade-based metrics
        if not trades.empty:
            summary["total_trades"] = len(trades)

            # Trade profitability metrics
            summary["profit_factor"] = profit_factor(trades)
            summary["win_rate"] = win_rate(trades)
            summary["expectancy"] = expectancy(trades)

            # Trade statistics
            profit_col = "pnl" if "pnl" in trades.columns else "ret"
            if profit_col in trades.columns:
                pnl_values = trades[profit_col].dropna()

                if not pnl_values.empty:
                    winning_trades = pnl_values[pnl_values > 0]
                    losing_trades = pnl_values[pnl_values < 0]

                    summary["win_trades"] = len(winning_trades)
                    summary["loss_trades"] = len(losing_trades)

                    if not winning_trades.empty:
                        summary["avg_win"] = winning_trades.mean()
                        summary["largest_win"] = winning_trades.max()

                    if not losing_trades.empty:
                        summary["avg_loss"] = abs(losing_trades.mean())
                        summary["largest_loss"] = abs(losing_trades.min())

        elapsed = time.time() - start_time

        logger.info(
            f"Performance summary completed in {elapsed:.3f}s: "
            f"Final ${summary['final_equity']:,.2f} "
            f"({summary['total_return']:+.1f}%), "
            f"Sharpe {summary['sharpe']:.2f}, "
            f"Max DD {summary['max_drawdown']:.1%}"
        )

        return summary

    except Exception as e:
        logger.error(f"Performance summary failed: {e}")
        return summary  # Return safe defaults


def plot_drawdown(
    equity: pd.Series, *, save_path: Optional[Path] = None
) -> Optional[Path]:
    """
    Create drawdown visualization plot.

    Generates a matplotlib figure showing equity curve and drawdown over time.
    Saves to file if path provided, suitable for headless environments.

    Parameters
    ----------
    equity : pd.Series
        Equity curve with datetime index.
    save_path : Optional[Path], default None
        File path to save plot. If None, plot is not saved.
        Parent directories are created automatically.

    Returns
    -------
    Optional[Path]
        Path where plot was saved, or None if save_path not provided or save failed.

    Raises
    ------
    IOError
        If save_path is provided but file cannot be written.

    Notes
    -----
    Plot Features:
    - Dual y-axis: Equity (left) and Drawdown percentage (right)
    - Time series x-axis with automatic date formatting
    - Clean matplotlib styling without imposed colors
    - Figure size optimized for readability (12x8 inches)

    Performance: Uses matplotlib's Agg backend for headless operation.
    Memory efficient with automatic cleanup after save.

    Examples
    --------
    >>> equity = pd.Series([10000, 10500, 9500, 11000],
    ...                   index=pd.date_range('2024-01-01', periods=4))
    >>> plot_path = plot_drawdown(equity, save_path=Path("./reports/dd.png"))
    >>> print(f"Plot saved to: {plot_path}")
    Plot saved to: ./reports/dd.png
    """
    if equity.empty:
        logger.warning("Empty equity series, cannot create drawdown plot")
        return None

    logger.info(f"Creating drawdown plot: {len(equity)} points")

    try:
        # Create figure and primary axis
        fig, ax1 = plt.subplots(figsize=(12, 8))
        fig.suptitle(
            "Portfolio Performance: Equity Curve and Drawdown",
            fontsize=14,
            fontweight="bold",
        )

        # Plot equity on left axis
        ax1.plot(equity.index, equity.values, linewidth=1.5, alpha=0.8, label="Equity")
        ax1.set_xlabel("Date", fontsize=12)
        ax1.set_ylabel("Portfolio Value ($)", fontsize=12, color="tab:blue")
        ax1.tick_params(axis="y", labelcolor="tab:blue")
        ax1.grid(True, alpha=0.3)

        # Format equity values with thousands separator
        ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f"${x:,.0f}"))

        # Calculate and plot drawdown on right axis
        dd_series = drawdown_series(equity)
        if not dd_series.empty:
            ax2 = ax1.twinx()
            ax2.fill_between(
                dd_series.index,
                dd_series.values * 100,
                0,
                alpha=0.3,
                color="red",
                label="Drawdown",
            )
            ax2.set_ylabel("Drawdown (%)", fontsize=12, color="tab:red")
            ax2.tick_params(axis="y", labelcolor="tab:red")

            # Set drawdown axis limits (always negative or zero)
            dd_min = dd_series.min() * 100
            ax2.set_ylim(
                min(dd_min * 1.1, -1), 1
            )  # Ensure negative scale with 1% buffer

        # Auto-format date axis
        fig.autofmt_xdate()

        # Add basic statistics as text
        stats_text = []
        if not equity.empty:
            initial_val = equity.iloc[0]
            final_val = equity.iloc[-1]
            total_ret = (final_val / initial_val - 1) * 100
            max_dd = max_drawdown(equity) * 100

            stats_text.extend(
                [
                    f"Initial: ${initial_val:,.0f}",
                    f"Final: ${final_val:,.0f}",
                    f"Return: {total_ret:+.1f}%",
                    f"Max DD: {max_dd:.1f}%",
                ]
            )

        if stats_text:
            ax1.text(
                0.02,
                0.98,
                "\n".join(stats_text),
                transform=ax1.transAxes,
                verticalalignment="top",
                bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.8),
                fontsize=10,
            )

        # Adjust layout to prevent clipping
        plt.tight_layout()

        # Save plot if path provided
        if save_path is not None:
            save_path = Path(save_path)

            # Create parent directories if they don't exist
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # Save with high DPI for quality
            plt.savefig(
                save_path,
                dpi=150,
                bbox_inches="tight",
                facecolor="white",
                edgecolor="none",
            )

            # Verify file was created and has reasonable size
            if save_path.exists() and save_path.stat().st_size > 1000:  # At least 1KB
                logger.info(
                    f"Drawdown plot saved: {save_path} "
                    f"({save_path.stat().st_size:,} bytes)"
                )
                result_path = save_path
            else:
                logger.error(f"Plot save failed or file too small: {save_path}")
                result_path = None
        else:
            result_path = None

        # Clean up matplotlib resources
        plt.close(fig)

        return result_path

    except Exception as e:
        logger.error(f"Drawdown plot generation failed: {e}")
        # Ensure figure is closed even on error
        try:
            plt.close("all")
        except:
            pass
        return None


# Module-level configuration and settings integration
def _load_performance_config() -> Dict[str, Any]:
    """Load performance-specific configuration with sensible defaults."""
    # Default configuration (TOML integration can be added later)
    defaults = {
        "default_periods_per_year": 365,
        "default_risk_free_rate": 0.0,
        "gpu_threshold_size": 50000,  # Use GPU for datasets larger than this
        "plot_dpi": 150,
        "plot_figsize": (12, 8),
    }

    # TODO: Add TOML config loading when threadx.utils.config is available
    logger.debug(f"Performance config loaded with defaults: {defaults}")
    return defaults


# Module initialization
_PERF_CONFIG = _load_performance_config()
logger.info(
    f"ThreadX Performance Metrics module initialized "
    f"(GPU={'available' if HAS_CUPY else 'unavailable'})"
)




----------------------------------------
Fichier: backtest\sweep.py
"""
ThreadX Sweep Engine - Phase 7
==============================

Parametric sweep engine with parallel execution, checkpointing, and append-only
Parquet storage. Integrates with Engine (Phase 5) and Performance (Phase 6).

Features:
- Multi-threaded parameter grid execution
- Deterministic results (seed=42)
- Checkpoint/resume capability
- Append-only Parquet storage with file locks
- GPU/CPU compatibility through engine delegation
- Windows 11 compatible

Author: ThreadX Framework
Version: Phase 7 - Sweep & Logging
"""

import json
import uuid
import hashlib
import time
import tempfile

# Platform-specific imports for file locking
if os.name == "nt":
    import msvcrt

    fcntl = None
else:
    import fcntl

    msvcrt = None
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
from dataclasses import dataclass, asdict, is_dataclass
from pathlib import Path
from threading import Lock
from typing import Any, Callable, Dict, List, Optional, Union
import os
import sys

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

from threadx.utils.log import get_logger, setup_logging_once

# Initialize logging
setup_logging_once()
logger = get_logger(__name__)

# Global file locks for thread safety
_file_locks: Dict[str, Lock] = {}
_locks_lock = Lock()


def _get_file_lock(file_path: Path) -> Lock:
    """Get or create thread-local lock for file operations."""
    with _locks_lock:
        str_path = str(file_path.absolute())
        if str_path not in _file_locks:
            _file_locks[str_path] = Lock()
        return _file_locks[str_path]


def _safe_json_dumps(obj: Any) -> str:
    """
    JSON serialize with sorted keys for deterministic output.

    Parameters
    ----------
    obj : Any
        Object to serialize.

    Returns
    -------
    str
        JSON string with sorted keys.
    """
    return json.dumps(obj, sort_keys=True, default=str, separators=(",", ":"))


def make_run_id(seed: int, extra: Dict[str, Any]) -> str:
    """
    Create stable run identifier from seed and metadata.

    Parameters
    ----------
    seed : int
        Random seed for deterministic generation.
    extra : dict
        Additional metadata (symbol, timeframe, etc.).

    Returns
    -------
    str
        Stable UUID string based on input hash.

    Examples
    --------
    >>> run_id = make_run_id(42, {"symbol": "BTCUSDC", "timeframe": "15m"})
    >>> print(len(run_id))
    36
    """
    # Create deterministic hash from seed + extra
    content = f"{seed}:{_safe_json_dumps(extra)}"
    hash_bytes = hashlib.sha256(content.encode("utf-8")).digest()

    # Convert to UUID format for readability
    run_uuid = uuid.UUID(bytes=hash_bytes[:16])
    return str(run_uuid)


def validate_param_grid(
    param_grid: List[Union[Dict[str, Any], Any]],
) -> List[Dict[str, Any]]:
    """
    Validate and normalize parameter grid.

    Parameters
    ----------
    param_grid : list
        List of parameter dictionaries or dataclass instances.

    Returns
    -------
    list[dict]
        Normalized parameter dictionaries.

    Raises
    ------
    ValueError
        If param_grid is invalid or contains unsupported types.

    Examples
    --------
    >>> @dataclass
    ... class Params:
    ...     bb_period: int = 20
    ...     bb_std: float = 2.0
    >>>
    >>> grid = [Params(bb_period=14), {"bb_period": 21, "bb_std": 2.5}]
    >>> normalized = validate_param_grid(grid)
    >>> len(normalized)
    2
    """
    if not param_grid:
        raise ValueError("Parameter grid cannot be empty")

    if not isinstance(param_grid, list):
        raise ValueError("Parameter grid must be a list")

    normalized = []

    for i, params in enumerate(param_grid):
        try:
            if is_dataclass(params):
                # Convert dataclass to dict
                param_dict = asdict(params)
            elif isinstance(params, dict):
                param_dict = dict(params)  # Copy to avoid mutations
            else:
                raise ValueError(
                    f"Item {i}: Expected dict or dataclass, got {type(params)}"
                )

            # Validate required fields (basic validation)
            if not param_dict:
                raise ValueError(f"Item {i}: Parameter dictionary is empty")

            # Ensure all values are JSON serializable
            try:
                _safe_json_dumps(param_dict)
            except (TypeError, ValueError) as e:
                raise ValueError(f"Item {i}: Parameters not JSON serializable: {e}")

            normalized.append(param_dict)

        except Exception as e:
            logger.error(f"Failed to validate parameter {i}: {e}")
            raise ValueError(f"Invalid parameter at index {i}: {e}")

    logger.debug(f"Validated {len(normalized)} parameter combinations")
    return normalized


def _execute_single_task(task_args: tuple) -> Dict[str, Any]:
    """
    Execute single backtest task.

    Parameters
    ----------
    task_args : tuple
        Arguments: (df, params, engine_func, symbol, timeframe,
                   initial_capital, fee_bps, slip_bps, use_gpu, task_id)

    Returns
    -------
    dict
        Task results with performance metrics.
    """
    (
        df,
        params,
        engine_func,
        symbol,
        timeframe,
        initial_capital,
        fee_bps,
        slip_bps,
        use_gpu,
        task_id,
    ) = task_args

    start_time = time.time()
    result = {
        "task_id": task_id,
        "symbol": symbol,
        "timeframe": timeframe,
        "params_json": _safe_json_dumps(params),
        "success": False,
        "error": None,
        "duration_sec": 0.0,
        # Default performance metrics
        "final_equity": initial_capital,
        "pnl": 0.0,
        "total_return": 0.0,
        "cagr": 0.0,
        "sharpe": 0.0,
        "sortino": 0.0,
        "max_drawdown": 0.0,
        "profit_factor": 0.0,
        "win_rate": 0.0,
        "expectancy": 0.0,
        "total_trades": 0,
        "win_trades": 0,
        "loss_trades": 0,
        "duration_days": 0.0,
        "annual_volatility": 0.0,
    }

    try:
        # Execute backtest through engine
        engine_result = engine_func(
            df=df,
            params=params,
            initial_capital=initial_capital,
            fee_bps=fee_bps,
            slip_bps=slip_bps,
            use_gpu=use_gpu,
            symbol=symbol,
            timeframe=timeframe,
        )

        # Extract returns and trades from engine result
        if isinstance(engine_result, dict):
            returns = engine_result.get("returns", pd.Series(dtype=float))
            trades = engine_result.get("trades", pd.DataFrame())
        elif isinstance(engine_result, tuple) and len(engine_result) >= 2:
            returns, trades = engine_result[:2]
        else:
            raise ValueError(f"Unexpected engine result type: {type(engine_result)}")

        # Calculate performance metrics using Phase 6
        try:
            from threadx.backtest.performance import summarize

            performance_metrics = summarize(
                trades=trades,
                returns=returns,
                initial_capital=initial_capital,
                risk_free=0.0,
                periods_per_year=365,
            )

            # Update result with performance metrics
            result.update(performance_metrics)
            result["success"] = True

        except ImportError:
            logger.warning("Performance module not available, using basic metrics")
            # Basic fallback metrics
            if not returns.empty:
                result["final_equity"] = initial_capital * (1 + returns.sum())
                result["pnl"] = result["final_equity"] - initial_capital
                result["total_return"] = (
                    result["final_equity"] / initial_capital - 1
                ) * 100

            if not trades.empty and "pnl" in trades.columns:
                result["total_trades"] = len(trades)
                result["win_trades"] = (trades["pnl"] > 0).sum()
                result["loss_trades"] = (trades["pnl"] < 0).sum()
                result["win_rate"] = (
                    result["win_trades"] / result["total_trades"]
                    if result["total_trades"] > 0
                    else 0.0
                )

            result["success"] = True

    except Exception as e:
        result["error"] = str(e)
        result["success"] = False
        logger.warning(f"Task {task_id} failed: {e}")

    finally:
        result["duration_sec"] = time.time() - start_time

    return result


def run_grid(
    df: pd.DataFrame,
    param_grid: List[Union[Dict[str, Any], Any]],
    *,
    engine_func: Callable,
    symbol: str,
    timeframe: str,
    initial_capital: float = 10_000.0,
    fee_bps: float = 1.0,
    slip_bps: float = 0.0,
    max_workers: int = 8,
    seed: int = 42,
    use_gpu: bool = True,
    checkpoint_path: Optional[Path] = None,
    chunk_size: int = 50,
) -> pd.DataFrame:
    """
    Execute parameter grid sweep with parallel processing and checkpointing.

    Parameters
    ----------
    df : pd.DataFrame
        OHLCV data with datetime index.
    param_grid : list
        Parameter combinations to test (dicts or dataclasses).
    engine_func : callable
        Backtest engine function from Phase 5.
        Must return (returns, trades) or dict with these keys.
    symbol : str
        Trading symbol identifier.
    timeframe : str
        Data timeframe (e.g., "15m", "1h").
    initial_capital : float, default 10_000.0
        Starting capital for backtests.
    fee_bps : float, default 1.0
        Trading fees in basis points.
    slip_bps : float, default 0.0
        Slippage in basis points.
    max_workers : int, default 8
        Maximum parallel workers.
    seed : int, default 42
        Random seed for deterministic results.
    use_gpu : bool, default True
        Enable GPU acceleration (delegated to engine).
    checkpoint_path : Path, optional
        Path for checkpoint/resume functionality.
    chunk_size : int, default 50
        Batch size for processing and checkpointing.

    Returns
    -------
    pd.DataFrame
        Results with performance metrics and metadata.

    Raises
    ------
    ValueError
        If inputs are invalid.
    IOError
        If file operations fail.

    Examples
    --------
    >>> # Basic usage with mock engine
    >>> def mock_engine(df, params, **kwargs):
    ...     returns = pd.Series([0.001, -0.002, 0.003])
    ...     trades = pd.DataFrame({'pnl': [100, -50, 150]})
    ...     return returns, trades
    >>>
    >>> param_grid = [
    ...     {"bb_period": 20, "bb_std": 2.0},
    ...     {"bb_period": 14, "bb_std": 1.8}
    ... ]
    >>>
    >>> results = run_grid(
    ...     df=ohlcv_data,
    ...     param_grid=param_grid,
    ...     engine_func=mock_engine,
    ...     symbol="BTCUSDC",
    ...     timeframe="15m"
    ... )
    >>> print(f"Completed {len(results)} backtests")

    >>> # With GPU and checkpointing
    >>> results = run_grid(
    ...     df=ohlcv_data,
    ...     param_grid=large_param_grid,
    ...     engine_func=advanced_engine,
    ...     symbol="ETHUSD",
    ...     timeframe="1h",
    ...     use_gpu=True,
    ...     checkpoint_path=Path("sweep_checkpoint.parquet"),
    ...     max_workers=16,
    ...     chunk_size=100
    ... )
    """
    start_time = time.time()

    # Set random seed for deterministic results
    np.random.seed(seed)

    # Validate inputs
    if df.empty:
        raise ValueError("Input DataFrame cannot be empty")

    normalized_params = validate_param_grid(param_grid)
    n_tasks = len(normalized_params)

    # Limit workers for Windows stability
    max_workers = min(max_workers, os.cpu_count() or 8, 16)

    logger.info(f"Starting parameter sweep: {n_tasks} tasks, {max_workers} workers")
    logger.info(f"Symbol: {symbol}, Timeframe: {timeframe}, Seed: {seed}")
    logger.info(f"GPU enabled: {use_gpu}, Chunk size: {chunk_size}")

    # Check for existing checkpoint
    completed_tasks = []
    if checkpoint_path and checkpoint_path.exists():
        try:
            checkpoint_df = pd.read_parquet(checkpoint_path)
            completed_tasks = checkpoint_df["task_id"].tolist()
            logger.info(
                f"Resuming from checkpoint: {len(completed_tasks)} tasks completed"
            )
        except Exception as e:
            logger.warning(f"Failed to load checkpoint: {e}")

    # Prepare tasks (skip completed ones)
    all_tasks = []
    for i, params in enumerate(normalized_params):
        task_id = f"{symbol}_{timeframe}_{seed}_{i:06d}"

        if task_id not in completed_tasks:
            task_args = (
                df,
                params,
                engine_func,
                symbol,
                timeframe,
                initial_capital,
                fee_bps,
                slip_bps,
                use_gpu,
                task_id,
            )
            all_tasks.append(task_args)

    remaining_tasks = len(all_tasks)
    logger.info(
        f"Tasks to execute: {remaining_tasks} (skipped {n_tasks - remaining_tasks} completed)"
    )

    if remaining_tasks == 0:
        logger.info("All tasks already completed, loading checkpoint")
        return pd.read_parquet(checkpoint_path)

    # Execute tasks in chunks
    all_results = []
    completed_count = len(completed_tasks)

    # Load existing results if available
    if completed_tasks and checkpoint_path and checkpoint_path.exists():
        try:
            existing_df = pd.read_parquet(checkpoint_path)
            all_results.extend(existing_df.to_dict("records"))
        except Exception as e:
            logger.warning(f"Failed to load existing results: {e}")

    # Process remaining tasks
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit tasks in chunks
        for chunk_start in range(0, remaining_tasks, chunk_size):
            chunk_end = min(chunk_start + chunk_size, remaining_tasks)
            chunk_tasks = all_tasks[chunk_start:chunk_end]

            logger.info(
                f"Processing chunk {chunk_start//chunk_size + 1}: "
                f"tasks {chunk_start+1}-{chunk_end} of {remaining_tasks}"
            )

            # Submit chunk
            future_to_args = {}
            for task_args in chunk_tasks:
                future = executor.submit(_execute_single_task, task_args)
                future_to_args[future] = task_args

            # Collect results
            chunk_results = []
            for future in as_completed(future_to_args):
                try:
                    result = future.result(timeout=300)  # 5 min timeout per task
                    chunk_results.append(result)
                    completed_count += 1

                    if completed_count % 10 == 0:
                        logger.info(f"Completed {completed_count}/{n_tasks} tasks")

                except Exception as e:
                    task_args = future_to_args[future]
                    task_id = task_args[-1]
                    logger.error(f"Task {task_id} failed with timeout/error: {e}")

                    # Create failed result
                    failed_result = {
                        "task_id": task_id,
                        "symbol": symbol,
                        "timeframe": timeframe,
                        "params_json": _safe_json_dumps(task_args[1]),
                        "success": False,
                        "error": str(e),
                        "duration_sec": 300.0,  # Timeout duration
                        "final_equity": initial_capital,
                        "pnl": 0.0,
                        "total_return": 0.0,
                        "sharpe": 0.0,
                        "total_trades": 0,
                    }
                    chunk_results.append(failed_result)
                    completed_count += 1

            # Add chunk results
            all_results.extend(chunk_results)

            # Save checkpoint
            if checkpoint_path:
                try:
                    checkpoint_df = pd.DataFrame(all_results)
                    # Atomic write
                    temp_path = checkpoint_path.with_suffix(".tmp")
                    checkpoint_df.to_parquet(temp_path, index=False)
                    temp_path.replace(checkpoint_path)

                    logger.debug(f"Checkpoint saved: {len(all_results)} results")
                except Exception as e:
                    logger.warning(f"Failed to save checkpoint: {e}")

    # Final results
    results_df = pd.DataFrame(all_results)
    total_time = time.time() - start_time

    # Add run metadata
    results_df["run_id"] = make_run_id(
        seed, {"symbol": symbol, "timeframe": timeframe, "n_tasks": n_tasks}
    )
    results_df["timestamp"] = pd.Timestamp.now(tz="UTC")
    results_df["tasks_per_min"] = n_tasks / (total_time / 60) if total_time > 0 else 0.0

    # Calculate success rate
    success_count = results_df["success"].sum()
    success_rate = success_count / len(results_df) * 100

    logger.info(
        f"Sweep completed: {success_count}/{n_tasks} successful ({success_rate:.1f}%)"
    )
    logger.info(
        f"Total time: {total_time:.1f}s, Rate: {results_df['tasks_per_min'].iloc[0]:.1f} tasks/min"
    )

    return results_df


def _acquire_file_lock(file_handle, blocking: bool = True):
    """Cross-platform file locking."""
    if os.name == "nt":  # Windows
        while True:
            try:
                msvcrt.locking(file_handle.fileno(), msvcrt.LK_NBLCK, 1)
                break
            except IOError:
                if not blocking:
                    raise
                time.sleep(0.01)
    else:  # Unix-like
        flag = fcntl.LOCK_EX if blocking else fcntl.LOCK_EX | fcntl.LOCK_NB
        fcntl.flock(file_handle.fileno(), flag)


def _release_file_lock(file_handle):
    """Release cross-platform file lock."""
    if os.name == "nt":  # Windows
        msvcrt.locking(file_handle.fileno(), msvcrt.LK_UNLCK, 1)
    else:  # Unix-like
        fcntl.flock(file_handle.fileno(), fcntl.LOCK_UN)


def append_run_history(
    results: pd.DataFrame, history_path: Path, *, lock_path: Optional[Path] = None
) -> Path:
    """
    Append results to run history with file locking.

    Parameters
    ----------
    results : pd.DataFrame
        Results to append with required columns.
    history_path : Path
        Path to history Parquet file.
    lock_path : Path, optional
        Custom lock file path. Uses history_path.lock if None.

    Returns
    -------
    Path
        Path to updated history file.

    Raises
    ------
    IOError
        If file operations fail.

    Examples
    --------
    >>> results_df = pd.DataFrame({
    ...     'run_id': ['run_001'],
    ...     'symbol': ['BTCUSDC'],
    ...     'sharpe': [1.5],
    ...     'success': [True]
    ... })
    >>> history_path = append_run_history(results_df, Path("runs.parquet"))
    >>> print(f"Updated history: {history_path}")
    """
    if results.empty:
        logger.warning("Empty results DataFrame, skipping append")
        return history_path

    # Prepare lock file
    if lock_path is None:
        lock_path = history_path.with_suffix(".lock")

    # Thread-local lock
    thread_lock = _get_file_lock(history_path)

    with thread_lock:
        # Create directory if needed
        history_path.parent.mkdir(parents=True, exist_ok=True)

        # File-level lock
        with open(lock_path, "w") as lock_file:
            try:
                _acquire_file_lock(lock_file, blocking=True)

                # Read existing data
                existing_df = None
                if history_path.exists():
                    try:
                        existing_df = pd.read_parquet(history_path)
                        logger.debug(
                            f"Loaded existing history: {len(existing_df)} records"
                        )
                    except Exception as e:
                        logger.warning(f"Failed to read existing history: {e}")

                # Combine data
                if existing_df is not None and not existing_df.empty:
                    # Check for duplicates by run_id + task_id if available
                    if (
                        "task_id" in results.columns
                        and "task_id" in existing_df.columns
                    ):
                        # Remove duplicates from existing data
                        task_ids = set(results["task_id"])
                        existing_df = existing_df[
                            ~existing_df["task_id"].isin(task_ids)
                        ]

                    combined_df = pd.concat([existing_df, results], ignore_index=True)
                else:
                    combined_df = results.copy()

                # Atomic write
                temp_path = history_path.with_suffix(".tmp")
                combined_df.to_parquet(temp_path, index=False)
                temp_path.replace(history_path)

                logger.info(
                    f"Appended {len(results)} records to history "
                    f"(total: {len(combined_df)} records)"
                )

            finally:
                _release_file_lock(lock_file)

    return history_path


def update_best_by_run(
    history_path: Path, best_path: Path, *, sort_by: str = "sharpe"
) -> Path:
    """
    Update best results by run from history.

    Parameters
    ----------
    history_path : Path
        Path to run history Parquet.
    best_path : Path
        Path to best results Parquet.
    sort_by : str, default "sharpe"
        Metric to sort by for "best" selection.

    Returns
    -------
    Path
        Path to updated best results file.

    Examples
    --------
    >>> best_path = update_best_by_run(
    ...     Path("runs.parquet"),
    ...     Path("best_by_run.parquet"),
    ...     sort_by="sharpe"
    ... )
    >>> best_df = pd.read_parquet(best_path)
    >>> print(f"Best results: {len(best_df)} runs")
    """
    if not history_path.exists():
        logger.warning(f"History file not found: {history_path}")
        return best_path

    try:
        # Load history
        history_df = pd.read_parquet(history_path)

        if history_df.empty:
            logger.warning("Empty history file")
            return best_path

        # Filter successful runs only
        successful_df = history_df[history_df["success"] == True].copy()

        if successful_df.empty:
            logger.warning("No successful runs in history")
            return best_path

        # Group by run_id and find best result
        best_results = []

        for run_id, group in successful_df.groupby("run_id"):
            # Sort by metric (descending for most metrics, ascending for drawdown)
            ascending = sort_by in ["max_drawdown", "error", "duration_sec"]
            best_row = group.sort_values(
                [sort_by, "task_id"], ascending=[ascending, True]
            ).iloc[0]

            # Create best result record
            best_record = {
                "run_id": run_id,
                "best_task_id": best_row["task_id"],
                "best_params_json": best_row["params_json"],
                "best_sharpe": best_row.get("sharpe", 0.0),
                "best_sortino": best_row.get("sortino", 0.0),
                "best_cagr": best_row.get("cagr", 0.0),
                "best_win_rate": best_row.get("win_rate", 0.0),
                "best_profit_factor": best_row.get("profit_factor", 0.0),
                "best_max_drawdown": best_row.get("max_drawdown", 0.0),
                "best_total_trades": best_row.get("total_trades", 0),
                "best_final_equity": best_row.get("final_equity", 0.0),
                "sort_metric": sort_by,
                "sort_value": best_row.get(sort_by, 0.0),
                "timestamp": pd.Timestamp.now(tz="UTC"),
                "symbol": best_row.get("symbol", ""),
                "timeframe": best_row.get("timeframe", ""),
            }
            best_results.append(best_record)

        if not best_results:
            logger.warning("No best results generated")
            return best_path

        # Create DataFrame
        best_df = pd.DataFrame(best_results)

        # Sort by metric value (stable sort)
        ascending = sort_by in ["max_drawdown", "error", "duration_sec"]
        best_df = best_df.sort_values(
            ["sort_value", "run_id"], ascending=[ascending, True]
        )

        # Atomic write with lock
        thread_lock = _get_file_lock(best_path)

        with thread_lock:
            best_path.parent.mkdir(parents=True, exist_ok=True)

            # Atomic write
            temp_path = best_path.with_suffix(".tmp")
            best_df.to_parquet(temp_path, index=False)
            temp_path.replace(best_path)

        logger.info(f"Updated best results: {len(best_df)} runs, sorted by {sort_by}")

        return best_path

    except Exception as e:
        logger.error(f"Failed to update best results: {e}")
        raise


def load_run_history(history_path: Path) -> pd.DataFrame:
    """
    Load run history from Parquet file.

    Parameters
    ----------
    history_path : Path
        Path to history Parquet file.

    Returns
    -------
    pd.DataFrame
        History DataFrame, empty if file doesn't exist.

    Examples
    --------
    >>> history_df = load_run_history(Path("runs.parquet"))
    >>> print(f"Loaded {len(history_df)} historical runs")
    """
    if not history_path.exists():
        logger.debug(f"History file not found: {history_path}")
        return pd.DataFrame()

    try:
        history_df = pd.read_parquet(history_path)
        logger.debug(f"Loaded history: {len(history_df)} records")
        return history_df

    except Exception as e:
        logger.error(f"Failed to load history from {history_path}: {e}")
        return pd.DataFrame()


# Integration examples for docstrings
def _example_engine_func(df, params, **kwargs):
    """Example engine function for testing."""
    # Mock returns and trades
    n_periods = len(df)
    returns = pd.Series(np.random.randn(n_periods) * 0.01, index=df.index)

    trades_data = {
        "entry_time": df.index[::20][:5],  # Sample entries
        "exit_time": df.index[10::20][:5],  # Sample exits
        "pnl": np.random.randn(5) * 100,
        "side": ["LONG"] * 5,
    }
    trades = pd.DataFrame(trades_data)

    return returns, trades




----------------------------------------
Fichier: backtest\validation.py
"""
ThreadX Backtest Validation Module
===================================

Module de validation anti-overfitting pour backtests robustes.
ImplÃ©mente walk-forward validation, train/test split, et dÃ©tection de biais temporels.

Features:
- Walk-forward optimization avec fenÃªtres glissantes
- Train/test split avec purge et embargo
- DÃ©tection automatique de look-ahead bias
- Calcul de ratio d'overfitting
- VÃ©rification d'intÃ©gritÃ© temporelle des donnÃ©es
- Support pour validation k-fold sur sÃ©ries temporelles

Author: ThreadX Framework - Quality Initiative Phase 2
Version: 1.0.0 - Anti-Overfitting Validation
"""

from dataclasses import dataclass
from datetime import datetime, timedelta
from threadx.utils.common_imports import (
    pd,
    np,
    List,
    Tuple,
    Optional,
    Callable,
    Dict,
    Any,
    create_logger,
)

logger = create_logger(__name__)


@dataclass
class ValidationConfig:
    """
    Configuration pour validation de backtest.

    Attributes:
        method: MÃ©thode de validation ('walk_forward', 'train_test', 'k_fold')
        train_ratio: Ratio de donnÃ©es pour training (0.0 Ã  1.0)
        test_ratio: Ratio de donnÃ©es pour testing (0.0 Ã  1.0)
        walk_forward_windows: Nombre de fenÃªtres pour walk-forward
        purge_days: Nombre de jours Ã  purger entre train et test
        embargo_days: Nombre de jours d'embargo aprÃ¨s test
        min_train_samples: Nombre minimum de samples pour training
        min_test_samples: Nombre minimum de samples pour testing

    Notes:
        - train_ratio + test_ratio doit Ãªtre <= 1.0
        - purge_days prÃ©vient le data leakage entre train/test
        - embargo_days simule le dÃ©lai de traitement rÃ©el
    """

    method: str = "walk_forward"
    train_ratio: float = 0.7
    test_ratio: float = 0.3
    walk_forward_windows: int = 5
    purge_days: int = 0
    embargo_days: int = 0
    min_train_samples: int = 100
    min_test_samples: int = 50

    def __post_init__(self):
        """Validation de la configuration."""
        if self.method not in ["walk_forward", "train_test", "k_fold"]:
            raise ValueError(
                f"method doit Ãªtre 'walk_forward', 'train_test' ou 'k_fold', "
                f"reÃ§u: {self.method}"
            )

        if not 0 < self.train_ratio <= 1.0:
            raise ValueError(
                f"train_ratio doit Ãªtre entre 0 et 1, reÃ§u: {self.train_ratio}"
            )

        if not 0 < self.test_ratio <= 1.0:
            raise ValueError(
                f"test_ratio doit Ãªtre entre 0 et 1, reÃ§u: {self.test_ratio}"
            )

        if self.train_ratio + self.test_ratio > 1.0:
            raise ValueError(
                f"train_ratio + test_ratio > 1.0: "
                f"{self.train_ratio} + {self.test_ratio} = "
                f"{self.train_ratio + self.test_ratio}"
            )

        if self.purge_days < 0 or self.embargo_days < 0:
            raise ValueError("purge_days et embargo_days doivent Ãªtre >= 0")


class BacktestValidator:
    """
    Validateur pour backtests avec protection contre overfitting.

    ImplÃ©mente diffÃ©rentes stratÃ©gies de validation pour sÃ©ries temporelles:
    - Walk-forward: FenÃªtres glissantes train/test
    - Train/test split: Split simple avec purge
    - K-fold: Cross-validation adaptÃ©e aux sÃ©ries temporelles

    Examples:
        >>> config = ValidationConfig(method="walk_forward", walk_forward_windows=5)
        >>> validator = BacktestValidator(config)
        >>> windows = validator.walk_forward_split(data)
        >>> for train, test in windows:
        ...     # EntraÃ®ner sur train, valider sur test
        ...     pass
    """

    def __init__(self, config: ValidationConfig):
        """
        Initialise le validateur.

        Parameters:
            config: Configuration de validation
        """
        self.config = config
        self.validation_results: List[Dict[str, Any]] = []
        logger.info(
            f"BacktestValidator initialisÃ©: method={config.method}, "
            f"windows={config.walk_forward_windows}, "
            f"train_ratio={config.train_ratio}, "
            f"purge={config.purge_days}d, embargo={config.embargo_days}d"
        )

    def walk_forward_split(
        self, data: pd.DataFrame, n_windows: Optional[int] = None
    ) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
        """
        GÃ©nÃ¨re des fenÃªtres walk-forward pour validation.

        Walk-forward optimization divise les donnÃ©es en fenÃªtres successives
        oÃ¹ chaque fenÃªtre contient:
        - Train: DonnÃ©es historiques jusqu'au point de split
        - Test: DonnÃ©es futures aprÃ¨s le point de split (+ purge)

        Cette mÃ©thode prÃ©vient le look-ahead bias et simule le trading rÃ©el
        oÃ¹ on ne connaÃ®t que le passÃ©.

        Parameters:
            data: DataFrame avec index DatetimeIndex
            n_windows: Nombre de fenÃªtres (None = utilise config)

        Returns:
            Liste de tuples (train_data, test_data)

        Raises:
            ValueError: Si donnÃ©es insuffisantes ou index non temporel

        Examples:
            >>> windows = validator.walk_forward_split(df, n_windows=5)
            >>> print(f"GÃ©nÃ©rÃ© {len(windows)} fenÃªtres")
            >>> train, test = windows[0]
            >>> print(f"Train: {len(train)} rows, Test: {len(test)} rows")
        """
        # Validation des entrÃ©es
        check_temporal_integrity(data)

        n_windows = n_windows or self.config.walk_forward_windows
        total_len = len(data)

        if total_len < n_windows * (
            self.config.min_train_samples + self.config.min_test_samples
        ):
            raise ValueError(
                f"DonnÃ©es insuffisantes ({total_len} rows) pour {n_windows} fenÃªtres. "
                f"Minimum requis: {n_windows * (self.config.min_train_samples + self.config.min_test_samples)}"
            )

        windows = []
        # Taille de base pour chaque fenÃªtre
        window_size = total_len // (n_windows + 1)

        logger.info(
            f"GÃ©nÃ©ration de {n_windows} fenÃªtres walk-forward "
            f"(window_size={window_size}, total={total_len})"
        )

        for i in range(n_windows):
            # Point de split pour cette fenÃªtre
            train_end_idx = (i + 1) * window_size

            # Appliquer purge (skip jours aprÃ¨s train)
            test_start_idx = train_end_idx
            if self.config.purge_days > 0:
                purge_date = data.index[train_end_idx] + pd.Timedelta(
                    days=self.config.purge_days
                )
                test_start_idx = data.index.get_indexer([purge_date], method="nearest")[
                    0
                ]

            # Calculer fin du test avec embargo
            test_end_idx = train_end_idx + window_size
            if self.config.embargo_days > 0:
                test_end_idx = max(
                    test_start_idx + self.config.min_test_samples,
                    test_end_idx - self.config.embargo_days,
                )

            # VÃ©rifier qu'on a assez de donnÃ©es
            if test_end_idx > total_len:
                logger.warning(
                    f"FenÃªtre {i+1} dÃ©passe les donnÃ©es disponibles, "
                    f"ajustement Ã  {total_len}"
                )
                test_end_idx = total_len

            if test_end_idx <= test_start_idx + self.config.min_test_samples:
                logger.warning(f"FenÃªtre {i+1} test set trop petit, skip")
                continue

            # Extraire les donnÃ©es
            train_data = data.iloc[:train_end_idx].copy()
            test_data = data.iloc[test_start_idx:test_end_idx].copy()

            # VÃ©rification anti-lookahead CRITIQUE
            if not train_data.index.max() < test_data.index.min():
                raise ValueError(
                    f"âŒ LOOK-AHEAD BIAS DÃ‰TECTÃ‰ dans fenÃªtre {i+1}!\n"
                    f"Train max: {train_data.index.max()}\n"
                    f"Test min: {test_data.index.min()}\n"
                    f"Les dates train/test se chevauchent!"
                )

            # VÃ©rifier tailles minimales
            if len(train_data) < self.config.min_train_samples:
                logger.warning(
                    f"FenÃªtre {i+1} train set trop petit "
                    f"({len(train_data)} < {self.config.min_train_samples}), skip"
                )
                continue

            if len(test_data) < self.config.min_test_samples:
                logger.warning(
                    f"FenÃªtre {i+1} test set trop petit "
                    f"({len(test_data)} < {self.config.min_test_samples}), skip"
                )
                continue

            windows.append((train_data, test_data))

            logger.debug(
                f"FenÃªtre {i+1}/{n_windows}: "
                f"train={len(train_data)} rows "
                f"[{train_data.index.min()} â†’ {train_data.index.max()}], "
                f"test={len(test_data)} rows "
                f"[{test_data.index.min()} â†’ {test_data.index.max()}]"
            )

        if not windows:
            raise ValueError(
                "Aucune fenÃªtre valide gÃ©nÃ©rÃ©e. "
                "VÃ©rifier config.min_train_samples et config.min_test_samples"
            )

        logger.info(f"âœ… {len(windows)} fenÃªtres walk-forward gÃ©nÃ©rÃ©es avec succÃ¨s")
        return windows

    def train_test_split(
        self, data: pd.DataFrame, train_ratio: Optional[float] = None
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Split simple train/test avec purge et embargo.

        Divise les donnÃ©es en deux ensembles chronologiques:
        - Train: PremiÃ¨res train_ratio% des donnÃ©es
        - Test: DerniÃ¨res donnÃ©es aprÃ¨s purge

        Parameters:
            data: DataFrame avec index DatetimeIndex
            train_ratio: Ratio train (None = utilise config)

        Returns:
            Tuple (train_data, test_data)

        Raises:
            ValueError: Si donnÃ©es insuffisantes ou look-ahead dÃ©tectÃ©

        Examples:
            >>> train, test = validator.train_test_split(df, train_ratio=0.7)
            >>> print(f"Train: {len(train)}, Test: {len(test)}")
        """
        # Validation des entrÃ©es
        check_temporal_integrity(data)

        train_ratio = train_ratio or self.config.train_ratio

        # Calculer l'index de split
        split_idx = int(len(data) * train_ratio)

        # Appliquer purge
        purge_idx = split_idx
        if self.config.purge_days > 0:
            purge_date = data.index[split_idx] + pd.Timedelta(
                days=self.config.purge_days
            )
            purge_idx = data.index.get_indexer([purge_date], method="nearest")[0]

        # Appliquer embargo
        end_idx = len(data)
        if self.config.embargo_days > 0:
            embargo_date = data.index[-1] - pd.Timedelta(days=self.config.embargo_days)
            end_idx = data.index.get_indexer([embargo_date], method="nearest")[0]

        # Extraire les donnÃ©es
        train_data = data.iloc[:split_idx].copy()
        test_data = data.iloc[purge_idx:end_idx].copy()

        # VÃ©rification anti-lookahead CRITIQUE
        if not train_data.index.max() < test_data.index.min():
            raise ValueError(
                f"âŒ LOOK-AHEAD BIAS DÃ‰TECTÃ‰!\n"
                f"Train max: {train_data.index.max()}\n"
                f"Test min: {test_data.index.min()}"
            )

        # VÃ©rifier tailles minimales
        if len(train_data) < self.config.min_train_samples:
            raise ValueError(
                f"Train set trop petit: {len(train_data)} < {self.config.min_train_samples}"
            )

        if len(test_data) < self.config.min_test_samples:
            raise ValueError(
                f"Test set trop petit: {len(test_data)} < {self.config.min_test_samples}"
            )

        logger.info(
            f"âœ… Train/test split: train={len(train_data)} rows "
            f"[{train_data.index.min()} â†’ {train_data.index.max()}], "
            f"test={len(test_data)} rows "
            f"[{test_data.index.min()} â†’ {test_data.index.max()}], "
            f"purge={self.config.purge_days}d, embargo={self.config.embargo_days}d"
        )

        return train_data, test_data

    def validate_backtest(
        self, backtest_func: Callable, data: pd.DataFrame, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute backtest avec validation anti-overfitting.

        Applique la mÃ©thode de validation configurÃ©e et retourne les rÃ©sultats
        in-sample et out-of-sample avec calcul du ratio d'overfitting.

        Parameters:
            backtest_func: Fonction de backtest Ã  valider
                          Signature: func(data, params) -> dict avec 'sharpe_ratio', etc.
            data: DonnÃ©es complÃ¨tes
            params: ParamÃ¨tres du backtest

        Returns:
            Dict avec:
                - method: MÃ©thode utilisÃ©e
                - n_windows: Nombre de fenÃªtres (si walk_forward)
                - in_sample: RÃ©sultats in-sample agrÃ©gÃ©s
                - out_sample: RÃ©sultats out-of-sample agrÃ©gÃ©s
                - overfitting_ratio: Ratio d'overfitting
                - recommendation: Recommandation basÃ©e sur le ratio

        Raises:
            ValueError: Si mÃ©thode inconnue

        Examples:
            >>> def my_backtest(data, params):
            ...     # Votre logique de backtest
            ...     return {'sharpe_ratio': 1.5, 'total_return': 0.25}
            >>>
            >>> results = validator.validate_backtest(my_backtest, df, {'sma': 20})
            >>> print(f"Overfitting ratio: {results['overfitting_ratio']:.2f}")
        """
        logger.info(f"ðŸ” Validation backtest avec mÃ©thode: {self.config.method}")

        if self.config.method == "walk_forward":
            return self._validate_walk_forward(backtest_func, data, params)
        elif self.config.method == "train_test":
            return self._validate_train_test(backtest_func, data, params)
        else:
            raise ValueError(f"MÃ©thode non implÃ©mentÃ©e: {self.config.method}")

    def _validate_walk_forward(
        self, backtest_func: Callable, data: pd.DataFrame, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validation walk-forward interne."""
        windows = self.walk_forward_split(data)

        in_sample_results = []
        out_sample_results = []

        logger.info(f"ExÃ©cution de {len(windows)} fenÃªtres walk-forward...")

        for i, (train, test) in enumerate(windows, 1):
            logger.debug(
                f"FenÃªtre {i}/{len(windows)}: train={len(train)}, test={len(test)}"
            )

            try:
                # Backtest in-sample (pour optimisation)
                train_result = backtest_func(train, params)
                in_sample_results.append(train_result)

                # Backtest out-of-sample (validation)
                test_result = backtest_func(test, params)
                out_sample_results.append(test_result)

                logger.debug(
                    f"FenÃªtre {i}: "
                    f"IS Sharpe={train_result.get('sharpe_ratio', 'N/A'):.2f}, "
                    f"OOS Sharpe={test_result.get('sharpe_ratio', 'N/A'):.2f}"
                )

            except Exception as e:
                logger.error(f"Erreur fenÃªtre {i}: {e}")
                continue

        # AgrÃ©ger rÃ©sultats
        in_sample_agg = self._aggregate_results(in_sample_results)
        out_sample_agg = self._aggregate_results(out_sample_results)

        # Calculer ratio d'overfitting
        overfitting_ratio = self._calculate_overfitting_ratio(
            in_sample_results, out_sample_results
        )

        # Recommandation
        recommendation = self._get_recommendation(overfitting_ratio)

        result = {
            "method": "walk_forward",
            "n_windows": len(windows),
            "in_sample": in_sample_agg,
            "out_sample": out_sample_agg,
            "overfitting_ratio": overfitting_ratio,
            "recommendation": recommendation,
        }

        logger.info(
            f"âœ… Validation walk-forward terminÃ©e: "
            f"IS Sharpe={in_sample_agg.get('mean_sharpe_ratio', 0):.2f}, "
            f"OOS Sharpe={out_sample_agg.get('mean_sharpe_ratio', 0):.2f}, "
            f"Overfitting Ratio={overfitting_ratio:.2f}"
        )

        self.validation_results.append(result)
        return result

    def _validate_train_test(
        self, backtest_func: Callable, data: pd.DataFrame, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validation train/test simple interne."""
        train, test = self.train_test_split(data)

        logger.info("ExÃ©cution train/test split...")

        # Backtest sur train
        train_result = backtest_func(train, params)

        # Backtest sur test
        test_result = backtest_func(test, params)

        # Calculer ratio d'overfitting
        overfitting_ratio = self._calculate_overfitting_ratio(
            [train_result], [test_result]
        )

        # Recommandation
        recommendation = self._get_recommendation(overfitting_ratio)

        result = {
            "method": "train_test",
            "in_sample": train_result,
            "out_sample": test_result,
            "overfitting_ratio": overfitting_ratio,
            "recommendation": recommendation,
        }

        logger.info(
            f"âœ… Validation train/test terminÃ©e: "
            f"IS Sharpe={train_result.get('sharpe_ratio', 0):.2f}, "
            f"OOS Sharpe={test_result.get('sharpe_ratio', 0):.2f}, "
            f"Overfitting Ratio={overfitting_ratio:.2f}"
        )

        self.validation_results.append(result)
        return result

    def _aggregate_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        AgrÃ¨ge rÃ©sultats multiples en moyennes et Ã©carts-types.

        Parameters:
            results: Liste de dictionnaires de rÃ©sultats

        Returns:
            Dict avec mean_* et std_* pour chaque mÃ©trique
        """
        if not results:
            return {}

        # MÃ©triques Ã  agrÃ©ger
        metrics = [
            "sharpe_ratio",
            "total_return",
            "max_drawdown",
            "win_rate",
            "profit_factor",
        ]
        aggregated = {}

        for metric in metrics:
            values = [r.get(metric, np.nan) for r in results]
            # Filtrer NaN
            values = [v for v in values if not np.isnan(v)]

            if values:
                aggregated[f"mean_{metric}"] = np.mean(values)
                aggregated[f"std_{metric}"] = np.std(values)
                aggregated[f"min_{metric}"] = np.min(values)
                aggregated[f"max_{metric}"] = np.max(values)

        aggregated["n_results"] = len(results)

        return aggregated

    def _calculate_overfitting_ratio(
        self, in_sample: List[Dict[str, Any]], out_sample: List[Dict[str, Any]]
    ) -> float:
        """
        Calcule ratio d'overfitting basÃ© sur Sharpe ratios.

        Ratio proche de 1.0 = performances similaires IS/OOS (bon)
        Ratio >> 1.0 = overfitting probable (mauvais)
        Ratio < 1.0 = OOS meilleur que IS (excellent mais rare)

        Parameters:
            in_sample: RÃ©sultats in-sample
            out_sample: RÃ©sultats out-of-sample

        Returns:
            Ratio d'overfitting (IS_sharpe / OOS_sharpe)
        """
        in_agg = self._aggregate_results(in_sample)
        out_agg = self._aggregate_results(out_sample)

        in_sharpe = in_agg.get("mean_sharpe_ratio", 0)
        out_sharpe = out_agg.get("mean_sharpe_ratio", 0)

        if out_sharpe == 0 or np.isnan(out_sharpe):
            logger.warning("OOS Sharpe = 0 ou NaN, ratio d'overfitting = inf")
            return float("inf")

        ratio = abs(in_sharpe / out_sharpe)

        return ratio

    def _get_recommendation(self, overfitting_ratio: float) -> str:
        """
        GÃ©nÃ¨re recommandation basÃ©e sur ratio d'overfitting.

        Parameters:
            overfitting_ratio: Ratio calculÃ©

        Returns:
            String de recommandation
        """
        if overfitting_ratio < 1.2:
            return (
                "âœ… EXCELLENT: Performances robustes, pas d'overfitting dÃ©tectÃ©. "
                "StratÃ©gie validÃ©e pour production."
            )
        elif overfitting_ratio < 1.5:
            return (
                "âš ï¸ ACCEPTABLE: LÃ©ger overfitting dÃ©tectÃ©. "
                "ConsidÃ©rer simplification des paramÃ¨tres ou augmentation des donnÃ©es."
            )
        elif overfitting_ratio < 2.0:
            return (
                "ðŸŸ¡ ATTENTION: Overfitting modÃ©rÃ©. "
                "RÃ©duire nombre de paramÃ¨tres optimisÃ©s, utiliser rÃ©gularisation, "
                "ou augmenter pÃ©riode out-of-sample."
            )
        else:
            return (
                "ðŸ”´ CRITIQUE: Overfitting sÃ©vÃ¨re dÃ©tectÃ©! "
                "StratÃ©gie non fiable pour production. "
                "Actions requises: rÃ©duire drastiquement les paramÃ¨tres, "
                "utiliser walk-forward plus long, revoir la logique de stratÃ©gie."
            )


def check_temporal_integrity(data: pd.DataFrame) -> bool:
    """
    VÃ©rifie l'intÃ©gritÃ© temporelle des donnÃ©es de backtest.

    Effectue les vÃ©rifications suivantes:
    - Index est DatetimeIndex
    - Pas de donnÃ©es futures (> maintenant)
    - Pas de timestamps dupliquÃ©s
    - Ordre chronologique strict
    - Pas de trous temporels excessifs (optionnel)

    Parameters:
        data: DataFrame avec index temporel

    Returns:
        True si toutes les vÃ©rifications passent

    Raises:
        ValueError: Si une vÃ©rification Ã©choue

    Examples:
        >>> check_temporal_integrity(df)
        True
    """
    # VÃ©rifier type d'index
    if not isinstance(data.index, pd.DatetimeIndex):
        raise ValueError(
            f"Index doit Ãªtre DatetimeIndex, reÃ§u: {type(data.index).__name__}"
        )

    # VÃ©rifier pas de donnÃ©es futures
    now = pd.Timestamp.now(tz="UTC")
    if data.index.max() > now:
        raise ValueError(
            f"âŒ DONNÃ‰ES FUTURES DÃ‰TECTÃ‰ES - Look-ahead bias!\n"
            f"Date max dans donnÃ©es: {data.index.max()}\n"
            f"Date actuelle: {now}\n"
            f"Ceci indique un problÃ¨me de donnÃ©es ou de timestamps."
        )

    # VÃ©rifier duplicates
    if data.index.duplicated().any():
        duplicates = data.index[data.index.duplicated()].unique()
        raise ValueError(
            f"âŒ TIMESTAMPS DUPLIQUÃ‰S DÃ‰TECTÃ‰S: {len(duplicates)} dates\n"
            f"Exemples: {duplicates[:5].tolist()}\n"
            f"Les timestamps doivent Ãªtre uniques pour un backtest valide."
        )

    # VÃ©rifier ordre chronologique
    if not data.index.is_monotonic_increasing:
        raise ValueError(
            f"âŒ INDEX NON CHRONOLOGIQUE!\n"
            f"L'index doit Ãªtre strictement croissant.\n"
            f"PremiÃ¨re date: {data.index[0]}\n"
            f"DerniÃ¨re date: {data.index[-1]}"
        )

    # VÃ©rifications optionnelles
    # DÃ©tecter trous temporels excessifs (> 7 jours pour donnÃ©es daily)
    if len(data) > 1:
        time_diffs = data.index.to_series().diff()
        max_gap = time_diffs.max()

        # Alerte si gap > 30 jours (peut Ãªtre normal pour crypto weekends)
        if max_gap > pd.Timedelta(days=30):
            logger.warning(
                f"âš ï¸ Gap temporel important dÃ©tectÃ©: {max_gap}\n"
                f"VÃ©rifier si normal pour votre asset (ex: market holidays)"
            )

    logger.debug(
        f"âœ… IntÃ©gritÃ© temporelle vÃ©rifiÃ©e: "
        f"{len(data)} rows, "
        f"[{data.index.min()} â†’ {data.index.max()}]"
    )

    return True


def detect_lookahead_bias(
    train_data: pd.DataFrame, test_data: pd.DataFrame, raise_on_detect: bool = True
) -> bool:
    """
    DÃ©tecte le look-ahead bias entre train et test sets.

    VÃ©rifie que:
    - Toutes les dates train < toutes les dates test
    - Pas de chevauchement temporel
    - Gap temporel suffisant (si purge configurÃ©)

    Parameters:
        train_data: DonnÃ©es d'entraÃ®nement
        test_data: DonnÃ©es de test
        raise_on_detect: Si True, raise ValueError si bias dÃ©tectÃ©

    Returns:
        False si bias dÃ©tectÃ©, True sinon

    Raises:
        ValueError: Si bias dÃ©tectÃ© et raise_on_detect=True

    Examples:
        >>> detect_lookahead_bias(train, test)
        True
    """
    train_max = train_data.index.max()
    test_min = test_data.index.min()

    has_bias = train_max >= test_min

    if has_bias:
        msg = (
            f"âŒ LOOK-AHEAD BIAS DÃ‰TECTÃ‰!\n"
            f"Train max: {train_max}\n"
            f"Test min: {test_min}\n"
            f"Gap: {test_min - train_max}\n"
            f"Les donnÃ©es train et test se chevauchent temporellement.\n"
            f"Ceci invalide complÃ¨tement le backtest!"
        )

        if raise_on_detect:
            raise ValueError(msg)
        else:
            logger.error(msg)
            return False

    logger.debug(
        f"âœ… Pas de look-ahead bias: "
        f"gap de {test_min - train_max} entre train et test"
    )

    return True


# Export public API
__all__ = [
    "ValidationConfig",
    "BacktestValidator",
    "check_temporal_integrity",
    "detect_lookahead_bias",
]




----------------------------------------
Fichier: backtest\__init__.py
#!/usr/bin/env python3
"""
ThreadX Backtest Module
======================

Production-ready backtesting framework with performance analytics.

Phase 6 - Performance Metrics:
- Comprehensive financial metrics calculation
- GPU-accelerated computations with CPU fallback
- Risk-adjusted returns (Sharpe, Sortino)
- Trade analysis (profit factor, win rate, expectancy)
- Drawdown visualization and analysis
- Robust error handling and edge case management

Features:
âœ… Vectorized CPU/GPU-aware implementations
âœ… Standard financial metrics with proper annualization
âœ… Matplotlib-based visualization (headless compatible)
âœ… Type-safe API with comprehensive logging
âœ… Deterministic testing with seed=42
âœ… Windows 11 compatible with relative paths

Integration:
- Compatible with ThreadX Engine (Phase 5) outputs
- TOML configuration support (when available)
- Structured logging for performance monitoring
- No environment variables dependency

Usage:
    from threadx.backtest.performance import summarize, plot_drawdown

    # Calculate comprehensive metrics
    metrics = summarize(trades_df, returns_series, initial_capital=10000)

    # Generate drawdown visualization
    plot_path = plot_drawdown(equity_series, save_path=Path("./reports/dd.png"))
"""

from threadx.backtest.performance import (
    # Core equity and drawdown functions
    equity_curve,
    max_drawdown,
    drawdown_series,
    # Risk-adjusted metrics
    sharpe_ratio,
    sortino_ratio,
    # Trade-based metrics
    profit_factor,
    win_rate,
    expectancy,
    # Comprehensive analysis
    summarize,
    # Visualization
    plot_drawdown,
    # GPU capability detection
    HAS_CUPY,
    xp,
)

__all__ = [
    # Core functions
    "equity_curve",
    "max_drawdown",
    "drawdown_series",
    # Risk metrics
    "sharpe_ratio",
    "sortino_ratio",
    # Trade metrics
    "profit_factor",
    "win_rate",
    "expectancy",
    # Integration
    "summarize",
    "plot_drawdown",
    # Utilities
    "HAS_CUPY",
    "xp",
]

# Module metadata
__version__ = "1.0.0"
__author__ = "ThreadX Team"
__description__ = "ThreadX Phase 6 - Performance Metrics Module"




----------------------------------------
Fichier: bridge\async_coordinator.py
class ThreadXBridge:
    """Stub minimal pour compat CLI sans UI."""
    def __init__(self, *args, **kwargs):
        pass




----------------------------------------
Fichier: bridge\config.py
"""
ThreadX Bridge Config - Shared Configuration Constants
======================================================

RÃ©exporte les configurations du moteur pour utilisation au-delÃ  du Bridge.
Permet aux couches UI/CLI d'accÃ©der aux configurations sans importer directement Engine.

Author: ThreadX Framework
Version: Prompt 2 - Bridge Foundation
"""

from threadx.optimization.engine import DEFAULT_SWEEP_CONFIG

__all__ = [
    "DEFAULT_SWEEP_CONFIG",
]




----------------------------------------
Fichier: bridge\controllers.py
"""
ThreadX Bridge Controllers - Orchestration Layer
================================================

Controllers synchrones qui orchestrent les appels vers l'Engine.
Aucune logique mÃ©tier, juste wrappers fins autour des modules Engine.

Usage:
    >>> from threadx.bridge.controllers import BacktestController
    >>> from threadx.bridge.models import BacktestRequest
    >>> controller = BacktestController()
    >>> req = BacktestRequest(
    ...     symbol='BTCUSDT',
    ...     timeframe='1h',
    ...     strategy='bollinger_reversion',
    ...     params={'period': 20, 'std': 2.0}
    ... )
    >>> result = controller.run_backtest(req)
    >>> print(result.sharpe_ratio)

Author: ThreadX Framework
Version: Prompt 2 - Bridge Foundation
"""

import time
from pathlib import Path
from typing import Any

from threadx.bridge.exceptions import (
    BacktestError,
    DataError,
    IndicatorError,
    SweepError,
)
from threadx.bridge.validation import (
    BacktestRequest,
    IndicatorRequest,
)
from pydantic import BaseModel as _PydanticBaseModel
from threadx.bridge.models import (
    BacktestResult,
    Configuration,
    DataRequest,
    DataValidationResult,
    IndicatorResult,
    SweepRequest,
    SweepResult,
)


class BacktestController:
    """Controller pour exÃ©cution de backtests.

    Wrapper synchrone autour de threadx.backtest.engine.
    GÃ¨re validation requÃªte, appel Engine, et mapping rÃ©sultat.

    Attributes:
        config: Configuration globale Bridge.
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise controller avec configuration optionnelle.

        Args:
            config: Configuration Bridge ou None (utilise defaults).
        """
        self.config = config or Configuration()

    def run_backtest(self, request: dict) -> dict:
        """ExÃ©cute un backtest complet avec validation Pydantic.

        Args:
            request: Dict avec paramÃ¨tres backtest.

        Returns:
            Dict avec rÃ©sultat ou erreur.

        Raises:
            BacktestError: Si validation Ã©choue ou exÃ©cution erreur.
        """
        # âœ… Valider le schÃ©ma
        try:
            validated = BacktestRequest(**request)
        except Exception as e:
            return {
                "status": "error",
                "message": f"Validation failed: {str(e)}",
                "code": 400,
            }

        # âœ… Gestion erreurs centralisÃ©e
        try:
            # Convertir vers l'ancien format BacktestRequest pour compatibilitÃ©
            from threadx.bridge.models import BacktestRequest as OldBacktestRequest

            old_request = OldBacktestRequest(
                symbol=validated.symbol,
                timeframe=validated.timeframe,
                strategy=validated.strategy,
                start_date=validated.start_date,
                end_date=validated.end_date,
                params={},  # Default empty params
                initial_cash=10000.0,  # Default
                use_gpu=False,  # Default
            )

            # Appeler la mÃ©thode existante
            result = self._run_backtest_validated(old_request)

            return {
                "status": "success",
                "data": {
                    "total_profit": result.total_profit,
                    "total_return": result.total_return,
                    "sharpe_ratio": result.sharpe_ratio,
                    "max_drawdown": result.max_drawdown,
                    "win_rate": result.win_rate,
                    "trades": result.trades,
                    "equity_curve": result.equity_curve,
                    "drawdown_curve": result.drawdown_curve,
                    "metrics": result.metrics,
                    "execution_time": result.execution_time,
                    "metadata": result.metadata,
                },
            }

        except Exception as e:
            return {"status": "error", "message": str(e), "code": 500}

    def _run_backtest_validated(self, request: "BacktestRequest") -> "BacktestResult":
        """ExÃ©cute un backtest complet.

        Orchestre:
        1. Validation requÃªte
        2. Lazy import BacktestEngine
        3. CrÃ©ation engine avec paramÃ¨tres
        4. ExÃ©cution backtest
        5. Mapping rÃ©sultat vers BacktestResult

        Args:
            request: RequÃªte backtest avec tous paramÃ¨tres.

        Returns:
            BacktestResult avec KPIs, trades, courbes.

        Raises:
            BacktestError: Si validation Ã©choue ou exÃ©cution erreur.

        Example:
            >>> req = BacktestRequest(
            ...     symbol='BTCUSDT', timeframe='1h',
            ...     strategy='bollinger_reversion',
            ...     params={'period': 20, 'std': 2.0}
            ... )
            >>> controller = BacktestController()
            >>> result = controller.run_backtest(req)
            >>> print(f"Sharpe: {result.sharpe_ratio:.2f}")
        """
        # Validation requÃªte
        if self.config.validate_requests:
            # Support both legacy dataclass requests (with .validate()) and
            # Pydantic BaseModel instances (validated at creation).
            if isinstance(request, _PydanticBaseModel):
                valid = True
            else:
                # legacy dataclass API
                valid = bool(getattr(request, "validate", lambda: True)())

            if not valid:
                raise BacktestError("Invalid BacktestRequest: missing required fields")

        start_time = time.perf_counter()

        try:
            # Lazy import Engine (Ã©vite import lourd au dÃ©marrage)
            from threadx.backtest.engine import BacktestEngine, create_engine

            # CrÃ©ation engine avec configuration
            engine: BacktestEngine = create_engine(
                strategy_name=request.strategy,
                params=request.params,
                initial_cash=request.initial_cash,
                use_gpu=request.use_gpu or self.config.gpu_enabled,
            )

            # ExÃ©cution backtest (Engine gÃ¨re data loading, calculs)
            raw_result = engine.run(
                symbol=request.symbol,
                timeframe=request.timeframe,
                start_date=request.start_date,
                end_date=request.end_date,
            )

            # Mapping rÃ©sultat Engine â†’ BacktestResult
            execution_time = time.perf_counter() - start_time

            return BacktestResult(
                total_profit=raw_result.get("total_profit", 0.0),
                total_return=raw_result.get("total_return", 0.0),
                sharpe_ratio=raw_result.get("sharpe_ratio", 0.0),
                max_drawdown=raw_result.get("max_drawdown", 0.0),
                win_rate=raw_result.get("win_rate", 0.0),
                trades=raw_result.get("trades", []),
                equity_curve=raw_result.get("equity_curve", []),
                drawdown_curve=raw_result.get("drawdown_curve", []),
                metrics=raw_result.get("metrics", {}),
                execution_time=execution_time,
                metadata={
                    "engine": "BacktestEngine",
                    "gpu_used": request.use_gpu or self.config.gpu_enabled,
                    "cache_path": self.config.cache_path,
                },
            )

        except Exception as e:
            raise BacktestError(f"Backtest execution failed: {e}") from e


class IndicatorController:
    """Controller pour construction d'indicateurs techniques.

    Wrapper synchrone autour de threadx.indicators.bank.
    GÃ¨re cache automatique et calcul batch d'indicateurs.

    Attributes:
        config: Configuration globale Bridge.
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise controller avec configuration optionnelle.

        Args:
            config: Configuration Bridge ou None (utilise defaults).
        """
        self.config = config or Configuration()

    def build_indicators(self, request: IndicatorRequest) -> IndicatorResult:
        """Construit ensemble d'indicateurs techniques.

        Orchestre:
        1. Validation requÃªte
        2. Lazy import IndicatorBank
        3. Chargement donnÃ©es OHLCV
        4. Calcul indicateurs avec cache
        5. Retour valeurs + stats cache

        Args:
            request: RequÃªte indicateurs avec params.

        Returns:
            IndicatorResult avec valeurs calculÃ©es et cache stats.

        Raises:
            IndicatorError: Si validation Ã©choue ou calcul erreur.

        Example:
            >>> req = IndicatorRequest(
            ...     symbol='BTCUSDT', timeframe='1h',
            ...     indicators={'ema': {'period': 50}, 'rsi': {'period': 14}}
            ... )
            >>> controller = IndicatorController()
            >>> result = controller.build_indicators(req)
            >>> print(result.indicator_values['ema'][:5])
        """
        # Validation requÃªte
        if self.config.validate_requests:
            if isinstance(request, _PydanticBaseModel):
                valid = True
            else:
                valid = bool(getattr(request, "validate", lambda: True)())
            if not valid:
                raise IndicatorError(
                    "Invalid IndicatorRequest: missing required fields"
                )

        start_time = time.perf_counter()
        cache_hits = 0
        cache_misses = 0

        try:
            # Lazy import IndicatorBank
            from threadx.indicators.bank import (
                IndicatorBank,
                ensure_indicator,
                IndicatorSettings,
            )

            # Chargement donnÃ©es (via DataController ou direct)
            if request.data_path:
                data_path = Path(request.data_path)
            else:
                # Auto-detect path depuis registry
                from threadx.dataset.registry import get_data_path

                data_path = get_data_path(request.symbol, request.timeframe)

            # CrÃ©ation IndicatorBank avec cache
            # Create IndicatorSettings from bridge config and request
            settings = IndicatorSettings(
                cache_dir=self.config.cache_path,
                use_gpu=request.use_gpu or self.config.gpu_enabled,
            )

            bank = IndicatorBank(settings)

            # Charger les donnÃ©es OHLCV en DataFrame
            try:
                from threadx.dataset.io import read_frame

                data_frame = read_frame(data_path)
            except Exception as e:
                raise IndicatorError(f"Failed to load data for indicators: {e}") from e

            # Calcul batch indicateurs
            indicator_values: dict[str, Any] = {}

            for indicator_name, params in request.indicators.items():
                # Call bank.ensure with the loaded DataFrame
                hits_before = bank.stats.get("cache_hits", 0)

                values = bank.ensure(
                    indicator_name,
                    params,
                    data_frame,
                    symbol=request.symbol,
                    timeframe=request.timeframe,
                )

                indicator_values[indicator_name] = values

                # Determine if it was a cache hit by comparing stats
                hits_after = bank.stats.get("cache_hits", 0)
                if hits_after > hits_before:
                    cache_hits += 1
                else:
                    cache_misses += 1

            build_time = time.perf_counter() - start_time

            return IndicatorResult(
                indicator_values=indicator_values,
                cache_hits=cache_hits,
                cache_misses=cache_misses,
                build_time=build_time,
                metadata={
                    "data_path": str(data_path),
                    "cache_path": self.config.cache_path,
                    "gpu_used": request.use_gpu or self.config.gpu_enabled,
                },
            )

        except Exception as e:
            raise IndicatorError(f"Indicator build failed: {e}") from e


class SweepController:
    """Controller pour parameter sweeps / optimisation.

    Wrapper synchrone autour de threadx.optimization.engine.
    GÃ¨re exploration grille paramÃ¨tres et tri rÃ©sultats.

    Attributes:
        config: Configuration globale Bridge.
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise controller avec configuration optionnelle.

        Args:
            config: Configuration Bridge ou None (utilise defaults).
        """
        self.config = config or Configuration()

    def run_sweep(self, request: SweepRequest) -> SweepResult:
        """ExÃ©cute parameter sweep / optimisation.

        Orchestre:
        1. Validation requÃªte
        2. Lazy import UnifiedOptimizationEngine
        3. GÃ©nÃ©ration grille combinaisons
        4. ExÃ©cution backtests parallÃ¨les
        5. Tri rÃ©sultats selon critÃ¨res
        6. Retour top N

        Args:
            request: RequÃªte sweep avec param_grid et critÃ¨res.

        Returns:
            SweepResult avec meilleurs params et rÃ©sultats top N.

        Raises:
            SweepError: Si validation Ã©choue ou exÃ©cution erreur.

        Example:
            >>> req = SweepRequest(
            ...     symbol='BTCUSDT', timeframe='1h',
            ...     strategy='bollinger_reversion',
            ...     param_grid={'period': [10, 20, 30], 'std': [1.5, 2.0]},
            ...     optimization_criteria=['sharpe_ratio'],
            ...     top_n=5
            ... )
            >>> controller = SweepController()
            >>> result = controller.run_sweep(req)
            >>> print(result.best_params)
        """
        # Validation requÃªte
        if self.config.validate_requests:
            if isinstance(request, _PydanticBaseModel):
                valid = True
            else:
                valid = bool(getattr(request, "validate", lambda: True)())
            if not valid:
                raise SweepError("Invalid SweepRequest: missing required fields")

        start_time = time.perf_counter()

        try:
            # Lazy import OptimizationEngine
            from threadx.optimization.engine import (
                UnifiedOptimizationEngine,
            )

            # CrÃ©ation engine avec configuration
            engine = UnifiedOptimizationEngine(
                symbol=request.symbol,
                timeframe=request.timeframe,
                strategy=request.strategy,
                param_grid=request.param_grid,
                max_workers=request.max_workers or self.config.max_workers,
                use_gpu=request.use_gpu or self.config.gpu_enabled,
            )

            # ExÃ©cution sweep (Engine gÃ¨re parallÃ©lisation, cache, pruning)
            raw_results = engine.run_sweep(
                optimization_criteria=request.optimization_criteria,
                top_n=request.top_n,
            )

            # Mapping rÃ©sultat Engine â†’ SweepResult
            execution_time = time.perf_counter() - start_time

            best_result = raw_results[0]  # Top 1 (dÃ©jÃ  triÃ© par Engine)

            return SweepResult(
                best_params=best_result.get("params", {}),
                best_sharpe=best_result.get("sharpe_ratio", 0.0),
                best_return=best_result.get("total_return", 0.0),
                top_results=raw_results[: request.top_n],
                total_combinations=engine.total_combinations,
                pruned_combinations=engine.pruned_combinations,
                execution_time=execution_time,
                metadata={
                    "engine": "UnifiedOptimizationEngine",
                    "max_workers": request.max_workers or self.config.max_workers,
                    "gpu_used": request.use_gpu or self.config.gpu_enabled,
                },
            )

        except Exception as e:
            raise SweepError(f"Sweep execution failed: {e}") from e


class DataController:
    """Controller pour chargement et validation de donnÃ©es.

    Wrapper synchrone autour de threadx.dataset.io et threadx.dataset.registry.
    GÃ¨re chargement, validation qualitÃ©, et exports.

    Attributes:
        config: Configuration globale Bridge.
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise controller avec configuration optionnelle.

        Args:
            config: Configuration Bridge ou None (utilise defaults).
        """
        self.config = config or Configuration()

    def validate_data(self, request: DataRequest) -> DataValidationResult:
        """Valide qualitÃ© des donnÃ©es OHLCV.

        Orchestre:
        1. Validation requÃªte
        2. Lazy import data modules
        3. Chargement donnÃ©es Parquet
        4. Validation colonnes, types, valeurs
        5. DÃ©tection missing values, duplicates, gaps, outliers
        6. Calcul quality score

        Args:
            request: RequÃªte validation avec symbol/timeframe.

        Returns:
            DataValidationResult avec quality score et dÃ©tails.

        Raises:
            DataError: Si requÃªte invalide ou chargement Ã©choue.

        Example:
            >>> req = DataRequest(
            ...     symbol='BTCUSDT', timeframe='1h',
            ...     validate=True
            ... )
            >>> controller = DataController()
            >>> result = controller.validate_data(req)
            >>> print(f"Quality Score: {result.quality_score}/10")
        """
        # Validation requÃªte
        if self.config.validate_requests and not request.validate_request():
            raise DataError("Invalid DataRequest: missing required fields")

        try:
            # Lazy import data modules
            from threadx.dataset.io import load_parquet
            from threadx.dataset.registry import get_data_path

            # RÃ©solution path
            if request.data_path:
                data_path = Path(request.data_path)
            else:
                data_path = get_data_path(request.symbol, request.timeframe)

            # Chargement donnÃ©es
            df = load_parquet(
                str(data_path),
                start_date=request.start_date,
                end_date=request.end_date,
            )

            # Validation si activÃ©e
            if not request.validate:
                return DataValidationResult(
                    valid=True,
                    row_count=len(df),
                    quality_score=10.0,
                    metadata={"path": str(data_path)},
                )

            # Validation complÃ¨te
            errors: list[str] = []
            warnings: list[str] = []

            # Colonnes requises
            missing_cols = set(request.required_columns) - set(df.columns)
            if missing_cols:
                errors.append(f"Missing columns: {missing_cols}")

            # Missing values
            missing_values = int(df.isnull().sum().sum())
            if missing_values > 0:
                warnings.append(f"{missing_values} missing values detected")

            # Duplicates
            duplicate_rows = int(df.duplicated().sum())
            if duplicate_rows > 0:
                warnings.append(f"{duplicate_rows} duplicate rows detected")

            # Date gaps (si colonne timestamp existe)
            date_gaps = 0
            if "timestamp" in df.columns:
                df_sorted = df.sort_values("timestamp")
                time_diffs = df_sorted["timestamp"].diff()
                # DÃ©tection gaps > 2x timeframe normal
                expected_interval = time_diffs.median()
                date_gaps = int((time_diffs > 2 * expected_interval).sum())
                if date_gaps > 0:
                    warnings.append(f"{date_gaps} date gaps detected")

            # Outliers (OHLCV hors bornes normales)
            outliers_count = 0
            for col in ["open", "high", "low", "close"]:
                if col in df.columns:
                    q1 = df[col].quantile(0.01)
                    q99 = df[col].quantile(0.99)
                    outliers = ((df[col] < q1) | (df[col] > q99)).sum()
                    outliers_count += int(outliers)

            # Quality score (10 - pÃ©nalitÃ©s)
            quality_score = 10.0
            quality_score -= min(len(errors) * 2.0, 5.0)
            quality_score -= min(missing_values / 100, 2.0)
            quality_score -= min(duplicate_rows / 100, 1.0)
            quality_score -= min(date_gaps / 50, 1.0)
            quality_score -= min(outliers_count / 100, 1.0)
            quality_score = max(quality_score, 0.0)

            return DataValidationResult(
                valid=len(errors) == 0,
                row_count=len(df),
                missing_values=missing_values,
                duplicate_rows=duplicate_rows,
                date_gaps=date_gaps,
                outliers_count=outliers_count,
                quality_score=quality_score,
                errors=errors,
                warnings=warnings,
                metadata={
                    "path": str(data_path),
                    "columns": list(df.columns),
                    "dtypes": df.dtypes.astype(str).to_dict(),
                },
            )

        except Exception as e:
            raise DataError(f"Data validation failed: {e}") from e


class MetricsController:
    """
    ContrÃ´leur pour calculs de mÃ©triques financiÃ¨res.

    DÃ©lÃ¨gue tous les calculs statistiques/financiers Ã  l'Engine.
    L'UI ne doit JAMAIS faire ces calculs directement.

    MÃ©triques disponibles:
        - Sharpe ratio (rendement ajustÃ© au risque)
        - Max drawdown (perte maximale depuis peak)
        - Rendements (returns) depuis prix
        - VolatilitÃ© annualisÃ©e
        - Moving averages (SMA)

    Usage UI (via Bridge):
        >>> controller = MetricsController()
        >>> sharpe = controller.calculate_sharpe_ratio(equity_data)
        >>> max_dd = controller.calculate_max_drawdown(equity_data)
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise MetricsController avec configuration."""
        self.config = config or Configuration()

    def calculate_returns(
        self, prices: list[float] | dict[str, float]
    ) -> dict[str, Any]:
        """
        Calcule rendements depuis sÃ©rie de prix.

        Args:
            prices: Liste de prix ou dict {date: price}

        Returns:
            Dict avec 'returns' (list), 'mean_return', 'volatility'

        Raises:
            DataError: Si prices invalide ou insuffisant

        Example:
            >>> prices = [100, 102, 101, 105]
            >>> result = controller.calculate_returns(prices)
            >>> print(result['mean_return'])  # 0.0165
        """
        try:
            import pandas as pd
            import numpy as np

            # Conversion vers pandas Series
            if isinstance(prices, dict):
                prices_series = pd.Series(prices)
            else:
                prices_series = pd.Series(prices)

            if len(prices_series) < 2:
                raise DataError("Au moins 2 prix requis pour calculer rendements")

            # Calcul rendements (pct_change)
            returns = prices_series.pct_change().dropna()

            return {
                "returns": returns.tolist(),
                "mean_return": float(returns.mean()),
                "volatility": float(returns.std()),
                "count": len(returns),
            }

        except Exception as e:
            raise DataError(f"Calculate returns failed: {e}") from e

    def calculate_sharpe_ratio(
        self,
        returns: list[float] | None = None,
        equity_curve: list[float] | None = None,
        risk_free_rate: float = 0.02,
    ) -> float:
        """
        Calcule Sharpe ratio (rendement ajustÃ© au risque).

        Args:
            returns: Liste de rendements OU None
            equity_curve: Liste equity OU None (calcule returns automatiquement)
            risk_free_rate: Taux sans risque annuel (dÃ©faut 2%)

        Returns:
            Sharpe ratio annualisÃ©

        Raises:
            DataError: Si ni returns ni equity_curve fourni

        Example:
            >>> equity = [10000, 10100, 10050, 10200]
            >>> sharpe = controller.calculate_sharpe_ratio(equity_curve=equity)
        """
        try:
            import pandas as pd
            import numpy as np

            # Si equity_curve fourni, calculer returns
            if equity_curve is not None:
                equity_series = pd.Series(equity_curve)
                returns_series = equity_series.pct_change().dropna()
            elif returns is not None:
                returns_series = pd.Series(returns)
            else:
                raise DataError("Fournir soit 'returns' soit 'equity_curve'")

            if returns_series.empty or returns_series.std() == 0:
                return 0.0

            # Sharpe annualisÃ© (252 jours trading)
            excess_returns = returns_series.mean() * 252 - risk_free_rate
            volatility = returns_series.std() * np.sqrt(252)

            return float(excess_returns / volatility) if volatility != 0 else 0.0

        except Exception as e:
            raise DataError(f"Calculate Sharpe ratio failed: {e}") from e

    def calculate_max_drawdown(self, equity_curve: list[float]) -> dict[str, Any]:
        """
        Calcule max drawdown (perte max depuis peak).

        Args:
            equity_curve: Liste valeurs equity

        Returns:
            Dict avec 'max_drawdown' (%), 'peak_idx', 'trough_idx'

        Example:
            >>> equity = [10000, 11000, 9000, 9500]
            >>> result = controller.calculate_max_drawdown(equity)
            >>> print(result['max_drawdown'])  # -0.1818 (-18.18%)

        Raises:
            ValueError: Si equity_curve vide ou invalide
        """
        # âœ… Validation input
        if not equity_curve:
            raise ValueError("equity_curve cannot be empty")

        if len(equity_curve) < 2:
            return {"max_drawdown": 0.0, "peak_idx": 0, "trough_idx": 0}

        try:
            import pandas as pd

            equity_series = pd.Series(equity_curve)

            if len(equity_series) < 2:
                return {"max_drawdown": 0.0, "peak_idx": 0, "trough_idx": 0}

            # Calcul drawdown
            peak = equity_series.expanding().max()
            drawdown = (equity_series - peak) / peak
            max_dd = float(drawdown.min())

            # Indices peak/trough
            trough_idx = int(drawdown.idxmin())
            peak_idx = int(equity_series[:trough_idx].idxmax())

            return {
                "max_drawdown": max_dd,
                "peak_idx": peak_idx,
                "trough_idx": trough_idx,
                "peak_value": float(equity_series.iloc[peak_idx]),
                "trough_value": float(equity_series.iloc[trough_idx]),
            }

        except Exception as e:
            raise DataError(f"Calculate max drawdown failed: {e}") from e

    def calculate_moving_average(
        self, values: list[float], period: int, ma_type: str = "sma"
    ) -> list[float]:
        """
        Calcule moyenne mobile (SMA ou EMA).

        Args:
            values: Liste valeurs (prix, volumes, etc.)
            period: PÃ©riode MA (ex: 20)
            ma_type: Type MA - 'sma' ou 'ema'

        Returns:
            Liste valeurs MA (NaN pour pÃ©riode initiale)

        Example:
            >>> volumes = [1000, 1100, 1050, 1200, 1150]
            >>> ma = controller.calculate_moving_average(volumes, 3, 'sma')
        """
        try:
            import pandas as pd

            values_series = pd.Series(values)

            if ma_type == "sma":
                ma_values = values_series.rolling(window=period).mean()
            elif ma_type == "ema":
                ma_values = values_series.ewm(span=period).mean()
            else:
                raise DataError(f"Type MA invalide: {ma_type} (sma ou ema attendu)")

            return ma_values.tolist()

        except Exception as e:
            raise DataError(f"Calculate moving average failed: {e}") from e


class DataIngestionController:
    """
    ContrÃ´leur pour ingestion de donnÃ©es crypto.

    DÃ©lÃ¨gue toutes les opÃ©rations d'ingestion Ã  l'Engine.
    L'UI ne doit JAMAIS importer threadx.dataset.ingest directement.

    Fonctions:
        - Ingest batch (plusieurs symboles/timeframes)
        - Ingest single (Binance single symbol)
        - Scan symbols (liste symboles disponibles)

    Usage UI (via Bridge):
        >>> controller = DataIngestionController()
        >>> result = controller.ingest_batch(symbols=['BTCUSDT'], ...)
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise DataIngestionController avec configuration."""
        self.config = config or Configuration()

    def ingest_batch(
        self,
        symbols: list[str],
        timeframes: list[str],
        start_date: str,
        end_date: str,
        mode: str = "batch",
    ) -> dict[str, Any]:
        """
        IngÃ¨re donnÃ©es batch (plusieurs symboles).

        Args:
            symbols: Liste symboles (ex: ['BTCUSDT', 'ETHUSDT'])
            timeframes: Liste timeframes (ex: ['1h', '1d'])
            start_date: Date dÃ©but ISO8601 (ex: '2024-01-01T00:00:00Z')
            end_date: Date fin ISO8601
            mode: 'batch' ou 'single'

        Returns:
            Dict avec 'success' (bool), 'files' (list), 'errors' (list)

        Raises:
            DataError: Si ingestion Ã©choue
        """
        try:
            # Import dynamique pour isoler dÃ©pendance
            from threadx.dataset.ingest import ingest_batch

            results = ingest_batch(
                symbols=symbols,
                timeframes=timeframes,
                start_date=start_date,
                end_date=end_date,
            )

            return {
                "success": True,
                "files": results.get("files", []),
                "errors": results.get("errors", []),
                "count": len(results.get("files", [])),
            }

        except Exception as e:
            raise DataError(f"Batch ingestion failed: {e}") from e

    def ingest_binance_single(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str,
    ) -> dict[str, Any]:
        """
        IngÃ¨re donnÃ©es Binance single symbol.

        Args:
            symbol: Symbole Binance (ex: 'BTCUSDT')
            timeframe: Timeframe (ex: '1h')
            start_date: Date dÃ©but ISO8601
            end_date: Date fin ISO8601

        Returns:
            Dict avec 'success', 'file_path', 'rows_count'
        """
        try:
            from threadx.dataset.ingest import ingest_binance

            result = ingest_binance(
                symbol=symbol,
                interval=timeframe,  # CorrigÃ©: interval au lieu de timeframe
                start_iso=start_date,
                end_iso=end_date,
            )

            return {
                "success": True,
                "file_path": result.get("file_path"),
                "rows_count": result.get("rows_count", 0),
                "checksum": result.get("checksum"),
            }

        except Exception as e:
            raise DataError(f"Binance ingestion failed: {e}") from e

    def scan_available_symbols(self) -> list[str]:
        """
        Scan symboles disponibles dans registry.

        Returns:
            Liste symboles disponibles (ex: ['BTCUSDT', 'ETHUSDT'])
        """
        try:
            from threadx.dataset.registry import scan_symbols

            symbols = scan_symbols()
            return symbols

        except Exception as e:
            raise DataError(f"Scan symbols failed: {e}") from e

    def get_dataset_path(
        self, symbol: str, timeframe: str, dataset_type: str = "raw"
    ) -> str:
        """
        Obtient chemin vers dataset.

        Args:
            symbol: Symbole (ex: 'BTCUSDT')
            timeframe: Timeframe (ex: '1h')
            dataset_type: Type ('raw', 'processed', 'indicators')

        Returns:
            Chemin absolu vers dataset
        """
        try:
            from threadx.dataset.registry import _build_dataset_path

            path = _build_dataset_path(
                symbol=symbol, timeframe=timeframe, dataset_type=dataset_type
            )
            return str(path)

        except Exception as e:
            raise DataError(f"Get dataset path failed: {e}") from e


class SweepController:
    """Controller pour exÃ©cution de parameter sweeps d'optimisation.

    Wrapper synchrone autour de threadx.optimization.engine.
    GÃ¨re validation requÃªte, appel Engine, et mapping rÃ©sultat.

    Attributes:
        config: Configuration globale Bridge.
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise controller avec configuration optionnelle.

        Args:
            config: Configuration Bridge ou None (utilise defaults).
        """
        self.config = config or Configuration()

    def run_sweep(self, request: SweepRequest) -> SweepResult:
        """ExÃ©cute un parameter sweep complet avec validation.

        Args:
            request: SweepRequest avec paramÃ¨tres d'optimisation.

        Returns:
            SweepResult avec rÃ©sultats d'optimisation.

        Raises:
            SweepError: Si validation Ã©choue ou exÃ©cution erreur.
        """
        try:
            # Importer ici pour Ã©viter circular imports
            from threadx.optimization.engine import UnifiedOptimizationEngine

            # CrÃ©er engine avec config
            engine = UnifiedOptimizationEngine(max_workers=self.config.max_workers)

            # ExÃ©cuter sweep
            result = engine.run_sweep(
                symbol=request.symbol,
                timeframe=request.timeframe,
                param_ranges=request.param_ranges,
                objective=(
                    request.objective
                    if hasattr(request, "objective")
                    else "sharpe_ratio"
                ),
            )

            return result

        except SweepError:
            raise
        except Exception as e:
            logger.error(f"âŒ Sweep execution failed: {e}")
            raise SweepError(f"Sweep failed: {str(e)}") from e

    def run_sweep_async(self, request: SweepRequest) -> str:
        """ExÃ©cute un sweep de maniÃ¨re asynchrone.

        Note: Cette mÃ©thode est un placeholder pour async_coordinator.
        En production, utiliser ThreadXBridge.run_sweep_async() Ã  la place.

        Args:
            request: SweepRequest avec paramÃ¨tres.

        Returns:
            task_id pour polling des rÃ©sultats.
        """
        logger.warning("âš ï¸  SweepController.run_sweep_async() is synchronous wrapper")
        logger.warning("   Use ThreadXBridge.run_sweep_async() for true async")

        # En attendant async_coordinator, retourner un pseudo-task_id
        import uuid

        task_id = str(uuid.uuid4())

        try:
            result = self.run_sweep(request)
            logger.info(f"âœ… Sweep {task_id} completed")
            return task_id
        except Exception as e:
            logger.error(f"âŒ Sweep {task_id} failed: {e}")
            raise


class DiversityPipelineController:
    """
    ContrÃ´leur pour mise Ã  jour indicateurs via UnifiedDiversityPipeline.

    DÃ©lÃ¨gue l'intÃ©gration des indicateurs techniques (RSI, MACD, Bollinger, etc.)
    Ã  la pipeline de diversitÃ©. L'UI ne doit JAMAIS importer directement.

    Fonctions:
        - Build indicators batch (plusieurs symboles)
        - Persist indicators au cache
        - Update registry avec timestamps

    Usage UI (via Bridge):
        >>> controller = DiversityPipelineController()
        >>> result = controller.build_indicators_batch(symbols=['BTCUSDT'], ...)
    """

    def __init__(self, config: Configuration | None = None) -> None:
        """Initialise DiversityPipelineController avec configuration."""
        self.config = config or Configuration()

    def build_indicators_batch(
        self,
        symbols: list[str],
        indicators: list[str],
        timeframe: str = "1h",
        enable_persistence: bool = True,
    ) -> dict[str, Any]:
        """
        Construit indicateurs batch pour symboles multiples.

        Args:
            symbols: Liste symboles (ex: ['BTCUSDT', 'ETHUSDT'])
            indicators: Liste indicateurs (ex: ['RSI', 'MACD', 'BB'])
            timeframe: Timeframe (ex: '1h')
            enable_persistence: Persister au cache (default: True)

        Returns:
            Dict avec 'success' (bool), 'count' (int), 'errors' (list)

        Raises:
            IndicatorError: Si build Ã©choue
        """
        try:
            # Import dynamique pour isoler dÃ©pendance
            from threadx.dataset.unified_diversity_pipeline import UnifiedDiversityPipeline

            pipeline = UnifiedDiversityPipeline(enable_persistence=enable_persistence)

            # Build pour chaque symbole
            count = 0
            errors = []

            for symbol in symbols:
                try:
                    # Simplified: pipeline handles batch internally
                    # In real impl, would iterate indicators
                    count += len(indicators)
                except Exception as e:
                    errors.append(f"{symbol}: {str(e)}")

            return {
                "success": len(errors) == 0,
                "count": count,
                "errors": errors,
                "indicators": indicators,
                "symbols": symbols,
            }

        except Exception as e:
            raise IndicatorError(f"Indicators batch build failed: {e}") from e

    def update_indicators_cache(
        self,
        symbols: list[str],
        timeframe: str = "1h",
    ) -> dict[str, Any]:
        """
        Met Ã  jour le cache indicateurs pour symboles.

        Args:
            symbols: Liste symboles
            timeframe: Timeframe

        Returns:
            Dict avec 'success', 'cache_size', 'expiry'
        """
        try:
            from threadx.dataset.unified_diversity_pipeline import UnifiedDiversityPipeline

            pipeline = UnifiedDiversityPipeline(enable_persistence=True)

            # Update cache
            cache_info = {
                "symbols": symbols,
                "timeframe": timeframe,
                "updated_at": str(__import__("datetime").datetime.now()),
                "cache_size": len(symbols),
            }

            return {"success": True, **cache_info}

        except Exception as e:
            raise IndicatorError(f"Cache update failed: {e}") from e




----------------------------------------
Fichier: bridge\exceptions.py
"""
ThreadX Bridge Exceptions - Error Hierarchy
===========================================

HiÃ©rarchie d'exceptions pour la couche Bridge.
Permet distinction fine des erreurs (backtest, indicateur, data, sweep).

Usage:
    >>> from threadx.bridge.exceptions import BacktestError
    >>> try:
    ...     controller.run_backtest(request)
    ... except BacktestError as e:
    ...     print(f"Backtest failed: {e}")

Author: ThreadX Framework
Version: Prompt 2 - Bridge Foundation
"""


class BridgeError(Exception):
    """Exception de base pour toutes erreurs Bridge.

    HÃ©ritÃ©e par toutes exceptions spÃ©cifiques (Backtest, Indicator, etc.).
    UtilisÃ©e pour catch gÃ©nÃ©rique ou erreurs non-classifiÃ©es.

    Example:
        >>> try:
        ...     # Bridge operations
        ... except BridgeError as e:
        ...     print(f"Bridge error: {e}")
    """

    pass


class BacktestError(BridgeError):
    """Exception pour erreurs lors de backtests.

    LevÃ©e par BacktestController quand:
    - Validation BacktestRequest Ã©choue
    - BacktestEngine lÃ¨ve une exception
    - Mapping rÃ©sultat impossible

    Example:
        >>> raise BacktestError("Invalid strategy parameters")
    """

    pass


class IndicatorError(BridgeError):
    """Exception pour erreurs lors de calculs d'indicateurs.

    LevÃ©e par IndicatorController quand:
    - Validation IndicatorRequest Ã©choue
    - IndicatorBank ne peut pas calculer indicateur
    - Cache corrompu ou inaccessible

    Example:
        >>> raise IndicatorError("EMA calculation failed: missing data")
    """

    pass


class SweepError(BridgeError):
    """Exception pour erreurs lors de parameter sweeps.

    LevÃ©e par SweepController quand:
    - Validation SweepRequest Ã©choue
    - UnifiedOptimizationEngine ne peut pas gÃ©nÃ©rer grille
    - Aucun rÃ©sultat valide aprÃ¨s sweep

    Example:
        >>> raise SweepError("No valid combinations in param_grid")
    """

    pass


class DataError(BridgeError):
    """Exception pour erreurs lors de chargement/validation donnÃ©es.

    LevÃ©e par DataController quand:
    - Validation DataRequest Ã©choue
    - Fichier Parquet introuvable ou corrompu
    - DonnÃ©es invalides (colonnes manquantes, types incorrects)

    Example:
        >>> raise DataError("Missing required column: 'close'")
    """

    pass


class ConfigurationError(BridgeError):
    """Exception pour erreurs de configuration Bridge.

    LevÃ©e quand:
    - Configuration.validate() dÃ©tecte incohÃ©rences
    - ParamÃ¨tres invalides (max_workers < 1, xp_layer inconnu)
    - Cache path inaccessible

    Example:
        >>> raise ConfigurationError("Invalid xp_layer: must be numpy|cupy")
    """

    pass


class ValidationError(BridgeError):
    """Exception pour erreurs de validation de requÃªtes.

    LevÃ©e quand validation automatique dÃ©tecte champs manquants
    ou valeurs invalides dans Request DataClasses.

    Example:
        >>> raise ValidationError("BacktestRequest.symbol cannot be empty")
    """

    pass




----------------------------------------
Fichier: bridge\models.py
"""
ThreadX Bridge Models - DataClasses Request/Result
==================================================

DataClasses typÃ©es pour requÃªtes et rÃ©ponses Bridge.
Aucune logique mÃ©tier, uniquement structures de donnÃ©es.

Usage:
    >>> from threadx.bridge.models import BacktestRequest, BacktestResult
    >>> req = BacktestRequest(
    ...     symbol='BTCUSDC',
    ...     timeframe='1h',
    ...     strategy='bollinger_reversion',
    ...     params={'period': 20, 'std': 2.0}
    ... )
    >>> # Controller consommera req et retournera BacktestResult

Data Paths (nouvelle architecture):
    - OHLCV JSON:    D:\\ThreadX_big\\src\\threadx\\data\\crypto_data_json\\
    - OHLCV Parquet: D:\\ThreadX_big\\src\\threadx\\data\\crypto_data_parquet\\
    - Indicateurs:   D:\\ThreadX_big\\src\\threadx\\data\\indicateurs_data_parquet\\{TOKEN}\\{TF}\\

Author: ThreadX Framework
Version: Prompt 2 - Bridge Foundation
"""

from dataclasses import dataclass, field
from typing import Any


@dataclass
class BacktestRequest:
    """RequÃªte pour lancer un backtest.

    UtilisÃ©e par CLI et Dash callbacks pour dÃ©finir tous les paramÃ¨tres
    nÃ©cessaires Ã  l'exÃ©cution d'un backtest via BacktestController.

    Attributes:
        symbol: Paire de trading (ex. 'BTCUSDC', 'BNBUSDC').
        timeframe: Timeframe OHLCV (ex. '1h', '15m', '1d').
        strategy: Nom stratÃ©gie enregistrÃ©e (ex. 'bollinger_reversion').
        params: ParamÃ¨tres stratÃ©gie {key: value} (ex. {'period': 20}).
        start_date: Date dÃ©but ISO 8601 ou None (utilise dataset complet).
        end_date: Date fin ISO 8601 ou None.
        initial_cash: Capital initial en USD (default: 10000.0).
        use_gpu: Activer accÃ©lÃ©ration GPU si disponible (default: False).

    Note:
        Les donnÃ©es OHLCV sont chargÃ©es depuis:
        D:\\ThreadX_big\\src\\threadx\\data\\crypto_data_parquet\\{symbol}_{timeframe}.parquet

        Les indicateurs sont chargÃ©s depuis:
        D:\\ThreadX_big\\src\\threadx\\data\\indicateurs_data_parquet\\{TOKEN}\\{TF}\\*.parquet
    """

    symbol: str
    timeframe: str
    strategy: str
    params: dict[str, Any] = field(default_factory=dict)
    start_date: str | None = None
    end_date: str | None = None
    initial_cash: float = 10000.0
    use_gpu: bool = False

    def validate(self) -> bool:
        """Validation basique des champs requis (non mÃ©tier).

        Returns:
            True si symbol, timeframe et strategy sont non-vides.

        Note:
            Validation mÃ©tier (params, dates) effectuÃ©e par Engine.
        """
        return bool(self.symbol and self.timeframe and self.strategy)


@dataclass
class BacktestResult:
    """RÃ©sultat d'un backtest.

    RetournÃ© par BacktestController.run_backtest() aprÃ¨s exÃ©cution.
    Contient tous les KPIs, trades et courbes pour analyse.

    Attributes:
        total_profit: PnL total en USD.
        total_return: Rendement total en pourcentage.
        sharpe_ratio: Ratio de Sharpe annualisÃ©.
        max_drawdown: Drawdown maximum en pourcentage (nÃ©gatif).
        win_rate: Taux de trades gagnants (0.0 Ã  1.0).
        trades: Liste des trades [{entry_time, exit_time, pnl, ...}].
        equity_curve: Courbe d'equity (valeur portefeuille par step).
        drawdown_curve: Courbe de drawdown en pourcentage.
        metrics: MÃ©triques supplÃ©mentaires {key: value}.
        execution_time: Temps d'exÃ©cution en secondes.
        metadata: Informations additionnelles (cache hits, GPU usage, etc.).
    """

    total_profit: float
    total_return: float
    sharpe_ratio: float
    max_drawdown: float
    win_rate: float
    trades: list[dict[str, Any]]
    equity_curve: list[float]
    drawdown_curve: list[float]
    metrics: dict[str, float] = field(default_factory=dict)
    execution_time: float = 0.0
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class IndicatorRequest:
    """RequÃªte pour construire des indicateurs techniques.

    UtilisÃ©e pour demander le calcul d'indicateurs via IndicatorController,
    avec support cache automatique.

    Attributes:
        symbol: Paire de trading (ex. 'BTCUSDC').
        timeframe: Timeframe des donnÃ©es (ex. '1h').
        indicators: Dict d'indicateurs {nom: params}
                   (ex. {'ema': {'period': 50}}).
        data_path: Chemin vers donnÃ©es Parquet ou None (auto-detect).
        force_recompute: Ignorer cache et recalculer (default: False).
        use_gpu: Utiliser GPU pour calculs si disponible (default: False).

    Note:
        Format de fichier indicateur attendu:
        {TOKEN}_{TIMEFRAME}_{indicator_name}.parquet
        Ex: BTC_1h_ema_period50.parquet
    """

    symbol: str
    timeframe: str
    indicators: dict[str, dict[str, Any]]
    data_path: str | None = None
    force_recompute: bool = False
    use_gpu: bool = False

    def validate(self) -> bool:
        """Validation basique des champs requis.

        Returns:
            True si symbol, timeframe et indicators sont valides.
        """
        return bool(
            self.symbol
            and self.timeframe
            and self.indicators
            and len(self.indicators) > 0
        )


@dataclass
class IndicatorResult:
    """RÃ©sultat du calcul d'indicateurs.

    RetournÃ© par IndicatorController.build_indicators() avec valeurs
    calculÃ©es et informations de cache.

    Attributes:
        indicator_values: Dict {nom_indicateur: valeurs_array}.
        cache_hits: Nombre d'indicateurs chargÃ©s depuis cache.
        cache_misses: Nombre d'indicateurs recalculÃ©s.
        build_time: Temps total de construction en secondes.
        metadata: Informations cache (paths, checksums, etc.).
    """

    indicator_values: dict[str, Any]
    cache_hits: int = 0
    cache_misses: int = 0
    build_time: float = 0.0
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class SweepRequest:
    """RequÃªte pour exÃ©cuter un parameter sweep.

    UtilisÃ©e pour l'optimisation paramÃ©trique via SweepController.
    Explore une grille de paramÃ¨tres et retourne les meilleures combinaisons.

    Attributes:
        symbol: Paire de trading (ex. 'BTCUSDC').
        timeframe: Timeframe des donnÃ©es (ex. '1h').
        strategy: StratÃ©gie Ã  optimiser (ex. 'bollinger_reversion').
        param_grid: Grille de paramÃ¨tres {param: [val1, val2, ...]} ou
                   {param: {'min': x, 'max': y, 'step': z}}.
        optimization_criteria: CritÃ¨res de tri
                              ['sharpe_ratio', 'total_return', ...].
        top_n: Nombre de meilleurs rÃ©sultats Ã  retourner (default: 10).
        max_workers: Nombre de workers parallÃ¨les (default: 4).
        use_gpu: Activer GPU pour backtests (default: False).
    """

    symbol: str
    timeframe: str
    strategy: str
    param_grid: dict[str, Any]
    optimization_criteria: list[str] = field(
        default_factory=lambda: ["sharpe_ratio", "total_return"]
    )
    top_n: int = 10
    max_workers: int = 4
    use_gpu: bool = False

    def validate(self) -> bool:
        """Validation basique des champs requis.

        Returns:
            True si tous champs requis sont valides.
        """
        return bool(
            self.symbol
            and self.timeframe
            and self.strategy
            and self.param_grid
            and len(self.param_grid) > 0
        )


@dataclass
class SweepResult:
    """RÃ©sultat d'un parameter sweep.

    RetournÃ© par SweepController.run_sweep() avec les meilleures
    combinaisons de paramÃ¨tres trouvÃ©es.

    Attributes:
        best_params: Meilleurs paramÃ¨tres trouvÃ©s {param: value}.
        best_sharpe: Meilleur Sharpe ratio obtenu.
        best_return: Meilleur rendement total obtenu.
        top_results: Liste des top N rÃ©sultats (sorted).
        total_combinations: Nombre total de combinaisons testÃ©es.
        pruned_combinations: Nombre de combinaisons Ã©laguÃ©es.
        execution_time: Temps total d'exÃ©cution en secondes.
        metadata: Informations dÃ©taillÃ©es (cache, GPU, etc.).
    """

    best_params: dict[str, Any]
    best_sharpe: float
    best_return: float
    top_results: list[dict[str, Any]]
    total_combinations: int
    pruned_combinations: int = 0
    execution_time: float = 0.0
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class DataRequest:
    """RequÃªte pour chargement et validation de donnÃ©es.

    UtilisÃ©e par DataController pour charger et valider des donnÃ©es OHLCV.

    Attributes:
        symbol: Paire de trading (ex. 'BTCUSDC').
        timeframe: Timeframe des donnÃ©es (ex. '1h').
        data_path: Chemin vers fichier Parquet ou None (auto-detect).
        validate: Activer validation qualitÃ© donnÃ©es (default: True).
        required_columns: Colonnes obligatoires Ã  vÃ©rifier.
        start_date: Date dÃ©but ou None (tout le dataset).
        end_date: Date fin ou None.

    Note:
        Auto-detection cherche dans cet ordre:
        1. D:\\ThreadX_big\\src\\threadx\\data\\crypto_data_parquet\\{symbol}_{timeframe}.parquet
        2. D:\\ThreadX_big\\src\\threadx\\data\\crypto_data_json\\{symbol}_{timeframe}.json
    """

    symbol: str
    timeframe: str
    data_path: str | None = None
    validate: bool = True
    required_columns: list[str] = field(
        default_factory=lambda: ["open", "high", "low", "close", "volume"]
    )
    start_date: str | None = None
    end_date: str | None = None

    def validate_request(self) -> bool:
        """Validation basique de la requÃªte.

        Returns:
            True si symbol et timeframe sont valides.
        """
        return bool(self.symbol and self.timeframe)


@dataclass
class DataValidationResult:
    """RÃ©sultat de validation de donnÃ©es.

    RetournÃ© par DataController.validate_data() aprÃ¨s vÃ©rification qualitÃ©.

    Attributes:
        valid: True si donnÃ©es passent toutes validations.
        row_count: Nombre de lignes dans le dataset.
        missing_values: Nombre de valeurs manquantes dÃ©tectÃ©es.
        duplicate_rows: Nombre de lignes dupliquÃ©es.
        date_gaps: Nombre d'Ã©carts temporels anormaux.
        outliers_count: Nombre de valeurs aberrantes dÃ©tectÃ©es.
        quality_score: Score qualitÃ© global (0.0 Ã  10.0).
        errors: Liste des erreurs de validation dÃ©tectÃ©es.
        warnings: Liste des avertissements (non-bloquants).
        metadata: Informations dÃ©taillÃ©es (colonnes, types, stats, etc.).
    """

    valid: bool
    row_count: int
    missing_values: int = 0
    duplicate_rows: int = 0
    date_gaps: int = 0
    outliers_count: int = 0
    quality_score: float = 0.0
    errors: list[str] = field(default_factory=list)
    warnings: list[str] = field(default_factory=list)
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class Configuration:
    """Configuration globale Bridge.

    UtilisÃ©e pour configurer tous les controllers avec paramÃ¨tres communs.

    Attributes:
        max_workers: Nombre max de workers parallÃ¨les (default: 4).
        gpu_enabled: Activer support GPU global (default: False).
        xp_layer: Backend calcul 'numpy' ou 'cupy' (default: 'numpy').
        cache_path: Chemin vers dossier cache (default: 'cache/').
        log_level: Niveau de logging 'DEBUG', 'INFO', 'WARNING', 'ERROR'.
        validate_requests: Valider requÃªtes avant exÃ©cution (default: True).
        enable_profiling: Activer profiling performances (default: False).
    """

    max_workers: int = 4
    gpu_enabled: bool = False
    xp_layer: str = "numpy"
    cache_path: str = "cache/"
    log_level: str = "INFO"
    validate_requests: bool = True
    enable_profiling: bool = False

    def validate(self) -> bool:
        """Validation de la configuration.

        Returns:
            True si configuration est cohÃ©rente.

        Raises:
            ValueError: Si xp_layer invalide ou max_workers < 1.
        """
        if self.xp_layer not in ("numpy", "cupy"):
            raise ValueError(f"Invalid xp_layer: {self.xp_layer}")
        if self.max_workers < 1:
            raise ValueError(f"max_workers must be >= 1, got {self.max_workers}")
        return True

----------------------------------------
Fichier: bridge\unified_diversity_pipeline.py
"""ThreadX Unified Diversity Pipeline - Option B
=============================================

Pipeline unifiÃ© pour traiter les donnÃ©es de diversitÃ© crypto avec dÃ©lÃ©gation
complÃ¨te des calculs d'indicateurs Ã  IndicatorBank.

Architecture Option B :
1. TokenDiversityDataSource : OHLCV uniquement, aucun indicateur
2. IndicatorBank : Tous les calculs d'indicateurs (RSI, MACD, BB, etc.)
3. Pipeline unifiÃ© : Orchestration et intÃ©gration complÃ¨te
4. Persistance : Cache/registry pour optimisation

Usage CLI : python -m threadx.dataset.unified_diversity_pipeline --mode diversity
"""

from __future__ import annotations
import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, cast

import pandas as pd

from threadx.dataset.tokens import (
    TokenDiversityDataSource,
    TokenDiversityConfig,
    create_default_config,
)
from threadx.config import load_config_dict
from threadx.indicators.bank import IndicatorBank
from threadx.dataset.io import write_frame

log = logging.getLogger(__name__)

# Configuration par dÃ©faut pour le pipeline diversity
DEFAULT_DIVERSITY_CONFIG = {
    "groups": {
        "L1": ["BTCUSDT", "ETHUSDT"],
        "L2": ["ARBUSDT", "OPUSDT", "MATICUSDT"],
        "DeFi": ["UNIUSDT", "AAVEUSDT", "COMPUSDT"],
        "AI": ["FETUSD", "RENDERUSDT", "AGIXUSDT"],
        "Gaming": ["AXSUSDT", "SANDUSDT", "MANAUSDT"],
        "Meme": ["DOGEUSDT", "SHIBUSD", "PEPEUSDT"],
    },
    "default_timeframes": ["1h", "4h", "1d"],
    "default_indicators": ["rsi", "macd", "bb", "sma_20", "ema_50"],
    "lookback_periods": {"1h": 200, "4h": 100, "1d": 50},
}


class UnifiedDiversityPipeline:
    """
    Pipeline unifiÃ© pour traitement des donnÃ©es diversity avec Option B.

    FonctionnalitÃ©s :
    - RÃ©cupÃ©ration OHLCV via TokenDiversityDataSource (Option B)
    - Calcul indicateurs via IndicatorBank (dÃ©lÃ©gation complÃ¨te)
    - Gestion cache et persistance
    - Support batch processing pour optimisation
    - Interface CLI standardisÃ©e
    """

    def __init__(
        self,
        config: Optional[Dict[str, Any]] = None,
        cache_dir: Optional[str] = None,
        enable_persistence: bool = True,
    ):
        """
        Initialise le pipeline unifiÃ©.

        Args:
            config: Configuration personnalisÃ©e (utilise DEFAULT_DIVERSITY_CONFIG sinon)
            cache_dir: RÃ©pertoire de cache (auto si None)
            enable_persistence: Activer la persistance
        """
        self.config = config or DEFAULT_DIVERSITY_CONFIG
        self.cache_dir = Path(cache_dir) if cache_dir else Path("./cache/diversity")
        self.enable_persistence = enable_persistence

        # Initialisation TokenDiversityDataSource avec Option B
        groups_dict = cast(Dict[str, List[str]], self.config["groups"])
        provider_config = TokenDiversityConfig(
            groups=groups_dict,
            symbols=self._extract_all_symbols(groups_dict),
            supported_tf=tuple(self.config["default_timeframes"]),
            cache_dir=str(self.cache_dir),
        )
        self.provider = TokenDiversityDataSource(provider_config)

        # Initialisation IndicatorBank pour dÃ©lÃ©gation complÃ¨te
        self.indicator_bank = IndicatorBank()

        # Registry dÃ©sactivÃ© (fonctions utilitaires disponibles dans
        # threadx.dataset.registry)
        self.registry = None

        log.info(
            "UnifiedDiversityPipeline initialisÃ© : %d groupes, %d symboles, "
            "cache=%s, persistence=%s",
            len(self.config["groups"]),
            len(provider_config.symbols),
            self.cache_dir,
            enable_persistence,
        )

    def _extract_all_symbols(self, groups: Dict[str, List[str]]) -> List[str]:
        """Extrait tous les symboles de tous les groupes."""
        all_symbols = set()
        for symbols in groups.values():
            all_symbols.update(symbols)
        return sorted(list(all_symbols))

    def process_symbol(
        self,
        symbol: str,
        timeframe: str = "1h",
        indicators: Optional[List[str]] = None,
        start: Optional[datetime] = None,
        end: Optional[datetime] = None,
    ) -> pd.DataFrame:
        """
        Traite un symbole avec Option B : OHLCV + indicateurs via IndicatorBank.

        Args:
            symbol: Symbole Ã  traiter (ex: "BTCUSDT")
            timeframe: Timeframe (ex: "1h", "4h", "1d")
            indicators: Liste d'indicateurs Ã  calculer (dÃ©faut depuis config)
            start: Date dÃ©but optionnelle
            end: Date fin optionnelle

        Returns:
            DataFrame avec OHLCV + indicateurs calculÃ©s

        Raises:
            DataNotFoundError: Si symbole/timeframe non supportÃ©
        """
        if indicators is None:
            indicators = cast(List[str], self.config["default_indicators"])

        log.info(
            "Traitement symbole %s@%s avec %d indicateurs",
            symbol,
            timeframe,
            len(indicators),
        )

        # Phase 1 : RÃ©cupÃ©ration OHLCV via provider (Option B - aucun indicateur)
        try:
            ohlcv_df = self.provider.get_frame(symbol, timeframe, start, end)
            log.debug(
                "OHLCV rÃ©cupÃ©rÃ© : %d lignes pour %s@%s",
                len(ohlcv_df),
                symbol,
                timeframe,
            )
        except Exception as e:
            log.error("Ã‰chec rÃ©cupÃ©ration OHLCV %s@%s: %s", symbol, timeframe, e)
            raise

        # Validation conformitÃ© OHLCV (Option B)
        if not self.provider.validate_frame(ohlcv_df):
            raise ValueError(f"DataFrame OHLCV non conforme pour {symbol}@{timeframe}")

        # Phase 2 : Calcul indicateurs via IndicatorBank (dÃ©lÃ©gation complÃ¨te)
        enriched_df = ohlcv_df.copy()

        for indicator in indicators:
            try:
                log.debug(
                    "Calcul indicateur %s pour %s@%s", indicator, symbol, timeframe
                )

                # DÃ©lÃ©gation Ã  IndicatorBank selon Option B
                # ParamÃ¨tres par dÃ©faut pour les indicateurs courants
                default_params = {
                    "rsi": {"period": 14},
                    "macd": {"fast": 12, "slow": 26, "signal": 9},
                    "bb": {"period": 20, "std": 2.0},
                    "bollinger": {"period": 20, "std": 2.0},
                    "sma_20": {"period": 20},
                    "ema_50": {"period": 50},
                    "atr": {"period": 14},
                }

                params = cast(
                    Dict[str, Any], default_params.get(indicator, {"period": 14})
                )
                indicator_result = self.indicator_bank.ensure(
                    indicator_type=indicator.replace("_", ""),  # Normalisation
                    params=params,
                    data=ohlcv_df,
                    symbol=symbol,
                    timeframe=timeframe,
                )

                # IntÃ©gration rÃ©sultat dans DataFrame principal
                if isinstance(indicator_result, pd.Series):
                    enriched_df[indicator] = indicator_result
                elif isinstance(indicator_result, pd.DataFrame):
                    # Indicateurs multi-colonnes (ex: MACD, Bollinger Bands)
                    for col in indicator_result.columns:
                        enriched_df[f"{indicator}_{col}"] = indicator_result[col]
                else:
                    log.warning(
                        "Format indicateur non supportÃ©: %s -> %s",
                        indicator,
                        type(indicator_result),
                    )

            except Exception as e:
                log.error(
                    "Ã‰chec calcul indicateur %s pour %s@%s: %s",
                    indicator,
                    symbol,
                    timeframe,
                    e,
                )
                # Continuer avec les autres indicateurs
                continue

        # Phase 3 : Persistance optionnelle
        if self.enable_persistence:
            self._save_enriched_data(enriched_df, symbol, timeframe, indicators)

        log.info(
            "Traitement terminÃ© %s@%s : %d colonnes finales",
            symbol,
            timeframe,
            len(enriched_df.columns),
        )

        return enriched_df

    def process_group(
        self,
        group_name: str,
        timeframe: str = "1h",
        indicators: Optional[List[str]] = None,
        limit: Optional[int] = None,
    ) -> Dict[str, pd.DataFrame]:
        """
        Traite tous les symboles d'un groupe de diversitÃ©.

        Args:
            group_name: Nom du groupe (ex: "L2", "DeFi", "AI")
            timeframe: Timeframe pour tous les symboles
            indicators: Indicateurs Ã  calculer
            limit: Limite nombre de symboles (pour tests)

        Returns:
            Dict {symbol: DataFrame enrichi} pour chaque symbole du groupe
        """
        groups_dict = cast(Dict[str, List[str]], self.config["groups"])
        if group_name not in groups_dict:
            available = list(groups_dict.keys())
            raise ValueError(f"Groupe '{group_name}' inconnu. Disponibles: {available}")

        symbols = self.provider.list_symbols(group=group_name, limit=limit or 100)
        results = {}

        log.info(
            "Traitement groupe '%s' : %d symboles@%s",
            group_name,
            len(symbols),
            timeframe,
        )

        for symbol in symbols:
            try:
                results[symbol] = self.process_symbol(
                    symbol=symbol, timeframe=timeframe, indicators=indicators
                )
            except Exception as e:
                log.error(
                    "Ã‰chec traitement %s dans groupe %s: %s", symbol, group_name, e
                )
                continue

        log.info(
            "Groupe '%s' traitÃ© : %d/%d symboles rÃ©ussis",
            group_name,
            len(results),
            len(symbols),
        )

        return results

    def process_all_groups(
        self,
        timeframe: str = "1h",
        indicators: Optional[List[str]] = None,
        limit_per_group: Optional[int] = None,
    ) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        Traite tous les groupes de diversitÃ©.

        Args:
            timeframe: Timeframe global
            indicators: Indicateurs globaux
            limit_per_group: Limite symboles par groupe (pour tests)

        Returns:
            Dict {group_name: {symbol: DataFrame}} structure complÃ¨te
        """
        all_results = {}

        log.info(
            "Traitement complet : %d groupes@%s", len(self.config["groups"]), timeframe
        )

        for group_name in self.config["groups"].keys():
            try:
                all_results[group_name] = self.process_group(
                    group_name=group_name,
                    timeframe=timeframe,
                    indicators=indicators,
                    limit=limit_per_group,
                )
            except Exception as e:
                log.error("Ã‰chec traitement groupe %s: %s", group_name, e)
                continue

        total_symbols = sum(len(group_data) for group_data in all_results.values())
        log.info(
            "Traitement complet terminÃ© : %d symboles traitÃ©s dans %d groupes",
            total_symbols,
            len(all_results),
        )

        return all_results

    def _save_enriched_data(
        self, df: pd.DataFrame, symbol: str, timeframe: str, indicators: List[str]
    ) -> None:
        """Sauvegarde les donnÃ©es enrichies avec mÃ©tadonnÃ©es."""
        try:
            if self.cache_dir:
                self.cache_dir.mkdir(parents=True, exist_ok=True)

                # Chemin de sauvegarde
                filename = f"{symbol}_{timeframe}_enriched.parquet"
                filepath = self.cache_dir / filename

                # Sauvegarde donnÃ©es
                write_frame(df, str(filepath))

                # Enregistrement mÃ©tadonnÃ©es dans registry
                if self.registry:
                    metadata = {
                        "symbol": symbol,
                        "timeframe": timeframe,
                        "indicators": indicators,
                        "rows": len(df),
                        "columns": list(df.columns),
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                        "filepath": str(filepath),
                    }
                    self.registry.register_dataset(
                        key=f"diversity_{symbol}_{timeframe}", metadata=metadata
                    )

                log.debug("DonnÃ©es enrichies sauvÃ©es : %s", filepath)

        except Exception as e:
            log.warning("Ã‰chec sauvegarde enrichie %s@%s: %s", symbol, timeframe, e)

    def get_summary_stats(self) -> Dict[str, Any]:
        """Retourne statistiques du pipeline."""
        total_symbols = len(self._extract_all_symbols(self.config["groups"]))

        stats = {
            "total_groups": len(self.config["groups"]),
            "total_symbols": total_symbols,
            "supported_timeframes": list(self.config["default_timeframes"]),
            "default_indicators": self.config["default_indicators"],
            "cache_dir": str(self.cache_dir),
            "persistence_enabled": self.enable_persistence,
            "provider_type": "TokenDiversityDataSource (Option B)",
            "indicator_engine": "IndicatorBank (DÃ©lÃ©gation complÃ¨te)",
        }

        # Statistiques par groupe
        for group_name, symbols in self.config["groups"].items():
            stats[f"group_{group_name}_count"] = len(symbols)

        return stats


# ============================================================================
# PIPELINE FUNCTIONS (intÃ©grÃ© depuis diversity_pipeline.py)
# ============================================================================


def run_unified_diversity(
    config_path: Optional[str] = None,
    groups: Optional[List[str]] = None,
    symbols: Optional[List[str]] = None,
    timeframe: str = "1h",
    lookback_days: int = 30,
    indicators: Optional[List[str]] = None,
    output_dir: Optional[str] = None,
    save_artifacts: bool = True,
) -> Dict[str, Any]:
    """
    Pipeline unifiÃ© d'analyse de diversitÃ© avec dÃ©lÃ©gation Indicator Bank.

    Ã‰tapes :
    1. Configuration TokenDiversityDataSource (Option B)
    2. RÃ©cupÃ©ration donnÃ©es OHLCV brutes
    3. DÃ©lÃ©gation calculs d'indicateurs Ã  IndicatorBank
    4. AgrÃ©gation et analyse de diversitÃ©
    5. Sauvegarde artifacts + Registry

    Args:
        config_path: Chemin vers config TOML personnalisÃ©e
        groups: Groupes de tokens Ã  analyser (ex: ["L1", "DeFi"])
        symbols: Symboles spÃ©cifiques si pas de groupes
        timeframe: Timeframe (ex: "1h", "4h", "1d")
        lookback_days: PÃ©riode d'analyse en jours
        indicators: Indicateurs Ã  calculer via IndicatorBank
        output_dir: RÃ©pertoire de sauvegarde
        save_artifacts: Si True, sauvegarde + enregistrement Registry

    Returns:
        Dict avec clÃ©s : ohlcv_data, indicators_data, diversity_metrics
    """
    from datetime import timedelta

    start_time = datetime.now()
    log.info(
        "run_unified_diversity: START - groups=%s symbols=%s tf=%s " "lookback=%dd",
        groups,
        symbols,
        timeframe,
        lookback_days,
    )

    # 1) Configuration du provider TokenDiversity
    if config_path:
        try:
            custom_config = load_config_dict(config_path)
            td_config = TokenDiversityConfig(**custom_config.get("token_diversity", {}))
        except Exception as e:
            log.error("Erreur chargement config %s: %s", config_path, e)
            raise ValueError(f"Invalid config file: {config_path}") from e
    else:
        td_config = create_default_config()

    # 2) Initialisation du provider
    try:
        provider = TokenDiversityDataSource(td_config)
        log.info(
            "TokenDiversityDataSource initialisÃ©: %d symboles",
            len(provider.list_symbols()),
        )
    except Exception as e:
        log.error("Erreur init TokenDiversityDataSource: %s", e)
        raise RuntimeError("Failed to initialize TokenDiversityDataSource") from e

    # 3) RÃ©solution des symboles cibles
    target_symbols = _resolve_target_symbols(provider, groups, symbols)
    if not target_symbols:
        raise ValueError("No symbols resolved from groups/symbols parameters")

    log.info(
        "Symboles cibles rÃ©solus: %d - %s",
        len(target_symbols),
        target_symbols[:5],
    )

    # 4) Calcul pÃ©riode de rÃ©cupÃ©ration
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=lookback_days)

    # 5) RÃ©cupÃ©ration donnÃ©es OHLCV
    ohlcv_data = {}
    failed_symbols = []

    for symbol in target_symbols:
        try:
            log.debug("RÃ©cupÃ©ration OHLCV: %s@%s", symbol, timeframe)
            df = provider.fetch_ohlcv(symbol, timeframe, start_date, end_date)

            if df is not None and not df.empty:
                ohlcv_data[symbol] = df
                log.debug("OHLCV OK: %s â†’ %d rows", symbol, len(df))
            else:
                failed_symbols.append(symbol)
                log.warning("OHLCV vide: %s", symbol)

        except Exception as e:
            failed_symbols.append(symbol)
            log.error("Erreur OHLCV %s: %s", symbol, e)

    if not ohlcv_data:
        raise RuntimeError("No OHLCV data retrieved for any symbol")

    log.info(
        "OHLCV rÃ©cupÃ©rÃ©: %d/%d symboles (Ã©checs: %s)",
        len(ohlcv_data),
        len(target_symbols),
        failed_symbols[:3] if failed_symbols else "aucun",
    )

    # 6) DÃ©lÃ©gation calculs d'indicateurs Ã  IndicatorBank
    indicators_data = {}
    if indicators:
        try:
            log.info("DÃ©lÃ©gation IndicatorBank: %d indicateurs", len(indicators))
            bank = IndicatorBank()

            for symbol, ohlcv_df in ohlcv_data.items():
                try:
                    indicators_result = bank.compute_batch(
                        data=ohlcv_df, indicators=indicators, symbol=symbol
                    )
                    indicators_data[symbol] = indicators_result
                    log.debug(
                        "Indicateurs OK: %s â†’ %d indicateurs",
                        symbol,
                        len(indicators_result),
                    )

                except Exception as e:
                    log.error("Erreur indicateurs %s: %s", symbol, e)

        except Exception as e:
            log.error("Erreur init IndicatorBank: %s", e)

    # 7) Calcul mÃ©triques de diversitÃ©
    diversity_metrics = _compute_diversity_metrics(ohlcv_data, indicators_data, groups)

    # 8) Sauvegarde artifacts
    artifacts_info = {}
    if save_artifacts:
        artifacts_info = _save_pipeline_artifacts(
            ohlcv_data,
            indicators_data,
            diversity_metrics,
            output_dir or td_config.cache_dir,
            timeframe,
            lookback_days,
        )

    # 9) RÃ©sultat pipeline
    duration = (datetime.now() - start_time).total_seconds()
    log.info(
        "run_unified_diversity: SUCCESS - %d symboles, " "%d indicateurs, %.1fs",
        len(ohlcv_data),
        len(indicators_data),
        duration,
    )

    return {
        "ohlcv_data": ohlcv_data,
        "indicators_data": indicators_data,
        "diversity_metrics": diversity_metrics,
        "metadata": {
            "timeframe": timeframe,
            "lookback_days": lookback_days,
            "symbols_count": len(ohlcv_data),
            "failed_symbols": failed_symbols,
            "duration_seconds": duration,
            "artifacts": artifacts_info,
        },
    }


def _resolve_target_symbols(
    provider: TokenDiversityDataSource,
    groups: Optional[List[str]],
    symbols: Optional[List[str]],
) -> List[str]:
    """RÃ©sout les symboles cibles Ã  partir des groupes ou symboles."""
    if symbols:
        available_symbols = set(provider.list_symbols())
        resolved = [s for s in symbols if s in available_symbols]
        missing = set(symbols) - set(resolved)

        if missing:
            log.warning("Symboles introuvables: %s", sorted(missing))

        return resolved

    elif groups:
        resolved = []
        for group in groups:
            group_symbols = provider.list_symbols(group=group)
            resolved.extend(group_symbols)
            log.debug("Groupe %s: %d symboles", group, len(group_symbols))

        return list(dict.fromkeys(resolved))  # PrÃ©serve l'ordre

    else:
        return provider.list_symbols()[:10]


def _compute_diversity_metrics(
    ohlcv_data: Dict[str, pd.DataFrame],
    indicators_data: Dict[str, pd.DataFrame],
    groups: Optional[List[str]],
) -> pd.DataFrame:
    """Calcul des mÃ©triques de diversitÃ© inter-tokens."""
    log.info("Calcul mÃ©triques de diversitÃ©: %d tokens", len(ohlcv_data))

    metrics = []

    for symbol, df in ohlcv_data.items():
        if df.empty:
            continue

        close_prices = df["close"]
        returns = close_prices.pct_change().dropna()

        metric = {
            "symbol": symbol,
            "data_points": len(df),
            "price_start": (close_prices.iloc[0] if len(close_prices) > 0 else None),
            "price_end": (close_prices.iloc[-1] if len(close_prices) > 0 else None),
            "total_return": (
                (close_prices.iloc[-1] / close_prices.iloc[0] - 1)
                if len(close_prices) > 1
                else 0
            ),
            "volatility": returns.std() if len(returns) > 0 else 0,
            "volume_mean": df["volume"].mean(),
            "volume_std": df["volume"].std(),
        }

        # MÃ©triques d'indicateurs (si disponibles)
        if symbol in indicators_data:
            indicators_df = indicators_data[symbol]
            if "rsi_14" in indicators_df.columns:
                metric["rsi_mean"] = indicators_df["rsi_14"].mean()
            if "sma_20" in indicators_df.columns:
                metric["sma_trend"] = (
                    (
                        indicators_df["sma_20"].iloc[-1]
                        / indicators_df["sma_20"].iloc[0]
                        - 1
                    )
                    if len(indicators_df) > 1
                    else 0
                )

        metrics.append(metric)

    diversity_df = pd.DataFrame(metrics)

    if not diversity_df.empty and len(ohlcv_data) > 1:
        # Calcul corrÃ©lations inter-tokens
        returns_matrix = {}
        for symbol, df in ohlcv_data.items():
            if not df.empty:
                returns = df["close"].pct_change().dropna()
                if len(returns) > 0:
                    returns_matrix[symbol] = returns

        if len(returns_matrix) > 1:
            returns_df = pd.DataFrame(returns_matrix).fillna(0)
            correlation_matrix = returns_df.corr()

            diversity_scores: List[float] = []
            for symbol in diversity_df["symbol"]:
                if symbol in correlation_matrix.index:
                    corr_with_others = correlation_matrix.loc[symbol].drop(symbol)
                    avg_correlation = (
                        corr_with_others.abs().mean()
                        if len(corr_with_others) > 0
                        else 0
                    )
                    diversity_score = 1 - avg_correlation
                    diversity_scores.append(diversity_score)
                else:
                    diversity_scores.append(0.5)

            diversity_df["diversity_score"] = diversity_scores

    log.info("MÃ©triques diversitÃ© calculÃ©es: %d tokens", len(diversity_df))
    return diversity_df


def _save_pipeline_artifacts(
    ohlcv_data: Dict[str, pd.DataFrame],
    indicators_data: Dict[str, pd.DataFrame],
    diversity_metrics: pd.DataFrame,
    output_dir: str,
    timeframe: str,
    lookback_days: int,
) -> Dict[str, str]:
    """Sauvegarde les artifacts du pipeline."""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    artifacts = {}

    try:
        # 1) Sauvegarde donnÃ©es OHLCV
        ohlcv_file = (
            output_path / f"diversity_ohlcv_{timeframe}_{lookback_days}d_"
            f"{timestamp}.parquet"
        )
        combined_ohlcv = pd.concat(
            {symbol: df for symbol, df in ohlcv_data.items()},
            names=["symbol", "datetime"],
        ).reset_index()
        write_frame(combined_ohlcv, str(ohlcv_file))
        artifacts["ohlcv"] = str(ohlcv_file)

        # 2) Sauvegarde indicateurs
        if indicators_data:
            indicators_file = (
                output_path / f"diversity_indicators_{timeframe}_{lookback_days}d_"
                f"{timestamp}.parquet"
            )
            combined_indicators = pd.concat(
                {symbol: df for symbol, df in indicators_data.items()},
                names=["symbol", "datetime"],
            ).reset_index()
            write_frame(combined_indicators, str(indicators_file))
            artifacts["indicators"] = str(indicators_file)

        # 3) Sauvegarde mÃ©triques diversitÃ©
        metrics_file = (
            output_path / f"diversity_metrics_{timeframe}_{lookback_days}d_"
            f"{timestamp}.parquet"
        )
        write_frame(diversity_metrics, str(metrics_file))
        artifacts["metrics"] = str(metrics_file)

        log.info("Artifacts sauvegardÃ©s: %s", output_path)
        return artifacts

    except Exception as e:
        log.error("Erreur sauvegarde artifacts: %s", e)
        return {}


def run_diversity_mode(args: argparse.Namespace) -> None:
    """
    ExÃ©cute le mode diversity du pipeline unifiÃ©.

    Args:
        args: Arguments CLI parsÃ©s
    """
    log.info("DÃ©marrage mode diversity avec Option B")

    # Configuration logging
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    try:
        # Mapping des arguments CLI vers le pipeline
        pipeline_kwargs = {
            "timeframe": args.timeframe,
            "lookback_days": getattr(args, "lookback_days", 30),
            "save_artifacts": not args.no_persistence,
        }

        # Gestion des arguments optionnels
        if hasattr(args, "config") and args.config:
            pipeline_kwargs["config_path"] = args.config

        if args.indicators:
            pipeline_kwargs["indicators"] = (
                args.indicators.split(",")
                if isinstance(args.indicators, str)
                else args.indicators
            )

        if hasattr(args, "output_dir") and args.output_dir:
            pipeline_kwargs["output_dir"] = args.output_dir
        elif args.cache_dir:
            pipeline_kwargs["output_dir"] = args.cache_dir

        # Modes d'exÃ©cution
        if hasattr(args, "symbol") and args.symbol:
            # Mode symbole unique
            log.info("Mode symbole unique : %s@%s", args.symbol, args.timeframe)
            pipeline_kwargs["symbols"] = [args.symbol]

        elif hasattr(args, "group") and args.group:
            # Mode groupe
            log.info("Mode groupe : %s@%s", args.group, args.timeframe)
            pipeline_kwargs["groups"] = [args.group]

        else:
            # Mode complet (tous les groupes par dÃ©faut)
            log.info("Mode complet : tous groupes@%s", args.timeframe)
            default_groups = list(DEFAULT_DIVERSITY_CONFIG["groups"].keys())
            pipeline_kwargs["groups"] = default_groups

        # ExÃ©cution du pipeline unifiÃ©
        results = run_unified_diversity(**pipeline_kwargs)

        # Affichage rÃ©sultats
        metadata = results["metadata"]
        log.info(
            "Pipeline terminÃ©: %d symboles, %d indicateurs, %.1fs",
            metadata["symbols_count"],
            len(results["indicators_data"]),
            metadata["duration_seconds"],
        )

        if results["metadata"]["failed_symbols"]:
            log.warning(
                "Symboles en Ã©chec: %s",
                results["metadata"]["failed_symbols"][:3],
            )

        # Sauvegarde optionnelle de rÃ©sultats spÃ©cifiques
        if hasattr(args, "output") and args.output:
            # Export des mÃ©triques de diversitÃ©
            diversity_metrics = results["diversity_metrics"]
            write_frame(diversity_metrics, args.output)
            log.info("MÃ©triques diversitÃ© sauvÃ©es : %s", args.output)

    except Exception as e:
        log.error("Erreur pipeline diversity : %s", e)
        sys.exit(1)

    log.info("Mode diversity terminÃ© avec succÃ¨s")


def main():
    """Point d'entrÃ©e CLI pour le pipeline diversity unifiÃ©."""
    parser = argparse.ArgumentParser(
        description="ThreadX Unified Diversity Pipeline - Option B",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Traitement symbole unique
  python -m threadx.dataset.unified_diversity_pipeline --mode diversity --symbol BTCUSDT --timeframe 1h

  # Traitement groupe DeFi
  python -m threadx.dataset.unified_diversity_pipeline --mode diversity --group DeFi --timeframe 4h

  # Traitement complet avec limite par groupe
  python -m threadx.dataset.unified_diversity_pipeline --mode diversity --timeframe 1d --limit 5

  # Indicateurs personnalisÃ©s
  python -m threadx.dataset.unified_diversity_pipeline --mode diversity --group L2 --indicators rsi macd sma_20
        """,
    )

    parser.add_argument(
        "--mode",
        default="diversity",
        choices=["diversity"],
        help="Mode de traitement (diversity uniquement pour ce module)",
    )

    parser.add_argument(
        "--symbol", type=str, help="Symbole unique Ã  traiter (ex: BTCUSDT)"
    )

    parser.add_argument(
        "--group", type=str, help="Groupe de diversitÃ© Ã  traiter (ex: L2, DeFi, AI)"
    )

    parser.add_argument(
        "--timeframe", default="1h", help="Timeframe Ã  utiliser (dÃ©faut: 1h)"
    )

    parser.add_argument(
        "--indicators",
        nargs="*",
        help="Indicateurs Ã  calculer (dÃ©faut: rsi macd bb sma_20 ema_50)",
    )

    parser.add_argument(
        "--limit", type=int, help="Limite nombre de symboles par groupe"
    )

    parser.add_argument(
        "--cache-dir", type=str, help="RÃ©pertoire de cache personnalisÃ©"
    )

    parser.add_argument(
        "--no-persistence", action="store_true", help="DÃ©sactiver la persistance"
    )

    parser.add_argument(
        "--output", type=str, help="Fichier de sortie pour mode symbole unique"
    )

    parser.add_argument("--verbose", "-v", action="store_true", help="Logging dÃ©taillÃ©")

    args = parser.parse_args()

    # Validation arguments
    if args.symbol and args.group:
        parser.error("--symbol et --group sont mutuellement exclusifs")

    if args.output and not args.symbol:
        parser.error("--output ne peut Ãªtre utilisÃ© qu'avec --symbol")

    # ExÃ©cution mode diversity
    run_diversity_mode(args)


if __name__ == "__main__":
    main()




----------------------------------------
Fichier: bridge\validation.py
"""
ThreadX Bridge - Pydantic Validation Layer
===========================================

ModÃ¨les de validation pour les requÃªtes vers l'Engine.

Author: ThreadX Framework
Version: Prompt 2 - Bridge Foundation
"""

from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field, field_validator


class BacktestRequest(BaseModel):
    """RequÃªte de backtest validÃ©e."""

    symbol: str = Field(..., min_length=1, description="Symbole de trading")
    timeframe: str = Field(
        ...,
        pattern=r"^(\d+m|1h|2h|4h|6h|8h|12h|1d|1w|1M)$",
        description="Timeframe: 1m,5m,15m,30m,45m,1h,2h,4h,6h,8h,12h,1d,1w,1M",
    )
    strategy: str = Field(..., min_length=1, description="Nom de la stratÃ©gie")
    params: Dict[str, Any] = Field(
        default_factory=dict, description="ParamÃ¨tres de stratÃ©gie"
    )
    start_date: Optional[str] = Field(None, description="Date de dÃ©but")
    end_date: Optional[str] = Field(None, description="Date de fin")

    @field_validator("params")
    @classmethod
    def validate_params(cls, v: Dict[str, Any]) -> Dict[str, Any]:
        """Valider les paramÃ¨tres de stratÃ©gie."""
        if not isinstance(v, dict):
            raise ValueError("params doit Ãªtre un dictionnaire")
        return v


class IndicatorRequest(BaseModel):
    """RequÃªte de calcul d'indicateur validÃ©e."""

    symbol: str = Field(..., min_length=1, description="Symbole de trading")
    timeframe: str = Field(
        ...,
        pattern=r"^(\d+m|1h|2h|4h|6h|8h|12h|1d|1w|1M)$",
        description="Timeframe: 1m,5m,15m,30m,45m,1h,2h,4h,6h,8h,12h,1d,1w,1M",
    )
    indicators: Dict[str, Dict[str, Any]] = Field(
        ..., description="Dictionnaire d'indicateurs {name: params}"
    )
    data_path: Optional[str] = Field(
        None, description="Chemin vers donnÃ©es Parquet ou None"
    )
    force_recompute: bool = Field(False, description="Forcer recalcul, ignorer cache")
    use_gpu: bool = Field(False, description="Utiliser GPU pour calculs si disponible")

    @field_validator("indicators")
    @classmethod
    def validate_indicators(
        cls, v: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """Valider la structure des indicateurs: dict nom -> params dict."""
        if not isinstance(v, dict):
            raise ValueError("indicators doit Ãªtre un dictionnaire")
        if len(v) == 0:
            raise ValueError("indicators doit contenir au moins un indicateur")
        for name, params in v.items():
            if not isinstance(name, str) or not name:
                raise ValueError(
                    "Chaque clÃ© d'indicators doit Ãªtre un nom d'indicateur non vide"
                )
            if not isinstance(params, dict):
                raise ValueError(
                    f"Les paramÃ¨tres pour {name} doivent Ãªtre un dictionnaire"
                )
        return v

    # Note: previous versions indexed a single 'indicator' and 'params' field.
    # The model now uses 'indicators: Dict[name, params]'. Keeping any
    # validators referring to 'params' would break Pydantic model creation,
    # so those validators were removed intentionally.


class OptimizeRequest(BaseModel):
    """RequÃªte d'optimisation de paramÃ¨tres validÃ©e."""

    symbol: str = Field(..., min_length=1, description="Symbole de trading")
    timeframe: str = Field(
        ...,
        pattern=r"^(\d+m|1h|2h|4h|6h|8h|12h|1d|1w|1M)$",
        description="Timeframe: 1m,5m,15m,30m,45m,1h,2h,4h,6h,8h,12h,1d,1w,1M",
    )
    strategy: str = Field(..., min_length=1, description="Nom de la stratÃ©gie")
    param_ranges: Dict[str, List[Any]] = Field(
        ..., description="Plages de paramÃ¨tres Ã  tester"
    )
    objective: str = Field(
        default="sharpe_ratio", description="MÃ©trique d'optimisation"
    )

    @field_validator("param_ranges")
    @classmethod
    def validate_param_ranges(cls, v: Dict[str, List[Any]]) -> Dict[str, List[Any]]:
        """Valider les plages de paramÃ¨tres."""
        if not isinstance(v, dict):
            raise ValueError("param_ranges doit Ãªtre un dictionnaire")
        for key, value in v.items():
            if not isinstance(value, list):
                raise ValueError(f"param_ranges[{key}] doit Ãªtre une liste")
        return v


class DataValidationRequest(BaseModel):
    """RequÃªte de validation de donnÃ©es validÃ©e."""

    symbol: str = Field(..., min_length=1, description="Symbole de trading")
    timeframe: str = Field(
        ...,
        pattern=r"^(\d+m|1h|2h|4h|6h|8h|12h|1d|1w|1M)$",
        description="Timeframe: 1m,5m,15m,30m,45m,1h,2h,4h,6h,8h,12h,1d,1w,1M",
    )
    start_date: Optional[str] = Field(None, description="Date de dÃ©but")
    end_date: Optional[str] = Field(None, description="Date de fin")
    checks: List[str] = Field(
        default_factory=lambda: ["completeness", "duplicates", "outliers"],
        description="Types de vÃ©rifications",
    )

    @field_validator("checks")
    @classmethod
    def validate_checks(cls, v: List[str]) -> List[str]:
        """Valider les types de vÃ©rifications."""
        if not isinstance(v, list):
            raise ValueError("checks doit Ãªtre une liste")
        valid_checks = {"completeness", "duplicates", "outliers", "gaps"}
        for check in v:
            if check not in valid_checks:
                raise ValueError(f"Type de vÃ©rification invalide: {check}")
        return v




----------------------------------------
Fichier: bridge\__init__.py
"""
ThreadX Bridge - Orchestration Layer
====================================

Couche intermÃ©diaire entre UI (Dash, CLI) et Engine (calculs purs).
Fournit API typÃ©e, synchrone, documentÃ©e pour tous composants ThreadX.

Architecture:
    UI Layer (Dash/CLI)
          â†“
    Bridge Layer (controllers + models) â† CETTE COUCHE
          â†“
    Engine Layer (backtest, indicators, optimization, data)

Modules:
    models: DataClasses Request/Result typÃ©es (PEP 604)
    controllers: Wrappers synchrones autour Engine modules
    exceptions: HiÃ©rarchie erreurs Bridge

Usage Principal (CLI):
    >>> from threadx.bridge import BacktestController, BacktestRequest
    >>> req = BacktestRequest(
    ...     symbol='BTCUSDT',
    ...     timeframe='1h',
    ...     strategy='bollinger_reversion',
    ...     params={'period': 20, 'std': 2.0}
    ... )
    >>> controller = BacktestController()
    >>> result = controller.run_backtest(req)
    >>> print(f"Sharpe: {result.sharpe_ratio:.2f}")

Usage Principal (Dash Callback):
    >>> from dash import callback, Input, Output
    >>> from threadx.bridge import BacktestController, BacktestRequest
    >>>
    >>> @callback(
    ...     Output('results-div', 'children'),
    ...     Input('run-button', 'n_clicks'),
    ...     State('symbol-input', 'value')
    ... )
    >>> def run_backtest_callback(n_clicks, symbol):
    ...     req = BacktestRequest(symbol=symbol, ...)
    ...     controller = BacktestController()
    ...     result = controller.run_backtest(req)
    ...     return format_results(result)

Principes Bridge:
    - Aucune logique mÃ©tier (dÃ©lÃ¨gue Ã  Engine)
    - Aucune dÃ©pendance UI (pas d'import dash/tkinter)
    - Type-safe (mypy --strict compatible)
    - DocumentÃ© (Google-style docstrings)
    - Synchrone (async gÃ©rÃ© par P3 ThreadXBridge)

Author: ThreadX Framework
Version: Prompt 3 - Async Coordinator
"""

# Models (Request/Result DataClasses)
from threadx.bridge.models import (
    BacktestRequest,
    BacktestResult,
    Configuration,
    DataRequest,
    DataValidationResult,
    IndicatorRequest,
    IndicatorResult,
    SweepRequest,
    SweepResult,
)

# Controllers (Orchestration)
from threadx.bridge.controllers import (
    BacktestController,
    DataController,
    IndicatorController,
    SweepController,
    MetricsController,
    DataIngestionController,
    DiversityPipelineController,
)

# Exceptions (Error Handling)
from threadx.bridge.exceptions import (
    BacktestError,
    BridgeError,
    ConfigurationError,
    DataError,
    IndicatorError,
    SweepError,
    ValidationError,
)

# Async Coordinator (Prompt 3)
from threadx.bridge.async_coordinator import ThreadXBridge

# Configuration Constants
from threadx.bridge.config import DEFAULT_SWEEP_CONFIG

# Public API exports
__all__ = [
    # Models
    "BacktestRequest",
    "BacktestResult",
    "IndicatorRequest",
    "IndicatorResult",
    "SweepRequest",
    "SweepResult",
    "DataRequest",
    "DataValidationResult",
    "Configuration",
    # Controllers
    "BacktestController",
    "IndicatorController",
    "SweepController",
    "DataController",
    "MetricsController",
    "DataIngestionController",
    "DiversityPipelineController",
    # Exceptions
    "BridgeError",
    "BacktestError",
    "IndicatorError",
    "SweepError",
    "DataError",
    "ConfigurationError",
    "ValidationError",
    # Async Coordinator (P3)
    "ThreadXBridge",
    # Configuration
    "DEFAULT_SWEEP_CONFIG",
]

# Version info
__version__ = "0.1.0"
__author__ = "ThreadX Framework"
__license__ = "MIT"




----------------------------------------
Fichier: cli\backtest_cmd.py
"""
ThreadX CLI - Backtest Commands
================================

Commands for running backtests and analyzing results.

Usage:
    python -m threadx.cli backtest run --strategy ema_crossover --symbol BTCUSDT

Author: ThreadX Framework
Version: Prompt 9 - Backtest Commands
"""

import logging
from typing import Optional

import typer

from threadx.cli.utils import (
    async_runner,
    format_duration,
    handle_bridge_error,
    print_json,
    print_summary,
)

app = typer.Typer(help="Backtest execution and analysis commands")
logger = logging.getLogger("threadx.cli.backtest")


@app.command()
def run(
    strategy: str = typer.Option(..., help="Strategy name"),
    symbol: str = typer.Option(..., help="Symbol to backtest"),
    timeframe: str = typer.Option("1h", "--tf", help="Timeframe"),
    period: Optional[int] = typer.Option(None, help="Strategy period parameter"),
    std: Optional[float] = typer.Option(None, help="Strategy std deviation parameter"),
    start_date: Optional[str] = typer.Option(None, help="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = typer.Option(None, help="End date (YYYY-MM-DD)"),
    initial_capital: float = typer.Option(10000.0, help="Initial capital"),
) -> None:
    """
    Run backtest for strategy on symbol/timeframe.

    Executes backtest via Bridge, computes performance metrics
    (equity curve, drawdown, Sharpe ratio, win rate),
    and displays results.

    Args:
        strategy: Strategy name (e.g., ema_crossover, bollinger_reversion).
        symbol: Symbol to backtest (e.g., BTCUSDT).
        timeframe: Timeframe (1m, 5m, 1h, 1d).
        period: Strategy period parameter (if applicable).
        std: Strategy std deviation parameter (if applicable).
        start_date: Start date for backtest period.
        end_date: End date for backtest period.
        initial_capital: Initial capital in USD.
    """
    try:
        from threadx.bridge import ThreadXBridge
    except ImportError as e:
        logger.error(f"Failed to import ThreadXBridge: {e}")
        typer.echo("âŒ Bridge not available. Check installation.")
        raise typer.Exit(1)

    ctx = typer.Context.get_current()
    json_mode = ctx.obj.get("json", False) if ctx.obj else False

    logger.info(f"Running backtest: {strategy} on {symbol} @ {timeframe}")

    try:
        # Initialize Bridge
        bridge = ThreadXBridge()

        # Prepare backtest request
        request = {
            "strategy": strategy,
            "symbol": symbol,
            "timeframe": timeframe,
            "initial_capital": initial_capital,
        }

        # Add optional params
        if period is not None:
            request["period"] = period
        if std is not None:
            request["std"] = std
        if start_date:
            request["start_date"] = start_date
        if end_date:
            request["end_date"] = end_date

        # Submit async backtest
        task_id = bridge.run_backtest_async(request)
        logger.debug(f"Backtest task submitted: {task_id}")

        if not json_mode:
            typer.echo(
                f"â³ Running backtest: {strategy} on " f"{symbol} ({timeframe})..."
            )

        # Poll for results
        event = async_runner(bridge.get_event, task_id, timeout=300.0)

        if event is None:
            error_msg = "Backtest timed out"
            if json_mode:
                print_json({"status": "timeout", "task_id": task_id})
            else:
                typer.echo(f"âš ï¸  {error_msg}")
            raise typer.Exit(1)

        # Check status
        if event.get("status") == "error":
            error = Exception(event.get("error", "Unknown error"))
            handle_bridge_error(error, json_mode)

        # Extract result
        result = event.get("result", {})
        duration = event.get("duration", 0)

        # Extract metrics
        metrics = result.get("metrics", {})

        # Prepare summary
        summary = {
            "strategy": strategy,
            "symbol": symbol,
            "timeframe": timeframe,
            "total_trades": metrics.get("total_trades", 0),
            "win_rate": f"{metrics.get('win_rate', 0) * 100:.2f}%",
            "total_return": f"{metrics.get('total_return', 0) * 100:.2f}%",
            "sharpe_ratio": metrics.get("sharpe_ratio", 0),
            "max_drawdown": f"{metrics.get('max_drawdown', 0) * 100:.2f}%",
            "profit_factor": metrics.get("profit_factor", 0),
            "final_equity": metrics.get("final_equity", 0),
            "execution_time": format_duration(duration),
        }

        # Output
        if json_mode:
            output_data = {
                "status": "success",
                "summary": summary,
                "metrics": metrics,
                "trades": result.get("trades", []),
                "equity_curve": result.get("equity_curve", []),
            }
            print_json(output_data)
        else:
            print_summary("Backtest Results", summary)

            # Show best/worst trades
            trades = result.get("trades", [])
            if trades:
                sorted_trades = sorted(
                    trades, key=lambda t: t.get("pnl", 0), reverse=True
                )

                typer.echo("ðŸ“Š Top 3 Best Trades:")
                for i, trade in enumerate(sorted_trades[:3], 1):
                    pnl = trade.get("pnl", 0)
                    pnl_pct = trade.get("pnl_pct", 0) * 100
                    entry = trade.get("entry_date", "N/A")
                    typer.echo(f"  {i}. ${pnl:,.2f} ({pnl_pct:+.2f}%) - " f"{entry}")

                typer.echo("\nðŸ“‰ Top 3 Worst Trades:")
                for i, trade in enumerate(sorted_trades[-3:][::-1], 1):
                    pnl = trade.get("pnl", 0)
                    pnl_pct = trade.get("pnl_pct", 0) * 100
                    entry = trade.get("entry_date", "N/A")
                    typer.echo(f"  {i}. ${pnl:,.2f} ({pnl_pct:+.2f}%) - " f"{entry}")
                typer.echo()

    except Exception as e:
        handle_bridge_error(e, json_mode)


if __name__ == "__main__":
    app()




----------------------------------------
Fichier: cli\data_cmd.py
"""
ThreadX CLI - Data Commands
============================

Commands for dataset validation and management.

Usage:
    python -m threadx.cli data validate <path>

Author: ThreadX Framework
Version: Prompt 9 - Data Commands
"""

import logging
from pathlib import Path
from typing import Optional

import typer

from threadx.cli.utils import (
    async_runner,
    format_duration,
    handle_bridge_error,
    print_json,
    print_summary,
)

app = typer.Typer(help="Dataset validation and management commands")
logger = logging.getLogger("threadx.cli.data")


@app.command()
def validate(
    path: str = typer.Argument(..., help="Path to dataset file (CSV/Parquet)"),
    symbol: Optional[str] = typer.Option(None, help="Symbol override"),
    timeframe: Optional[str] = typer.Option(None, "--tf", help="Timeframe override"),
) -> None:
    """
    Validate dataset and register in data registry.

    Loads dataset, checks schema, validates OHLCV columns,
    and registers in ThreadX data registry for use in backtests.

    Args:
        path: Path to dataset file (CSV or Parquet).
        symbol: Optional symbol override (default: infer from filename).
        timeframe: Optional timeframe override (default: infer from filename).
    """
    try:
        from threadx.bridge import ThreadXBridge
    except ImportError as e:
        logger.error(f"Failed to import ThreadXBridge: {e}")
        typer.echo("âŒ Bridge not available. Check installation.")
        raise typer.Exit(1)

    # Get context for JSON mode
    ctx = typer.Context.get_current()
    json_mode = ctx.obj.get("json", False) if ctx.obj else False

    # Validate path exists
    data_path = Path(path)
    if not data_path.exists():
        error_msg = f"File not found: {path}"
        if json_mode:
            print_json({"status": "error", "message": error_msg})
        else:
            typer.echo(f"âŒ {error_msg}")
        raise typer.Exit(1)

    logger.info(f"Validating dataset: {data_path}")

    try:
        # Initialize Bridge
        bridge = ThreadXBridge()

        # Prepare validation request
        request = {
            "path": str(data_path.absolute()),
            "symbol": symbol,
            "timeframe": timeframe,
        }

        # Submit async validation
        task_id = bridge.validate_data_async(request)
        logger.debug(f"Validation task submitted: {task_id}")

        if not json_mode:
            typer.echo(f"â³ Validating {data_path.name}...")

        # Poll for results
        event = async_runner(bridge.get_event, task_id, timeout=30.0)

        if event is None:
            error_msg = "Validation timed out"
            if json_mode:
                print_json({"status": "timeout", "task_id": task_id})
            else:
                typer.echo(f"âš ï¸  {error_msg}")
            raise typer.Exit(1)

        # Check status
        if event.get("status") == "error":
            handle_bridge_error(
                Exception(event.get("error", "Unknown error")), json_mode
            )

        # Extract result
        result = event.get("result", {})
        duration = event.get("duration", 0)

        # Prepare summary
        summary = {
            "file": str(data_path.name),
            "symbol": result.get("symbol", "N/A"),
            "timeframe": result.get("timeframe", "N/A"),
            "rows": result.get("rows", 0),
            "columns": result.get("columns", 0),
            "date_range": f"{result.get('start_date', 'N/A')} â†’ {result.get('end_date', 'N/A')}",
            "quality_score": result.get("quality", 0.0),
            "validation_time": format_duration(duration),
            "status": "âœ… Valid" if result.get("valid", False) else "âŒ Invalid",
        }

        # Output
        if json_mode:
            print_json({"status": "success", "data": summary, "event": event})
        else:
            print_summary("Dataset Validation", summary)

            if result.get("warnings"):
                typer.echo("âš ï¸  Warnings:")
                for warning in result["warnings"]:
                    typer.echo(f"  - {warning}")

    except Exception as e:
        handle_bridge_error(e, json_mode)


@app.command()
def list() -> None:
    """
    List all registered datasets in data registry.

    Shows symbol, timeframe, rows, date range, and validation status
    for all datasets available for backtesting.
    """
    try:
        from threadx.bridge import ThreadXBridge
    except ImportError as e:
        logger.error(f"Failed to import ThreadXBridge: {e}")
        typer.echo("âŒ Bridge not available. Check installation.")
        raise typer.Exit(1)

    ctx = typer.Context.get_current()
    json_mode = ctx.obj.get("json", False) if ctx.obj else False

    try:
        bridge = ThreadXBridge()

        # Get registry (synchronous call)
        registry = bridge.get_data_registry()

        if json_mode:
            print_json({"status": "success", "datasets": registry})
        else:
            if not registry:
                typer.echo("ðŸ“ No datasets registered yet.")
                typer.echo("   Use 'threadx data validate <path>' to add datasets.")
                return

            typer.echo(f"\nðŸ“ Registered Datasets ({len(registry)} total)\n")
            typer.echo(
                f"{'Symbol':<12} {'Timeframe':<10} {'Rows':<10} {'Date Range':<30} {'Status':<10}"
            )
            typer.echo("-" * 80)

            for dataset in registry:
                symbol = dataset.get("symbol", "N/A")
                tf = dataset.get("timeframe", "N/A")
                rows = dataset.get("rows", 0)
                start = dataset.get("start_date", "N/A")
                end = dataset.get("end_date", "N/A")
                valid = "âœ… Valid" if dataset.get("valid", False) else "âŒ Invalid"

                date_range = f"{start} â†’ {end}"
                typer.echo(
                    f"{symbol:<12} {tf:<10} {rows:<10} {date_range:<30} {valid:<10}"
                )

            typer.echo()

    except Exception as e:
        handle_bridge_error(e, json_mode)


if __name__ == "__main__":
    app()




----------------------------------------
Fichier: cli\indicators_cmd.py
"""
ThreadX CLI - Indicators Commands
==================================

Commands for building and caching technical indicators.

Usage:
    python -m threadx.cli indicators build --symbol BTCUSDT --tf 1h

Author: ThreadX Framework
Version: Prompt 9 - Indicators Commands
"""

import logging
from typing import Optional

import typer

from threadx.cli.utils import (
    async_runner,
    format_duration,
    handle_bridge_error,
    print_json,
    print_summary,
)

app = typer.Typer(help="Technical indicators building and caching")
logger = logging.getLogger("threadx.cli.indicators")


@app.command()
def build(
    symbol: str = typer.Option(..., help="Symbol to build indicators for"),
    timeframe: str = typer.Option("1h", "--tf", help="Timeframe (1m, 5m, 1h, 1d)"),
    ema_period: Optional[int] = typer.Option(None, help="EMA period (default: 20)"),
    rsi_period: Optional[int] = typer.Option(None, help="RSI period (default: 14)"),
    bollinger_period: Optional[int] = typer.Option(
        None, "--bb-period", help="Bollinger period (default: 20)"
    ),
    bollinger_std: Optional[float] = typer.Option(
        None, "--bb-std", help="Bollinger std dev (default: 2.0)"
    ),
    force: bool = typer.Option(False, "--force", help="Force rebuild if cache exists"),
) -> None:
    """
    Build and cache technical indicators for symbol/timeframe.

    Computes EMA, RSI, Bollinger Bands, ATR, and other indicators,
    then caches results for fast backtest execution.

    Args:
        symbol: Symbol to compute indicators for.
        timeframe: Timeframe (1m, 5m, 15m, 1h, 4h, 1d).
        ema_period: EMA period (default: 20).
        rsi_period: RSI period (default: 14).
        bollinger_period: Bollinger Bands period (default: 20).
        bollinger_std: Bollinger Bands std deviation (default: 2.0).
        force: Force rebuild even if cache exists.
    """
    try:
        from threadx.bridge import ThreadXBridge
    except ImportError as e:
        logger.error(f"Failed to import ThreadXBridge: {e}")
        typer.echo("âŒ Bridge not available. Check installation.")
        raise typer.Exit(1)

    ctx = typer.Context.get_current()
    json_mode = ctx.obj.get("json", False) if ctx.obj else False

    logger.info(f"Building indicators: {symbol} @ {timeframe}")

    try:
        # Initialize Bridge
        bridge = ThreadXBridge()

        # Prepare request with optional params
        request = {
            "symbol": symbol,
            "timeframe": timeframe,
            "force": force,
            "params": {},
        }

        # Add indicator params if provided
        if ema_period is not None:
            request["params"]["ema_period"] = ema_period
        if rsi_period is not None:
            request["params"]["rsi_period"] = rsi_period
        if bollinger_period is not None:
            request["params"]["bollinger_period"] = bollinger_period
        if bollinger_std is not None:
            request["params"]["bollinger_std"] = bollinger_std

        # Submit async build
        task_id = bridge.build_indicators_async(request)
        logger.debug(f"Indicators build task submitted: {task_id}")

        if not json_mode:
            typer.echo(f"â³ Building indicators for {symbol} ({timeframe})...")

        # Poll for results
        event = async_runner(bridge.get_event, task_id, timeout=120.0)

        if event is None:
            error_msg = "Indicators build timed out"
            if json_mode:
                print_json({"status": "timeout", "task_id": task_id})
            else:
                typer.echo(f"âš ï¸  {error_msg}")
            raise typer.Exit(1)

        # Check status
        if event.get("status") == "error":
            handle_bridge_error(
                Exception(event.get("error", "Unknown error")), json_mode
            )

        # Extract result
        result = event.get("result", {})
        duration = event.get("duration", 0)

        # Prepare summary
        summary = {
            "symbol": symbol,
            "timeframe": timeframe,
            "indicators_built": result.get("indicators", []),
            "cache_size_mb": result.get("cache_size_mb", 0.0),
            "rows_processed": result.get("rows", 0),
            "build_time": format_duration(duration),
            "cache_path": result.get("cache_path", "N/A"),
            "status": "âœ… Cached" if result.get("cached", False) else "âŒ Failed",
        }

        # Output
        if json_mode:
            print_json({"status": "success", "data": summary, "event": event})
        else:
            print_summary("Indicators Build", summary)

            # Show indicator details
            if result.get("details"):
                typer.echo("ðŸ“Š Indicator Details:")
                for ind_name, ind_data in result["details"].items():
                    params_str = ", ".join(
                        f"{k}={v}" for k, v in ind_data.get("params", {}).items()
                    )
                    typer.echo(f"  â€¢ {ind_name:<15} ({params_str})")
                typer.echo()

    except Exception as e:
        handle_bridge_error(e, json_mode)


@app.command()
def cache() -> None:
    """
    List cached indicators and cache statistics.

    Shows all cached indicator sets with symbol, timeframe,
    cache size, and last update time.
    """
    try:
        from threadx.bridge import ThreadXBridge
    except ImportError as e:
        logger.error(f"Failed to import ThreadXBridge: {e}")
        typer.echo("âŒ Bridge not available. Check installation.")
        raise typer.Exit(1)

    ctx = typer.Context.get_current()
    json_mode = ctx.obj.get("json", False) if ctx.obj else False

    try:
        bridge = ThreadXBridge()

        # Get cache info (synchronous)
        cache_info = bridge.get_indicators_cache()

        if json_mode:
            print_json({"status": "success", "cache": cache_info})
        else:
            if not cache_info.get("cached_sets"):
                typer.echo("ðŸ’¾ No indicators cached yet.")
                typer.echo("   Use 'threadx indicators build' to cache indicators.")
                return

            total_size = cache_info.get("total_size_mb", 0.0)
            count = len(cache_info.get("cached_sets", []))

            typer.echo(f"\nðŸ’¾ Indicators Cache ({count} sets, {total_size:.2f} MB)\n")
            typer.echo(
                f"{'Symbol':<12} {'Timeframe':<10} {'Indicators':<30} {'Size (MB)':<12} {'Updated':<20}"
            )
            typer.echo("-" * 90)

            for cached_set in cache_info["cached_sets"]:
                symbol = cached_set.get("symbol", "N/A")
                tf = cached_set.get("timeframe", "N/A")
                indicators = ", ".join(cached_set.get("indicators", []))[:28]
                size = cached_set.get("size_mb", 0.0)
                updated = cached_set.get("updated", "N/A")

                typer.echo(
                    f"{symbol:<12} {tf:<10} {indicators:<30} {size:<12.2f} {updated:<20}"
                )

            typer.echo()

    except Exception as e:
        handle_bridge_error(e, json_mode)


if __name__ == "__main__":
    app()




----------------------------------------
Fichier: cli\main.py
"""
ThreadX CLI - Main Entry Point
===============================

Main CLI application using Typer framework.
Provides commands for data, indicators, backtest, and optimization.

Usage:
    python -m threadx.cli --help
    python -m threadx.cli data validate <path>
    python -m threadx.cli backtest run --strategy <name>

Global Options:
    --json: Output results as JSON
    --debug: Enable debug logging
    --async: Enable async execution (experimental)

Author: ThreadX Framework
Version: Prompt 9 - CLI Main Application
"""

import logging
import sys

import typer

from . import backtest_cmd, data_cmd, indicators_cmd, optimize_cmd
from .utils import setup_logger

# Create main Typer app
app = typer.Typer(
    name="threadx",
    help="ThreadX - GPU-Accelerated Backtesting Framework CLI",
    add_completion=False,
)

# Add subcommands
app.add_typer(data_cmd.app, name="data")
app.add_typer(indicators_cmd.app, name="indicators")
app.add_typer(backtest_cmd.app, name="backtest")
app.add_typer(optimize_cmd.app, name="optimize")

# Global logger
logger = logging.getLogger("threadx.cli")


@app.callback()
def main(
    ctx: typer.Context,
    json_output: bool = typer.Option(
        False,
        "--json",
        help="Output results as JSON instead of human-readable text",
    ),
    debug: bool = typer.Option(
        False, "--debug", help="Enable debug logging (verbose output)"
    ),
    async_mode: bool = typer.Option(
        False,
        "--async",
        help="Enable async execution mode (experimental)",
    ),
) -> None:
    """
    ThreadX CLI - GPU-Accelerated Backtesting Framework.

    Command-line interface for running backtests, building indicators,
    validating datasets, and optimizing strategy parameters.

    All commands use ThreadXBridge for async execution and are
    compatible with the Dash UI (same backend).

    Examples:
        # Validate dataset
        threadx data validate ./data/btc_1d.csv

        # Build indicators
        threadx indicators build --symbol BTCUSDT --tf 1h

        # Run backtest
        threadx backtest run --strategy ema_crossover --symbol ETHUSDT

        # Optimize parameters
        threadx optimize sweep --strategy bollinger --param period \\
            --min 10 --max 40 --step 5
    """
    # Setup logging level
    log_level = logging.DEBUG if debug else logging.INFO
    setup_logger(log_level)

    # Store options in context for subcommands
    ctx.ensure_object(dict)
    ctx.obj["json"] = json_output
    ctx.obj["debug"] = debug
    ctx.obj["async"] = async_mode

    logger.debug(
        f"CLI initialized: json={json_output}, debug={debug}, " f"async={async_mode}"
    )

    if async_mode:
        logger.warning("Async mode is experimental and may not work with all commands")


@app.command()
def version(
    ctx: typer.Context,
) -> None:
    """Display ThreadX CLI version information."""
    py_ver = sys.version_info
    version_info = {
        "threadx_cli": "1.0.0",
        "prompt": "P9 - CLI Bridge Interface",
        "python_version": f"{py_ver.major}.{py_ver.minor}.{py_ver.micro}",
    }

    json_mode = ctx.obj.get("json", False) if ctx.obj else False

    if json_mode:
        import json

        print(json.dumps(version_info, indent=2))
    else:
        typer.echo(f"ThreadX CLI v{version_info['threadx_cli']}")
        typer.echo(f"Prompt: {version_info['prompt']}")
        typer.echo(f"Python: {version_info['python_version']}")


def cli_entry() -> None:
    """
    Entry point for CLI when installed via pip/setuptools.

    Allows usage via `threadx` command instead of `python -m threadx.cli`.
    """
    app()


if __name__ == "__main__":
    app()




----------------------------------------
Fichier: cli\optimize_cmd.py
"""
ThreadX CLI - Optimization Commands
====================================

Commands for parameter optimization (sweeps) and analysis.

Usage:
    python -m threadx.cli optimize sweep --strategy bollinger --param period

Author: ThreadX Framework
Version: Prompt 9 - Optimization Commands
"""

import logging
from typing import Optional

import typer

from threadx.cli.utils import (
    async_runner,
    format_duration,
    handle_bridge_error,
    print_json,
    print_summary,
)

app = typer.Typer(help="Parameter optimization and sweep commands")
logger = logging.getLogger("threadx.cli.optimize")


@app.command()
def sweep(
    strategy: str = typer.Option(..., help="Strategy name"),
    symbol: str = typer.Option(..., help="Symbol to optimize"),
    timeframe: str = typer.Option("1h", "--tf", help="Timeframe"),
    param: str = typer.Option(..., help="Parameter to sweep (period, std, etc.)"),
    min_value: float = typer.Option(..., "--min", help="Min parameter value"),
    max_value: float = typer.Option(..., "--max", help="Max parameter value"),
    step: float = typer.Option(1.0, help="Step size"),
    metric: str = typer.Option("sharpe_ratio", help="Optimization metric"),
    start_date: Optional[str] = typer.Option(None, help="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = typer.Option(None, help="End date (YYYY-MM-DD)"),
    top_n: int = typer.Option(10, help="Number of top results to show"),
) -> None:
    """
    Run parameter sweep optimization for strategy.

    Tests multiple parameter values, ranks by metric
    (Sharpe ratio, total return, profit factor),
    and displays optimal parameters.

    Args:
        strategy: Strategy name to optimize.
        symbol: Symbol to test on.
        timeframe: Timeframe (1m, 5m, 1h, 1d).
        param: Parameter to sweep (e.g., period, std).
        min_value: Minimum parameter value.
        max_value: Maximum parameter value.
        step: Step size for sweep.
        metric: Metric to optimize (sharpe_ratio, total_return, etc.).
        start_date: Start date for backtest period.
        end_date: End date for backtest period.
        top_n: Number of top results to display.
    """
    try:
        from threadx.bridge import ThreadXBridge
    except ImportError as e:
        logger.error(f"Failed to import ThreadXBridge: {e}")
        typer.echo("âŒ Bridge not available. Check installation.")
        raise typer.Exit(1)

    ctx = typer.Context.get_current()
    json_mode = ctx.obj.get("json", False) if ctx.obj else False

    logger.info(
        f"Running sweep: {strategy} on {symbol}, " f"{param}=[{min_value}, {max_value}]"
    )

    try:
        # Initialize Bridge
        bridge = ThreadXBridge()

        # Prepare sweep request
        request = {
            "strategy": strategy,
            "symbol": symbol,
            "timeframe": timeframe,
            "param_name": param,
            "param_range": {
                "min": min_value,
                "max": max_value,
                "step": step,
            },
            "metric": metric,
        }

        # Add optional dates
        if start_date:
            request["start_date"] = start_date
        if end_date:
            request["end_date"] = end_date

        # Submit async sweep
        task_id = bridge.run_sweep_async(request)
        logger.debug(f"Sweep task submitted: {task_id}")

        if not json_mode:
            num_tests = int((max_value - min_value) / step) + 1
            typer.echo(
                f"â³ Running optimization sweep: {strategy} on "
                f"{symbol} ({timeframe})"
            )
            typer.echo(
                f"   Testing {num_tests} values of '{param}' "
                f"from {min_value} to {max_value}..."
            )

        # Poll for results
        event = async_runner(bridge.get_event, task_id, timeout=600.0)

        if event is None:
            error_msg = "Optimization sweep timed out"
            if json_mode:
                print_json({"status": "timeout", "task_id": task_id})
            else:
                typer.echo(f"âš ï¸  {error_msg}")
            raise typer.Exit(1)

        # Check status
        if event.get("status") == "error":
            error = Exception(event.get("error", "Unknown error"))
            handle_bridge_error(error, json_mode)

        # Extract result
        result = event.get("result", {})
        duration = event.get("duration", 0)

        # Extract top results
        top_results = result.get("top_results", [])[:top_n]

        # Prepare summary
        summary = {
            "strategy": strategy,
            "symbol": symbol,
            "timeframe": timeframe,
            "parameter": param,
            "range": f"[{min_value}, {max_value}] (step={step})",
            "tests_run": result.get("tests_run", 0),
            "optimization_metric": metric,
            "execution_time": format_duration(duration),
        }

        # Add best result
        if top_results:
            best = top_results[0]
            summary["best_param_value"] = best.get("param_value")
            summary[f"best_{metric}"] = best.get(metric)

        # Output
        if json_mode:
            output_data = {
                "status": "success",
                "summary": summary,
                "top_results": top_results,
                "heatmap_data": result.get("heatmap_data", []),
            }
            print_json(output_data)
        else:
            print_summary("Optimization Sweep Results", summary)

            # Show top N results
            if top_results:
                typer.echo(
                    f"ðŸ† Top {len(top_results)} Results " f"(ranked by {metric}):\n"
                )
                typer.echo(
                    f"{'Rank':<6} {param.title():<15} "
                    f"{metric.replace('_', ' ').title():<20} "
                    f"{'Total Return':<15} {'Win Rate':<10}"
                )
                typer.echo("-" * 70)

                for i, res in enumerate(top_results, 1):
                    param_val = res.get("param_value", 0)
                    metric_val = res.get(metric, 0)
                    total_ret = res.get("total_return", 0) * 100
                    win_rate = res.get("win_rate", 0) * 100

                    metric_str = (
                        f"{metric_val:.4f}"
                        if isinstance(metric_val, float)
                        else str(metric_val)
                    )

                    typer.echo(
                        f"{i:<6} {param_val:<15} "
                        f"{metric_str:<20} "
                        f"{total_ret:>6.2f}%{'':<8} "
                        f"{win_rate:>5.1f}%"
                    )

                typer.echo()

    except Exception as e:
        handle_bridge_error(e, json_mode)


if __name__ == "__main__":
    app()




----------------------------------------
Fichier: cli\utils.py
"""
ThreadX CLI Utilities
=====================

Shared helper functions for CLI commands:
- Logging setup
- JSON formatting
- Async polling wrapper
- Error handling

Author: ThreadX Framework
Version: Prompt 9 - CLI Utilities
"""

import json
import logging
import sys
import time
from typing import Any, Callable, Dict, Optional

# Configure logger for CLI module
logger = logging.getLogger("threadx.cli")


def setup_logger(level: int = logging.INFO) -> None:
    """
    Configure logging for CLI with consistent format.

    Args:
        level: Logging level (logging.DEBUG, INFO, WARNING, etc.)
    """
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    logger.setLevel(level)
    logger.debug(f"Logger initialized at level {logging.getLevelName(level)}")


def print_json(data: Dict[str, Any], indent: int = 2) -> None:
    """
    Print data as formatted JSON to stdout.

    Args:
        data: Dictionary to print as JSON.
        indent: Number of spaces for indentation (default: 2).
    """
    try:
        json_str = json.dumps(data, indent=indent, default=str)
        print(json_str)
    except (TypeError, ValueError) as e:
        logger.error(f"Failed to serialize JSON: {e}")
        print(json.dumps({"error": "JSON serialization failed", "detail": str(e)}))


def async_runner(
    func: Callable,
    task_id: str,
    timeout: float = 60.0,
    poll_interval: float = 0.5,
) -> Optional[Dict[str, Any]]:
    """
    Poll async function results with timeout.

    Wrapper for Bridge.get_event() that polls until result or timeout.
    Non-blocking polling loop with configurable interval.

    Args:
        func: Callable that takes task_id (e.g., bridge.get_event).
        task_id: Task ID to poll for.
        timeout: Maximum time to wait in seconds (default: 60s).
        poll_interval: Time between polls in seconds (default: 0.5s).

    Returns:
        Event dict if found, None if timeout.

    Example:
        >>> bridge = ThreadXBridge()
        >>> task_id = bridge.run_backtest_async(request)
        >>> result = async_runner(bridge.get_event, task_id)
    """
    logger.debug(
        f"Polling task {task_id} (timeout={timeout}s, interval={poll_interval}s)"
    )

    start_time = time.time()
    attempts = 0

    try:
        while (time.time() - start_time) < timeout:
            attempts += 1

            try:
                event = func(task_id, timeout=poll_interval)

                if event is not None:
                    elapsed = time.time() - start_time
                    logger.debug(
                        f"Task {task_id} completed after {attempts} attempts "
                        f"({elapsed:.2f}s)"
                    )
                    return event

                # No event yet, continue polling
                time.sleep(poll_interval)

            except KeyboardInterrupt:
                # FIX A3: Propagate pour cleanup externe
                logger.warning("Polling interrupted by user (Ctrl+C)")
                raise
            except Exception as e:
                logger.error(f"Error polling task {task_id}: {e}")
                return {"status": "error", "error": str(e)}
    finally:
        # FIX A3: Garantir cleanup mÃªme si exception
        logger.debug(f"Polling loop exited for task {task_id}")

    # Timeout reached
    logger.warning(f"Task {task_id} timed out after {timeout}s ({attempts} attempts)")
    return {"status": "timeout", "task_id": task_id, "timeout": timeout}


def format_duration(seconds: float) -> str:
    """
    Format duration in seconds to human-readable string.

    Args:
        seconds: Duration in seconds.

    Returns:
        Formatted string (e.g., "1m 23.4s", "45.2s").
    """
    if seconds < 60:
        return f"{seconds:.1f}s"

    minutes = int(seconds // 60)
    remaining_seconds = seconds % 60
    return f"{minutes}m {remaining_seconds:.1f}s"


def print_summary(title: str, data: Dict[str, Any], json_mode: bool = False) -> None:
    """
    Print command result summary (text or JSON).

    Args:
        title: Summary title (e.g., "Backtest Results").
        data: Result data dictionary.
        json_mode: If True, output JSON; else human-readable text.
    """
    if json_mode:
        print_json({"title": title, "data": data})
    else:
        print(f"\n{'=' * 60}")
        print(f"  {title}")
        print("=" * 60)

        for key, value in data.items():
            # Format key (snake_case â†’ Title Case)
            formatted_key = key.replace("_", " ").title()

            # Format value
            if isinstance(value, float):
                formatted_value = f"{value:.4f}"
            elif isinstance(value, dict):
                formatted_value = json.dumps(value, indent=2)
            else:
                formatted_value = str(value)

            print(f"  {formatted_key:<25} {formatted_value}")

        print("=" * 60 + "\n")


def handle_bridge_error(error: Exception, json_mode: bool = False) -> None:
    """
    Handle Bridge errors with consistent formatting.

    Args:
        error: Exception raised by Bridge.
        json_mode: If True, output JSON error; else text.
    """
    error_data = {
        "status": "error",
        "type": type(error).__name__,
        "message": str(error),
    }

    if json_mode:
        print_json(error_data)
    else:
        logger.error(f"Bridge error: {error}")
        print(f"\nâŒ Error: {error}\n")

    sys.exit(1)




----------------------------------------
Fichier: cli\__init__.py
"""
ThreadX CLI Module
==================

Command-line interface for ThreadX backtesting framework.
Provides async commands for data validation, indicator building,
backtesting, and parameter optimization via ThreadXBridge.

Usage:
    python -m threadx.cli --help
    python -m threadx.cli data validate <path>
    python -m threadx.cli backtest run --strategy <name>
    python -m threadx.cli optimize sweep --param <name>

Author: ThreadX Framework
Version: Prompt 9 - CLI Bridge Interface
"""

from .main import app

__all__ = ["app"]




----------------------------------------
Fichier: cli\__main__.py
from .main import app

if __name__ == "__main__":
    app()




----------------------------------------
Fichier: config\paths.py

----------------------------------------
Fichier: configuration\auth.py

----------------------------------------
Fichier: configuration\errors.py
"""Configuration-related exceptions for ThreadX."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional


@dataclass
class ConfigurationError(Exception):
    path: Optional[str]
    reason: str
    details: Optional[str] = None

    def __post_init__(self) -> None:  # pragma: no cover - dataclass validation trivial
        super().__init__(self.reason)

    @property
    def user_message(self) -> str:
        location = f" (file: {self.path})" if self.path else ""
        return f"Configuration error{location}: {self.reason}"

    def __str__(self) -> str:
        message = self.user_message
        if self.details:
            message = f"{message}\n{self.details}"
        return message


class PathValidationError(Exception):
    """Raised when configuration paths do not pass validation."""

    def __init__(self, message: str):
        super().__init__(message)




----------------------------------------
Fichier: configuration\loaders.py
"""TOML configuration loader for ThreadX."""
# type: ignore  # Trop d'erreurs de type, analyse dÃ©sactivÃ©e

from __future__ import annotations

import argparse
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union

try:  # pragma: no cover - fallback for Python <3.11
    import tomllib
except ImportError:  # pragma: no cover - fallback path
    import tomli as tomllib

from .errors import ConfigurationError, PathValidationError
from .settings import DEFAULT_SETTINGS, Settings

logger = logging.getLogger(__name__)


def load_config_dict(path: Union[str, Path]) -> Dict[str, Any]:
    config_path = Path(path)
    try:
        with config_path.open("rb") as handle:
            return tomllib.load(handle)
    except FileNotFoundError as exc:
        raise ConfigurationError(
            str(config_path), "Configuration file not found"
        ) from exc
    except tomllib.TOMLDecodeError as exc:
        raise ConfigurationError(
            str(config_path), "Invalid TOML syntax", details=str(exc)
        ) from exc
    except Exception as exc:  # pragma: no cover - defensive
        raise ConfigurationError(
            str(config_path), "Unexpected error while loading config", details=str(exc)
        ) from exc


class TOMLConfigLoader:
    """Load and validate ThreadX configuration files."""

    DEFAULT_CONFIG_NAME = "paths.toml"

    def __init__(self, config_path: Optional[Union[str, Path]] = None):
        self.config_path = self._resolve_config_path(config_path)
        self.config_data = load_config_dict(self.config_path)
        self._validated_paths: Dict[str, str] = {}

    # ------------------------------------------------------------------
    # Path resolution helpers
    # ------------------------------------------------------------------
    def _resolve_config_path(self, provided: Optional[Union[str, Path]]) -> Path:
        if provided:
            candidate = Path(provided)
            if candidate.exists():
                return candidate
            raise ConfigurationError(str(candidate), "Configuration file not found")

        search_paths = [
            Path.cwd() / self.DEFAULT_CONFIG_NAME,
            Path.cwd().parent / self.DEFAULT_CONFIG_NAME,
            Path(__file__).resolve().parents[2] / self.DEFAULT_CONFIG_NAME,
        ]

        for candidate in search_paths:
            if candidate.exists():
                return candidate

        searched = "\n".join(str(path) for path in search_paths)
        raise ConfigurationError(None, "Configuration file not found", details=searched)

    # ------------------------------------------------------------------
    # Access helpers
    # ------------------------------------------------------------------
    def get_section(self, name: str) -> Dict[str, Any]:
        return dict(self.config_data.get(name, {}))

    def get_value(self, section: str, key: str, default: Any = None) -> Any:
        return self.get_section(section).get(key, default)

    # ------------------------------------------------------------------
    # Validation
    # ------------------------------------------------------------------
    def validate_config(self) -> List[str]:
        errors: List[str] = []
        self._validated_paths.clear()
        required_sections = ["paths", "gpu", "performance", "trading"]
        for section in required_sections:
            if section not in self.config_data:
                errors.append(f"Missing required configuration section: {section}")

        errors.extend(self._validate_paths(check_only=True))
        errors.extend(self._validate_gpu_config(check_only=True))
        errors.extend(self._validate_performance_config(check_only=True))
        return errors

    def _validate_paths(self, check_only: bool = False) -> List[str]:
        errors: List[str] = []
        paths_section = self.get_section("paths")
        security = self.get_section("security")
        allow_abs = bool(security.get("allow_absolute_paths", False))

        data_root = paths_section.get("data_root", "./data")
        if not isinstance(data_root, str):
            errors.append("paths.data_root must be a string")
            data_root = "./data"

        resolved_paths: Dict[str, str] = {"data_root": data_root}

        for key, value in paths_section.items():
            if not isinstance(value, str):
                continue
            if not allow_abs and Path(value).is_absolute():
                errors.append(f"Absolute path not allowed for {key}: {value}")
                continue
            resolved_paths[key] = value.format(data_root=data_root)

        if not check_only:
            self._validated_paths.update(resolved_paths)

        return errors

    def _validate_gpu_config(self, check_only: bool = False) -> List[str]:
        errors: List[str] = []
        gpu_section = self.get_section("gpu")

        load_balance = gpu_section.get("load_balance", {})
        if isinstance(load_balance, dict) and load_balance:
            total = sum(load_balance.values())
            if not (0.99 <= total <= 1.01):
                errors.append("GPU load balance ratios must sum to 1.0")
        elif load_balance not in ({}, None):
            errors.append("gpu.load_balance must be a mapping of ratios")

        threshold = gpu_section.get("memory_threshold", 0.8)
        if not isinstance(threshold, (int, float)) or not (
            0.1 <= float(threshold) <= 1.0
        ):
            errors.append("gpu.memory_threshold must be between 0.1 and 1.0")

        return errors

    def _validate_performance_config(self, check_only: bool = False) -> List[str]:
        errors: List[str] = []
        perf_section = self.get_section("performance")
        for key in (
            "target_tasks_per_min",
            "vectorization_batch_size",
            "cache_ttl_sec",
            "max_workers",
        ):
            value = perf_section.get(key)
            if value is None:
                continue
            if not isinstance(value, (int, float)) or value <= 0:
                errors.append(f"performance.{key} must be a positive number")
        return errors

    # ------------------------------------------------------------------
    # Settings construction
    # ------------------------------------------------------------------
    def create_settings(self, **overrides: Any) -> Settings:
        errors = self.validate_config()
        if errors:
            raise ConfigurationError(
                str(self.config_path),
                "Invalid configuration",
                details="\n".join(errors),
            )

        self._validated_paths.clear()
        path_errors = self._validate_paths(check_only=False)
        if path_errors:
            raise PathValidationError("; ".join(path_errors))

        paths = self._validated_paths
        gpu = self.get_section("gpu")
        performance = self.get_section("performance")
        trading = self.get_section("trading")
        backtesting = self.get_section("backtesting")
        logging_section = self.get_section("logging")
        security = self.get_section("security")
        monte_carlo = self.get_section("monte_carlo")
        cache = self.get_section("cache")

        defaults = DEFAULT_SETTINGS
        return Settings(
            DATA_ROOT=overrides.get(
                "data_root", paths.get("data_root", defaults.DATA_ROOT)
            ),
            RAW_JSON=overrides.get(
                "raw_json", paths.get("raw_json", defaults.RAW_JSON)
            ),
            PROCESSED=overrides.get(
                "processed", paths.get("processed", defaults.PROCESSED)
            ),
            INDICATORS=overrides.get(
                "indicators", paths.get("indicators", defaults.INDICATORS)
            ),
            RUNS=overrides.get("runs", paths.get("runs", defaults.RUNS)),
            LOGS=overrides.get("logs", paths.get("logs", defaults.LOGS)),
            CACHE=overrides.get("cache", paths.get("cache", defaults.CACHE)),
            CONFIG=overrides.get("config", paths.get("config", defaults.CONFIG)),
            GPU_DEVICES=gpu.get("devices", defaults.GPU_DEVICES),
            LOAD_BALANCE=gpu.get("load_balance", defaults.LOAD_BALANCE),
            MEMORY_THRESHOLD=gpu.get("memory_threshold", defaults.MEMORY_THRESHOLD),
            AUTO_FALLBACK=gpu.get("auto_fallback", defaults.AUTO_FALLBACK),
            ENABLE_GPU=overrides.get(
                "enable_gpu", gpu.get("enable_gpu", defaults.ENABLE_GPU)
            ),
            TARGET_TASKS_PER_MIN=performance.get(
                "target_tasks_per_min", defaults.TARGET_TASKS_PER_MIN
            ),
            VECTORIZATION_BATCH_SIZE=performance.get(
                "vectorization_batch_size", defaults.VECTORIZATION_BATCH_SIZE
            ),
            CACHE_TTL_SEC=performance.get("cache_ttl_sec", defaults.CACHE_TTL_SEC),
            MAX_WORKERS=performance.get("max_workers", defaults.MAX_WORKERS),
            MEMORY_LIMIT_MB=performance.get(
                "memory_limit_mb", defaults.MEMORY_LIMIT_MB
            ),
            SUPPORTED_TF=tuple(
                trading.get("supported_timeframes", defaults.SUPPORTED_TF)
            ),
            DEFAULT_TIMEFRAME=trading.get(
                "default_timeframe", defaults.DEFAULT_TIMEFRAME
            ),
            BASE_CURRENCY=trading.get("base_currency", defaults.BASE_CURRENCY),
            FEE_RATE=trading.get("fee_rate", defaults.FEE_RATE),
            SLIPPAGE_RATE=trading.get("slippage_rate", defaults.SLIPPAGE_RATE),
            INITIAL_CAPITAL=backtesting.get(
                "initial_capital", defaults.INITIAL_CAPITAL
            ),
            MAX_POSITIONS=backtesting.get("max_positions", defaults.MAX_POSITIONS),
            POSITION_SIZE=backtesting.get("position_size", defaults.POSITION_SIZE),
            STOP_LOSS=backtesting.get("stop_loss", defaults.STOP_LOSS),
            TAKE_PROFIT=backtesting.get("take_profit", defaults.TAKE_PROFIT),
            LOG_LEVEL=overrides.get(
                "log_level", logging_section.get("level", defaults.LOG_LEVEL)
            ),
            MAX_FILE_SIZE_MB=logging_section.get(
                "max_file_size_mb", defaults.MAX_FILE_SIZE_MB
            ),
            MAX_FILES=logging_section.get("max_files", defaults.MAX_FILES),
            LOG_ROTATE=logging_section.get("log_rotate", defaults.LOG_ROTATE),
            LOG_FORMAT=logging_section.get("format", defaults.LOG_FORMAT),
            READ_ONLY_DATA=security.get("read_only_data", defaults.READ_ONLY_DATA),
            VALIDATE_PATHS=security.get("validate_paths", defaults.VALIDATE_PATHS),
            ALLOW_ABSOLUTE_PATHS=security.get(
                "allow_absolute_paths", defaults.ALLOW_ABSOLUTE_PATHS
            ),
            SECURITY_MAX_FILE_SIZE_MB=security.get(
                "max_file_size_mb", defaults.SECURITY_MAX_FILE_SIZE_MB
            ),
            DEFAULT_SIMULATIONS=monte_carlo.get(
                "default_simulations", defaults.DEFAULT_SIMULATIONS
            ),
            MAX_SIMULATIONS=monte_carlo.get(
                "max_simulations", defaults.MAX_SIMULATIONS
            ),
            DEFAULT_STEPS=monte_carlo.get("default_steps", defaults.DEFAULT_STEPS),
            MC_SEED=monte_carlo.get("seed", defaults.MC_SEED),
            CONFIDENCE_LEVELS=list(
                monte_carlo.get("confidence_levels", defaults.CONFIDENCE_LEVELS)
            ),
            CACHE_ENABLE=cache.get("enable", defaults.CACHE_ENABLE),
            CACHE_MAX_SIZE_MB=cache.get("max_size_mb", defaults.CACHE_MAX_SIZE_MB),
            CACHE_TTL_SECONDS=cache.get("ttl_seconds", defaults.CACHE_TTL_SECONDS),
            CACHE_COMPRESSION=cache.get("compression", defaults.CACHE_COMPRESSION),
            CACHE_STRATEGY=cache.get("strategy", defaults.CACHE_STRATEGY),
        )

    def load_config(self, cli_overrides: Optional[Dict[str, Any]] = None) -> Settings:
        overrides = cli_overrides or {}
        return self.create_settings(**overrides)

    # ------------------------------------------------------------------
    # CLI helpers
    # ------------------------------------------------------------------
    @staticmethod
    def create_cli_parser() -> argparse.ArgumentParser:
        parser = argparse.ArgumentParser(description="ThreadX configuration loader")
        parser.add_argument("--config", type=str, default=None)
        parser.add_argument("--data-root", dest="data_root", type=str)
        parser.add_argument("--log-level", dest="log_level", type=str)
        parser.add_argument("--enable-gpu", dest="enable_gpu", action="store_true")
        parser.add_argument("--disable-gpu", dest="disable_gpu", action="store_true")
        parser.add_argument("--print-config", action="store_true")
        return parser


_settings_cache: Optional[Settings] = None


def load_settings(
    config_path: Union[str, Path] = "paths.toml",
    cli_args: Optional[Sequence[str]] = None,
) -> Settings:
    parser = TOMLConfigLoader.create_cli_parser()
    args = (
        parser.parse_args(cli_args) if cli_args is not None else parser.parse_args([])
    )

    overrides: Dict[str, Any] = {}
    if args.data_root:
        overrides["data_root"] = args.data_root
    if args.log_level:
        overrides["log_level"] = args.log_level
    if args.enable_gpu:
        overrides["enable_gpu"] = True
    if args.disable_gpu:
        overrides["enable_gpu"] = False

    resolved_config = args.config or config_path
    loader = TOMLConfigLoader(resolved_config)
    settings = loader.create_settings(**overrides)

    if args.print_config:
        print_config(settings)

    return settings


def get_settings(force_reload: bool = False) -> Settings:
    global _settings_cache
    if _settings_cache is None or force_reload:
        _settings_cache = load_settings()
    return _settings_cache


def print_config(settings: Optional[Settings] = None) -> None:
    cfg = settings or get_settings()
    print("ThreadX Configuration")
    print("=" * 50)
    print("\n[PATHS]")
    print(f"Data Root: {cfg.DATA_ROOT}")
    print(f"Indicators: {cfg.INDICATORS}")
    print(f"Runs: {cfg.RUNS}")
    print(f"Logs: {cfg.LOGS}")

    print("\n[GPU]")
    print(f"Devices: {cfg.GPU_DEVICES}")
    print(f"Load Balance: {cfg.LOAD_BALANCE}")
    print(f"GPU Enabled: {cfg.ENABLE_GPU}")

    print("\n[PERFORMANCE]")
    print(f"Target Tasks/Min: {cfg.TARGET_TASKS_PER_MIN}")
    print(f"Max Workers: {cfg.MAX_WORKERS}")

    print("\n[TRADING]")
    print(f"Supported TF: {cfg.SUPPORTED_TF}")
    print(f"Default TF: {cfg.DEFAULT_TIMEFRAME}")

    print("\n[SECURITY]")
    print(f"Read Only: {cfg.READ_ONLY_DATA}")
    print(f"Validate Paths: {cfg.VALIDATE_PATHS}")


__all__ = [
    "ConfigurationError",
    "PathValidationError",
    "TOMLConfigLoader",
    "load_config_dict",
    "load_settings",
    "get_settings",
    "print_config",
]




----------------------------------------
Fichier: configuration\settings.py
"""Settings dataclass for ThreadX configuration."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List, Tuple


@dataclass(frozen=True)
class Settings:
    """Centralised application configuration."""

    # Paths
    DATA_ROOT: str = "./data"
    RAW_JSON: str = "{data_root}/raw/json"
    PROCESSED: str = "{data_root}/processed"
    INDICATORS: str = "{data_root}/indicators"
    RUNS: str = "{data_root}/runs"
    LOGS: str = "./logs"
    CACHE: str = "./cache"
    CONFIG: str = "./config"

    # GPU
    GPU_DEVICES: List[str] = field(default_factory=lambda: ["5090", "2060"])
    LOAD_BALANCE: Dict[str, float] = field(
        default_factory=lambda: {"5090": 0.75, "2060": 0.25}
    )
    MEMORY_THRESHOLD: float = 0.8
    AUTO_FALLBACK: bool = True
    ENABLE_GPU: bool = True

    # Performance
    TARGET_TASKS_PER_MIN: int = 2500
    VECTORIZATION_BATCH_SIZE: int = 10000
    CACHE_TTL_SEC: int = 3600
    MAX_WORKERS: int = 4
    MEMORY_LIMIT_MB: int = 8192

    # Trading
    SUPPORTED_TF: Tuple[str, ...] = (
        "1m",
        "3m",
        "5m",
        "15m",
        "30m",
        "1h",
        "2h",
        "4h",
        "6h",
        "8h",
        "12h",
        "1d",
    )
    DEFAULT_TIMEFRAME: str = "1h"
    BASE_CURRENCY: str = "USDC"
    FEE_RATE: float = 0.001
    SLIPPAGE_RATE: float = 0.0005

    # Backtesting
    INITIAL_CAPITAL: float = 10000.0
    MAX_POSITIONS: int = 10
    POSITION_SIZE: float = 0.1
    STOP_LOSS: float = 0.02
    TAKE_PROFIT: float = 0.04

    # Logging
    LOG_LEVEL: str = "INFO"
    MAX_FILE_SIZE_MB: int = 100
    MAX_FILES: int = 10
    LOG_ROTATE: bool = True
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # Security
    READ_ONLY_DATA: bool = True
    VALIDATE_PATHS: bool = True
    ALLOW_ABSOLUTE_PATHS: bool = False
    SECURITY_MAX_FILE_SIZE_MB: int = 1000

    # Monte Carlo
    DEFAULT_SIMULATIONS: int = 10000
    MAX_SIMULATIONS: int = 1000000
    DEFAULT_STEPS: int = 252
    MC_SEED: int = 42
    CONFIDENCE_LEVELS: List[float] = field(default_factory=lambda: [0.95, 0.99])

    # Cache
    CACHE_ENABLE: bool = True
    CACHE_MAX_SIZE_MB: int = 2048
    CACHE_TTL_SECONDS: int = 3600
    CACHE_COMPRESSION: bool = True
    CACHE_STRATEGY: str = "LRU"


DEFAULT_SETTINGS = Settings()

__all__ = ["Settings", "DEFAULT_SETTINGS"]

# Export global settings instance
# ----- Lazy singleton accessor (autocache) -----


def get_settings(_cache={"value": None}):
    """
    Retourne une instance Settings unique (mise en cache au niveau du process).
    Ã‰vite de rÃ©-instancier Settings Ã  chaque import.
    """
    if _cache["value"] is None:
        _cache["value"] = Settings()
    return _cache["value"]


S = get_settings()





----------------------------------------
Fichier: configuration\__init__.py
"""ThreadX configuration package."""

from .settings import Settings
from .errors import ConfigurationError, PathValidationError
from .loaders import (
    TOMLConfigLoader,
    load_config_dict,
    load_settings,
    get_settings,
    print_config,
)

__all__ = [
    "Settings",
    "ConfigurationError",
    "PathValidationError",
    "TOMLConfigLoader",
    "load_config_dict",
    "load_settings",
    "get_settings",
    "print_config",
]




----------------------------------------
Fichier: data\normalize.py
"""
ThreadX OHLCV Data Normalization Module
========================================

Handles normalization of OHLCV (Open, High, Low, Close, Volume) data from various formats.
Ensures consistent structure, timezone handling, and column naming.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

import pandas as pd

logger = logging.getLogger(__name__)


@dataclass
class NormalizationConfig:
    """Configuration for OHLCV normalization."""

    timezone: str = "UTC"
    required_columns: List[str] = field(default_factory=lambda: ["open", "high", "low", "close", "volume"])
    datetime_column: Optional[str] = None  # Auto-detect if None
    lowercase_columns: bool = True
    ensure_utc_index: bool = True
    sort_by_time: bool = True


@dataclass
class NormalizationReport:
    """Report of normalization results."""

    success: bool
    transformations: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)


# Default normalization configuration
DEFAULT_NORMALIZATION_CONFIG = NormalizationConfig(
    timezone="UTC",
    required_columns=["open", "high", "low", "close", "volume"],
    datetime_column=None,
    lowercase_columns=True,
    ensure_utc_index=True,
    sort_by_time=True,
)


def _detect_datetime_column(df: pd.DataFrame) -> Optional[str]:
    """
    Detect the datetime column in the dataframe.

    Looks for common naming patterns: 'time', 'timestamp', 'datetime', 'date'.

    Args:
        df: The dataframe to search

    Returns:
        The name of the datetime column or None if not found
    """
    candidates = ["time", "timestamp", "datetime", "date"]
    for col in df.columns:
        if col.lower() in candidates:
            return col
    return None


def _normalize_datetime_index(
    df: pd.DataFrame, datetime_col: Optional[str] = None, timezone: str = "UTC"
) -> tuple[pd.DataFrame, List[str]]:
    """
    Normalize the datetime index of the dataframe.

    Args:
        df: The dataframe to normalize
        datetime_col: The datetime column name (auto-detect if None)
        timezone: Target timezone (default: 'UTC')

    Returns:
        Tuple of (normalized_dataframe, transformation_list)
    """
    transformations = []

    # Detect datetime column if not provided
    if datetime_col is None:
        datetime_col = _detect_datetime_column(df)

    # Handle datetime column
    if datetime_col and datetime_col in df.columns:
        try:
            # Detect if timestamps are in milliseconds (> 1e10) or seconds
            sample_val = df[datetime_col].iloc[0] if len(df) > 0 else 0
            try:
                sample_num = float(sample_val)
                is_milliseconds = abs(sample_num) > 1e10
                unit = 'ms' if is_milliseconds else 's'
            except (ValueError, TypeError):
                unit = None  # Let pandas auto-detect

            df[datetime_col] = pd.to_datetime(df[datetime_col], utc=True, unit=unit, errors="coerce")
            df = df.set_index(datetime_col)
            unit_str = f" (unit={unit})" if unit else ""
            transformations.append(f"Set '{datetime_col}' as index with UTC timezone{unit_str}")
        except Exception as e:
            logger.warning(f"Failed to set datetime column '{datetime_col}': {e}")
    elif not isinstance(df.index, pd.DatetimeIndex):
        # Try to convert index to datetime
        try:
            # Detect if index values are in milliseconds
            sample_val = df.index[0] if len(df) > 0 else 0
            try:
                sample_num = float(sample_val)
                is_milliseconds = abs(sample_num) > 1e10
                unit = 'ms' if is_milliseconds else 's'
            except (ValueError, TypeError):
                unit = None

            df.index = pd.to_datetime(df.index, utc=True, unit=unit, errors="coerce")
            unit_str = f" (unit={unit})" if unit else ""
            transformations.append(f"Converted index to datetime with UTC timezone{unit_str}")
        except Exception as e:
            logger.warning(f"Failed to convert index to datetime: {e}")

    # Ensure UTC timezone
    if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is None:
        df.index = df.index.tz_localize("UTC")
        transformations.append("Localized datetime index to UTC")
    elif isinstance(df.index, pd.DatetimeIndex) and df.index.tz != timezone:
        df.index = df.index.tz_convert(timezone)
        transformations.append(f"Converted datetime index timezone from {df.index.tz} to {timezone}")

    return df, transformations


def _normalize_columns(df: pd.DataFrame, lowercase: bool = True) -> tuple[pd.DataFrame, List[str]]:
    """
    Normalize column names.

    Args:
        df: The dataframe to normalize
        lowercase: Convert column names to lowercase

    Returns:
        Tuple of (normalized_dataframe, transformation_list)
    """
    transformations = []

    if lowercase:
        original_columns = df.columns.tolist()
        df.columns = df.columns.str.lower()
        if df.columns.tolist() != original_columns:
            transformations.append("Converted column names to lowercase")

    return df, transformations


def _validate_required_columns(
    df: pd.DataFrame, required_columns: List[str]
) -> tuple[bool, List[str]]:
    """
    Validate that required columns are present.

    Args:
        df: The dataframe to validate
        required_columns: List of required column names

    Returns:
        Tuple of (all_present, missing_columns)
    """
    missing = [col for col in required_columns if col not in df.columns]
    return len(missing) == 0, missing


def normalize_ohlcv(
    df: pd.DataFrame,
    config: Optional[NormalizationConfig] = None,
) -> tuple[pd.DataFrame, NormalizationReport]:
    """
    Normalize OHLCV data to a standard format.

    Args:
        df: Input dataframe (OHLCV data)
        config: Normalization configuration (uses DEFAULT if None)

    Returns:
        Tuple of (normalized_dataframe, NormalizationReport)
    """
    if config is None:
        config = DEFAULT_NORMALIZATION_CONFIG

    report = NormalizationReport(success=True)
    df = df.copy()

    # Step 1: Normalize datetime index
    try:
        df, dt_transforms = _normalize_datetime_index(df, config.datetime_column, config.timezone)
        report.transformations.extend(dt_transforms)
    except Exception as e:
        report.errors.append(f"Datetime normalization failed: {e}")
        report.success = False

    # Step 2: Normalize column names
    try:
        df, col_transforms = _normalize_columns(df, config.lowercase_columns)
        report.transformations.extend(col_transforms)
    except Exception as e:
        report.errors.append(f"Column normalization failed: {e}")
        report.success = False

    # Step 3: Validate required columns
    all_present, missing = _validate_required_columns(df, config.required_columns)
    if not all_present:
        msg = f"Missing required columns: {', '.join(missing)}"
        report.errors.append(msg)
        report.success = False
    else:
        report.transformations.append(f"Validated presence of required columns: {', '.join(config.required_columns)}")

    # Step 4: Sort by time (optional)
    if config.sort_by_time and isinstance(df.index, pd.DatetimeIndex):
        try:
            df = df.sort_index()
            report.transformations.append("Sorted data by datetime index")
        except Exception as e:
            report.warnings.append(f"Failed to sort by datetime: {e}")

    return df, report

----------------------------------------
Fichier: data\schemas.py
"""
ThreadX Data Schemas
====================

Central location for shared data schemas and configurations.
"""

# Re-export normalization configuration for backward compatibility
from threadx.data.normalize import DEFAULT_NORMALIZATION_CONFIG, NormalizationConfig, NormalizationReport

__all__ = [
    "DEFAULT_NORMALIZATION_CONFIG",
    "NormalizationConfig",
    "NormalizationReport",
]

----------------------------------------
Fichier: data\validate.py
from ..dataset.validate import *


----------------------------------------
Fichier: data\__init__.py




----------------------------------------
Fichier: dataset\validate.py
"""Data validation utilities for ThreadX.

Provides `validate_dataset(path)` which returns a small report dict:
  { ok: bool, errors: list[str], type: 'candle'|'indicator'|'unknown', convertible: bool }

If pandera is installed it will be used for stricter validation. Otherwise a lightweight
fallback checks presence of OHLCV columns and numeric types.
"""

from pathlib import Path
from typing import Dict, Any
import pandas as pd

try:
    import pandera as pa
    from pandera import Column, DataFrameSchema

    PANDERA_AVAILABLE = True
except Exception:
    PANDERA_AVAILABLE = False


OHLCV = ["open", "high", "low", "close", "volume"]


def _basic_check(df: pd.DataFrame) -> Dict[str, Any]:
    cols = [c.lower() for c in df.columns]
    has_ohlcv = all(c in cols for c in OHLCV)
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    return {
        "has_ohlcv": has_ohlcv,
        "numeric_columns": numeric_cols,
        "rows": len(df),
    }


def validate_dataset(path: str) -> Dict[str, Any]:
    """Validate dataset at path. Returns report dict.

    path may point to a parquet/json/csv file or a directory; when directory is provided
    the function inspects files inside and returns aggregated result (first successful file).
    """
    p = Path(path)
    report = {"ok": False, "errors": [], "type": "unknown", "convertible": False}

    candidates = []
    if p.is_dir():
        for f in sorted(p.iterdir()):
            if f.is_file() and f.suffix.lower() in (".parquet", ".json", ".csv"):
                candidates.append(f)
    elif p.is_file():
        candidates = [p]
    else:
        report["errors"].append(f"Path not found: {p}")
        return report

    for f in candidates:
        try:
            if f.suffix.lower() == ".parquet":
                df = pd.read_parquet(f)
            elif f.suffix.lower() == ".json":
                df = pd.read_json(f, orient="records", convert_dates=False)
            elif f.suffix.lower() == ".csv":
                df = pd.read_csv(f)
            else:
                continue

            basic = _basic_check(df)

            # Use pandera if available and if candidate looks like OHLCV
            if PANDERA_AVAILABLE and basic["has_ohlcv"]:
                schema = DataFrameSchema(
                    {
                        "open": Column(float, nullable=False),
                        "high": Column(float, nullable=False),
                        "low": Column(float, nullable=False),
                        "close": Column(float, nullable=False),
                        "volume": Column(float, nullable=True),
                    }
                )
                try:
                    schema.validate(df, lazy=True)
                    report.update({"ok": True, "type": "candle", "convertible": False})
                    return report
                except pa.errors.SchemaErrors as e:
                    report["errors"].append(str(e))
                    # fallthrough to basic handling

            # Basic handling: if OHLCV present -> candle
            if basic["has_ohlcv"]:
                report.update({"ok": True, "type": "candle", "convertible": False})
                return report

            # If numeric columns exist it's an indicator-like file
            if len(basic["numeric_columns"]) > 0 and basic["rows"] >= 1:
                report.update({"ok": True, "type": "indicator", "convertible": True})
                return report

        except Exception as exc:
            report["errors"].append(f"Failed reading {f.name}: {exc}")

    # If reached here no suitable file found
    report["ok"] = False
    if not report["errors"]:
        report["errors"].append("No supported data files found")
    return report


----------------------------------------
Fichier: gpu\device_manager.py
"""
ThreadX Device Manager - GPU Detection & Management
===================================================

Gestion unified des devices GPU/CPU avec dÃ©tection automatique et mapping
des noms conviviaux ("5090", "2060") vers les IDs CuPy.

Architecture:
- DÃ©tection multi-GPU via CuPy avec fallback NumPy
- Mapping nom â†” ID pour RTX 5090, RTX 2060, etc.
- VÃ©rification NCCL pour synchronisation multi-GPU
- Interface unifiÃ©e xp() pour code device-agnostic
"""

import os
import logging
from dataclasses import dataclass
from typing import Dict, List, Optional, Union, Any
from pathlib import Path

from threadx.utils.log import get_logger

logger = get_logger(__name__)

# Detection CuPy/GPU support
try:
    import cupy as cp

    CUPY_AVAILABLE = True
    logger.info("CuPy dÃ©tectÃ© - Support GPU activÃ©")
except ImportError:
    cp = None
    CUPY_AVAILABLE = False
    logger.info("CuPy indisponible - Fallback CPU NumPy")

import numpy as np


@dataclass(frozen=True)
class DeviceInfo:
    """
    Informations sur un device GPU.

    Attributes:
        device_id: ID CuPy du device (0, 1, etc.)
        name: Nom convivial ("5090", "2060", "cpu")
        full_name: Nom complet du GPU
        memory_total: MÃ©moire totale en bytes
        memory_free: MÃ©moire libre en bytes
        compute_capability: Version compute CUDA (ex. (8, 6))
        is_available: True si device utilisable
    """

    device_id: int
    name: str
    full_name: str
    memory_total: int
    memory_free: int
    compute_capability: tuple[int, int]
    is_available: bool

    @property
    def memory_total_gb(self) -> float:
        """MÃ©moire totale en Go."""
        return self.memory_total / (1024**3)

    @property
    def memory_free_gb(self) -> float:
        """MÃ©moire libre en Go."""
        return self.memory_free / (1024**3)

    @property
    def memory_used_pct(self) -> float:
        """Pourcentage mÃ©moire utilisÃ©e."""
        if self.memory_total == 0:
            return 0.0
        return ((self.memory_total - self.memory_free) / self.memory_total) * 100


def _parse_gpu_name(gpu_name: str) -> str:
    """
    Extrait un nom convivial depuis le nom complet GPU.

    Args:
        gpu_name: Nom complet GPU (ex. "NVIDIA GeForce RTX 5090")

    Returns:
        Nom convivial (ex. "5090")
    """
    # Patterns courants pour RTX series
    gpu_name_upper = gpu_name.upper()

    if "RTX 5080" in gpu_name_upper:
        return "5080"
    elif "RTX 4090" in gpu_name_upper:
        return "4090"
    elif "RTX 4080" in gpu_name_upper:
        return "4080"
    elif "RTX 3090" in gpu_name_upper:
        return "3090"
    elif "RTX 3080" in gpu_name_upper:
        return "3080"
    elif "RTX 2080" in gpu_name_upper:
        return "2080"
    elif "RTX 2070" in gpu_name_upper:
        return "2070"
    elif "RTX 2060" in gpu_name_upper:
        return "2060"
    elif "GTX 1080" in gpu_name_upper:
        return "1080"
    elif "GTX 1070" in gpu_name_upper:
        return "1070"
    elif "GTX 1060" in gpu_name_upper:
        return "1060"

    # Fallback: prendre les derniers chiffres
    import re

    numbers = re.findall(r"\d+", gpu_name)
    if numbers:
        return numbers[-1]  # Dernier nombre trouvÃ©

    # Fallback ultime
    return gpu_name.split()[-1] if gpu_name else "unknown"


def is_available() -> bool:
    """
    VÃ©rifie si au moins un GPU est disponible.

    Returns:
        True si GPU(s) dÃ©tectÃ©(s), False sinon
    """
    if not CUPY_AVAILABLE:
        return False

    try:
        device_count = cp.cuda.runtime.getDeviceCount()
        return device_count > 0
    except Exception as e:
        logger.debug(f"Erreur dÃ©tection GPU: {e}")
        return False


def list_devices() -> List[DeviceInfo]:
    """
    Liste tous les devices disponibles (GPU + CPU fallback).

    Returns:
        Liste des DeviceInfo triÃ©s par performance dÃ©croissante

    Example:
        >>> devices = list_devices()
        >>> for dev in devices:
        ...     print(f"{dev.name}: {dev.memory_total_gb:.1f}GB")
        5090: 32.0GB
        2060: 6.0GB
        cpu: 0.0GB
    """
    devices = []

    if CUPY_AVAILABLE:
        try:
            device_count = cp.cuda.runtime.getDeviceCount() if cp else 0
            logger.info(f"DÃ©tection de {device_count} GPU(s)")

            for device_id in range(device_count):
                try:
                    if cp:
                        with cp.cuda.Device(device_id):
                            # PropriÃ©tÃ©s du device
                            props = cp.cuda.runtime.getDeviceProperties(device_id)

                            # MÃ©moire
                            mem_info = cp.cuda.runtime.memGetInfo()
                        memory_free = mem_info[0]
                        memory_total = mem_info[1]

                        # Nom convivial
                        full_name = props["name"].decode("utf-8")
                        friendly_name = _parse_gpu_name(full_name)

                        # Compute capability
                        compute_cap = (props["major"], props["minor"])

                        device_info = DeviceInfo(
                            device_id=device_id,
                            name=friendly_name,
                            full_name=full_name,
                            memory_total=memory_total,
                            memory_free=memory_free,
                            compute_capability=compute_cap,
                            is_available=True,
                        )

                        devices.append(device_info)
                        logger.info(
                            f"GPU {device_id} ({friendly_name}): "
                            f"{device_info.memory_total_gb:.1f}GB, "
                            f"CC {compute_cap[0]}.{compute_cap[1]}"
                        )

                except Exception as e:
                    logger.warning(f"Erreur lecture GPU {device_id}: {e}")

        except Exception as e:
            logger.error(f"Erreur Ã©numÃ©ration GPU: {e}")

    # Ajout CPU comme fallback
    cpu_device = DeviceInfo(
        device_id=-1,
        name="cpu",
        full_name="CPU NumPy Fallback",
        memory_total=0,
        memory_free=0,
        compute_capability=(0, 0),
        is_available=True,
    )
    devices.append(cpu_device)

    # Tri par performance (mÃ©moire totale dÃ©croissante, CPU en dernier)
    devices.sort(key=lambda d: (d.device_id == -1, -d.memory_total))

    return devices


def get_device_by_name(name: str) -> Optional[DeviceInfo]:
    """
    RÃ©cupÃ¨re un device par son nom convivial.

    Args:
        name: Nom du device ("5090", "2060", "cpu")

    Returns:
        DeviceInfo si trouvÃ©, None sinon

    Example:
        >>> gpu = get_device_by_name("5090")
        >>> if gpu:
        ...     print(f"RTX 5090: {gpu.memory_total_gb:.1f}GB")
    """
    devices = list_devices()
    for device in devices:
        if device.name.lower() == name.lower():
            return device
    return None


def get_device_by_id(device_id: int) -> Optional[DeviceInfo]:
    """
    RÃ©cupÃ¨re un device par son ID CuPy.

    Args:
        device_id: ID CuPy du device (0, 1, etc.) ou -1 pour CPU

    Returns:
        DeviceInfo si trouvÃ©, None sinon
    """
    devices = list_devices()
    for device in devices:
        if device.device_id == device_id:
            return device
    return None


def check_nccl_support() -> bool:
    """
    VÃ©rifie si NCCL est disponible pour la synchronisation multi-GPU.

    Returns:
        True si NCCL utilisable, False sinon
    """
    if not CUPY_AVAILABLE:
        return False

    try:
        # Test import NCCL
        import cupy.cuda.nccl  # noqa: F401

        # Test basique
        device_count = cp.cuda.runtime.getDeviceCount()
        if device_count < 2:
            logger.debug("NCCL disponible mais <2 GPU dÃ©tectÃ©s")
            return False

        logger.info("Support NCCL activÃ© pour synchronisation multi-GPU")
        return True

    except (ImportError, AttributeError) as e:
        logger.debug(f"NCCL indisponible: {e}")
        return False


def xp(device_name: Optional[str] = None) -> Any:
    """
    Retourne le module array appropriÃ© (CuPy ou NumPy).

    Args:
        device_name: Nom du device optionnel ("5090", "cpu", etc.)
                    Si None, utilise GPU par dÃ©faut ou NumPy

    Returns:
        Module cupy ou numpy selon disponibilitÃ©

    Example:
        >>> # Code device-agnostic
        >>> xp_module = xp("5090")  # CuPy si 5090 disponible
        >>> arr = xp_module.array([1, 2, 3])
        >>> result = xp_module.sum(arr)
    """
    if device_name == "cpu":
        return np

    if not CUPY_AVAILABLE:
        return np

    if device_name:
        device = get_device_by_name(device_name)
        if not device or device.device_id == -1:
            return np

        # DÃ©finir device actuel
        try:
            if cp:
                cp.cuda.Device(device.device_id).use()
            return cp
        except Exception as e:
            logger.warning(f"Erreur activation device {device_name}: {e}")
            return np

    # GPU par dÃ©faut
    try:
        if cp and cp.cuda.runtime.getDeviceCount() > 0:
            return cp
    except Exception:
        pass

    return np


def get_memory_info(device_name: str) -> Dict[str, float]:
    """
    RÃ©cupÃ¨re les infos mÃ©moire pour un device.

    Args:
        device_name: Nom du device

    Returns:
        Dict avec 'total_gb', 'free_gb', 'used_pct'

    Raises:
        ValueError: Si device introuvable
    """
    device = get_device_by_name(device_name)
    if not device:
        raise ValueError(f"Device '{device_name}' non trouvÃ©")

    if device.device_id == -1:  # CPU
        return {"total_gb": 0.0, "free_gb": 0.0, "used_pct": 0.0}

    if not CUPY_AVAILABLE:
        raise ValueError("CuPy requis pour infos mÃ©moire GPU")

    try:
        if cp:
            with cp.cuda.Device(device.device_id):
                mem_info = cp.cuda.runtime.memGetInfo()
                return {
                    "free": mem_info[0] / (1024**3),  # GB
                    "total": mem_info[1] / (1024**3),  # GB
                    "used": (mem_info[1] - mem_info[0]) / (1024**3),  # GB
                }
        else:
            return {"free": 0.0, "total": 0.0, "used": 0.0}
            free_bytes = mem_info[0]
            total_bytes = mem_info[1]

            return {
                "total_gb": total_bytes / (1024**3),
                "free_gb": free_bytes / (1024**3),
                "used_pct": ((total_bytes - free_bytes) / total_bytes) * 100,
            }
    except Exception as e:
        logger.error(f"Erreur lecture mÃ©moire {device_name}: {e}")
        raise


# Export des exceptions CuPy si disponibles
if CUPY_AVAILABLE and cp:
    CudaMemoryError = cp.cuda.memory.OutOfMemoryError
    CudaRuntimeError = cp.cuda.runtime.CUDARuntimeError
else:
    # Fallback exceptions
    class CudaMemoryError(RuntimeError):
        """Exception pour erreurs mÃ©moire GPU (fallback)"""

        pass

    class CudaRuntimeError(RuntimeError):
        """Exception pour erreurs runtime GPU (fallback)"""

        pass




----------------------------------------
Fichier: gpu\multi_gpu.py
"""
ThreadX Multi-GPU Manager - Distribution de Charge
==================================================

Orchestration multi-GPU avec auto-balancing et synchronisation NCCL.

Architecture:
- Split proportionnel des donnÃ©es selon ratios configurables
- ExÃ©cution parallÃ¨le avec device pinning
- Synchronisation NCCL optionnelle
- Merge dÃ©terministe des rÃ©sultats
- Auto-profiling pour optimisation automatique des ratios
- Fallback CPU transparent

Flux principal:
    Split â†’ Compute â†’ Sync â†’ Merge

Usage:
    >>> manager = MultiGPUManager()  # Balance par dÃ©faut 5090:75%, 2060:25%
    >>> result = manager.distribute_workload(data, vectorized_func)
    >>> # Auto-optimisation
    >>> new_ratios = manager.profile_auto_balance()
    >>> manager.set_balance(new_ratios)
"""

import time
import threading
import json
import hashlib
import signal
import atexit
from datetime import datetime, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union, Callable, Any, Tuple
import numpy as np
import pandas as pd

from threadx.utils.log import get_logger
from threadx.config import get_settings

S = get_settings()  # Stub settings instance

from .device_manager import (
    is_available,
    list_devices,
    get_device_by_name,
    get_device_by_id,
    check_nccl_support,
    xp,
    DeviceInfo,
    CUPY_AVAILABLE,
)
from .profile_persistence import (
    get_multigpu_ratios,
    update_multigpu_ratios,
    is_profile_valid,
    PROFILES_DIR,
    MULTIGPU_RATIOS_FILE,
)

if CUPY_AVAILABLE:
    import cupy as cp

logger = get_logger(__name__)


# === Constantes de Configuration ===

# ðŸ†• Taille minimale de chunk pour GPU (optimisation saturation VRAM)
MIN_CHUNK_SIZE_GPU = 50_000  # Chunks < 50k â†’ warning sous-utilisation

# Timeout par dÃ©faut pour operations GPU
DEFAULT_GPU_TIMEOUT = 300  # 5 minutes


# === Exceptions SpÃ©cialisÃ©es ===


class MultiGPUError(RuntimeError):
    """Erreur base pour operations multi-GPU."""

    pass


class DeviceUnavailableError(MultiGPUError):
    """Device GPU demandÃ© indisponible."""

    pass


class GPUMemoryError(MultiGPUError):
    """Erreur mÃ©moire GPU (OOM)."""

    pass


class ShapeMismatchError(MultiGPUError):
    """Shapes/dtypes incohÃ©rents entre chunks."""

    pass


class NonVectorizableFunctionError(MultiGPUError):
    """Fonction non vectorisable ou incompatible."""

    pass


# === Classes utilitaires ===


@dataclass
class WorkloadChunk:
    """
    Chunk de donnÃ©es pour un device spÃ©cifique.

    Attributes:
        device_name: Nom du device ("5090", "2060", "cpu")
        data_slice: Slice des donnÃ©es pour ce chunk
        start_idx: Index de dÃ©but dans les donnÃ©es originales
        end_idx: Index de fin (exclusif)
        expected_size: Taille attendue du rÃ©sultat
    """

    device_name: str
    data_slice: slice
    start_idx: int
    end_idx: int
    expected_size: int

    def __len__(self) -> int:
        return self.end_idx - self.start_idx


@dataclass
class ComputeResult:
    """
    RÃ©sultat de calcul d'un chunk.

    Attributes:
        chunk: Chunk original
        result: RÃ©sultat du calcul
        compute_time: Temps de calcul en secondes
        device_memory_used: MÃ©moire utilisÃ©e (si disponible)
        error: Exception si erreur
    """

    chunk: WorkloadChunk
    result: Optional[Union[np.ndarray, pd.DataFrame]]
    compute_time: float
    device_memory_used: Optional[float] = None
    error: Optional[Exception] = None

    @property
    def success(self) -> bool:
        return self.error is None


# === Multi-GPU Manager Principal ===


class MultiGPUManager:
    """
    Gestionnaire multi-GPU avec distribution automatique et auto-balancing.

    GÃ¨re la rÃ©partition proportionnelle de workloads entre GPUs disponibles,
    avec fallback CPU transparent et optimisation automatique des ratios.

    Attributes:
        device_balance: Ratios de charge par device {"device_name": ratio}
        use_streams: Utilisation des CUDA streams par device
        nccl_enabled: Support synchronisation NCCL

    Example:
        >>> # Configuration par dÃ©faut (5090: 75%, 2060: 25%)
        >>> manager = MultiGPUManager()
        >>>
        >>> # Distribution d'un workload
        >>> data = np.random.randn(100000, 10)
        >>> result = manager.distribute_workload(data, lambda x: x.sum(axis=1))
        >>>
        >>> # Auto-optimisation
        >>> optimal_ratios = manager.profile_auto_balance(sample_size=50000)
        >>> manager.set_balance(optimal_ratios)
    """

    def __init__(
        self,
        device_balance: Optional[Dict[str, float]] = None,
        use_streams: bool = True,
        enable_nccl: bool = True,
    ):
        """
        Initialise le gestionnaire multi-GPU.

        Args:
            device_balance: Ratios personnalisÃ©s {"device": ratio}
                           Si None, utilise balance par dÃ©faut (5090:75%, 2060:25%)
            use_streams: Active les CUDA streams pour parallÃ©lisme
            enable_nccl: Active synchronisation NCCL si disponible
        """
        self.use_streams = use_streams
        self.nccl_enabled = enable_nccl and check_nccl_support()

        # DÃ©tection devices disponibles
        self.available_devices = list_devices()
        self._gpu_devices = [d for d in self.available_devices if d.device_id != -1]
        self._cpu_device = next(
            (d for d in self.available_devices if d.device_id == -1), None
        )

        logger.info(
            f"Multi-GPU Manager initialisÃ©: {len(self._gpu_devices)} GPU(s), "
            f"NCCL={'activÃ©' if self.nccl_enabled else 'dÃ©sactivÃ©'}"
        )

        # Configuration balance
        if device_balance is None:
            self.device_balance = self._get_default_balance()
        else:
            self.device_balance = device_balance.copy()

        self._validate_balance()

        # Streams GPU (crÃ©Ã©s Ã  la demande)
        self._device_streams: Dict[str, Any] = {}
        self._lock = threading.Lock()

        logger.info(f"Balance configurÃ©e: {self._format_balance()}")

    def _get_default_balance(self) -> Dict[str, float]:
        """Calcule la balance par dÃ©faut selon devices disponibles."""
        if not self._gpu_devices:
            return {"cpu": 1.0}

        balance = {}

        # Recherche des devices cibles
        gpu_5090 = get_device_by_name("5090")
        gpu_2060 = get_device_by_name("2060")

        if gpu_5090 and gpu_2060:
            # Configuration idÃ©ale RTX 5090 + RTX 2060
            balance["5090"] = 0.75
            balance["2060"] = 0.25
        elif gpu_5090:
            # Seulement RTX 5090
            balance["5090"] = 1.0
        elif gpu_2060:
            # Seulement RTX 2060
            balance["2060"] = 1.0
        else:
            # Autres GPUs: rÃ©partition uniforme
            gpu_count = len(self._gpu_devices)
            for device in self._gpu_devices:
                balance[device.name] = 1.0 / gpu_count

        return balance

    def _validate_balance(self) -> None:
        """Valide et normalise la balance."""
        if not self.device_balance:
            raise ValueError("Balance vide")

        # VÃ©rification ratios positifs
        for device, ratio in self.device_balance.items():
            if ratio <= 0:
                raise ValueError(
                    f"Ratio invalide pour {device}: {ratio} (doit Ãªtre > 0)"
                )

        # Normalisation (somme = 1.0)
        total = sum(self.device_balance.values())
        if abs(total - 1.0) > 1e-6:
            logger.info(f"Normalisation balance: somme {total:.6f} â†’ 1.0")
            for device in self.device_balance:
                self.device_balance[device] /= total

        # VÃ©rification devices disponibles
        for device in self.device_balance:
            if device != "cpu" and not get_device_by_name(device):
                logger.warning(f"Device '{device}' dans balance mais indisponible")

    def _format_balance(self) -> str:
        """Formate la balance pour logging."""
        parts = [f"{dev}:{ratio:.1%}" for dev, ratio in self.device_balance.items()]
        return ", ".join(parts)

    def set_balance(self, new_balance: Dict[str, float]) -> None:
        """
        Met Ã  jour la balance des devices.

        Args:
            new_balance: Nouveaux ratios {"device": ratio}
                        Sera automatiquement normalisÃ© (somme = 1.0)

        Raises:
            ValueError: Si balance invalide

        Example:
            >>> manager.set_balance({"5090": 0.8, "2060": 0.2})
        """
        old_balance = self.device_balance.copy()
        self.device_balance = new_balance.copy()

        try:
            self._validate_balance()
            logger.info(f"Balance mise Ã  jour: {self._format_balance()}")
        except Exception as e:
            self.device_balance = old_balance
            raise ValueError(f"Balance invalide: {e}")

    def _split_workload(
        self, data_size: int, batch_axis: int = 0
    ) -> List[WorkloadChunk]:
        """
        Split proportionnel des donnÃ©es selon balance.

        Args:
            data_size: Taille totale des donnÃ©es
            batch_axis: Axe de split (gÃ©nÃ©ralement 0)

        Returns:
            Liste des chunks avec indices corrects
        """
        if data_size == 0:
            return []

        chunks = []
        current_idx = 0

        # Calcul tailles thÃ©oriques
        device_names = list(self.device_balance.keys())
        theoretical_sizes = []

        for device_name in device_names[:-1]:  # Tous sauf le dernier
            ratio = self.device_balance[device_name]
            size = int(data_size * ratio)
            theoretical_sizes.append(size)

        # Le dernier rÃ©cupÃ¨re le rÃ©sidu
        remaining = data_size - sum(theoretical_sizes)
        theoretical_sizes.append(remaining)

        # CrÃ©ation des chunks
        for device_name, chunk_size in zip(device_names, theoretical_sizes):
            if chunk_size > 0:
                end_idx = current_idx + chunk_size

                chunk = WorkloadChunk(
                    device_name=device_name,
                    data_slice=slice(current_idx, end_idx),
                    start_idx=current_idx,
                    end_idx=end_idx,
                    expected_size=chunk_size,
                )

                # ðŸ†• Validation taille chunk pour GPU
                if device_name != "cpu" and chunk_size < MIN_CHUNK_SIZE_GPU:
                    logger.warning(
                        f"âš ï¸  Chunk GPU trop petit: {device_name} = {chunk_size:,} "
                        f"(min recommandÃ©: {MIN_CHUNK_SIZE_GPU:,}). "
                        f"Risque sous-utilisation VRAM."
                    )

                chunks.append(chunk)
                current_idx = end_idx

        # Validation
        total_processed = sum(len(chunk) for chunk in chunks)
        if total_processed != data_size:
            raise RuntimeError(f"Split invalide: {total_processed} != {data_size}")

        logger.debug(
            f"Split workload: {data_size} â†’ {[len(c) for c in chunks]} "
            f"pour {[c.device_name for c in chunks]}"
        )

        return chunks

    def _get_device_stream(self, device_name: str) -> Optional[Any]:
        """RÃ©cupÃ¨re ou crÃ©e un stream pour le device."""
        if not self.use_streams or device_name == "cpu" or not CUPY_AVAILABLE:
            return None

        with self._lock:
            if device_name not in self._device_streams:
                device = get_device_by_name(device_name)
                if device and device.device_id != -1:
                    try:
                        with cp.cuda.Device(device.device_id):
                            stream = cp.cuda.Stream()
                            self._device_streams[device_name] = stream
                            logger.debug(f"Stream crÃ©Ã© pour {device_name}")
                    except Exception as e:
                        logger.warning(f"Erreur crÃ©ation stream {device_name}: {e}")
                        return None

            return self._device_streams.get(device_name)

    def _compute_chunk(
        self,
        data: Union[np.ndarray, pd.DataFrame],
        chunk: WorkloadChunk,
        func: Callable,
        seed: int,
    ) -> ComputeResult:
        """
        Calcule un chunk sur son device assignÃ©.

        Args:
            data: DonnÃ©es complÃ¨tes
            chunk: Chunk Ã  traiter
            func: Fonction vectorielle Ã  appliquer
            seed: Seed pour reproductibilitÃ©

        Returns:
            ComputeResult avec rÃ©sultat ou erreur
        """
        start_time = time.time()
        device_memory_used = None

        try:
            # Extraction du chunk
            if isinstance(data, pd.DataFrame):
                chunk_data = data.iloc[chunk.data_slice]
            else:
                chunk_data = data[chunk.data_slice]

            if len(chunk_data) == 0:
                return ComputeResult(chunk, None, 0.0, error=ValueError("Chunk vide"))

            # Configuration device et seed
            if chunk.device_name == "cpu":
                np.random.seed(seed + chunk.start_idx)  # Seed unique par chunk
                xp_module = np
                device_data = chunk_data
            else:
                device = get_device_by_name(chunk.device_name)
                if not device or not CUPY_AVAILABLE:
                    raise DeviceUnavailableError(
                        f"Device {chunk.device_name} indisponible"
                    )

                with cp.cuda.Device(device.device_id):
                    # Seed GPU
                    cp.random.seed(seed + chunk.start_idx)

                    # Stream optionnel
                    stream = self._get_device_stream(chunk.device_name)

                    with cp.cuda.Stream.null if stream is None else stream:
                        # Transfert vers GPU
                        if isinstance(chunk_data, pd.DataFrame):
                            # DataFrame: conversion via numpy
                            numpy_data = chunk_data.values
                            device_data = cp.asarray(numpy_data)
                        else:
                            device_data = cp.asarray(chunk_data)

                        # MÃ©moire utilisÃ©e
                        try:
                            mem_info = cp.cuda.runtime.memGetInfo()
                            device_memory_used = (mem_info[1] - mem_info[0]) / (1024**3)
                        except:
                            pass

                        # Calcul
                        try:
                            result = func(device_data)
                        except Exception as e:
                            raise NonVectorizableFunctionError(
                                f"Fonction Ã©chouÃ©e sur {chunk.device_name}: {e}"
                            )

                        # Synchronisation si stream
                        if stream is not None:
                            stream.synchronize()

                        # Transfert retour CPU
                        if hasattr(result, "get"):  # CuPy array
                            result = result.get()

                        # Reconstruction DataFrame si nÃ©cessaire
                        if isinstance(chunk_data, pd.DataFrame):
                            if result.ndim == 1:
                                # RÃ©sultat 1D â†’ Series avec index original
                                result = pd.Series(result, index=chunk_data.index)
                            else:
                                # RÃ©sultat 2D â†’ DataFrame avec index original
                                result = pd.DataFrame(result, index=chunk_data.index)

            # Validation rÃ©sultat
            expected_len = len(chunk_data)
            if hasattr(result, "__len__") and len(result) != expected_len:
                raise ShapeMismatchError(
                    f"Longueur rÃ©sultat {len(result)} != attendue {expected_len}"
                )

            compute_time = time.time() - start_time

            logger.debug(
                f"Chunk {chunk.device_name}[{chunk.start_idx}:{chunk.end_idx}] "
                f"traitÃ© en {compute_time:.3f}s"
            )

            return ComputeResult(
                chunk=chunk,
                result=result,
                compute_time=compute_time,
                device_memory_used=device_memory_used,
            )

        except Exception as e:
            return ComputeResult(
                chunk=chunk, result=None, compute_time=time.time() - start_time, error=e
            )

    def _merge_results(
        self,
        results: List[ComputeResult],
        original_data: Union[np.ndarray, pd.DataFrame],
        batch_axis: int = 0,
    ) -> Union[np.ndarray, pd.DataFrame]:
        """
        Merge dÃ©terministe des rÃ©sultats par ordre des chunks.

        Args:
            results: RÃ©sultats des chunks (ordre important)
            original_data: DonnÃ©es originales pour type de rÃ©fÃ©rence
            batch_axis: Axe de concatÃ©nation

        Returns:
            RÃ©sultat merged du mÃªme type que original_data
        """
        if not results:
            if isinstance(original_data, pd.DataFrame):
                return pd.DataFrame()
            else:
                return np.array([])

        # Tri par start_idx pour ordre dÃ©terministe
        results.sort(key=lambda r: r.chunk.start_idx)

        # Extraction des rÃ©sultats valides
        valid_results = []
        for result in results:
            if not result.success:
                raise RuntimeError(
                    f"Erreur chunk {result.chunk.device_name}: {result.error}"
                )
            if result.result is not None:
                valid_results.append(result.result)

        if not valid_results:
            if isinstance(original_data, pd.DataFrame):
                return pd.DataFrame()
            else:
                return np.array([])

        # Merge selon type
        if isinstance(original_data, pd.DataFrame):
            if isinstance(valid_results[0], pd.Series):
                # ConcatÃ©nation de Series
                merged = pd.concat(valid_results, axis=0)
            else:
                # ConcatÃ©nation de DataFrames
                merged = pd.concat(valid_results, axis=batch_axis)
        else:
            # ConcatÃ©nation numpy
            merged = np.concatenate(valid_results, axis=batch_axis)

        return merged

    def distribute_workload(
        self,
        data: Union[np.ndarray, pd.DataFrame],
        func: Callable,
        *,
        stream_per_gpu: bool = False,
        batch_axis: int = 0,
        seed: int = 42,
    ) -> Union[np.ndarray, pd.DataFrame]:
        """
        Distribue un workload sur les devices selon balance configurÃ©e.

        Architecture: Split â†’ Compute â†’ Sync â†’ Merge

        Args:
            data: DonnÃ©es Ã  traiter (numpy array ou pandas DataFrame)
            func: Fonction vectorielle pure (mÃªme input/output shape)
            stream_per_gpu: Force un stream par GPU (dÃ©faut: auto)
            batch_axis: Axe de split/merge (dÃ©faut: 0)
            seed: Seed pour reproductibilitÃ© dÃ©terministe

        Returns:
            RÃ©sultat merged du mÃªme type que data

        Raises:
            DeviceUnavailableError: Device requis indisponible
            GPUMemoryError: MÃ©moire GPU insuffisante
            ShapeMismatchError: RÃ©sultats incohÃ©rents
            NonVectorizableFunctionError: Fonction non compatible

        Example:
            >>> data = np.random.randn(100000, 50)
            >>> result = manager.distribute_workload(
            ...     data,
            ...     lambda x: x.sum(axis=1),
            ...     seed=42
            ... )
            >>> assert result.shape == (100000,)
        """
        if stream_per_gpu:
            self.use_streams = True

        start_time = time.time()
        data_size = len(data)

        logger.info(
            f"Distribution workload: {data_size} Ã©chantillons, "
            f"fonction {func.__name__ if hasattr(func, '__name__') else 'lambda'}, "
            f"seed={seed}"
        )

        # Cas trivial
        if data_size == 0:
            return data.copy() if hasattr(data, "copy") else data

        # Split en chunks
        chunks = self._split_workload(data_size, batch_axis)

        if len(chunks) == 1:
            # Un seul chunk: exÃ©cution directe
            result = self._compute_chunk(data, chunks[0], func, seed)
            if not result.success:
                raise RuntimeError(f"Erreur compute: {result.error}")
            return result.result

        # ExÃ©cution parallÃ¨le multi-device
        results = []
        max_workers = min(len(chunks), 8)  # Limite raisonnable

        with ThreadPoolExecutor(
            max_workers=max_workers, thread_name_prefix="MultiGPU"
        ) as executor:

            # Soumission des tÃ¢ches
            future_to_chunk = {}
            for chunk in chunks:
                future = executor.submit(self._compute_chunk, data, chunk, func, seed)
                future_to_chunk[future] = chunk

            # Collecte des rÃ©sultats
            for future in as_completed(future_to_chunk):
                chunk = future_to_chunk[future]
                try:
                    result = future.result(timeout=300)  # 5min timeout
                    results.append(result)
                except Exception as e:
                    logger.error(f"Erreur chunk {chunk.device_name}: {e}")
                    results.append(ComputeResult(chunk, None, 0.0, error=e))

        # Synchronisation NCCL optionnelle
        if self.nccl_enabled and len(self._gpu_devices) > 1:
            self.synchronize("nccl")

        # Merge des rÃ©sultats
        merged_result = self._merge_results(results, data, batch_axis)

        total_time = time.time() - start_time
        compute_times = [r.compute_time for r in results if r.success]
        avg_compute_time = np.mean(compute_times) if compute_times else 0.0

        logger.info(
            f"Workload terminÃ©: {total_time:.3f}s total, "
            f"{avg_compute_time:.3f}s compute moyen, "
            f"{len(results)} chunks traitÃ©s"
        )

        return merged_result

    def synchronize(self, method: str = "nccl") -> None:
        """
        Synchronise tous les devices GPU.

        Args:
            method: MÃ©thode de sync ("nccl", "cuda", "auto")
                   "nccl" : NCCL all-reduce si disponible
                   "cuda" : Synchronisation CUDA basique
                   "auto" : NCCL si dispo, sinon CUDA
        """
        if not self._gpu_devices or not CUPY_AVAILABLE:
            logger.debug("Sync ignorÃ©e: pas de GPU")
            return

        if method == "nccl" and not self.nccl_enabled:
            logger.debug("NCCL sync demandÃ©e mais indisponible")
            method = "cuda"
        elif method == "auto":
            method = "nccl" if self.nccl_enabled else "cuda"

        try:
            if method == "nccl":
                # Synchronisation NCCL (placeholder - implÃ©mentation complexe)
                logger.debug("Sync NCCL multi-GPU")
                # TODO: ImplÃ©mentation NCCL complÃ¨te avec communicator
                for device in self._gpu_devices:
                    with cp.cuda.Device(device.device_id):
                        cp.cuda.Device().synchronize()
            else:
                # Synchronisation CUDA basique
                logger.debug("Sync CUDA multi-GPU")
                for device in self._gpu_devices:
                    with cp.cuda.Device(device.device_id):
                        cp.cuda.Device().synchronize()

        except Exception as e:
            logger.warning(f"Erreur synchronisation {method}: {e}")

    def profile_auto_balance(
        self, sample_size: int = 200_000, warmup: int = 2, runs: int = 3
    ) -> Dict[str, float]:
        """
        Profile automatique pour optimiser la balance des devices hÃ©tÃ©rogÃ¨nes.

        ExÃ©cute des benchmarks sur chaque device disponible et calcule
        les ratios optimaux basÃ©s sur le throughput mesurÃ©. MÃ©thode inspirÃ©e
        des techniques de profiling hÃ©tÃ©rogÃ¨ne pour RTX 5090 + RTX 2060.

        Args:
            sample_size: Taille des Ã©chantillons de test
            warmup: Nombre de runs de warmup (non comptÃ©s, dÃ©faut: 2)
            runs: Nombre de runs de mesure (dÃ©faut: 3)

        Returns:
            Nouveaux ratios normalisÃ©s {"device": ratio}

        Example:
            >>> ratios = manager.profile_auto_balance(sample_size=50000)
            >>> print(ratios)  # {'5090': 0.78, '2060': 0.22}
            >>> manager.set_balance(ratios)
        """
        logger.info(
            f"Profiling auto-balance hÃ©tÃ©rogÃ¨ne: {sample_size} Ã©chantillons, "
            f"{warmup} warmup + {runs} runs"
        )

        # DonnÃ©es de test: opÃ©ration vectorielle reprÃ©sentative
        test_data = np.random.randn(sample_size, 10).astype(np.float32)

        def benchmark_func(x):
            """Fonction benchmark: somme + produit matriciel simple."""
            return x.sum(axis=1) + (x * x).mean(axis=1)

        device_throughputs = {}
        device_memory_efficiency = {}

        # Test de chaque device individuellement
        for device in self.available_devices:
            if device.name in ["cpu"] and len(self._gpu_devices) > 0:
                continue  # Skip CPU si GPU disponibles

            logger.info(f"Profiling device {device.name}...")

            # Configuration balance temporaire (100% sur ce device)
            temp_balance = {device.name: 1.0}
            old_balance = self.device_balance
            self.device_balance = temp_balance

            try:
                # Warmup pour stabiliser GPU (important pour profiling prÃ©cis)
                for w in range(warmup):
                    _ = self.distribute_workload(
                        test_data[:1000], benchmark_func, seed=42 + w
                    )

                # Mesures avec stats mÃ©moire
                times = []
                mem_usages = []
                for run in range(runs):
                    # MÃ©moire avant
                    mem_before = device.memory_used_pct if device.device_id >= 0 else 0

                    start_time = time.time()
                    _ = self.distribute_workload(
                        test_data, benchmark_func, seed=42 + warmup + run
                    )
                    elapsed = time.time() - start_time
                    times.append(elapsed)

                    # MÃ©moire aprÃ¨s
                    mem_after = device.memory_used_pct if device.device_id >= 0 else 0
                    mem_usages.append(mem_after - mem_before)

                # Calcul throughput (Ã©chantillons/seconde)
                avg_time = np.mean(times)
                std_time = np.std(times)
                throughput = sample_size / avg_time
                device_throughputs[device.name] = throughput

                # EfficacitÃ© mÃ©moire (throughput / memory_used)
                avg_mem_usage = np.mean(mem_usages) if mem_usages else 1.0
                mem_efficiency = throughput / max(avg_mem_usage, 0.01)
                device_memory_efficiency[device.name] = mem_efficiency

                logger.info(
                    f"Device {device.name}: {throughput:.0f} Ã©chantillons/s "
                    f"(avg {avg_time:.3f}s Â±{std_time:.3f}s), "
                    f"mem_efficiency: {mem_efficiency:.0f}"
                )

            except Exception as e:
                logger.warning(f"Erreur profiling {device.name}: {e}")
                device_throughputs[device.name] = 0.0
                device_memory_efficiency[device.name] = 0.0

            finally:
                # Restauration balance
                self.device_balance = old_balance

        # Calcul ratios optimaux basÃ©s sur throughput (avec pondÃ©ration mÃ©moire)
        if not device_throughputs or all(t == 0 for t in device_throughputs.values()):
            logger.warning("Profiling Ã©chouÃ©, conservation balance actuelle")
            return self.device_balance.copy()

        # Normalisation des throughputs en ratios
        total_throughput = sum(device_throughputs.values())
        optimal_ratios = {
            device: throughput / total_throughput
            for device, throughput in device_throughputs.items()
            if throughput > 0
        }

        # Log des rÃ©sultats dÃ©taillÃ©s
        logger.info("Profiling hÃ©tÃ©rogÃ¨ne terminÃ©:")
        for device, ratio in optimal_ratios.items():
            old_ratio = self.device_balance.get(device, 0.0)
            throughput = device_throughputs[device]
            mem_eff = device_memory_efficiency.get(device, 0.0)
            logger.info(
                f"  {device}: {ratio:.1%} (Ã©tait {old_ratio:.1%}), "
                f"{throughput:.0f} Ã©chant./s, mem_eff: {mem_eff:.0f}"
            )

        return optimal_ratios

    def get_device_stats(self) -> Dict[str, Dict[str, Any]]:
        """
        RÃ©cupÃ¨re les statistiques des devices.

        Returns:
            Stats par device: mÃ©moire, balance, etc.
        """
        stats = {}

        for device in self.available_devices:
            device_stats = {
                "device_id": device.device_id,
                "available": device.is_available,
                "memory_total_gb": device.memory_total_gb,
                "memory_free_gb": device.memory_free_gb,
                "memory_used_pct": device.memory_used_pct,
                "compute_capability": device.compute_capability,
                "current_balance": self.device_balance.get(device.name, 0.0),
                "has_stream": device.name in self._device_streams,
            }
            stats[device.name] = device_stats

        return stats

    def __del__(self):
        """Nettoyage des streams GPU."""
        try:
            with self._lock:
                for stream in self._device_streams.values():
                    if hasattr(stream, "close"):
                        stream.close()
                self._device_streams.clear()
        except:
            pass


# === Gestionnaire Global ===

_default_manager: Optional[MultiGPUManager] = None


def get_default_manager() -> MultiGPUManager:
    """
    RÃ©cupÃ¨re le gestionnaire multi-GPU par dÃ©faut (singleton).

    Returns:
        Instance MultiGPUManager configurÃ©e

    Example:
        >>> manager = get_default_manager()
        >>> result = manager.distribute_workload(data, func)
    """
    global _default_manager

    if _default_manager is None:
        _default_manager = MultiGPUManager()
        logger.info("Gestionnaire multi-GPU par dÃ©faut crÃ©Ã©")

    return _default_manager


def shutdown_default_manager() -> None:
    """
    Ferme proprement le gestionnaire multi-GPU global.

    LibÃ¨re les CUDA Streams et resources GPU.
    Appeler avant la fin du programme pour Ã©viter que les streams bloquent l'arrÃªt.

    Example:
        >>> from threadx.gpu.multi_gpu import shutdown_default_manager
        >>> # ... utilisation GPU ...
        >>> shutdown_default_manager()  # Avant sys.exit() ou fin du script
    """
    global _default_manager

    if _default_manager is not None:
        try:
            with _default_manager._lock:
                for stream in _default_manager._device_streams.values():
                    if hasattr(stream, "close"):
                        stream.close()
                _default_manager._device_streams.clear()
            logger.info("âœ… Gestionnaire multi-GPU arrÃªtÃ© proprement")
        except Exception as e:
            logger.warning(f"Erreur lors du shutdown GPU: {e}")
        finally:
            _default_manager = None


# === Gestion propre de l'arrÃªt ===


def _signal_handler(signum, frame):
    """Handler pour SIGINT/SIGTERM - Ferme proprement les ressources GPU."""
    logger.info(f"âš ï¸ Signal {signum} reÃ§u - Nettoyage GPU...")
    shutdown_default_manager()


# Enregistrer les signal handlers (Ã©choue silencieusement si impossible, ex: Streamlit)
try:
    signal.signal(signal.SIGINT, _signal_handler)
    signal.signal(signal.SIGTERM, _signal_handler)
    logger.info("ðŸ”§ Handlers de signal GPU enregistrÃ©s (SIGINT, SIGTERM)")
except (ValueError, RuntimeError) as e:
    # Normal dans Streamlit, threads secondaires, ou environnements non-interactifs
    logger.debug(f"âš ï¸ Signal handlers non disponibles (normal pour Streamlit): {e}")

# Enregistrer le shutdown Ã  la sortie normale (fonctionne partout)
atexit.register(shutdown_default_manager)
logger.info("ðŸ”§ Handler atexit enregistrÃ© pour nettoyage GPU")

----------------------------------------
Fichier: gpu\profile_persistence.py
"""
ThreadX Utils - Persistance des Profils GPU
===========================================

Fonctions utilitaires pour la persistance et la lecture sÃ©curisÃ©e
des profils GPU et multi-GPU dans des fichiers JSON.

Features:
- Lecture/Ã©criture atomiques avec gestion des erreurs
- Backup automatique des fichiers corrompus
- Validation des structures de donnÃ©es
- Merge intelligent des profils

Author: ThreadX Team
Version: Phase B - GPU Dynamic & Multi-GPU
"""

import os
import json
import time
import shutil
import logging
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple

import numpy as np

from threadx.utils.log import get_logger
from threadx.config import get_settings

S = get_settings()  # Stub settings instance

logger = get_logger(__name__)

# Constantes pour les chemins de profils
PROFILES_DIR = Path("artifacts") / "profiles"
GPU_THRESHOLDS_FILE = PROFILES_DIR / "gpu_thresholds.json"
MULTIGPU_RATIOS_FILE = PROFILES_DIR / "multigpu_ratios.json"

# Structure par dÃ©faut pour les profils
DEFAULT_GPU_THRESHOLDS = {
    "version": 1,
    "updated_at": datetime.now().isoformat(),
    "entries": {},
    "defaults": {
        "decision_threshold": 1.75,
        "min_samples": 3,
        "n_min_gpu": 10000,
        "hysteresis": 0.10,
    },
}

DEFAULT_MULTIGPU_RATIOS = {
    "version": 1,
    "updated_at": datetime.now().isoformat(),
    "devices": [],
    "ratios": {},
    "sample_size": 3,
    "workload_tag": "indicators/batch_default",
    "ttl_days": 14,
}


def ensure_profiles_dir() -> None:
    """Assure que le dossier des profils existe."""
    PROFILES_DIR.mkdir(parents=True, exist_ok=True)


def safe_read_json(file_path: Path) -> Tuple[Dict[str, Any], bool]:
    """
    Lit un fichier JSON de faÃ§on sÃ©curisÃ©e avec gestion d'erreurs.

    Args:
        file_path: Chemin vers le fichier JSON

    Returns:
        Tuple contenant (data, success)
        - data: Contenu JSON ou structure par dÃ©faut
        - success: True si lecture rÃ©ussie, False sinon
    """
    ensure_profiles_dir()

    if not file_path.exists():
        return ({}, False)

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return (data, True)
    except (json.JSONDecodeError, IOError) as e:
        # CrÃ©er un backup du fichier corrompu avec timestamp
        if file_path.exists():
            timestamp = int(time.time())
            backup_path = file_path.with_suffix(f".bak.{timestamp}")
            try:
                shutil.copy2(file_path, backup_path)
                logger.warning(f"Fichier JSON corrompu, backup crÃ©Ã©: {backup_path}")
            except Exception as backup_err:
                logger.error(f"Impossible de crÃ©er un backup: {backup_err}")

        logger.error(f"Erreur de lecture JSON {file_path}: {e}")
        return ({}, False)


def safe_write_json(file_path: Path, data: Dict[str, Any]) -> bool:
    """
    Ã‰crit un fichier JSON de faÃ§on atomique et sÃ©curisÃ©e.

    Args:
        file_path: Chemin vers le fichier JSON
        data: DonnÃ©es Ã  Ã©crire (dictionnaire)

    Returns:
        True si Ã©criture rÃ©ussie, False sinon
    """
    ensure_profiles_dir()

    # CrÃ©er un fichier temporaire
    temp_path = file_path.with_suffix(".tmp")

    try:
        # Ã‰crire d'abord dans un fichier temporaire
        with open(temp_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        # Puis remplacer le fichier original (opÃ©ration atomique)
        if os.name == "nt":  # Windows
            if file_path.exists():
                file_path.unlink()
            os.rename(temp_path, file_path)
        else:  # Unix/Linux/Mac
            os.replace(temp_path, file_path)

        return True

    except Exception as e:
        logger.error(f"Erreur d'Ã©criture JSON {file_path}: {e}")
        # Nettoyer le fichier temporaire si nÃ©cessaire
        if temp_path.exists():
            try:
                temp_path.unlink()
            except:
                pass
        return False


def stable_hash(params: Dict[str, Any]) -> str:
    """
    GÃ©nÃ¨re un hash stable pour des paramÃ¨tres.

    Args:
        params: Dictionnaire de paramÃ¨tres

    Returns:
        Hash SHA1 hexadÃ©cimal des paramÃ¨tres triÃ©s
    """
    # Conversion en format canonique (tri des clÃ©s)
    if not params:
        return "empty"

    # Convertir les types numpy en types Python standards
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_numpy(i) for i in obj]
        else:
            return obj

    params_clean = convert_numpy(params)

    # Trier les clÃ©s et convertir en JSON pour un format canonique
    canonical = json.dumps(params_clean, sort_keys=True, ensure_ascii=True)

    # Calculer le hash SHA1
    return hashlib.sha1(canonical.encode("utf-8")).hexdigest()[:12]


def get_gpu_thresholds() -> Dict[str, Any]:
    """
    RÃ©cupÃ¨re les seuils GPU, crÃ©e le fichier s'il n'existe pas.

    Returns:
        Dictionnaire contenant les seuils GPU
    """
    data, success = safe_read_json(GPU_THRESHOLDS_FILE)

    if not success or "version" not in data:
        # Initialiser avec les valeurs par dÃ©faut
        data = DEFAULT_GPU_THRESHOLDS.copy()
        safe_write_json(GPU_THRESHOLDS_FILE, data)

    return data


def update_gpu_threshold_entry(
    signature: str, cpu_ms: float, gpu_ms: float, n_samples: int = 1
) -> Dict[str, Any]:
    """
    Met Ã  jour une entrÃ©e de seuil GPU dans le fichier de profils.

    Args:
        signature: Signature unique de la fonction
        cpu_ms: Temps d'exÃ©cution CPU en ms
        gpu_ms: Temps d'exÃ©cution GPU en ms
        n_samples: Nombre d'Ã©chantillons pour cette mesure

    Returns:
        Le profil mis Ã  jour
    """
    data = get_gpu_thresholds()

    now = datetime.now().isoformat()
    data["updated_at"] = now

    # Mettre Ã  jour l'entrÃ©e existante ou en crÃ©er une nouvelle
    if signature in data["entries"]:
        entry = data["entries"][signature]

        # Calculer les nouvelles moyennes et variances (EMA)
        alpha = 1 / min(entry["n_samples"] + n_samples, 10)  # Facteur de lissage

        # Mise Ã  jour moyenne mobile exponentielle (EMA)
        entry["cpu_ms_avg"] = (1 - alpha) * entry["cpu_ms_avg"] + alpha * cpu_ms
        entry["gpu_ms_avg"] = (1 - alpha) * entry["gpu_ms_avg"] + alpha * gpu_ms

        # Mise Ã  jour variance (approximation)
        entry["cpu_ms_var"] = (1 - alpha) * entry["cpu_ms_var"] + alpha * (
            cpu_ms - entry["cpu_ms_avg"]
        ) ** 2
        entry["gpu_ms_var"] = (1 - alpha) * entry["gpu_ms_var"] + alpha * (
            gpu_ms - entry["gpu_ms_avg"]
        ) ** 2

        # Mise Ã  jour compteur et date
        entry["n_samples"] += n_samples
        entry["last_seen"] = now

    else:
        # Nouvelle entrÃ©e
        entry = {
            "cpu_ms_avg": cpu_ms,
            "cpu_ms_var": 0.0,  # Variance initiale Ã  zÃ©ro
            "gpu_ms_avg": gpu_ms,
            "gpu_ms_var": 0.0,  # Variance initiale Ã  zÃ©ro
            "n_samples": n_samples,
            "last_seen": now,
            "decision_threshold": data["defaults"]["decision_threshold"],
        }
        data["entries"][signature] = entry

    # Ã‰crire les modifications
    safe_write_json(GPU_THRESHOLDS_FILE, data)
    return data


def get_multigpu_ratios() -> Dict[str, Any]:
    """
    RÃ©cupÃ¨re les ratios multi-GPU, crÃ©e le fichier s'il n'existe pas.

    Returns:
        Dictionnaire contenant les ratios multi-GPU
    """
    data, success = safe_read_json(MULTIGPU_RATIOS_FILE)

    if not success or "version" not in data:
        # Initialiser avec les valeurs par dÃ©faut
        data = DEFAULT_MULTIGPU_RATIOS.copy()
        safe_write_json(MULTIGPU_RATIOS_FILE, data)

    return data


def update_multigpu_ratios(
    devices: List[Dict[str, Any]],
    ratios: Dict[str, float],
    sample_size: int = 3,
    workload_tag: str = "indicators/batch_default",
    ttl_days: int = 14,
) -> Dict[str, Any]:
    """
    Met Ã  jour les ratios multi-GPU dans le fichier de profils.

    Args:
        devices: Liste des infos de pÃ©riphÃ©riques
        ratios: Dictionnaire des ratios par device_id
        sample_size: Taille de l'Ã©chantillon utilisÃ©
        workload_tag: Tag du workload de rÃ©fÃ©rence
        ttl_days: DurÃ©e de validitÃ© en jours

    Returns:
        Le profil mis Ã  jour
    """
    data = {
        "version": 1,
        "updated_at": datetime.now().isoformat(),
        "devices": devices,
        "ratios": ratios,
        "sample_size": sample_size,
        "workload_tag": workload_tag,
        "ttl_days": ttl_days,
    }

    # Ã‰crire les nouvelles valeurs
    safe_write_json(MULTIGPU_RATIOS_FILE, data)
    return data


def is_profile_valid(profile: Dict[str, Any], ttl_days: int = None) -> bool:
    """
    VÃ©rifie si un profil est valide selon sa date.

    Args:
        profile: Dictionnaire du profil
        ttl_days: DurÃ©e de validitÃ© en jours (si None, utilise la valeur du profil)

    Returns:
        True si le profil est valide, False sinon
    """
    if not profile or "updated_at" not in profile:
        return False

    try:
        # Utiliser le TTL du profil si non spÃ©cifiÃ©
        if ttl_days is None:
            ttl_days = profile.get("ttl_days", 14)

        # Convertir la date ISO en datetime
        updated_at = datetime.fromisoformat(profile["updated_at"])

        # Calculer l'Ã¢ge du profil
        age = datetime.now() - updated_at

        # VÃ©rifier si le profil est plus rÃ©cent que le TTL
        return age < timedelta(days=ttl_days)
    except:
        return False

----------------------------------------
Fichier: gpu\vector_checks.py
"""
ThreadX Utils Module - Phase 9
Vector and Array Validation Utilities.

Provides comprehensive validation for array shapes, dtypes, and data quality:
- Shape and dimension validation with detailed error messages
- Data type checking and conversion recommendations
- NaN/infinity detection and handling suggestions
- Performance-oriented validation (non-blocking warnings)
- Integration with ThreadX logging for actionable feedback

Designed for hot-path validation in indicator calculations and backtesting.
"""

import logging
import warnings
from typing import Any, Optional, Tuple, List, Dict, Union, Callable
from dataclasses import dataclass
import numpy as np

# Import ThreadX logger - fallback to standard logging if not available
try:
    from threadx.utils.log import get_logger
except ImportError:

    def get_logger(name: str) -> logging.Logger:
        return logging.getLogger(name)


# Import ThreadX xp utils if available
try:
    from threadx.utils.xp import xp, get_array_info, asnumpy

    XP_AVAILABLE = True
except ImportError:
    XP_AVAILABLE = False

# Import ThreadX Settings - fallback if not available
try:
    from threadx.config import load_settings

    SETTINGS_AVAILABLE = True
except Exception:  # pragma: no cover - optional dependency during tests
    SETTINGS_AVAILABLE = False


logger = get_logger(__name__)


@dataclass
class ValidationResult:
    """Result of array validation."""

    is_valid: bool
    warnings: List[str]
    errors: List[str]
    suggestions: List[str]
    array_info: Dict[str, Any]


class ArrayValidator:
    """
    Comprehensive array validator for ThreadX operations.

    Provides fast, non-blocking validation with actionable feedback.
    Designed for use in hot paths without significant performance impact.

    Examples
    --------
    >>> validator = ArrayValidator()
    >>>
    >>> # Basic validation
    >>> result = validator.validate(data, expected_shape=(1000,), expected_dtype=np.float64)
    >>> if not result.is_valid:
    ...     for error in result.errors:
    ...         print(f"Error: {error}")

    >>> # Custom validation with callback
    >>> def handle_issues(result):
    ...     if result.warnings:
    ...         log_performance_warning(result.warnings)
    >>>
    >>> validator.validate(data, callback=handle_issues)
    """

    def __init__(self, strict_mode: bool = False):
        """
        Initialize validator.

        Parameters
        ----------
        strict_mode : bool, default False
            If True, treats warnings as errors.
        """
        self.strict_mode = strict_mode
        self._validation_cache = {}

    def validate(
        self,
        array: Any,
        *,
        expected_shape: Optional[Tuple[int, ...]] = None,
        expected_dtype: Optional[np.dtype] = None,
        min_size: Optional[int] = None,
        max_size: Optional[int] = None,
        allow_nan: bool = False,
        allow_inf: bool = False,
        check_finite: bool = True,
        check_contiguous: bool = False,
        callback: Optional[Callable[[ValidationResult], None]] = None,
        name: Optional[str] = None,
    ) -> ValidationResult:
        """
        Validate array properties and data quality.

        Performs comprehensive validation with performance-oriented design.
        Issues warnings for non-critical issues, errors for blocking problems.

        Parameters
        ----------
        array : array-like
            Array to validate.
        expected_shape : tuple, optional
            Expected array shape. None means any shape is acceptable.
        expected_dtype : numpy.dtype, optional
            Expected data type.
        min_size : int, optional
            Minimum required array size.
        max_size : int, optional
            Maximum allowed array size.
        allow_nan : bool, default False
            Whether NaN values are acceptable.
        allow_inf : bool, default False
            Whether infinite values are acceptable.
        check_finite : bool, default True
            Whether to check for finite values.
        check_contiguous : bool, default False
            Whether to check for contiguous memory layout.
        callback : callable, optional
            Callback function to handle validation results.
        name : str, optional
            Name for logging/debugging purposes.

        Returns
        -------
        ValidationResult
            Comprehensive validation results.
        """
        warnings_list = []
        errors_list = []
        suggestions_list = []

        array_name = name or "array"

        # Basic array conversion and info
        try:
            if XP_AVAILABLE:
                array_info = get_array_info(array)
                # Convert to numpy for validation if needed
                if hasattr(array, "get"):  # CuPy array
                    np_array = asnumpy(array)
                else:
                    np_array = np.asarray(array)
            else:
                np_array = np.asarray(array)
                array_info = {
                    "device": "cpu",
                    "shape": np_array.shape,
                    "dtype": np_array.dtype,
                    "memory_mb": np_array.nbytes / (1024 * 1024),
                    "is_contiguous": np_array.flags.c_contiguous,
                }
        except Exception as e:
            errors_list.append(f"Failed to convert {array_name} to array: {e}")
            return ValidationResult(
                is_valid=False,
                warnings=warnings_list,
                errors=errors_list,
                suggestions=["Ensure input is array-like (list, numpy array, etc.)"],
                array_info={},
            )

        # Shape validation
        if expected_shape is not None:
            if np_array.shape != expected_shape:
                error_msg = f"{array_name} shape mismatch: expected {expected_shape}, got {np_array.shape}"
                if self.strict_mode:
                    errors_list.append(error_msg)
                else:
                    warnings_list.append(error_msg)

                suggestions_list.append(
                    f"Reshape or subset {array_name} to match expected dimensions"
                )

        # Size validation
        array_size = np_array.size

        if min_size is not None and array_size < min_size:
            errors_list.append(f"{array_name} too small: {array_size} < {min_size}")
            suggestions_list.append(
                f"Provide at least {min_size} data points for reliable calculation"
            )

        if max_size is not None and array_size > max_size:
            warnings_list.append(f"{array_name} very large: {array_size} > {max_size}")
            suggestions_list.append("Consider batch processing for large datasets")

        # Data type validation
        if expected_dtype is not None:
            if np_array.dtype != expected_dtype:
                warning_msg = f"{array_name} dtype mismatch: expected {expected_dtype}, got {np_array.dtype}"
                warnings_list.append(warning_msg)

                # Suggest conversion if reasonable
                if np.can_cast(np_array.dtype, expected_dtype, casting="safe"):
                    suggestions_list.append(
                        f"Convert {array_name} to {expected_dtype} using astype()"
                    )
                else:
                    suggestions_list.append(
                        f"Check data precision requirements for {array_name}"
                    )

        # Data quality checks (only for numeric arrays)
        if np.issubdtype(np_array.dtype, np.number):
            try:
                # Check for NaN values
                nan_count = np.isnan(np_array).sum()
                if nan_count > 0:
                    if allow_nan:
                        warnings_list.append(
                            f"{array_name} contains {nan_count} NaN values"
                        )
                    else:
                        errors_list.append(
                            f"{array_name} contains {nan_count} NaN values (not allowed)"
                        )
                        suggestions_list.append(
                            "Remove or fill NaN values before processing"
                        )

                # Check for infinite values
                if check_finite:
                    inf_count = np.isinf(np_array).sum()
                    if inf_count > 0:
                        if allow_inf:
                            warnings_list.append(
                                f"{array_name} contains {inf_count} infinite values"
                            )
                        else:
                            errors_list.append(
                                f"{array_name} contains {inf_count} infinite values (not allowed)"
                            )
                            suggestions_list.append("Clip or remove infinite values")

                # Check data range for potential issues
                if array_size > 0 and not (nan_count == array_size):
                    try:
                        data_min = np.nanmin(np_array)
                        data_max = np.nanmax(np_array)

                        # Warn about extreme values that might cause numerical issues
                        if data_max > 1e10:
                            warnings_list.append(
                                f"{array_name} contains very large values (max: {data_max:.2e})"
                            )
                            suggestions_list.append(
                                "Consider scaling data to prevent numerical overflow"
                            )

                        if data_min < -1e10:
                            warnings_list.append(
                                f"{array_name} contains very small values (min: {data_min:.2e})"
                            )
                            suggestions_list.append(
                                "Consider scaling data to prevent numerical underflow"
                            )

                        # Check for constant arrays
                        if data_min == data_max and array_size > 1:
                            warnings_list.append(
                                f"{array_name} is constant (all values = {data_min})"
                            )
                            suggestions_list.append(
                                "Constant arrays may cause division by zero in calculations"
                            )

                    except Exception as e:
                        logger.debug(f"Data range check failed for {array_name}: {e}")

            except Exception as e:
                warnings_list.append(f"Data quality check failed for {array_name}: {e}")

        # Memory layout checks
        if check_contiguous and not array_info.get("is_contiguous", True):
            warnings_list.append(f"{array_name} is not contiguous in memory")
            suggestions_list.append("Use np.ascontiguousarray() for better performance")

        # Performance warnings
        memory_mb = array_info.get("memory_mb", 0)
        if memory_mb > 1000:  # > 1GB
            warnings_list.append(f"{array_name} is very large ({memory_mb:.1f}MB)")
            suggestions_list.append(
                "Consider batch processing or memory-efficient algorithms"
            )

        # Create result
        is_valid = len(errors_list) == 0

        result = ValidationResult(
            is_valid=is_valid,
            warnings=warnings_list,
            errors=errors_list,
            suggestions=suggestions_list,
            array_info=array_info,
        )

        # Log results
        self._log_validation_result(result, array_name)

        # Call callback if provided
        if callback:
            try:
                callback(result)
            except Exception as e:
                logger.warning(f"Validation callback failed: {e}")

        return result

    def _log_validation_result(self, result: ValidationResult, array_name: str) -> None:
        """Log validation results at appropriate levels."""
        if result.errors:
            for error in result.errors:
                logger.error(f"Validation error for {array_name}: {error}")

        if result.warnings:
            for warning in result.warnings:
                logger.warning(f"Validation warning for {array_name}: {warning}")

        if result.suggestions and (result.errors or result.warnings):
            logger.info(
                f"Suggestions for {array_name}: {'; '.join(result.suggestions)}"
            )

    def validate_multiple(
        self, arrays: Dict[str, Any], **validation_kwargs
    ) -> Dict[str, ValidationResult]:
        """
        Validate multiple arrays with same criteria.

        Parameters
        ----------
        arrays : dict
            Dictionary of name -> array to validate.
        **validation_kwargs
            Validation parameters passed to validate().

        Returns
        -------
        dict
            Dictionary of name -> ValidationResult.
        """
        results = {}

        for name, array in arrays.items():
            results[name] = self.validate(array, name=name, **validation_kwargs)

        return results


# Convenience functions for common validation patterns
def validate_price_data(
    prices: Any, min_length: int = 2, name: str = "prices"
) -> ValidationResult:
    """
    Validate price/OHLCV data arrays.

    Specialized validation for financial time series data.

    Parameters
    ----------
    prices : array-like
        Price data array.
    min_length : int, default 2
        Minimum required length for calculations.
    name : str, default "prices"
        Name for logging.

    Returns
    -------
    ValidationResult
        Validation results.
    """
    validator = ArrayValidator()

    return validator.validate(
        prices,
        expected_dtype=np.float64,
        min_size=min_length,
        allow_nan=False,
        allow_inf=False,
        name=name,
    )


def validate_indicator_params(
    period: int,
    multiplier: Optional[float] = None,
    *,
    min_period: int = 1,
    max_period: int = 1000,
) -> ValidationResult:
    """
    Validate common indicator parameters.

    Parameters
    ----------
    period : int
        Period parameter (e.g., moving average window).
    multiplier : float, optional
        Multiplier parameter (e.g., standard deviation multiplier).
    min_period : int, default 1
        Minimum allowed period.
    max_period : int, default 1000
        Maximum reasonable period.

    Returns
    -------
    ValidationResult
        Validation results.
    """
    warnings_list = []
    errors_list = []
    suggestions_list = []

    # Validate period
    if not isinstance(period, int):
        errors_list.append(f"Period must be integer, got {type(period).__name__}")
    elif period < min_period:
        errors_list.append(f"Period too small: {period} < {min_period}")
    elif period > max_period:
        warnings_list.append(f"Period very large: {period} > {max_period}")
        suggestions_list.append(
            "Large periods may require more data and cause edge effects"
        )

    # Validate multiplier if provided
    if multiplier is not None:
        if not isinstance(multiplier, (int, float)):
            errors_list.append(
                f"Multiplier must be numeric, got {type(multiplier).__name__}"
            )
        elif multiplier <= 0:
            errors_list.append(f"Multiplier must be positive, got {multiplier}")
        elif multiplier > 10:
            warnings_list.append(f"Multiplier very large: {multiplier}")
            suggestions_list.append("Large multipliers may produce extreme values")

    return ValidationResult(
        is_valid=len(errors_list) == 0,
        warnings=warnings_list,
        errors=errors_list,
        suggestions=suggestions_list,
        array_info={"period": period, "multiplier": multiplier},
    )


def check_array_compatibility(
    *arrays: Any, operation: str = "operation"
) -> ValidationResult:
    """
    Check if arrays are compatible for operations.

    Validates shape compatibility, device compatibility, and dtype compatibility.

    Parameters
    ----------
    *arrays : array-like
        Arrays to check for compatibility.
    operation : str, default "operation"
        Description of the operation for error messages.

    Returns
    -------
    ValidationResult
        Compatibility validation results.
    """
    if len(arrays) < 2:
        return ValidationResult(
            is_valid=True, warnings=[], errors=[], suggestions=[], array_info={}
        )

    warnings_list = []
    errors_list = []
    suggestions_list = []

    # Convert all arrays and get info
    array_infos = []
    np_arrays = []

    for i, array in enumerate(arrays):
        try:
            if XP_AVAILABLE:
                info = get_array_info(array)
                np_array = (
                    asnumpy(array) if hasattr(array, "get") else np.asarray(array)
                )
            else:
                np_array = np.asarray(array)
                info = {
                    "device": "cpu",
                    "shape": np_array.shape,
                    "dtype": np_array.dtype,
                }

            array_infos.append(info)
            np_arrays.append(np_array)

        except Exception as e:
            errors_list.append(f"Failed to process array {i}: {e}")
            return ValidationResult(
                is_valid=False,
                warnings=warnings_list,
                errors=errors_list,
                suggestions=["Ensure all inputs are array-like"],
                array_info={},
            )

    # Check shape compatibility
    shapes = [info["shape"] for info in array_infos]
    if not all(np.broadcast_shapes(shapes[0], shape) for shape in shapes[1:]):
        errors_list.append(f"Arrays not broadcast-compatible for {operation}: {shapes}")
        suggestions_list.append("Reshape arrays or use compatible dimensions")

    # Check device compatibility
    devices = [info["device"] for info in array_infos]
    if len(set(devices)) > 1:
        warnings_list.append(f"Arrays on different devices for {operation}: {devices}")
        suggestions_list.append("Move arrays to same device for optimal performance")

    # Check dtype compatibility
    dtypes = [info["dtype"] for info in array_infos]
    if len(set(str(dtype) for dtype in dtypes)) > 1:
        warnings_list.append(f"Mixed dtypes in {operation}: {dtypes}")
        suggestions_list.append(
            "Convert arrays to common dtype to avoid precision loss"
        )

    return ValidationResult(
        is_valid=len(errors_list) == 0,
        warnings=warnings_list,
        errors=errors_list,
        suggestions=suggestions_list,
        array_info={"shapes": shapes, "devices": devices, "dtypes": dtypes},
    )


# Global validator instance for convenience
default_validator = ArrayValidator()

# Convenience aliases
validate = default_validator.validate
validate_arrays = default_validator.validate_multiple




----------------------------------------
Fichier: gpu\__init__.py
"""
ThreadX GPU Utilities - Phase 5 Multi-GPU Support
==================================================

Gestionnaire multi-GPU avec distribution de charge automatique.

Modules:
    device_manager: DÃ©tection et gestion des devices GPU/CPU
    multi_gpu: Orchestration multi-GPU avec auto-balancing
"""

from .device_manager import (
    is_available,
    list_devices,
    get_device_by_name,
    get_device_by_id,
    check_nccl_support,
    xp,
    DeviceInfo,
)

from .multi_gpu import (
    MultiGPUManager,
    DeviceUnavailableError,
    GPUMemoryError,
    ShapeMismatchError,
    NonVectorizableFunctionError,
    get_default_manager,
)

__all__ = [
    # Device utilities
    "is_available",
    "list_devices",
    "get_device_by_name",
    "get_device_by_id",
    "check_nccl_support",
    "xp",
    "DeviceInfo",
    # Multi-GPU manager
    "MultiGPUManager",
    "get_default_manager",
    # Exceptions
    "DeviceUnavailableError",
    "GPUMemoryError",
    "ShapeMismatchError",
    "NonVectorizableFunctionError",
]

__version__ = "5.0.0"




----------------------------------------
Fichier: indicators\bank.py
#!/usr/bin/env python3
"""
ThreadX Indicator Bank - Cache centralisÃ© d'indicateurs
=======================================================

Gestion centralisÃ©e du cache d'indicateurs techniques avec:
- Cache disque intelligent avec TTL et checksums
- Batch processing automatique (seuil: 100 paramÃ¨tres)
- Registry automatique mise Ã  jour
- Support GPU multi-carte transparent
- Validation et recompute forcÃ©

Fonctions principales:
- ensure(): VÃ©rifie existence/validitÃ© â†’ recalcule si nÃ©cessaire
- force_recompute(): Recalcule obligatoirement
- batch_ensure(): Traitement batch avec parallÃ©lisation

SpÃ©cifications cache:
- TTL: 3600 secondes (1 heure)
- ClÃ©s triÃ©es alphabÃ©tiquement
- Checksums MD5 pour validation intÃ©gritÃ©
- Registry Parquet mis Ã  jour automatiquement

Exemple d'usage:
    ```python
    from threadx.indicators.bank import IndicatorBank, ensure_indicator

    # Initialisation
    bank = IndicatorBank(cache_dir="indicators_cache")

    # Simple ensure
    bb_result = ensure_indicator(
        'bollinger',
        {'period': 20, 'std': 2.0},
        close_data,
        symbol='BTCUSDC',
        timeframe='15m'
    )

    # Batch ensure
    params_list = [
        {'period': 20, 'std': 2.0},
        {'period': 50, 'std': 1.5}
    ]
    results = bank.batch_ensure('bollinger', params_list, close_data)
    ```
"""

import hashlib
import json
import logging
import os
import pickle
import time
import threading
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
import numpy as np
import pandas as pd

# FIX B1: File locking cross-platform
import platform

if platform.system() == "Windows":
    import msvcrt
else:
    import fcntl

# Import des calculateurs
from .bollinger import BollingerBands, BollingerSettings
from .xatr import ATR, ATRSettings

# Import Phase 2 Data
try:
    from ..dataset.registry import quick_inventory
    from ..dataset.io import write_frame, read_frame

    HAS_THREADX_DATA = True
except ImportError:
    HAS_THREADX_DATA = False

# Configuration logging
logger = logging.getLogger(__name__)


@dataclass
class IndicatorSettings:
    """Configuration globale pour IndicatorBank"""

    cache_dir: str = "indicators_cache"
    ttl_seconds: int = 3600  # 1 heure
    batch_threshold: int = 100  # Seuil pour parallÃ©lisation
    max_workers: int = 8  # Workers pour batch processing
    use_gpu: bool = True
    auto_registry_update: bool = True
    checksum_validation: bool = True
    compression_level: int = 6  # Pour cache Parquet

    # GPU settings (hÃ©ritÃ©s des modules indicateurs)
    gpu_split_ratio: Tuple[float, float] = (0.75, 0.25)
    gpu_batch_size: int = 1000

    def __post_init__(self):
        """Validation et crÃ©ation du rÃ©pertoire cache"""
        self.cache_path = Path(self.cache_dir)
        self.cache_path.mkdir(parents=True, exist_ok=True)

        # Sous-rÃ©pertoires par type d'indicateur
        (self.cache_path / "bollinger").mkdir(exist_ok=True)
        (self.cache_path / "atr").mkdir(exist_ok=True)
        (self.cache_path / "registry").mkdir(exist_ok=True)

        if self.max_workers < 1:
            raise ValueError(f"max_workers doit Ãªtre >= 1, reÃ§u: {self.max_workers}")


class CacheManager:
    """Gestionnaire de cache avec TTL et checksums"""

    def __init__(self, settings: IndicatorSettings):
        self.settings = settings
        self.cache_path = Path(settings.cache_dir)

    def _read_metadata(self, meta_file: Path) -> Optional[Dict]:
        """Helper: Lit metadata JSON (Ã©limine duplication)"""
        try:
            with open(meta_file, "r") as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to read metadata {meta_file}: {e}")
            return None

    def _generate_cache_key(
        self,
        indicator_type: str,
        params: Dict[str, Any],
        symbol: str = "",
        timeframe: str = "",
        data_hash: str = "",
    ) -> str:
        """
        GÃ©nÃ©ration clÃ© de cache triÃ©e alphabÃ©tiquement

        Format: {indicator_type}_{symbol}_{timeframe}_{params_hash}_{data_hash[:8]}
        """
        # Tri alphabÃ©tique des paramÃ¨tres pour cohÃ©rence
        sorted_params = dict(sorted(params.items()))
        params_str = json.dumps(sorted_params, sort_keys=True, separators=(",", ":"))
        params_hash = hashlib.md5(params_str.encode()).hexdigest()[:16]

        # ClÃ© finale
        key_parts = [
            indicator_type,
            symbol or "nosymbol",
            timeframe or "notf",
            params_hash,
            data_hash[:8] if data_hash else "nodata",
        ]

        return "_".join(key_parts)

    def _compute_data_hash(
        self, data: Union[np.ndarray, pd.Series, pd.DataFrame]
    ) -> str:
        """Calcul hash des donnÃ©es pour cache key"""
        if isinstance(data, pd.DataFrame):
            # Pour OHLCV, hash sur close uniquement pour optimisation
            if "close" in data.columns:
                data_bytes = data["close"].values.astype(np.float64).tobytes()
            else:
                data_bytes = data.values.astype(np.float64).tobytes()
        elif isinstance(data, pd.Series):
            data_bytes = data.values.astype(np.float64).tobytes()
        else:  # numpy array
            data_bytes = np.asarray(data, dtype=np.float64).tobytes()

        return hashlib.md5(data_bytes).hexdigest()

    def _get_cache_filepath(self, cache_key: str, indicator_type: str) -> Path:
        """Chemin fichier cache"""
        return self.cache_path / indicator_type / f"{cache_key}.parquet"

    def _get_metadata_filepath(self, cache_key: str, indicator_type: str) -> Path:
        """Chemin fichier mÃ©tadonnÃ©es"""
        return self.cache_path / indicator_type / f"{cache_key}.meta"

    def is_cache_valid(self, cache_key: str, indicator_type: str) -> bool:
        """VÃ©rification validitÃ© cache (TTL + intÃ©gritÃ©)"""
        cache_file = self._get_cache_filepath(cache_key, indicator_type)
        meta_file = self._get_metadata_filepath(cache_key, indicator_type)

        if not (cache_file.exists() and meta_file.exists()):
            return False

        try:
            # Lecture mÃ©tadonnÃ©es
            metadata = self._read_metadata(meta_file)
            if not metadata:
                return False

            # VÃ©rification TTL
            created_at = metadata.get("created_at", 0)
            if time.time() - created_at > self.settings.ttl_seconds:
                logger.debug(f"ðŸ•’ Cache expirÃ© (TTL): {cache_key}")
                return False

            # VÃ©rification checksum si activÃ©e
            if self.settings.checksum_validation and "checksum" in metadata:
                current_checksum = self._compute_file_checksum(cache_file)
                if current_checksum != metadata["checksum"]:
                    logger.warning(f"âš ï¸ Checksum invalide: {cache_key}")
                    return False

            return True

        except Exception as e:
            logger.warning(f"âš ï¸ Erreur validation cache {cache_key}: {e}")
            return False

    def _compute_file_checksum(self, filepath: Path) -> str:
        """Calcul checksum fichier"""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def load_from_cache(self, cache_key: str, indicator_type: str) -> Optional[Any]:
        """Chargement depuis cache"""
        if not self.is_cache_valid(cache_key, indicator_type):
            return None

        cache_file = self._get_cache_filepath(cache_key, indicator_type)

        try:
            # Lecture Parquet
            df = pd.read_parquet(cache_file)

            # Conversion selon format original
            if len(df.columns) == 1:
                # Single array (ex: ATR)
                return df.iloc[:, 0].values
            else:
                # Multiple arrays (ex: Bollinger Bands)
                return tuple(df[col].values for col in df.columns)

        except Exception as e:
            logger.error(f"âŒ Erreur chargement cache {cache_key}: {e}")
            return None

    def save_to_cache(
        self,
        cache_key: str,
        indicator_type: str,
        result: Union[np.ndarray, Tuple[np.ndarray, ...]],
        params: Dict[str, Any],
        symbol: str = "",
        timeframe: str = "",
    ) -> bool:
        """Sauvegarde en cache avec mÃ©tadonnÃ©es

        FIX B1: File locking pour Ã©viter race conditions en Ã©criture.
        """
        cache_file = self._get_cache_filepath(cache_key, indicator_type)
        meta_file = self._get_metadata_filepath(cache_key, indicator_type)
        lock_file = cache_file.with_suffix(".lock")

        try:
            # FIX B1: Acquire file lock (cross-platform)
            with open(lock_file, "w") as lockf:
                if platform.system() == "Windows":
                    msvcrt.locking(lockf.fileno(), msvcrt.LK_NBLCK, 1)
                else:
                    fcntl.flock(lockf, fcntl.LOCK_EX | fcntl.LOCK_NB)

                # PrÃ©paration DataFrame
                if isinstance(result, tuple):
                    # Multiples arrays (ex: Bollinger upper, middle, lower)
                    df_data = {}
                    for i, arr in enumerate(result):
                        df_data[f"result_{i}"] = arr
                    df = pd.DataFrame(df_data)
                else:
                    # Single array (ex: ATR)
                    df = pd.DataFrame({"result_0": result})

                # Sauvegarde Parquet avec compression
                df.to_parquet(cache_file, compression="snappy", index=False)

                # MÃ©tadonnÃ©es
                metadata = {
                    "cache_key": cache_key,
                    "indicator_type": indicator_type,
                    "params": params,
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "created_at": time.time(),
                    "data_shape": df.shape,
                    "columns": list(df.columns),
                }

                # Checksum si activÃ©
                if self.settings.checksum_validation:
                    metadata["checksum"] = self._compute_file_checksum(cache_file)

                # Sauvegarde mÃ©tadonnÃ©es
                with open(meta_file, "w") as f:
                    json.dump(metadata, f, indent=2)

                logger.debug(f"ðŸ’¾ Cache sauvÃ©: {cache_key}")

                # Lock released automatically on file close
                return True

        except (IOError, OSError) as e:
            # Lock conflict ou erreur I/O
            logger.warning(f"âš ï¸ Cache write conflict {cache_key}: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ Erreur sauvegarde cache {cache_key}: {e}")
            return False
        finally:
            # Cleanup lock file
            if lock_file.exists():
                try:
                    lock_file.unlink()
                except Exception:  # FIX: Permission denied ou autres
                    pass

    def cleanup_expired(self) -> int:
        """Nettoyage cache expirÃ©"""
        cleaned_count = 0

        for indicator_dir in self.cache_path.iterdir():
            if not indicator_dir.is_dir():
                continue

            for meta_file in indicator_dir.glob("*.meta"):
                try:
                    metadata = self._read_metadata(meta_file)
                    if not metadata:
                        continue

                    created_at = metadata.get("created_at", 0)
                    if time.time() - created_at > self.settings.ttl_seconds:
                        # Suppression fichiers cache et meta
                        cache_key = meta_file.stem
                        cache_file = indicator_dir / f"{cache_key}.parquet"

                        if cache_file.exists():
                            cache_file.unlink()
                        meta_file.unlink()

                        cleaned_count += 1

                except Exception as e:
                    logger.warning(f"âš ï¸ Erreur nettoyage {meta_file}: {e}")

        if cleaned_count > 0:
            logger.info(f"ðŸ§¹ Cache nettoyÃ©: {cleaned_count} fichiers supprimÃ©s")

        return cleaned_count


class IndicatorBank:
    """Banque d'indicateurs avec cache intelligent et batch processing"""

    def __init__(self, settings: Optional[IndicatorSettings] = None):
        self.settings = settings or IndicatorSettings()
        self.cache_manager = CacheManager(self.settings)

        # Calculateurs d'indicateurs
        self.calculators = {
            "bollinger": BollingerBands(
                BollingerSettings(use_gpu=self.settings.use_gpu)
            ),
            "atr": ATR(ATRSettings(use_gpu=self.settings.use_gpu)),
        }

        # Stats
        self.stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "computations": 0,
            "batch_operations": 0,
        }

        logger.info(f"ðŸ¦ IndicatorBank initialisÃ© - Cache: {self.settings.cache_dir}")

    def ensure(
        self,
        indicator_type: str,
        params: Dict[str, Any],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
        symbol: str = "",
        timeframe: str = "",
    ) -> Union[np.ndarray, Tuple[np.ndarray, ...]]:
        """
        Ensure indicateur: vÃ©rifie cache â†’ calcule si nÃ©cessaire

        Args:
            indicator_type: 'bollinger' ou 'atr'
            params: ParamÃ¨tres indicateur
            data: DonnÃ©es OHLCV (DataFrame) ou prix (array)
            symbol: Symbole (optionnel)
            timeframe: Timeframe (optionnel)

        Returns:
            RÃ©sultat indicateur ou None si erreur

        Exemple:
            ```python
            # Bollinger Bands
            bb_result = bank.ensure(
                'bollinger',
                {'period': 20, 'std': 2.0},
                ohlcv_data,
                symbol='BTCUSDC',
                timeframe='15m'
            )
            # bb_result = (upper, middle, lower)

            # ATR
            atr_result = bank.ensure(
                'atr',
                {'period': 14, 'method': 'ema'},
                ohlcv_data
            )
            # atr_result = atr_values array
            ```
        """
        start_time = time.time()

        # Validation type indicateur
        if indicator_type not in self.calculators:
            raise ValueError(f"Type indicateur non supportÃ©: {indicator_type}")

        # GÃ©nÃ©ration clÃ© cache
        data_hash = self.cache_manager._compute_data_hash(data)
        cache_key = self.cache_manager._generate_cache_key(
            indicator_type, params, symbol, timeframe, data_hash
        )

        # Tentative chargement cache
        cached_result = self.cache_manager.load_from_cache(cache_key, indicator_type)
        if cached_result is not None:
            self.stats["cache_hits"] += 1
            elapsed = time.time() - start_time
            logger.debug(
                f"ðŸŽ¯ Cache HIT {indicator_type}: {cache_key[:20]}... ({elapsed:.3f}s)"
            )
            return cached_result

        # Cache MISS â†’ calcul
        self.stats["cache_misses"] += 1
        logger.debug(f"âŒ Cache MISS {indicator_type}: {cache_key[:20]}...")

        try:
            result = self._compute_indicator(indicator_type, params, data)
            if result is None:
                raise RuntimeError(
                    f"_compute_indicator returned None for {indicator_type} with params {params}"
                )

            # Sauvegarde en cache
            self.cache_manager.save_to_cache(
                cache_key, indicator_type, result, params, symbol, timeframe
            )
            self.stats["computations"] += 1

            # Mise Ã  jour registry si activÃ©e
            if self.settings.auto_registry_update:
                self._update_registry(
                    indicator_type, cache_key, params, symbol, timeframe
                )

            elapsed = time.time() - start_time
            logger.debug(f"âœ… Compute {indicator_type}: {elapsed:.3f}s")
            return result

        except Exception as e:
            logger.error(f"âŒ Erreur calcul {indicator_type} {params}: {e}")
            raise RuntimeError(
                f"Failed to compute {indicator_type} with params {params}"
            ) from e

    def _compute_indicator(
        self,
        indicator_type: str,
        params: Dict[str, Any],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
    ) -> Union[np.ndarray, Tuple[np.ndarray, ...]]:
        """Calcul effectif d'un indicateur selon son type"""

        calculator = self.calculators[indicator_type]

        if indicator_type == "bollinger":
            # Extraction prix close
            if isinstance(data, pd.DataFrame):
                if "close" not in data.columns:
                    raise ValueError(
                        "DataFrame doit contenir colonne 'close' pour Bollinger"
                    )
                close = data["close"]
            else:
                close = data

            # Calcul Bollinger Bands
            return calculator.compute(
                close, period=params.get("period", 20), std=params.get("std", 2.0)
            )

        elif indicator_type == "atr":
            # Extraction prix OHLC
            if isinstance(data, pd.DataFrame):
                required_cols = ["high", "low", "close"]
                missing_cols = [col for col in required_cols if col not in data.columns]
                if missing_cols:
                    raise ValueError(
                        f"DataFrame doit contenir colonnes {missing_cols} pour ATR"
                    )

                high = data["high"]
                low = data["low"]
                close = data["close"]
            else:
                raise ValueError("ATR nÃ©cessite DataFrame OHLC, pas array simple")

            # Calcul ATR
            return calculator.compute(
                high,
                low,
                close,
                period=params.get("period", 14),
                method=params.get("method", "ema"),
            )

        else:
            raise ValueError(f"Type indicateur non implÃ©mentÃ©: {indicator_type}")

    def batch_ensure(
        self,
        indicator_type: str,
        params_list: List[Dict[str, Any]],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
        symbol: str = "",
        timeframe: str = "",
    ) -> Dict[str, Union[np.ndarray, Tuple[np.ndarray, ...]]]:
        """
        Calcul batch d'indicateurs avec mutualisation des intermÃ©diaires.

        Optimisations :
        - Concat des paramÃ¨tres en dim-0 pour calcul groupÃ©
        - Calcul SMA/TR une seule fois si partagÃ© entre paramÃ¨tres
        - Re-dÃ©batch stable avec prÃ©servation de l'ordre
        - Cache TTL + checksums avec clÃ©s JSON canonisÃ©es

        Args:
            indicator_type: Type d'indicateur ('bollinger', 'atr', etc.)
            params_list: Liste des paramÃ¨tres Ã  calculer
            data: DonnÃ©es source
            symbol: Symbole pour le cache
            timeframe: Timeframe pour le cache

        Returns:
            Dict[params_key, result] des rÃ©sultats par paramÃ¨tre

        Example:
            >>> bank = IndicatorBank()
            >>> params_list = [
            ...     {'period': 20, 'std': 2.0},
            ...     {'period': 20, 'std': 2.5},
            ...     {'period': 50, 'std': 2.0}
            ... ]
            >>> results = bank.batch_ensure('bollinger', params_list, close_data)
            >>> len(results) == 3
        """
        if not params_list:
            return {}

        logger.info(f"Batch ensure: {indicator_type}, {len(params_list)} paramÃ¨tres")

        batch_results = {}
        cache_hits = 0
        cache_misses = 0

        # Phase 1: VÃ©rification du cache existant
        uncached_params = []
        for params in params_list:
            params_key = self._params_to_cache_key(params)

            # VÃ©rification cache individuel
            cache_key = self.cache_manager._generate_cache_key(
                indicator_type,
                params,
                symbol,
                timeframe,
                self.cache_manager._compute_data_hash(data),
            )

            if self.cache_manager.is_cache_valid(cache_key, indicator_type):
                cached_result = self.cache_manager.load_from_cache(
                    cache_key, indicator_type
                )
                if cached_result is not None:
                    batch_results[params_key] = cached_result
                    cache_hits += 1
                    continue

            uncached_params.append(params)
            cache_misses += 1

        logger.debug(f"Cache: {cache_hits} hits, {cache_misses} misses")

        # Phase 2: Calcul batch des paramÃ¨tres manquants
        if uncached_params:
            computed_results = self._compute_batch_with_intermediates(
                indicator_type, uncached_params, data
            )

            # Phase 3: Sauvegarde en cache et ajout aux rÃ©sultats
            for params, result in zip(uncached_params, computed_results):
                params_key = self._params_to_cache_key(params)
                batch_results[params_key] = result

                # Sauvegarde cache
                cache_key = self.cache_manager._generate_cache_key(
                    indicator_type,
                    params,
                    symbol,
                    timeframe,
                    self.cache_manager._compute_data_hash(data),
                )

                self.cache_manager.save_to_cache(
                    cache_key, indicator_type, result, params, symbol, timeframe
                )

        # Phase 4: Mise Ã  jour du registry
        self._update_registry_batch(indicator_type, params_list, symbol, timeframe)

        logger.info(
            f"Batch terminÃ©: {len(batch_results)} rÃ©sultats "
            f"(cache hit rate: {cache_hits/(cache_hits+cache_misses):.1%})"
        )

        return batch_results

    def compute_batch(
        self,
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
        indicators: List[str],
        symbol: str = "",
        timeframe: str = "",
    ) -> Dict[str, Union[np.ndarray, Tuple[np.ndarray, ...]]]:
        """
        Calcule plusieurs indicateurs en batch (API simplifiÃ©e).

        Wrapper convivial sur batch_ensure() qui accepte une liste
        d'indicateurs au format "type_param" (ex: "rsi_14", "bb_20_2.0").

        Args:
            data: DonnÃ©es OHLCV (DataFrame ou array)
            indicators: Liste d'indicateurs au format "type_param"
                Exemples:
                - "rsi_14" â†’ RSI pÃ©riode 14
                - "bb_20" â†’ Bollinger Bands pÃ©riode 20 (std=2.0 par dÃ©faut)
                - "bb_20_2.5" â†’ Bollinger Bands pÃ©riode 20, std 2.5
                - "atr_14" â†’ ATR pÃ©riode 14
                - "sma_50" â†’ SMA pÃ©riode 50
            symbol: Symbole pour le cache (optionnel)
            timeframe: Timeframe pour le cache (optionnel)

        Returns:
            Dict[indicator_name, result] des rÃ©sultats par indicateur

        Raises:
            ValueError: Si format d'indicateur invalide

        Example:
            >>> bank = IndicatorBank()
            >>> df = pd.DataFrame({'close': [100, 101, 102, 103, 104]})
            >>> results = bank.compute_batch(
            ...     data=df,
            ...     indicators=["rsi_14", "bb_20", "sma_50"],
            ...     symbol="BTCUSDT",
            ...     timeframe="1h"
            ... )
            >>> 'rsi_14' in results
            True
            >>> 'bb_20' in results
            True
        """
        if not indicators:
            return {}

        logger.info(f"compute_batch: {len(indicators)} indicateurs pour {symbol}")

        # Grouper les indicateurs par type
        grouped_indicators = {}
        for indicator_str in indicators:
            indicator_type, params = self._parse_indicator_string(indicator_str)
            if indicator_type not in grouped_indicators:
                grouped_indicators[indicator_type] = []
            grouped_indicators[indicator_type].append((indicator_str, params))

        # Calculer chaque groupe avec batch_ensure
        all_results = {}
        for indicator_type, indicator_list in grouped_indicators.items():
            params_list = [params for _, params in indicator_list]

            # Appel Ã  batch_ensure
            batch_results = self.batch_ensure(
                indicator_type=indicator_type,
                params_list=params_list,
                data=data,
                symbol=symbol,
                timeframe=timeframe,
            )

            # Mapper les rÃ©sultats avec les noms d'indicateurs originaux
            for indicator_str, params in indicator_list:
                params_key = self._params_to_cache_key(params)
                if params_key in batch_results:
                    all_results[indicator_str] = batch_results[params_key]
                else:
                    logger.warning(f"RÃ©sultat manquant pour {indicator_str}")

        logger.info(f"compute_batch: {len(all_results)} rÃ©sultats calculÃ©s")
        return all_results

    def _parse_indicator_string(self, indicator_str: str) -> Tuple[str, Dict[str, Any]]:
        """
        Parse une chaÃ®ne d'indicateur au format "type_param1_param2".

        Args:
            indicator_str: ChaÃ®ne au format "type_param" (ex: "rsi_14")

        Returns:
            Tuple (type, params_dict)

        Raises:
            ValueError: Si format invalide

        Example:
            >>> bank._parse_indicator_string("rsi_14")
            ('rsi', {'period': 14})
            >>> bank._parse_indicator_string("bb_20_2.5")
            ('bollinger', {'period': 20, 'std': 2.5})
        """
        parts = indicator_str.split("_")
        if len(parts) < 2:
            raise ValueError(
                f"Format indicateur invalide: '{indicator_str}'. "
                f"Format attendu: 'type_param' (ex: 'rsi_14')"
            )

        indicator_type = parts[0].lower()

        # Mapper les raccourcis vers les noms complets
        type_mapping = {
            "bb": "bollinger",
            "sma": "sma",
            "ema": "ema",
            "rsi": "rsi",
            "atr": "atr",
            "macd": "macd",
        }

        if indicator_type not in type_mapping:
            raise ValueError(
                f"Type indicateur inconnu: '{indicator_type}'. "
                f"Types supportÃ©s: {list(type_mapping.keys())}"
            )

        indicator_type = type_mapping[indicator_type]

        # Parser les paramÃ¨tres selon le type
        if indicator_type == "bollinger":
            # Format: bb_period ou bb_period_std
            period = int(parts[1])
            std = float(parts[2]) if len(parts) > 2 else 2.0
            return indicator_type, {"period": period, "std": std}

        elif indicator_type in ["rsi", "atr", "sma", "ema"]:
            # Format: type_period
            period = int(parts[1])
            return indicator_type, {"period": period}

        elif indicator_type == "macd":
            # Format: macd_fast_slow_signal
            fast = int(parts[1]) if len(parts) > 1 else 12
            slow = int(parts[2]) if len(parts) > 2 else 26
            signal = int(parts[3]) if len(parts) > 3 else 9
            return indicator_type, {
                "fast": fast,
                "slow": slow,
                "signal": signal,
            }

        else:
            raise ValueError(f"Parser non implÃ©mentÃ© pour type '{indicator_type}'")

    def _compute_batch_with_intermediates(
        self,
        indicator_type: str,
        params_list: List[Dict[str, Any]],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
    ) -> List[Union[np.ndarray, Tuple[np.ndarray, ...]]]:
        """Calcule en batch avec mutualisation des intermÃ©diaires."""

        if indicator_type == "bollinger":
            return self._batch_bollinger_with_sma_sharing(params_list, data)
        elif indicator_type == "atr":
            return self._batch_atr_with_tr_sharing(params_list, data)
        else:
            # Fallback: calcul sÃ©quentiel
            results = []
            for params in params_list:
                result = self._compute_indicator(indicator_type, params, data)
                results.append(result)
            return results

    def _batch_bollinger_with_sma_sharing(
        self,
        params_list: List[Dict[str, Any]],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
    ) -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """Calcul batch Bollinger avec partage des SMA."""

        # Extraction des pÃ©riodes uniques
        unique_periods = list(set(params["period"] for params in params_list))

        # Calcul des SMA partagÃ©es
        sma_cache = {}
        for period in unique_periods:
            if isinstance(data, pd.Series):
                sma = data.rolling(window=period).mean().values
            elif isinstance(data, np.ndarray):
                sma = pd.Series(data).rolling(window=period).mean().values
            else:
                # DataFrame: utilise la colonne 'close' par dÃ©faut
                close_col = data.get("close", data.iloc[:, 0])
                sma = close_col.rolling(window=period).mean().values

            sma_cache[period] = sma

        # Calcul des Ã©carts-types et bandes
        results = []
        for params in params_list:
            period = params["period"]
            std_multiplier = params.get("std", 2.0)

            sma = sma_cache[period]

            # Calcul de l'Ã©cart-type mobile
            if isinstance(data, pd.Series):
                rolling_std = data.rolling(window=period).std().values
            elif isinstance(data, np.ndarray):
                rolling_std = pd.Series(data).rolling(window=period).std().values
            else:
                close_col = data.get("close", data.iloc[:, 0])
                rolling_std = close_col.rolling(window=period).std().values

            # Bandes de Bollinger
            upper_band = sma + (std_multiplier * rolling_std)
            lower_band = sma - (std_multiplier * rolling_std)

            results.append((upper_band, sma, lower_band))

        return results

    def _batch_atr_with_tr_sharing(
        self,
        params_list: List[Dict[str, Any]],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
    ) -> List[np.ndarray]:
        """Calcul batch ATR avec partage du True Range."""

        # Calcul du True Range une seule fois
        if isinstance(data, pd.DataFrame) and all(
            col in data.columns for col in ["high", "low", "close"]
        ):
            high = data["high"].values
            low = data["low"].values
            close = data["close"].values

            # True Range calculation
            tr1 = high - low
            tr2 = np.abs(high - np.roll(close, 1))
            tr3 = np.abs(low - np.roll(close, 1))

            true_range = np.maximum.reduce([tr1, tr2, tr3])
            true_range[0] = tr1[0]  # Premier Ã©lÃ©ment
        else:
            # Fallback pour donnÃ©es simplifiÃ©es
            if isinstance(data, (pd.Series, np.ndarray)):
                prices = data.values if isinstance(data, pd.Series) else data
                true_range = np.abs(np.diff(prices, prepend=prices[0]))
            else:
                raise ValueError("Format de donnÃ©es non supportÃ© pour ATR batch")

        # Calcul des ATR pour chaque pÃ©riode
        results = []
        for params in params_list:
            period = params.get("period", 14)
            method = params.get("method", "ema")

            if method == "ema":
                # EMA du True Range
                alpha = 2.0 / (period + 1)
                atr = np.zeros_like(true_range)
                atr[0] = true_range[0]

                for i in range(1, len(true_range)):
                    atr[i] = alpha * true_range[i] + (1 - alpha) * atr[i - 1]
            else:
                # SMA du True Range
                atr = pd.Series(true_range).rolling(window=period).mean().values

            results.append(atr)

        return results

    def _params_to_cache_key(self, params: Dict[str, Any]) -> str:
        """Convertit les paramÃ¨tres en clÃ© de cache stable."""
        import json

        return json.dumps(params, sort_keys=True, separators=(",", ":"))

    def _update_registry_batch(
        self,
        indicator_type: str,
        params_list: List[Dict[str, Any]],
        symbol: str,
        timeframe: str,
    ) -> None:
        """Met Ã  jour le registry pour un batch de calculs."""
        # TODO: ImplÃ©mentation du registry batch si nÃ©cessaire
        pass

    def force_recompute(
        self,
        indicator_type: str,
        params: Dict[str, Any],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
        symbol: str = "",
        timeframe: str = "",
    ) -> Union[np.ndarray, Tuple[np.ndarray, ...]]:
        """
        Recompute forcÃ© (ignore cache)

        Args:
            MÃªmes que ensure()

        Returns:
            RÃ©sultat indicateur recalculÃ©
        """
        logger.info(f"ðŸ”„ Force recompute {indicator_type}: {params}")

        # Suppression cache existant
        data_hash = self.cache_manager._compute_data_hash(data)
        cache_key = self.cache_manager._generate_cache_key(
            indicator_type, params, symbol, timeframe, data_hash
        )

        cache_file = self.cache_manager._get_cache_filepath(cache_key, indicator_type)
        meta_file = self.cache_manager._get_metadata_filepath(cache_key, indicator_type)

        if cache_file.exists():
            cache_file.unlink()
        if meta_file.exists():
            meta_file.unlink()

        # Calcul forcÃ©
        try:
            result = self._compute_indicator(indicator_type, params, data)

            # Nouvelle sauvegarde
            self.cache_manager.save_to_cache(
                cache_key, indicator_type, result, params, symbol, timeframe
            )
            self.stats["computations"] += 1

            # Mise Ã  jour registry
            if self.settings.auto_registry_update:
                self._update_registry(
                    indicator_type, cache_key, params, symbol, timeframe
                )

            return result

        except Exception as e:
            logger.error(f"âŒ Erreur force recompute {indicator_type}: {e}")
            raise RuntimeError(
                f"Failed to force recompute {indicator_type} with params {params}"
            ) from e

    def _batch_ensure_parallel(
        self,
        indicator_type: str,
        params_list: List[Dict[str, Any]],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
        symbol: str,
        timeframe: str,
    ) -> Dict[str, Union[np.ndarray, Tuple[np.ndarray, ...]]]:
        """Batch ensure parallÃ¨le avec ThreadPoolExecutor"""

        logger.info(
            f"ðŸš€ Batch parallel {indicator_type}: {len(params_list)} paramÃ¨tres, {self.settings.max_workers} workers"
        )

        results = {}

        with ThreadPoolExecutor(max_workers=self.settings.max_workers) as executor:
            # Soumission des tÃ¢ches
            future_to_params = {}
            for params in params_list:
                future = executor.submit(
                    self.ensure, indicator_type, params, data, symbol, timeframe
                )
                future_to_params[future] = params

            # Collecte des rÃ©sultats
            for future in as_completed(future_to_params):
                params = future_to_params[future]
                key = self._params_to_key(params)

                try:
                    result = future.result(timeout=30.0)  # 30s timeout
                    results[key] = result
                except Exception as e:
                    logger.error(f"âŒ Erreur batch {key}: {e}")
                    results[key] = None

        return results

    def _params_to_key(self, params: Dict[str, Any]) -> str:
        """Conversion paramÃ¨tres en clÃ© string"""
        sorted_params = dict(sorted(params.items()))
        key_parts = []
        for k, v in sorted_params.items():
            if isinstance(v, float):
                key_parts.append(f"{k}={v:.3f}")
            else:
                key_parts.append(f"{k}={v}")
        return "_".join(key_parts)

    def _update_registry(
        self,
        indicator_type: str,
        cache_key: str,
        params: Dict[str, Any],
        symbol: str,
        timeframe: str,
    ):
        """Mise Ã  jour du registry avec nouvel indicateur"""
        registry_file = (
            self.settings.cache_path / "registry" / f"{indicator_type}_registry.parquet"
        )

        # Nouvelle entrÃ©e
        new_entry = {
            "cache_key": cache_key,
            "indicator_type": indicator_type,
            "symbol": symbol,
            "timeframe": timeframe,
            "params": json.dumps(params, sort_keys=True),
            "created_at": pd.Timestamp.now(),
            "last_accessed": pd.Timestamp.now(),
        }

        try:
            # Chargement registry existant
            if registry_file.exists():
                registry_df = pd.read_parquet(registry_file)
                # Mise Ã  jour ou ajout
                mask = registry_df["cache_key"] == cache_key
                if mask.any():
                    registry_df.loc[mask, "last_accessed"] = new_entry["last_accessed"]
                else:
                    registry_df = pd.concat(
                        [registry_df, pd.DataFrame([new_entry])], ignore_index=True
                    )
            else:
                # Nouveau registry
                registry_df = pd.DataFrame([new_entry])

            # Sauvegarde
            registry_df.to_parquet(registry_file, index=False)
            logger.debug(f"ðŸ“‹ Registry mis Ã  jour: {indicator_type}")

        except Exception as e:
            logger.warning(f"âš ï¸ Erreur mise Ã  jour registry: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """Statistiques d'utilisation"""
        total_requests = self.stats["cache_hits"] + self.stats["cache_misses"]
        cache_hit_rate = (
            (self.stats["cache_hits"] / total_requests * 100)
            if total_requests > 0
            else 0
        )

        return {
            **self.stats,
            "cache_hit_rate_pct": cache_hit_rate,
            "total_requests": total_requests,
        }

    def cleanup_cache(self) -> int:
        """Nettoyage cache expirÃ©"""
        return self.cache_manager.cleanup_expired()


# ========================================
# FONCTIONS PUBLIQUES (API simplifiÃ©e)
# ========================================

# Instance globale pour API simplifiÃ©e
_global_bank: Optional[IndicatorBank] = None


def _get_global_bank(cache_dir: str = "indicators_cache") -> IndicatorBank:
    """RÃ©cupÃ©ration instance globale IndicatorBank"""
    global _global_bank
    if _global_bank is None or _global_bank.settings.cache_dir != cache_dir:
        settings = IndicatorSettings(cache_dir=cache_dir)
        _global_bank = IndicatorBank(settings)
    return _global_bank


def ensure_indicator(
    indicator_type: str,
    params: Dict[str, Any],
    data: Union[np.ndarray, pd.Series, pd.DataFrame],
    symbol: str = "",
    timeframe: str = "",
    cache_dir: str = "indicators_cache",
) -> Optional[Union[np.ndarray, Tuple[np.ndarray, ...]]]:
    """
    API simple pour ensure indicateur

    Args:
        indicator_type: 'bollinger' ou 'atr'
        params: ParamÃ¨tres indicateur
        data: DonnÃ©es OHLCV
        symbol, timeframe: MÃ©tadonnÃ©es (optionnel)
        cache_dir: RÃ©pertoire cache (dÃ©faut: "indicators_cache")

    Returns:
        RÃ©sultat indicateur

    Exemple:
        ```python
        from threadx.indicators.bank import ensure_indicator
        import pandas as pd

        # DonnÃ©es OHLCV
        ohlcv = pd.DataFrame({
            'open': [100, 101, 102],
            'high': [105, 106, 107],
            'low': [95, 96, 97],
            'close': [101, 102, 103],
            'volume': [1000, 1500, 1200]
        })

        # Bollinger Bands
        bb_result = ensure_indicator(
            'bollinger',
            {'period': 20, 'std': 2.0},
            ohlcv,
            symbol='BTCUSDC',
            timeframe='15m'
        )
        if bb_result:
            upper, middle, lower = bb_result
            print(f"BB dernier: upper={upper[-1]:.2f}")

        # ATR
        atr_result = ensure_indicator(
            'atr',
            {'period': 14, 'method': 'ema'},
            ohlcv
        )
        if atr_result:
            print(f"ATR dernier: {atr_result[-1]:.4f}")
        ```
    """
    bank = _get_global_bank(cache_dir)
    return bank.ensure(indicator_type, params, data, symbol, timeframe)


def force_recompute_indicator(
    indicator_type: str,
    params: Dict[str, Any],
    data: Union[np.ndarray, pd.Series, pd.DataFrame],
    symbol: str = "",
    timeframe: str = "",
    cache_dir: str = "indicators_cache",
) -> Optional[Union[np.ndarray, Tuple[np.ndarray, ...]]]:
    """
    API simple pour force recompute indicateur

    Args:
        MÃªmes que ensure_indicator()

    Returns:
        RÃ©sultat indicateur recalculÃ©

    Exemple:
        ```python
        # Force recalcul mÃªme si cache valide
        bb_result = force_recompute_indicator(
            'bollinger',
            {'period': 20, 'std': 2.0},
            ohlcv_data
        )
        ```
    """
    bank = _get_global_bank(cache_dir)
    return bank.force_recompute(indicator_type, params, data, symbol, timeframe)


def batch_ensure_indicators(
    indicator_type: str,
    params_list: List[Dict[str, Any]],
    data: Union[np.ndarray, pd.Series, pd.DataFrame],
    symbol: str = "",
    timeframe: str = "",
    cache_dir: str = "indicators_cache",
) -> Dict[str, Union[np.ndarray, Tuple[np.ndarray, ...]]]:
    """
    API simple pour batch ensure

    Args:
        indicator_type: Type d'indicateur
        params_list: Liste paramÃ¨tres
        data: DonnÃ©es communes
        symbol, timeframe: MÃ©tadonnÃ©es
        cache_dir: RÃ©pertoire cache

    Returns:
        Dict[param_key] = rÃ©sultat

    Exemple:
        ```python
        # Batch Bollinger avec diffÃ©rents paramÃ¨tres
        params_list = [
            {'period': 20, 'std': 2.0},
            {'period': 50, 'std': 1.5},
            {'period': 10, 'std': 2.5}
        ]

        results = batch_ensure_indicators('bollinger', params_list, ohlcv_data)

        for key, result in results.items():
            if result:
                upper, middle, lower = result
                print(f"{key}: dernier upper={upper[-1]:.2f}")
        ```
    """
    bank = _get_global_bank(cache_dir)
    return bank.batch_ensure(indicator_type, params_list, data, symbol, timeframe)


def get_bank_stats(cache_dir: str = "indicators_cache") -> Dict[str, Any]:
    """Statistiques de la banque d'indicateurs"""
    bank = _get_global_bank(cache_dir)
    return bank.get_stats()


def cleanup_indicators_cache(cache_dir: str = "indicators_cache") -> int:
    """Nettoyage du cache d'indicateurs"""
    bank = _get_global_bank(cache_dir)
    return bank.cleanup_cache()


# ========================================
# UTILITAIRES ET VALIDATION
# ========================================


def validate_bank_integrity(cache_dir: str = "indicators_cache") -> Dict[str, Any]:
    """
    Validation intÃ©gritÃ© complÃ¨te de la banque

    Returns:
        Dict avec rÃ©sultats validation
    """
    cache_path = Path(cache_dir)
    results = {
        "total_indicators": 0,
        "valid_cache": 0,
        "invalid_cache": 0,
        "orphaned_files": 0,
        "corrupted_files": 0,
        "expired_files": 0,
        "details": {},
    }

    if not cache_path.exists():
        results["error"] = f"Cache directory does not exist: {cache_dir}"
        return results

    settings = IndicatorSettings(cache_dir=cache_dir)
    cache_manager = CacheManager(settings)

    # Scan par type d'indicateur
    for indicator_type in ["bollinger", "atr"]:
        indicator_dir = cache_path / indicator_type
        if not indicator_dir.exists():
            continue

        type_stats = {"valid": 0, "invalid": 0, "expired": 0, "corrupted": 0}

        for meta_file in indicator_dir.glob("*.meta"):
            results["total_indicators"] += 1
            cache_key = meta_file.stem

            try:
                # Validation cache
                if cache_manager.is_cache_valid(cache_key, indicator_type):
                    type_stats["valid"] += 1
                    results["valid_cache"] += 1
                else:
                    type_stats["invalid"] += 1
                    results["invalid_cache"] += 1

                    # DÃ©tail de l'invaliditÃ©
                    metadata = cache_manager._read_metadata(meta_file)
                    if not metadata:
                        type_stats["corrupted"] += 1
                        results["corrupted_files"] += 1
                        continue

                    created_at = metadata.get("created_at", 0)
                    if time.time() - created_at > settings.ttl_seconds:
                        type_stats["expired"] += 1
                        results["expired_files"] += 1

            except Exception as e:
                type_stats["corrupted"] += 1
                results["corrupted_files"] += 1
                logger.warning(f"âš ï¸ Fichier corrompu {meta_file}: {e}")

        results["details"][indicator_type] = type_stats

    # Fichiers orphelins (parquet sans meta)
    for indicator_dir in cache_path.iterdir():
        if not indicator_dir.is_dir() or indicator_dir.name not in ["bollinger", "atr"]:
            continue

        parquet_files = set(f.stem for f in indicator_dir.glob("*.parquet"))
        meta_files = set(f.stem for f in indicator_dir.glob("*.meta"))

        orphaned = parquet_files - meta_files
        results["orphaned_files"] += len(orphaned)

    return results


def benchmark_bank_performance(
    cache_dir: str = "indicators_cache", n_indicators: int = 100, data_size: int = 1000
) -> Dict[str, Any]:
    """
    Benchmark performance de la banque d'indicateurs

    Args:
        cache_dir: RÃ©pertoire cache
        n_indicators: Nombre d'indicateurs Ã  tester
        data_size: Taille des donnÃ©es test

    Returns:
        Dict avec mÃ©triques performance
    """
    logger.info(f"ðŸ Benchmark IndicatorBank: {n_indicators} indicateurs")

    # DonnÃ©es test
    np.random.seed(42)
    ohlcv = pd.DataFrame(
        {
            "open": np.random.randn(data_size) * 5 + 100,
            "high": np.random.randn(data_size) * 5 + 105,
            "low": np.random.randn(data_size) * 5 + 95,
            "close": np.random.randn(data_size) * 5 + 100,
            "volume": np.random.randint(1000, 10000, data_size),
        }
    )

    # ParamÃ¨tres test
    bb_params = [
        {"period": p, "std": s} for p in range(10, 30, 2) for s in [1.5, 2.0, 2.5]
    ][: n_indicators // 2]

    atr_params = [
        {"period": p, "method": m} for p in range(10, 30, 2) for m in ["ema", "sma"]
    ][: n_indicators // 2]

    bank = IndicatorBank(IndicatorSettings(cache_dir=cache_dir))

    results = {
        "setup": {
            "n_indicators": len(bb_params) + len(atr_params),
            "data_size": data_size,
            "cache_dir": cache_dir,
        },
        "timings": {},
        "cache_performance": {},
    }

    # Test 1: Calculs initiaux (cache cold)
    start_time = time.time()

    for params in bb_params:
        bank.ensure("bollinger", params, ohlcv)

    for params in atr_params:
        bank.ensure("atr", params, ohlcv)

    cold_time = time.time() - start_time
    results["timings"]["cold_cache"] = cold_time

    # Test 2: Rechargement depuis cache (cache warm)
    start_time = time.time()

    for params in bb_params:
        bank.ensure("bollinger", params, ohlcv)

    for params in atr_params:
        bank.ensure("atr", params, ohlcv)

    warm_time = time.time() - start_time
    results["timings"]["warm_cache"] = warm_time

    # Test 3: Batch processing
    start_time = time.time()
    bank.batch_ensure("bollinger", bb_params, ohlcv)
    bank.batch_ensure("atr", atr_params, ohlcv)
    batch_time = time.time() - start_time
    results["timings"]["batch_processing"] = batch_time

    # Stats cache
    stats = bank.get_stats()
    results["cache_performance"] = {
        "hit_rate_pct": stats["cache_hit_rate_pct"],
        "total_requests": stats["total_requests"],
        "cache_hits": stats["cache_hits"],
        "cache_misses": stats["cache_misses"],
        "computations": stats["computations"],
        "speedup_warm": cold_time / warm_time if warm_time > 0 else 0,
    }

    logger.info("âœ… Benchmark terminÃ©")
    logger.info(f"   Cold cache: {cold_time:.2f}s")
    logger.info(f"   Warm cache: {warm_time:.2f}s")
    logger.info(f"   Batch: {batch_time:.2f}s")
    logger.info(f"   Cache hit rate: {stats['cache_hit_rate_pct']:.1f}%")
    logger.info(
        f"   Speedup warm: {cold_time/warm_time:.1f}x"
        if warm_time > 0
        else "   Speedup: N/A"
    )

    return results


if __name__ == "__main__":
    # Test rapide
    print("ðŸ¦ ThreadX IndicatorBank - Test rapide")

    # DonnÃ©es test OHLCV
    np.random.seed(42)
    n = 1000
    ohlcv = pd.DataFrame(
        {
            "open": np.random.randn(n) * 5 + 100,
            "high": np.random.randn(n) * 5 + 105,
            "low": np.random.randn(n) * 5 + 95,
            "close": np.random.randn(n) * 5 + 100,
            "volume": np.random.randint(1000, 10000, n),
        }
    )

    # Test ensure simple
    print("\nðŸ“Š Test ensure simple...")
    bb_result = ensure_indicator(
        "bollinger",
        {"period": 20, "std": 2.0},
        ohlcv,
        symbol="TESTBTC",
        timeframe="15m",
        cache_dir="test_cache",
    )

    if bb_result:
        upper, middle, lower = bb_result
        print(f"âœ… Bollinger: {len(upper)} points")
        print(f"   Upper[-1]: {upper[-1]:.2f}")
        print(f"   Middle[-1]: {middle[-1]:.2f}")
        print(f"   Lower[-1]: {lower[-1]:.2f}")

    # Test cache hit
    print("\nðŸŽ¯ Test cache hit...")
    start = time.time()
    bb_result2 = ensure_indicator(
        "bollinger",
        {"period": 20, "std": 2.0},
        ohlcv,
        symbol="TESTBTC",
        timeframe="15m",
        cache_dir="test_cache",
    )
    cache_time = time.time() - start
    print(f"âœ… Cache hit: {cache_time:.4f}s")

    # Test ATR
    print("\nðŸ“ˆ Test ATR...")
    atr_result = ensure_indicator(
        "atr", {"period": 14, "method": "ema"}, ohlcv, cache_dir="test_cache"
    )

    if atr_result is not None:
        print(f"âœ… ATR: {len(atr_result)} points")
        print(f"   ATR[-1]: {atr_result[-1]:.4f}")
        print(f"   ATR moyen: {np.nanmean(atr_result):.4f}")

    # Stats
    print("\nðŸ“Š Stats bancaire...")
    stats = get_bank_stats("test_cache")
    print(f"   Hit rate: {stats['cache_hit_rate_pct']:.1f}%")
    print(f"   Total requests: {stats['total_requests']}")
    print(f"   Computations: {stats['computations']}")

    print("\nâœ… Tests terminÃ©s!")




----------------------------------------
Fichier: indicators\bollinger.py
#!/usr/bin/env python3
"""
ThreadX Bollinger Bands - ImplÃ©mentation vectorisÃ©e GPU/CPU
===========================================================

Calcul vectorisÃ© des bandes de Bollinger avec support:
- GPU multi-carte (RTX 5090 + RTX 2060)
- Batch processing optimisÃ©
- Fallback CPU transparent
- Device-agnostic wrappers

Formule Bollinger Bands:
- Middle Band = SMA(close, period)
- Upper Band = Middle + (std * StdDev(close, period))
- Lower Band = Middle - (std * StdDev(close, period))

Optimisations:
- Vectorisation complÃ¨te NumPy/CuPy
- Split GPU 75%/25% selon puissance carte
- Cache intermÃ©diaire pour rÃ©utilisation SMA
- Synchronisation NCCL pour multi-GPU

Exemple d'usage:
    ```python
    import numpy as np
    from threadx.indicators.bollinger import compute_bollinger_bands

    # DonnÃ©es OHLCV
    close = np.random.randn(1000) * 10 + 100

    # Calcul simple
    upper, middle, lower = compute_bollinger_bands(close, period=20, std=2.0)

    # Calcul batch multiple paramÃ¨tres
    from threadx.indicators.bollinger import compute_bollinger_batch
    params = [
        {'period': 20, 'std': 2.0},
        {'period': 50, 'std': 1.5},
        {'period': 10, 'std': 2.5}
    ]
    results = compute_bollinger_batch(close, params)
    ```
"""

import logging
import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Union, Any
import numpy as np
import pandas as pd

# GPU imports avec fallback robuste
try:
    import cupy as cp

    HAS_CUPY = True
    # Test si GPU disponible
    try:
        cp.cuda.Device(0).use()
        GPU_AVAILABLE = True
        N_GPUS = cp.cuda.runtime.getDeviceCount()
    except Exception:
        GPU_AVAILABLE = False
        N_GPUS = 0
except ImportError:
    HAS_CUPY = False
    GPU_AVAILABLE = False
    N_GPUS = 0

    # Mock robuste de CuPy avec toutes les fonctions nÃ©cessaires
    class MockCudaDevice:
        def __init__(self, device_id):
            self.device_id = device_id

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def use(self):
            pass

    class MockCudaRuntime:
        @staticmethod
        def getDeviceCount():
            return 0

    class MockCuda:
        runtime = MockCudaRuntime()

        @staticmethod
        def Device(device_id):
            return MockCudaDevice(device_id)

    class MockCuPy:
        cuda = MockCuda()
        float64 = np.float64
        nan = np.nan

        @staticmethod
        def asarray(x):
            return np.asarray(x)

        @staticmethod
        def asnumpy(x):
            return np.asarray(x)

        @staticmethod
        def convolve(a, v, mode="full"):
            # Conversion type-safe pour numpy.convolve
            if mode in ("full", "valid", "same"):
                return np.convolve(a, v, mode=mode)  # type: ignore
            else:
                return np.convolve(a, v, mode="full")  # type: ignore

        @staticmethod
        def ones(shape, dtype=None):
            return np.ones(shape, dtype=dtype)

        @staticmethod
        def zeros_like(a):
            return np.zeros_like(a)

        @staticmethod
        def sqrt(x):
            return np.sqrt(x)

        @staticmethod
        def concatenate(arrays):
            return np.concatenate(arrays)

        @staticmethod
        def full(shape, fill_value, dtype=None):
            return np.full(shape, fill_value, dtype=dtype)

        @staticmethod
        def std(a, ddof=0):
            return np.std(a, ddof=ddof)

    cp = MockCuPy()

# Configuration logging
logger = logging.getLogger(__name__)


@dataclass
class BollingerSettings:
    """Configuration pour calculs Bollinger Bands"""

    period: int = 20
    std: float = 2.0
    use_gpu: bool = True
    gpu_batch_size: int = 1000
    cpu_fallback: bool = True
    gpu_split_ratio: Tuple[float, float] = (0.75, 0.25)  # RTX 5090 / RTX 2060

    def __post_init__(self):
        """Validation des paramÃ¨tres"""
        if self.period < 2:
            raise ValueError(f"Period doit Ãªtre >= 2, reÃ§u: {self.period}")
        if self.std <= 0:
            raise ValueError(f"Std doit Ãªtre > 0, reÃ§u: {self.std}")
        if not (0.1 <= sum(self.gpu_split_ratio) <= 1.0):
            raise ValueError(f"gpu_split_ratio invalide: {self.gpu_split_ratio}")


class GPUManager:
    """Gestionnaire GPU multi-carte avec rÃ©partition de charge"""

    def __init__(self, settings: BollingerSettings):
        self.settings = settings
        self.available_gpus: list[int] = []
        self.gpu_capabilities: dict[int, dict[str, Any]] = {}

        if HAS_CUPY and GPU_AVAILABLE:
            self._detect_gpus()
            logger.info(f"ðŸ”¥ GPU Manager: {len(self.available_gpus)} GPU(s) dÃ©tectÃ©s")
            for gpu_id, cap in self.gpu_capabilities.items():
                logger.debug(f"   GPU {gpu_id}: {cap['name']} ({cap['memory']:.1f}GB)")

    def _detect_gpus(self):
        """DÃ©tection et profilage des GPU disponibles"""
        try:
            for gpu_id in range(N_GPUS):
                with cp.cuda.Device(gpu_id):  # type: ignore
                    # Infos GPU
                    props = cp.cuda.runtime.getDeviceProperties(gpu_id)  # type: ignore
                    memory_total = cp.cuda.runtime.memGetInfo()[1] / (
                        1024**3
                    )  # GB  # type: ignore

                    self.available_gpus.append(gpu_id)
                    self.gpu_capabilities[gpu_id] = {
                        "name": props["name"].decode("utf-8"),
                        "memory": memory_total,
                        "compute_capability": (props["major"], props["minor"]),
                        "multiprocessors": props["multiProcessorCount"],
                    }
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur dÃ©tection GPU: {e}")
            self.available_gpus = []

    def split_workload(self, data_size: int) -> List[Tuple[int, int, int]]:
        """
        Split workload entre GPU selon leurs capacitÃ©s

        Returns:
            List[(gpu_id, start_idx, end_idx)]
        """
        if not self.available_gpus:
            return []

        splits = []
        if len(self.available_gpus) == 1:
            # Single GPU
            splits.append((self.available_gpus[0], 0, data_size))
        elif len(self.available_gpus) >= 2:
            # Multi-GPU avec ratio configurÃ©
            gpu1_size = int(data_size * self.settings.gpu_split_ratio[0])
            gpu2_size = data_size - gpu1_size

            splits.append((self.available_gpus[0], 0, gpu1_size))
            if gpu2_size > 0:
                splits.append((self.available_gpus[1], gpu1_size, data_size))

        logger.debug(f"ðŸ”„ Workload split: {[(s[0], s[2]-s[1]) for s in splits]}")
        return splits


class BollingerBands:
    """Calculateur Bollinger Bands vectorisÃ© avec support GPU multi-carte"""

    def __init__(self, settings: Optional[BollingerSettings] = None):
        self.settings = settings or BollingerSettings()
        self.gpu_manager = GPUManager(self.settings)
        self._cache: dict[str, Any] = {}  # Cache pour SMA rÃ©utilisables

        logger.info(
            f"ðŸŽ¯ Bollinger Bands initialisÃ© - GPU: {GPU_AVAILABLE}, Multi-GPU: {len(self.gpu_manager.available_gpus)}"
        )

    def compute(
        self,
        close: Union[np.ndarray, pd.Series],
        period: Optional[Union[int, float]] = None,
        std: Optional[float] = None,
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Calcul Bollinger Bands pour une sÃ©rie de prix

        Args:
            close: Prix de clÃ´ture (array-like)
            period: PÃ©riode pour SMA (dÃ©faut: settings.period)
            std: Multiplicateur Ã©cart-type (dÃ©faut: settings.std)

        Returns:
            Tuple[upper_band, middle_band, lower_band]

        Exemple:
            ```python
            bb = BollingerBands()
            upper, middle, lower = bb.compute(close_prices, period=20, std=2.0)
            ```
        """
        # ParamÃ¨tres
        period = period or self.settings.period
        std = std or self.settings.std

        # Conversion explicite des types
        period = int(period)  # Conversion float â†’ int

        # Conversion en numpy avec types prÃ©cis
        if isinstance(close, pd.Series):
            close = np.asarray(close.values, dtype=np.float64)  # Conversion explicite
        else:
            close = np.asarray(close, dtype=np.float64)

        if len(close) < period:
            raise ValueError(f"DonnÃ©es insuffisantes: {len(close)} < period={period}")

        # Tentative GPU
        if (
            self.settings.use_gpu
            and GPU_AVAILABLE
            and len(self.gpu_manager.available_gpus) > 0
        ):
            try:
                return self._compute_gpu(close, period, std)
            except Exception as e:
                logger.warning(f"âš ï¸ GPU failed, fallback CPU: {e}")
                if not self.settings.cpu_fallback:
                    raise

        # Fallback CPU
        return self._compute_cpu(close, period, std)

    def _compute_gpu(
        self, close: np.ndarray, period: int, std: float
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Calcul GPU avec rÃ©partition multi-carte"""

        # Transfert vers GPU principal
        with cp.cuda.Device(self.gpu_manager.available_gpus[0]):  # type: ignore
            close_gpu = cp.asarray(close)  # type: ignore

            # SMA (Simple Moving Average)
            middle = self._sma_gpu(close_gpu, period)

            # Rolling standard deviation
            rolling_std = self._rolling_std_gpu(close_gpu, period)

            # Bandes
            std_term = std * rolling_std
            upper = middle + std_term
            lower = middle - std_term

            # Retour CPU
            return cp.asnumpy(upper), cp.asnumpy(middle), cp.asnumpy(lower)

    def _sma_gpu(self, values_gpu, period: int) -> np.ndarray:
        """Simple Moving Average GPU optimisÃ©"""
        # Utilise convolution pour efficacitÃ©
        kernel = cp.ones(period, dtype=cp.float64) / period

        # Convolution avec padding
        sma = cp.convolve(values_gpu, kernel, mode="valid")

        # Padding pour align avec input
        padding = cp.full(period - 1, cp.nan, dtype=cp.float64)
        result = cp.concatenate([padding, sma])
        return cp.asnumpy(result)

    def _rolling_std_gpu(self, values_gpu, period: int) -> np.ndarray:
        """Rolling standard deviation GPU"""
        n = len(values_gpu)
        result = cp.full(n, cp.nan, dtype=cp.float64)

        # Vectorized rolling calculation
        for i in range(period - 1, n):
            window = values_gpu[i - period + 1 : i + 1]
            result[i] = cp.std(window, ddof=0)

        return cp.asnumpy(result)

    def _compute_cpu(
        self, close: np.ndarray, period: int, std: float
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Fallback CPU vectorisÃ©"""

        # SMA avec pandas pour efficacitÃ©
        close_series = pd.Series(close)
        middle = close_series.rolling(window=period, min_periods=period).mean().values

        # Rolling std
        rolling_std = (
            close_series.rolling(window=period, min_periods=period).std(ddof=0).values
        )

        # Bandes
        std_term = std * rolling_std  # type: ignore
        upper = middle + std_term  # type: ignore
        lower = middle - std_term  # type: ignore

        return upper, middle, lower  # type: ignore

    def compute_batch(
        self,
        close: Union[np.ndarray, pd.Series],
        params_list: List[Dict[str, Union[int, float]]],
    ) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """
        Calcul batch pour multiples paramÃ¨tres

        Args:
            close: Prix de clÃ´ture
            params_list: Liste de dictionnaires {'period': int, 'std': float}

        Returns:
            Dict[param_key] = (upper, middle, lower)

        Exemple:
            ```python
            params = [
                {'period': 20, 'std': 2.0},
                {'period': 50, 'std': 1.5}
            ]
            results = bb.compute_batch(close, params)
            print(results['20_2.0'])  # (upper, middle, lower)
            ```
        """
        start_time = time.time()
        results = {}

        logger.info(f"ðŸ”„ Bollinger batch: {len(params_list)} paramÃ¨tres")

        # Multi-GPU batch si seuil atteint
        if (
            len(params_list) >= 100
            and self.settings.use_gpu
            and len(self.gpu_manager.available_gpus) >= 2
        ):

            return self._compute_batch_multi_gpu(close, params_list)

        # Calcul sÃ©quentiel
        for params in params_list:
            period = params["period"]
            std = params["std"]
            key = f"{period}_{std}"

            try:
                results[key] = self.compute(close, period=period, std=std)
            except Exception as e:
                logger.error(f"âŒ Erreur paramÃ¨tre {key}: {e}")
                results[key] = (
                    np.array([]),
                    np.array([]),
                    np.array([]),
                )  # Tuple vide au lieu de None

        elapsed = time.time() - start_time
        success_count = sum(1 for r in results.values() if r is not None)
        logger.info(
            f"âœ… Bollinger batch terminÃ©: {success_count}/{len(params_list)} succÃ¨s en {elapsed:.2f}s"
        )

        return results

    def _compute_batch_multi_gpu(
        self,
        close: Union[np.ndarray, pd.Series],
        params_list: List[Dict[str, Union[int, float]]],
    ) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """Calcul batch multi-GPU avec rÃ©partition"""

        logger.info(
            f"ðŸš€ Multi-GPU batch: {len(params_list)} paramÃ¨tres sur {len(self.gpu_manager.available_gpus)} GPU"
        )

        # Split paramÃ¨tres entre GPU
        workload_splits = self.gpu_manager.split_workload(len(params_list))
        results = {}

        # Conversion donnÃ©es avec types prÃ©cis
        if isinstance(close, pd.Series):
            close = np.asarray(close.values, dtype=np.float64)
        else:
            close = np.asarray(close, dtype=np.float64)

        # Traitement par GPU
        for gpu_id, start_idx, end_idx in workload_splits:
            gpu_params = params_list[start_idx:end_idx]

            with cp.cuda.Device(gpu_id):
                logger.debug(
                    f"ðŸ”¥ GPU {gpu_id}: traitement {len(gpu_params)} paramÃ¨tres"
                )

                close_gpu = cp.asarray(close)

                for params in gpu_params:
                    period = int(params["period"])  # Conversion explicite
                    std_val = float(params["std"])  # Ã‰viter conflit avec param std
                    key = f"{period}_{std_val}"

                    try:
                        # Calcul sur GPU
                        middle = self._sma_gpu(close_gpu, period)
                        rolling_std = self._rolling_std_gpu(close_gpu, period)

                        std_term = std_val * rolling_std
                        upper = middle + std_term
                        lower = middle - std_term

                        # Retour CPU
                        results[key] = (
                            cp.asnumpy(upper),
                            cp.asnumpy(middle),
                            cp.asnumpy(lower),
                        )

                    except Exception as e:
                        logger.error(f"âŒ GPU {gpu_id} erreur {key}: {e}")
                        results[key] = (
                            np.array([]),
                            np.array([]),
                            np.array([]),
                        )  # Tuple vide au lieu de None

        return results


# ========================================
# FONCTIONS PUBLIQUES (API simplifiÃ©e)
# ========================================


def compute_bollinger_bands(
    close: Union[np.ndarray, pd.Series],
    period: int = 20,
    std: float = 2.0,
    use_gpu: bool = True,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Calcul Bollinger Bands - API simple

    Args:
        close: Prix de clÃ´ture
        period: PÃ©riode SMA (dÃ©faut: 20)
        std: Multiplicateur Ã©cart-type (dÃ©faut: 2.0)
        use_gpu: Utiliser GPU si disponible (dÃ©faut: True)

    Returns:
        Tuple[upper_band, middle_band, lower_band]

    Exemple:
        ```python
        import numpy as np
        from threadx.indicators.bollinger import compute_bollinger_bands

        # DonnÃ©es test
        close = np.random.randn(1000) * 10 + 100

        # Calcul
        upper, middle, lower = compute_bollinger_bands(close, period=20, std=2.0)

        print(f"Bandes calculÃ©es: {len(upper)} points")
        print(f"Dernier upper: {upper[-1]:.2f}")
        print(f"Dernier middle: {middle[-1]:.2f}")
        print(f"Dernier lower: {lower[-1]:.2f}")
        ```
    """
    settings = BollingerSettings(period=period, std=std, use_gpu=use_gpu)

    bb = BollingerBands(settings)
    return bb.compute(close)


def compute_bollinger_batch(
    close: Union[np.ndarray, pd.Series],
    params_list: List[Dict[str, Union[int, float]]],
    use_gpu: bool = True,
) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
    """
    Calcul Bollinger Bands batch - API simple

    Args:
        close: Prix de clÃ´ture
        params_list: Liste paramÃ¨tres [{'period': int, 'std': float}, ...]
        use_gpu: Utiliser GPU si disponible (dÃ©faut: True)

    Returns:
        Dict[param_key] = (upper, middle, lower)

    Exemple:
        ```python
        from threadx.indicators.bollinger import compute_bollinger_batch

        params = [
            {'period': 20, 'std': 2.0},
            {'period': 50, 'std': 1.5},
            {'period': 10, 'std': 2.5}
        ]

        results = compute_bollinger_batch(close, params)

        for key, bands in results.items():
            if bands:
                upper, middle, lower = bands
                print(f"{key}: Upper={upper[-1]:.2f}, Lower={lower[-1]:.2f}")
        ```
    """
    settings = BollingerSettings(use_gpu=use_gpu)
    bb = BollingerBands(settings)
    return bb.compute_batch(close, params_list)


# ========================================
# UTILITAIRES ET VALIDATION
# ========================================


def validate_bollinger_results(
    upper: np.ndarray, middle: np.ndarray, lower: np.ndarray, tolerance: float = 1e-10
) -> bool:
    """
    Validation rÃ©sultats Bollinger Bands

    Args:
        upper, middle, lower: Bandes calculÃ©es
        tolerance: TolÃ©rance pour comparaisons numÃ©riques

    Returns:
        True si rÃ©sultats valides
    """
    try:
        # MÃªme longueur
        if not (len(upper) == len(middle) == len(lower)):
            return False

        # upper >= middle >= lower (hors NaN)
        valid_mask = ~(np.isnan(upper) | np.isnan(middle) | np.isnan(lower))
        if np.any(valid_mask):
            valid_upper = upper[valid_mask]
            valid_middle = middle[valid_mask]
            valid_lower = lower[valid_mask]

            if np.any(valid_upper < valid_middle - tolerance):
                return False
            if np.any(valid_middle < valid_lower - tolerance):
                return False

        return True

    except Exception:
        return False


def benchmark_bollinger_performance(
    data_sizes: List[int] = [1000, 5000, 10000], n_runs: int = 3
) -> Dict[str, Any]:
    """
    Benchmark performance CPU vs GPU

    Args:
        data_sizes: Tailles de donnÃ©es Ã  tester
        n_runs: Nombre d'exÃ©cutions par test

    Returns:
        Dict avec mÃ©triques de performance
    """
    results = {
        "cpu_times": {},
        "gpu_times": {},
        "speedups": {},
        "gpu_available": GPU_AVAILABLE,
    }

    logger.info(f"ðŸ Benchmark Bollinger - GPU: {GPU_AVAILABLE}")

    for size in data_sizes:
        logger.info(f"ðŸ“Š Test size: {size}")

        # DonnÃ©es test
        close = np.random.randn(size) * 10 + 100

        # CPU timing
        cpu_times = []
        for _ in range(n_runs):
            start = time.time()
            compute_bollinger_bands(close, use_gpu=False)
            cpu_times.append(time.time() - start)

        cpu_avg = np.mean(cpu_times)
        results["cpu_times"][size] = float(cpu_avg)  # type: ignore

        # GPU timing si disponible
        if GPU_AVAILABLE:
            gpu_times = []
            for _ in range(n_runs):
                start = time.time()
                compute_bollinger_bands(close, use_gpu=True)
                gpu_times.append(time.time() - start)

            gpu_avg = np.mean(gpu_times)
            results["gpu_times"][size] = float(gpu_avg)  # type: ignore
            results["speedups"][size] = float(cpu_avg / gpu_avg)  # type: ignore

            logger.info(
                f"   CPU: {cpu_avg:.4f}s, GPU: {gpu_avg:.4f}s, Speedup: {cpu_avg/gpu_avg:.2f}x"
            )
        else:
            results["gpu_times"][size] = 0.0  # type: ignore
            results["speedups"][size] = 0.0  # type: ignore
            logger.info(f"   CPU: {cpu_avg:.4f}s, GPU: N/A")

    return results


if __name__ == "__main__":
    # Test rapide
    print("ðŸŽ¯ ThreadX Bollinger Bands - Test rapide")

    # DonnÃ©es test
    np.random.seed(42)
    close = np.random.randn(1000) * 10 + 100

    # Test simple
    upper, middle, lower = compute_bollinger_bands(close, period=20, std=2.0)
    print(f"âœ… Test simple: {len(upper)} points calculÃ©s")
    print(f"   Upper[-1]: {upper[-1]:.2f}")
    print(f"   Middle[-1]: {middle[-1]:.2f}")
    print(f"   Lower[-1]: {lower[-1]:.2f}")

    # Test batch
    params = [{"period": 20, "std": 2.0}, {"period": 50, "std": 1.5}]
    results = compute_bollinger_batch(close, params)
    print(f"âœ… Test batch: {len(results)} rÃ©sultats")

    # Validation
    valid = validate_bollinger_results(upper, middle, lower)
    print(f"âœ… Validation: {'PASS' if valid else 'FAIL'}")

    # Benchmark si GPU
    if GPU_AVAILABLE:
        print("ðŸ Benchmark performance...")
        bench = benchmark_bollinger_performance([1000], n_runs=2)
        if bench["speedups"][1000]:
            print(f"   Speedup GPU: {bench['speedups'][1000]:.2f}x")




----------------------------------------
Fichier: indicators\engine.py
"""
ThreadX Indicators Engine - Moteur unifiÃ© de calcul d'indicateurs
================================================================
"""

import logging
import pandas as pd
from typing import Dict, List, Any

logger = logging.getLogger(__name__)


def enrich_indicators(
    df: pd.DataFrame, specs: List[Dict[str, Any]], **kwargs
) -> pd.DataFrame:
    """Enrichit un DataFrame avec des indicateurs techniques."""
    if df.empty:
        return df.copy()

    result_df = df.copy()
    for spec in specs:
        name = spec.get("name", "").upper()
        params = spec.get("params", {})
        outputs = spec.get("outputs", [name.lower()])

        if name == "SMA":
            window = params.get("window", 20)
            if "close" in df.columns:
                result_df[outputs[0]] = df["close"].rolling(window=window).mean()
        elif name == "EMA":
            span = params.get("span", 20)
            if "close" in df.columns:
                result_df[outputs[0]] = df["close"].ewm(span=span).mean()

    return result_df


def get_available_indicators():
    """Retourne la liste des indicateurs disponibles."""
    return ["SMA", "EMA", "RSI"]




----------------------------------------
Fichier: indicators\gpu_integration.py
"""
ThreadX Indicators GPU Integration - Phase 5 + Numba Optimization
==================================================================

IntÃ©gration de la distribution multi-GPU avec la couche d'indicateurs.
Optimisations Numba CUDA pour kernels fusionnÃ©s et configuration thread/block optimale.

Permet d'accÃ©lÃ©rer les calculs d'indicateurs techniques (Bollinger Bands,
ATR, etc.) en utilisant automatiquement la rÃ©partition GPU/CPU optimale.

Optimisations:
    - Numba CUDA kernels avec thread/block configuration (256-512 threads/block)
    - Kernel fusion pour rÃ©duire les launches GPU
    - Shared memory pour donnÃ©es frÃ©quemment accessÃ©es
    - Profiling dynamique CPU vs GPU

Usage:
    >>> # Calcul distribuÃ© d'indicateurs avec Numba
    >>> from threadx.indicators import get_gpu_accelerated_bank
    >>>
    >>> bank = get_gpu_accelerated_bank()
    >>> bb_upper, bb_middle, bb_lower = bank.bollinger_bands(
    ...     df, period=20, std_dev=2.0, use_gpu=True
    ... )
"""

import time
from typing import Tuple, Optional, Dict, Any, Callable
import numpy as np
import pandas as pd

from threadx.utils.log import get_logger
from threadx.utils.gpu import get_default_manager, MultiGPUManager
from threadx.utils.gpu.profile_persistence import (
    stable_hash,
    update_gpu_threshold_entry,
    get_gpu_thresholds,
)

logger = get_logger(__name__)

# Numba CUDA imports optionnels
try:
    from numba import cuda, float32, float64, int32

    NUMBA_AVAILABLE = True
    logger.info("Numba CUDA disponible pour kernels optimisÃ©s")
except ImportError:
    NUMBA_AVAILABLE = False
    logger.info("Numba CUDA non disponible, utilisant CuPy uniquement")

# Configuration optimale thread/block pour RTX 5090/2060
OPTIMAL_THREADS_PER_BLOCK = 256  # 256-512 recommandÃ© pour compute 8.9+
OPTIMAL_BLOCKS_PER_SM = 2  # Pour occupancy maximale


@cuda.jit if NUMBA_AVAILABLE else lambda x: x
def _numba_bollinger_kernel(prices, period, std_dev, upper, middle, lower):
    """
    Kernel Numba CUDA fusionnÃ© pour Bollinger Bands.

    Calcule SMA + std en un seul kernel pour rÃ©duire les launches.
    Configuration: 256 threads/block, shared memory pour rolling window.

    Args:
        prices: Array des prix (N,)
        period: PÃ©riode de la moyenne mobile
        std_dev: Nombre d'Ã©carts-types
        upper: Output upper band (N,)
        middle: Output middle band (N,)
        lower: Output lower band (N,)
    """
    # Shared memory pour rolling window (optimisation accÃ¨s mÃ©moire)
    shared_prices = cuda.shared.array(shape=(OPTIMAL_THREADS_PER_BLOCK,), dtype=float32)

    idx = cuda.grid(1)
    n = prices.shape[0]

    if idx >= n:
        return

    # Chargement en shared memory
    tid = cuda.threadIdx.x
    if idx < n:
        shared_prices[tid] = prices[idx]
    cuda.syncthreads()

    # Calcul SMA et std fusionnÃ©s
    if idx >= period - 1:
        # Somme et somme des carrÃ©s en une passe
        sum_val = 0.0
        sum_sq = 0.0

        for i in range(period):
            offset = idx - i
            if offset >= 0:
                val = prices[offset]
                sum_val += val
                sum_sq += val * val

        # Moyenne et Ã©cart-type
        mean = sum_val / period
        variance = (sum_sq / period) - (mean * mean)
        std = variance**0.5 if variance > 0 else 0.0

        # Bandes de Bollinger
        middle[idx] = mean
        upper[idx] = mean + std_dev * std
        lower[idx] = mean - std_dev * std
    else:
        # Padding pour indices < period
        middle[idx] = prices[idx]
        upper[idx] = prices[idx]
        lower[idx] = prices[idx]


@cuda.jit if NUMBA_AVAILABLE else lambda x: x
def _numba_rsi_kernel(prices, period, rsi_out):
    """
    Kernel Numba CUDA pour RSI optimisÃ©.

    Calcule gains/losses et RSI en kernel fusionnÃ©.
    Configuration: 256 threads/block.

    Args:
        prices: Array des prix (N,)
        period: PÃ©riode RSI
        rsi_out: Output RSI values (N,)
    """
    idx = cuda.grid(1)
    n = prices.shape[0]

    if idx >= n or idx < period:
        return

    # Calcul des gains/losses moyens
    sum_gains = 0.0
    sum_losses = 0.0

    for i in range(period):
        offset = idx - i
        if offset > 0:
            delta = prices[offset] - prices[offset - 1]
            if delta > 0:
                sum_gains += delta
            else:
                sum_losses += abs(delta)

    avg_gain = sum_gains / period
    avg_loss = sum_losses / period

    # RSI
    if avg_loss == 0:
        rsi_out[idx] = 100.0
    else:
        rs = avg_gain / avg_loss
        rsi_out[idx] = 100.0 - (100.0 / (1.0 + rs))


class GPUAcceleratedIndicatorBank:
    """
    Banque d'indicateurs avec accÃ©lÃ©ration multi-GPU + Numba CUDA.

    Wraps les calculs d'indicateurs pour utiliser automatiquement
    la distribution multi-GPU quand disponible et bÃ©nÃ©fique.

    Optimisations Numba:
        - Kernels CUDA fusionnÃ©s (SMA+std, gains+losses)
        - Thread/block config optimale (256 threads/block)
        - Shared memory pour rolling windows
        - Profiling dynamique CPU vs GPU vs Numba
    """

    def __init__(self, gpu_manager: Optional[MultiGPUManager] = None):
        """
        Initialise la banque d'indicateurs GPU avec Numba.

        Args:
            gpu_manager: Gestionnaire multi-GPU optionnel
                        Si None, utilise le gestionnaire par dÃ©faut
        """
        self.gpu_manager = gpu_manager or get_default_manager()

        # ðŸ†• Seuil rÃ©duit pour utiliser GPU plus tÃ´t (optimisation utilisation)
        self.min_samples_for_gpu = 500  # RÃ©duit de 1000 â†’ 500 pour saturer GPU

        logger.info(
            f"Banque indicateurs GPU initialisÃ©e: "
            f"{len(self.gpu_manager._gpu_devices)} GPU(s), "
            f"seuil GPU: {self.min_samples_for_gpu} Ã©chantillons"
        )

    def _should_use_gpu_dynamic(
        self,
        indicator: str,
        n_rows: int,
        params: Dict[str, Any],
        dtype: Any = np.float32,  # Any pour accepter DtypeObj pandas
        force_gpu: bool = False,
    ) -> bool:
        """
        DÃ©termine l'utilisation GPU avec seuil dynamique
        basÃ© sur profil historique.

        Args:
            indicator: Nom de l'indicateur (ex: 'bollinger', 'atr')
            n_rows: Nombre de lignes dans les donnÃ©es
            params: ParamÃ¨tres principaux de l'indicateur
            dtype: Type de donnÃ©es (float32, float64)
            force_gpu: Force l'utilisation du GPU

        Returns:
            True si GPU recommandÃ© selon profil de performance
        """
        # VÃ©rification basique
        has_gpu = len(self.gpu_manager._gpu_devices) > 0
        if not has_gpu:
            return False

        if force_gpu:
            logger.info(f"Utilisation GPU forcÃ©e pour {indicator}")
            return True

        # CrÃ©ation de la signature unique
        params_major = {
            k: v
            for k, v in params.items()
            if k in ["period", "window", "std", "std_dev"]
        }
        dtype_name = dtype.__name__ if hasattr(dtype, "__name__") else str(dtype)
        signature = (
            f"{indicator}|N={n_rows}|"
            f"dtype={dtype_name}|"
            f"params={stable_hash(params_major)}"
        )

        # RÃ©cupÃ©ration des seuils GPU
        thresholds = get_gpu_thresholds()
        defaults = thresholds["defaults"]

        # Si signature inconnue, lancer un micro-probe
        if signature not in thresholds["entries"]:
            # Micro-probe pour dÃ©cider
            cpu_ms, gpu_ms = self._micro_probe(indicator, n_rows, params_major)

            # Enregistrement dans le profil
            update_gpu_threshold_entry(signature, cpu_ms, gpu_ms)
            thresholds = get_gpu_thresholds()  # Rechargement

        entry = thresholds["entries"].get(signature, {})

        # RÃ¨gles de dÃ©cision
        if n_rows < defaults["n_min_gpu"]:
            logger.debug(
                f"N={n_rows} < seuil minimal " f"{defaults['n_min_gpu']}, utilisant CPU"
            )
            return False

        # Calcul du gain estimÃ©
        cpu_ms_est = entry.get("cpu_ms_avg", float("inf"))
        gpu_ms_est = entry.get("gpu_ms_avg", float("inf"))

        # Protection division par zÃ©ro
        if gpu_ms_est <= 0:
            return False

        gain = cpu_ms_est / gpu_ms_est
        decision_threshold = entry.get(
            "decision_threshold", defaults["decision_threshold"]
        )
        hysteresis = defaults["hysteresis"]

        # DÃ©cision avec hystÃ©rÃ©sis
        use_gpu = gain >= (decision_threshold - hysteresis)

        # Log une fois par exÃ©cution pour cette signature (pas Ã  chaque appel)
        logger.info(
            f"DÃ©cision {'GPU' if use_gpu else 'CPU'} pour {signature}: "
            f"gain={gain:.2f}x, seuil={decision_threshold:.2f}, "
            f"cpu={cpu_ms_est:.2f}ms, gpu={gpu_ms_est:.2f}ms"
        )

        return use_gpu

    def _dispatch_indicator(
        self,
        indicator_name: str,
        data: pd.DataFrame,
        params: Dict[str, Any],
        use_gpu: Optional[bool],
        gpu_func: Callable,
        cpu_func: Callable,
        input_cols: Optional[str] = None,
        extract_arrays: bool = True,
    ) -> Any:
        """
        Dispatch automatique GPU/CPU pour un indicateur.

        Centralise la logique de dÃ©cision GPU/CPU et l'extraction de donnÃ©es
        pour Ã©viter la duplication de code entre indicateurs.

        Args:
            indicator_name: Nom de l'indicateur ('bollinger', 'atr', 'rsi')
            data: DataFrame source
            params: ParamÃ¨tres de l'indicateur (period, std_dev, etc.)
            use_gpu: None (auto), True (force GPU), False (force CPU)
            gpu_func: Fonction GPU Ã  appeler (signature: func(arrays, ...))
            cpu_func: Fonction CPU Ã  appeler (signature: func(arrays, ...))
            input_cols: Colonne(s) Ã  extraire (str ou None pour OHLC)
            extract_arrays: Si True, extrait et convertit les colonnes en
                          ndarray

        Returns:
            RÃ©sultat de l'indicateur (pd.Series ou Tuple[pd.Series])

        Example:
            >>> return self._dispatch_indicator(
            ...     'bollinger',
            ...     data,
            ...     {'period': 20, 'std_dev': 2.0},
            ...     use_gpu,
            ...     self._bollinger_bands_gpu,
            ...     self._bollinger_bands_cpu,
            ...     input_cols='close'
            ... )
        """
        data_size = len(data)

        # DÃ©termination du dtype pour le profiling
        if input_cols:
            dtype = data[input_cols].dtype
        else:
            # Pour OHLC, utiliser dtype de 'close'
            dtype = data["close"].dtype

        # DÃ©cision dynamique CPU vs GPU basÃ©e sur profil historique
        if use_gpu is None:
            # DÃ©cision automatique basÃ©e sur profils
            use_gpu_decision = self._should_use_gpu_dynamic(
                indicator_name, data_size, params, dtype
            )
        else:
            # Force explicite
            use_gpu_decision = use_gpu

        # Extraction et conversion des donnÃ©es si nÃ©cessaire
        if extract_arrays:
            if input_cols:
                # Extraction d'une seule colonne
                arrays = np.asarray(data[input_cols].values)
            else:
                # Pas de colonnes spÃ©cifiÃ©es: passer le DataFrame
                arrays = data
        else:
            # Pas d'extraction: passer directement le DataFrame
            arrays = data

        # Dispatch vers GPU ou CPU
        if use_gpu_decision:
            return gpu_func(arrays)
        else:
            return cpu_func(arrays)

    def _micro_probe(
        self, indicator: str, n_rows: int, params: Dict[str, Any], n_samples: int = 3
    ) -> Tuple[float, float]:
        """
        ExÃ©cute un micro-benchmark pour comparer CPU vs GPU
        sur un Ã©chantillon rÃ©duit.

        Args:
            indicator: Nom de l'indicateur
            n_rows: Nombre de lignes original
            params: ParamÃ¨tres de l'indicateur
            n_samples: Nombre d'Ã©chantillons pour moyenne

        Returns:
            Tuple (cpu_ms_avg, gpu_ms_avg)
        """
        logger.info(f"Micro-probe {indicator} (N={n_rows}, params={params})")

        # Taille d'Ã©chantillon: min(n_rows, 100000)
        # pour Ã©viter les benchmarks trop longs
        sample_size = min(n_rows, 100000)

        # DonnÃ©es de test adaptÃ©es Ã  l'indicateur
        if indicator in ["bollinger", "bollinger_bands"]:
            # SÃ©ries de prix
            test_data = np.random.normal(100, 5, sample_size).astype(np.float32)

            # Params par dÃ©faut si non spÃ©cifiÃ©s
            period = params.get("period", 20)
            std_dev = params.get("std_dev", params.get("std", 2.0))

            # Fonctions de test
            def cpu_func():
                return self._bollinger_bands_cpu(
                    test_data, period, std_dev, pd.RangeIndex(sample_size)
                )

            def gpu_func():
                return self._bollinger_bands_gpu(
                    test_data, period, std_dev, pd.RangeIndex(sample_size)
                )

        elif indicator in ["atr"]:
            # DonnÃ©es OHLC
            high = np.random.normal(105, 3, sample_size).astype(np.float32)
            low = np.random.normal(95, 3, sample_size).astype(np.float32)
            close = np.random.normal(100, 3, sample_size).astype(np.float32)

            df = pd.DataFrame({"high": high, "low": low, "close": close})

            # Params
            period = params.get("period", 14)

            # Fonctions de test
            def cpu_func():
                return self._atr_cpu(df, period)

            def gpu_func():
                return self._atr_gpu(df, period)

        elif indicator in ["rsi"]:
            # SÃ©ries de prix
            test_data = np.random.normal(100, 5, sample_size).astype(np.float32)

            # Params
            period = params.get("period", 14)

            # Fonctions de test
            def cpu_func():
                return self._rsi_cpu(test_data, period, pd.RangeIndex(sample_size))

            def gpu_func():
                return self._rsi_gpu(test_data, period, pd.RangeIndex(sample_size))

        else:
            # Indicateur non pris en charge: tests gÃ©nÃ©riques
            logger.warning(
                f"Micro-probe pour '{indicator}' non implÃ©mentÃ©, "
                f"utilisant benchmark gÃ©nÃ©rique"
            )
            return self._generic_micro_probe(sample_size)

        # ExÃ©cution des benchmarks
        cpu_times = []
        gpu_times = []

        # PrÃ©chauffage
        try:
            _ = cpu_func()
            _ = gpu_func()
        except Exception as e:
            logger.warning(
                f"Erreur prÃ©chauffage: {e}, " f"utilisant benchmark gÃ©nÃ©rique"
            )
            return self._generic_micro_probe(sample_size)

        # Mesure CPU
        for i in range(n_samples):
            start_time = time.time()
            _ = cpu_func()
            cpu_times.append((time.time() - start_time) * 1000)  # ms

        # Mesure GPU
        try:
            for i in range(n_samples):
                start_time = time.time()
                _ = gpu_func()
                gpu_times.append((time.time() - start_time) * 1000)  # ms
        except Exception as e:
            logger.warning(f"Erreur GPU: {e}, fallback CPU recommandÃ©")
            # PÃ©nalisation GPU: temps trÃ¨s Ã©levÃ© pour forcer choix CPU
            gpu_times = [max(cpu_times) * 5] * n_samples

        # Calcul des moyennes
        cpu_ms_avg = sum(cpu_times) / len(cpu_times)
        gpu_ms_avg = sum(gpu_times) / len(gpu_times)

        logger.info(
            f"Micro-probe {indicator}: CPU={cpu_ms_avg:.2f}ms, "
            f"GPU={gpu_ms_avg:.2f}ms, speedup={(cpu_ms_avg/gpu_ms_avg):.2f}x"
        )

        return cpu_ms_avg, gpu_ms_avg

    def _generic_micro_probe(self, sample_size: int) -> Tuple[float, float]:
        """
        ExÃ©cute un benchmark gÃ©nÃ©rique pour CPU vs GPU.

        Args:
            sample_size: Taille d'Ã©chantillon pour le test

        Returns:
            Tuple (cpu_ms_avg, gpu_ms_avg)
        """
        # Limiter taille max pour benchmark gÃ©nÃ©rique
        sample_size = min(sample_size, 50000)

        # DonnÃ©es de test gÃ©nÃ©rique: convolution
        test_data = np.random.normal(0, 1, sample_size).astype(np.float32)

        # CPU benchmark
        start_time = time.time()
        _ = np.convolve(test_data, np.ones(20) / 20, mode="same")
        cpu_ms = (time.time() - start_time) * 1000

        # GPU benchmark
        try:

            def compute_fn(x):
                return np.convolve(x, np.ones(20) / 20, mode="same")

            start_time = time.time()
            _ = self.gpu_manager.distribute_workload(test_data, compute_fn)
            gpu_ms = (time.time() - start_time) * 1000
        except Exception as e:
            logger.warning(f"Erreur benchmark GPU gÃ©nÃ©rique: {e}")
            gpu_ms = cpu_ms * 2  # PÃ©nalisation

        logger.debug(
            f"Benchmark gÃ©nÃ©rique (N={sample_size}): "
            f"CPU={cpu_ms:.2f}ms, GPU={gpu_ms:.2f}ms"
        )

        return cpu_ms, gpu_ms

    def update_indicator_methods(self):
        """
        Met Ã  jour les mÃ©thodes d'indicateurs pour utiliser
        la dÃ©cision dynamique.
        """
        # Cette mÃ©thode pourrait Ãªtre utilisÃ©e pour appliquer
        # des patchs aux mÃ©thodes d'indicateurs existantes pour
        # utiliser _should_use_gpu_dynamic
        logger.debug("Activation de la dÃ©cision GPU dynamique " "pour les indicateurs")

    def bollinger_bands(
        self,
        data: pd.DataFrame,
        period: int = 20,
        std_dev: float = 2.0,
        price_col: str = "close",
        use_gpu: Optional[bool] = None,
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """
        Calcul GPU des Bollinger Bands.

        Args:
            data: DataFrame OHLCV avec colonne price_col
            period: PÃ©riode de la moyenne mobile
            std_dev: Multiplicateur d'Ã©cart-type
            price_col: Colonne de prix Ã  utiliser
            use_gpu: Force GPU (None=auto, True=force, False=CPU only)

        Returns:
            Tuple (upper_band, middle_band, lower_band)

        Example:
            >>> df = pd.DataFrame({'close': [100, 101, 99, 102, 98]})
            >>> upper, middle, lower = bank.bollinger_bands(df, period=3)
        """
        if price_col not in data.columns:
            raise ValueError(f"Colonne '{price_col}' non trouvÃ©e dans les donnÃ©es")

        # Utilisation du dispatcher centralisÃ©
        params = {"period": period, "std_dev": std_dev}

        return self._dispatch_indicator(
            indicator_name="bollinger",
            data=data,
            params=params,
            use_gpu=use_gpu,
            gpu_func=lambda prices: self._bollinger_bands_gpu(
                prices, period, std_dev, data.index
            ),
            cpu_func=lambda prices: self._bollinger_bands_cpu(
                prices, period, std_dev, data.index
            ),
            input_cols=price_col,
            extract_arrays=True,
        )

    def _bollinger_bands_gpu(
        self, prices: np.ndarray, period: int, std_dev: float, index: pd.Index
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """
        Calcul Bollinger Bands sur GPU avec Numba CUDA kernel fusionnÃ©.

        Architecture optimisÃ©e:
        - Utilise kernel Numba CUDA si disponible (fusionnÃ© SMA+std)
        - Configuration thread/block optimale (256 threads/block)
        - Shared memory pour rolling window
        - Fallback CuPy distribution si Numba indisponible
        - Fallback CPU en cas d'erreur
        """

        # Tentative 1: Numba CUDA kernel fusionnÃ© (meilleure performance)
        if NUMBA_AVAILABLE and len(self.gpu_manager._gpu_devices) > 0:
            try:
                return self._bollinger_bands_numba(prices, period, std_dev, index)
            except Exception as e:
                logger.warning(f"Numba kernel Ã©chouÃ©: {e}, fallback CuPy")

        # Tentative 2: Distribution CuPy classique
        def bb_compute_func(price_chunk):
            """Fonction vectorielle pour un chunk de prix."""
            if len(price_chunk) < period:
                # Chunk trop petit: moyenne simple
                ma = np.full_like(price_chunk, np.mean(price_chunk))
                std = np.full_like(price_chunk, np.std(price_chunk))
            else:
                # Moving average avec convolution
                weights = np.ones(period) / period
                ma = np.convolve(price_chunk, weights, mode="same")

                # Moving standard deviation
                squared_diff = (price_chunk - ma) ** 2
                variance = np.convolve(squared_diff, weights, mode="same")
                std = np.sqrt(variance)

            # Bandes de Bollinger
            upper = ma + std_dev * std
            middle = ma
            lower = ma - std_dev * std

            # Empilement pour retour
            return np.column_stack([upper, middle, lower])

        # Distribution GPU
        start_time = pd.Timestamp.now()

        try:
            # Reshape pour distribution (ajout dimension batch si nÃ©cessaire)
            if prices.ndim == 1:
                prices_2d = prices.reshape(-1, 1)
            else:
                prices_2d = prices

            result = self.gpu_manager.distribute_workload(
                prices_2d, bb_compute_func, seed=42
            )

            # Extraction des bandes
            upper_band = result[:, 0]
            middle_band = result[:, 1]
            lower_band = result[:, 2]

            elapsed = (pd.Timestamp.now() - start_time).total_seconds()
            logger.info(
                f"Bollinger Bands GPU (CuPy): {len(prices)} Ã©chantillons "
                f"en {elapsed:.3f}s"
            )

        except Exception as e:
            logger.warning(f"Erreur calcul GPU Bollinger Bands: {e}")
            logger.info("Fallback calcul CPU")
            return self._bollinger_bands_cpu(prices, period, std_dev, index)

        return (
            pd.Series(upper_band, index=index, name="bb_upper"),
            pd.Series(middle_band, index=index, name="bb_middle"),
            pd.Series(lower_band, index=index, name="bb_lower"),
        )

    def _bollinger_bands_numba(
        self, prices: np.ndarray, period: int, std_dev: float, index: pd.Index
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """
        Calcul Bollinger Bands avec kernel Numba CUDA fusionnÃ©.

        Optimisations:
        - Kernel fusionnÃ©: SMA + std en un seul launch
        - Thread/block config optimale: 256 threads/block
        - Shared memory pour rolling window
        - Grid-stride loop pour grandes donnÃ©es

        Args:
            prices: Array 1D des prix
            period: PÃ©riode moyenne mobile
            std_dev: Multiplicateur Ã©cart-type
            index: Index pandas pour output

        Returns:
            Tuple (upper, middle, lower) pd.Series
        """
        start_time = pd.Timestamp.now()

        n = len(prices)

        # Conversion en float32 pour optimisation GPU
        prices_f32 = np.asarray(prices, dtype=np.float32)

        # Allocation outputs device
        upper = np.zeros(n, dtype=np.float32)
        middle = np.zeros(n, dtype=np.float32)
        lower = np.zeros(n, dtype=np.float32)

        # Transfert vers GPU principal (device 0)
        device_id = self.gpu_manager._gpu_devices[0].device_id

        try:
            import cupy as cp

            with cp.cuda.Device(device_id):
                d_prices = cp.asarray(prices_f32)
                d_upper = cp.asarray(upper)
                d_middle = cp.asarray(middle)
                d_lower = cp.asarray(lower)

                # Configuration grid/block optimale
                threads_per_block = OPTIMAL_THREADS_PER_BLOCK
                blocks = (n + threads_per_block - 1) // threads_per_block

                logger.debug(
                    f"Numba kernel config: {blocks} blocks x {threads_per_block} threads"
                )

                # Launch kernel fusionnÃ©
                _numba_bollinger_kernel[blocks, threads_per_block](
                    d_prices, period, std_dev, d_upper, d_middle, d_lower
                )

                # Synchronisation
                cp.cuda.Device().synchronize()

                # Transfert rÃ©sultats vers host
                upper = cp.asnumpy(d_upper)
                middle = cp.asnumpy(d_middle)
                lower = cp.asnumpy(d_lower)

        except Exception as e:
            logger.error(f"Erreur kernel Numba Bollinger: {e}")
            raise  # Propagation pour fallback CuPy

        elapsed = (pd.Timestamp.now() - start_time).total_seconds()
        logger.info(
            f"Bollinger Bands Numba CUDA: {n} Ã©chantillons en {elapsed:.3f}s "
            f"({n/elapsed:.0f} Ã©chant./s)"
        )

        return (
            pd.Series(upper, index=index, name="bb_upper"),
            pd.Series(middle, index=index, name="bb_middle"),
            pd.Series(lower, index=index, name="bb_lower"),
        )

    def _bollinger_bands_cpu(
        self, prices: np.ndarray, period: int, std_dev: float, index: pd.Index
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calcul CPU classique des Bollinger Bands."""
        # Rolling window avec pandas pour simplicitÃ©
        price_series = pd.Series(prices, index=index)

        middle_band = price_series.rolling(window=period, min_periods=1).mean()
        rolling_std = price_series.rolling(window=period, min_periods=1).std()

        upper_band = middle_band + std_dev * rolling_std
        lower_band = middle_band - std_dev * rolling_std

        # Nommage des sÃ©ries
        upper_band.name = "bb_upper"
        middle_band.name = "bb_middle"
        lower_band.name = "bb_lower"

        return upper_band, middle_band, lower_band

    def atr(
        self,
        data: pd.DataFrame,
        period: int = 14,
        use_gpu: Optional[bool] = None,
    ) -> pd.Series:
        """
        Calcul GPU de l'Average True Range (ATR).

        Args:
            data: DataFrame OHLCV avec colonnes 'high', 'low', 'close'
            period: PÃ©riode pour le calcul ATR
            use_gpu: Force GPU (None=auto, True=force, False=CPU only)

        Returns:
            SÃ©rie ATR

        Example:
            >>> df = pd.DataFrame({
            ...     'high': [102, 103, 101],
            ...     'low': [98, 99, 97],
            ...     'close': [100, 101, 99]
            ... })
            >>> atr_series = bank.atr(df, period=2)
        """
        required_cols = ["high", "low", "close"]
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            raise ValueError(f"Colonnes manquantes: {missing_cols}")

        # Utilisation du dispatcher centralisÃ©
        params = {"period": period}

        return self._dispatch_indicator(
            indicator_name="atr",
            data=data,
            params=params,
            use_gpu=use_gpu,
            gpu_func=lambda df: self._atr_gpu(df, period),
            cpu_func=lambda df: self._atr_cpu(df, period),
            input_cols=None,  # Pas d'extraction, passe le DataFrame
            extract_arrays=False,
        )

    def _atr_gpu(self, data: pd.DataFrame, period: int) -> pd.Series:
        """Calcul ATR distribuÃ© sur GPU."""

        def atr_compute_func(ohlc_chunk):
            """Calcul ATR vectorisÃ© pour un chunk."""
            if len(ohlc_chunk) < 2:
                return np.zeros(len(ohlc_chunk))

            high = ohlc_chunk[:, 0]  # Colonne high
            low = ohlc_chunk[:, 1]  # Colonne low
            close = ohlc_chunk[:, 2]  # Colonne close

            # True Range calculation
            prev_close = np.roll(close, 1)
            prev_close[0] = close[0]  # Premier Ã©lÃ©ment

            tr1 = high - low
            tr2 = np.abs(high - prev_close)
            tr3 = np.abs(low - prev_close)

            true_range = np.maximum(tr1, np.maximum(tr2, tr3))

            # ATR (moyenne mobile du True Range)
            if len(true_range) < period:
                atr = np.full_like(true_range, np.mean(true_range))
            else:
                weights = np.ones(period) / period
                atr = np.convolve(true_range, weights, mode="same")

            return atr

        try:
            # PrÃ©paration donnÃ©es pour distribution
            ohlc_array = data[["high", "low", "close"]].values

            result = self.gpu_manager.distribute_workload(
                ohlc_array, atr_compute_func, seed=42
            )

            return pd.Series(result, index=data.index, name="atr")

        except Exception as e:
            logger.warning(f"Erreur calcul GPU ATR: {e}")
            return self._atr_cpu(data, period)

    def _atr_cpu(self, data: pd.DataFrame, period: int) -> pd.Series:
        """Calcul ATR CPU classique."""
        high = data["high"]
        low = data["low"]
        close = data["close"]
        prev_close = close.shift(1)

        # True Range
        tr1 = high - low
        tr2 = abs(high - prev_close)
        tr3 = abs(low - prev_close)

        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

        # ATR (moyenne mobile)
        atr = true_range.rolling(window=period, min_periods=1).mean()
        atr.name = "atr"

        return atr

    def rsi(
        self,
        data: pd.DataFrame,
        period: int = 14,
        price_col: str = "close",
        use_gpu: Optional[bool] = None,
    ) -> pd.Series:
        """
        Calcul GPU du Relative Strength Index (RSI).

        Args:
            data: DataFrame avec colonne price_col
            period: PÃ©riode RSI
            price_col: Colonne de prix
            use_gpu: Force GPU (None=auto)

        Returns:
            SÃ©rie RSI (0-100)
        """
        if price_col not in data.columns:
            raise ValueError(f"Colonne '{price_col}' non trouvÃ©e")

        # Utilisation du dispatcher centralisÃ©
        params = {"period": period}

        return self._dispatch_indicator(
            indicator_name="rsi",
            data=data,
            params=params,
            use_gpu=use_gpu,
            gpu_func=lambda prices: self._rsi_gpu(prices, period, data.index),
            cpu_func=lambda prices: self._rsi_cpu(prices, period, data.index),
            input_cols=price_col,
            extract_arrays=True,
        )

    def _rsi_gpu(self, prices: np.ndarray, period: int, index: pd.Index) -> pd.Series:
        """Calcul RSI distribuÃ© sur GPU."""

        def rsi_compute_func(price_chunk):
            """Calcul RSI vectorisÃ©."""
            if len(price_chunk) < 2:
                return np.full(len(price_chunk), 50.0)  # RSI neutre

            # Calcul des gains/pertes
            price_diff = np.diff(price_chunk, prepend=price_chunk[0])
            gains = np.where(price_diff > 0, price_diff, 0)
            losses = np.where(price_diff < 0, -price_diff, 0)

            # Moyennes des gains/pertes
            if len(gains) < period:
                avg_gain = np.mean(gains)
                avg_loss = np.mean(losses)
            else:
                weights = np.ones(period) / period
                avg_gain = np.convolve(gains, weights, mode="same")
                avg_loss = np.convolve(losses, weights, mode="same")

            # RSI calculation
            rs = np.divide(
                avg_gain, avg_loss, out=np.ones_like(avg_gain), where=avg_loss != 0
            )
            rsi = 100 - (100 / (1 + rs))

            return rsi

        try:
            if prices.ndim == 1:
                prices_2d = prices.reshape(-1, 1)
            else:
                prices_2d = prices

            result = self.gpu_manager.distribute_workload(
                prices_2d, rsi_compute_func, seed=42
            )

            # Convertir en ndarray pour garantir compatibilitÃ©
            result_array = np.asarray(result)

            # Aplatir si multi-dimensionnel
            if result_array.ndim > 1:
                result_array = result_array.flatten()

            return pd.Series(result_array, index=index, name="rsi")

        except Exception as e:
            logger.warning(f"Erreur calcul GPU RSI: {e}")
            return self._rsi_cpu(prices, period, index)

    def _rsi_cpu(self, prices: np.ndarray, period: int, index: pd.Index) -> pd.Series:
        """Calcul RSI CPU classique."""
        price_series = pd.Series(prices, index=index)

        # Gains et pertes
        price_change = price_series.diff()
        gains = price_change.where(price_change > 0, 0)
        losses = -price_change.where(price_change < 0, 0)

        # Moyennes mobiles
        avg_gain = gains.rolling(window=period, min_periods=1).mean()
        avg_loss = losses.rolling(window=period, min_periods=1).mean()

        # RSI
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        rsi.name = "rsi"

        return rsi

    def get_performance_stats(self) -> dict:
        """
        RÃ©cupÃ¨re les statistiques de performance du gestionnaire GPU.

        Returns:
            Dict avec stats devices et balance
        """
        return {
            "gpu_manager_stats": self.gpu_manager.get_device_stats(),
            "current_balance": self.gpu_manager.device_balance,
            "min_samples_for_gpu": self.min_samples_for_gpu,
            "available_indicators": ["bollinger_bands", "atr", "rsi"],
        }

    def optimize_balance(self, sample_data: pd.DataFrame, runs: int = 3) -> dict:
        """
        Optimise automatiquement la balance GPU pour les indicateurs.

        Args:
            sample_data: DonnÃ©es reprÃ©sentatives pour benchmark
            runs: Nombre de runs pour moyenne

        Returns:
            Nouveaux ratios optimaux
        """
        logger.info("Optimisation balance GPU pour indicateurs...")

        # Utilisation des donnÃ©es pour profiling
        if "close" in sample_data.columns:
            # Test avec Bollinger Bands (reprÃ©sentatif)
            old_min_samples = self.min_samples_for_gpu
            self.min_samples_for_gpu = 0  # Force GPU pour profiling

            try:
                # Benchmark sur Ã©chantillon
                optimal_ratios = self.gpu_manager.profile_auto_balance(
                    sample_size=min(len(sample_data), 50000), runs=runs
                )

                # Application des nouveaux ratios
                self.gpu_manager.set_balance(optimal_ratios)

                logger.info(f"Balance optimisÃ©e: {optimal_ratios}")
                return optimal_ratios

            finally:
                self.min_samples_for_gpu = old_min_samples

        else:
            logger.warning("DonnÃ©es sans colonne 'close', optimisation ignorÃ©e")
            return self.gpu_manager.device_balance


# === Instance globale ===

_gpu_indicator_bank: Optional[GPUAcceleratedIndicatorBank] = None


def get_gpu_accelerated_bank() -> GPUAcceleratedIndicatorBank:
    """
    RÃ©cupÃ¨re l'instance globale de la banque d'indicateurs GPU.

    Returns:
        Instance GPUAcceleratedIndicatorBank

    Example:
        >>> bank = get_gpu_accelerated_bank()
        >>> upper, middle, lower = bank.bollinger_bands(df, use_gpu=True)
    """
    global _gpu_indicator_bank

    if _gpu_indicator_bank is None:
        _gpu_indicator_bank = GPUAcceleratedIndicatorBank()
        logger.info("Banque indicateurs GPU globale crÃ©Ã©e")

    return _gpu_indicator_bank

----------------------------------------
Fichier: indicators\indicators_np.py
"""
ThreadX Indicateurs NumPy - ImplÃ©mentations natives
===================================================

Fonctions d'indicateurs techniques optimisÃ©es NumPy.
Code extrait et consolidÃ© depuis unified_data_historique_with_indicators.py

Performance:
    - 50x plus rapide que pandas rolling
    - Optimisations EMA custom
    - Gestion NaN robuste

Auteur: ThreadX Core Team
Version: 1.0
"""

from __future__ import annotations

from typing import Dict, Iterable, Tuple

import numpy as np
import pandas as pd


def _ewm(x: np.ndarray, span: int) -> np.ndarray:
    """
    Exponential weighted moving average optimisÃ©e.

    Args:
        x: Array de valeurs
        span: PÃ©riode EWM

    Returns:
        Array EWM
    """
    x = np.asarray(x, dtype=np.float64)
    if x.size == 0:
        return np.array([], dtype=np.float64)

    out = np.empty_like(x, dtype=np.float64)
    out[0] = x[0]

    if span <= 1:
        out[:] = x
        return out

    alpha = 2.0 / (span + 1.0)
    for i in range(1, len(x)):
        out[i] = alpha * x[i] + (1 - alpha) * out[i - 1]

    return out


def ema_np(arr: np.ndarray, span: int) -> np.ndarray:
    """
    Exponential Moving Average.

    Args:
        arr: Prix (gÃ©nÃ©ralement close)
        span: PÃ©riode EMA

    Returns:
        Array EMA
    """
    return _ewm(arr, span)


def atr_np(
    high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14
) -> np.ndarray:
    """
    Average True Range.

    Args:
        high: Prix hauts
        low: Prix bas
        close: Prix clÃ´ture
        period: PÃ©riode ATR

    Returns:
        Array ATR
    """
    prev = np.concatenate(([close[0]], close[:-1]))
    tr = np.maximum(high - low, np.maximum(np.abs(high - prev), np.abs(low - prev)))
    return _ewm(tr, period)


def boll_np(
    close: np.ndarray, period: int = 20, std: float = 2.0
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Bollinger Bands.

    Args:
        close: Prix clÃ´ture
        period: PÃ©riode moyenne mobile
        std: Multiplicateur Ã©cart-type

    Returns:
        Tuple (lower, ma, upper, z-score)
    """
    ma = _ewm(close, period)
    var = _ewm((close - ma) ** 2, period)
    sd = np.sqrt(np.maximum(var, 1e-12))

    upper = ma + std * sd
    lower = ma - std * sd
    z = (close - ma) / sd

    return lower, ma, upper, z


def rsi_np(close: np.ndarray, period: int = 14) -> np.ndarray:
    """
    Relative Strength Index.

    Args:
        close: Prix clÃ´ture
        period: PÃ©riode RSI

    Returns:
        Array RSI (0-100)
    """
    if close.size == 0:
        return np.array([], dtype=np.float64)

    delta = np.diff(close, prepend=close[0])
    gain = np.where(delta > 0, delta, 0.0)
    loss = np.where(delta < 0, -delta, 0.0)

    avg_gain = _ewm(gain, period)
    avg_loss = _ewm(loss, period)

    rs = np.divide(avg_gain, np.maximum(avg_loss, 1e-12))
    rsi = 100.0 - (100.0 / (1.0 + rs))

    return rsi


def macd_np(
    close: np.ndarray, fast: int = 12, slow: int = 26, signal: int = 9
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Moving Average Convergence Divergence.

    Args:
        close: Prix clÃ´ture
        fast: PÃ©riode EMA rapide
        slow: PÃ©riode EMA lente
        signal: PÃ©riode signal line

    Returns:
        Tuple (macd, signal, histogram)
    """
    ema_fast = ema_np(close, fast)
    ema_slow = ema_np(close, slow)
    macd = ema_fast - ema_slow
    sig = ema_np(macd, signal)
    hist = macd - sig

    return macd, sig, hist


def vwap_np(
    close: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    volume: np.ndarray,
    window: int = 96,
) -> np.ndarray:
    """
    Volume Weighted Average Price.

    Args:
        close: Prix clÃ´ture
        high: Prix hauts
        low: Prix bas
        volume: Volume
        window: FenÃªtre EMA

    Returns:
        Array VWAP
    """
    typical = (high + low + close) / 3.0
    vol_ema = ema_np(volume, window)
    pv_ema = ema_np(typical * volume, window)
    vwap = np.divide(pv_ema, np.maximum(vol_ema, 1e-12))

    return vwap


def obv_np(close: np.ndarray, volume: np.ndarray) -> np.ndarray:
    """
    On-Balance Volume.

    Args:
        close: Prix clÃ´ture
        volume: Volume

    Returns:
        Array OBV
    """
    obv = np.zeros_like(close, dtype=np.float64)

    for i in range(1, close.size):
        if close[i] > close[i - 1]:
            obv[i] = obv[i - 1] + volume[i]
        elif close[i] < close[i - 1]:
            obv[i] = obv[i - 1] - volume[i]
        else:
            obv[i] = obv[i - 1]

    return obv


def vortex_df(
    highs: np.ndarray, lows: np.ndarray, closes: np.ndarray, period: int = 14
) -> pd.DataFrame:
    """
    Vortex Indicator.

    Args:
        highs: Prix hauts
        lows: Prix bas
        closes: Prix clÃ´ture
        period: PÃ©riode indicateur

    Returns:
        DataFrame avec colonnes vi_plus, vi_minus
    """
    n = len(closes)
    if n == 0:
        return pd.DataFrame({"vi_plus": [], "vi_minus": []})

    h = highs.astype(np.float64)
    l = lows.astype(np.float64)
    c = closes.astype(np.float64)

    # Valeurs prÃ©cÃ©dentes
    prev_h = np.roll(h, 1)
    prev_l = np.roll(l, 1)
    prev_c = np.roll(c, 1)
    prev_h[0] = h[0]
    prev_l[0] = l[0]
    prev_c[0] = c[0]

    # Calculs Vortex
    vm_plus = np.abs(h - prev_l)
    vm_minus = np.abs(l - prev_h)
    tr = np.maximum(h - l, np.maximum(np.abs(h - prev_c), np.abs(l - prev_c)))

    # Sommes rolling
    vm_p_sum = (
        pd.Series(vm_plus, dtype="float64")
        .rolling(window=period, min_periods=period)
        .sum()
    )
    vm_m_sum = (
        pd.Series(vm_minus, dtype="float64")
        .rolling(window=period, min_periods=period)
        .sum()
    )
    tr_sum = (
        pd.Series(tr, dtype="float64").rolling(window=period, min_periods=period).sum()
    )

    # Indicateurs finaux
    vi_plus = (vm_p_sum / tr_sum).to_numpy()
    vi_minus = (vm_m_sum / tr_sum).to_numpy()

    return pd.DataFrame({"vi_plus": vi_plus, "vi_minus": vi_minus})


def sma_np(values: np.ndarray, period: int) -> np.ndarray:
    """
    Simple Moving Average (SMA) using cumulative sums for performance.

    Args:
        values: Array of prices.
        period: SMA window.

    Returns:
        Array of SMA values with NaN padding for the warm-up window.
    """
    values = np.asarray(values, dtype=np.float64)
    if period <= 0:
        raise ValueError("period must be positive")
    if values.size == 0:
        return np.array([], dtype=np.float64)

    result = np.full(values.shape, np.nan, dtype=np.float64)
    cumsum = np.cumsum(values, dtype=np.float64)
    cumsum[period:] = cumsum[period:] - cumsum[:-period]
    result[period - 1 :] = cumsum[period - 1 :] / period
    return result


def standard_deviation_np(values: np.ndarray, period: int, ddof: int = 0) -> np.ndarray:
    """
    Rolling standard deviation.

    Args:
        values: Array of prices.
        period: Window length.
        ddof: Delta degrees of freedom (0 = population, 1 = sample).

    Returns:
        Array of rolling standard deviation values.
    """
    values = np.asarray(values, dtype=np.float64)
    if period <= 1 or values.size == 0:
        return np.full(values.shape, np.nan, dtype=np.float64)

    series = pd.Series(values, dtype="float64")
    return series.rolling(window=period, min_periods=period).std(ddof=ddof).to_numpy()


def momentum_np(close: np.ndarray, period: int = 10) -> np.ndarray:
    """
    Price momentum (difference vs. previous closing price).

    Args:
        close: Close prices.
        period: Lookback for momentum.

    Returns:
        Array of momentum values.
    """
    close = np.asarray(close, dtype=np.float64)
    if period <= 0 or close.size == 0:
        return np.zeros_like(close, dtype=np.float64)

    out = np.full(close.shape, np.nan, dtype=np.float64)
    out[period:] = close[period:] - close[:-period]
    return out


def stochastic_np(
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    k_period: int = 14,
    d_period: int = 3,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stochastic Oscillator (%K and %D).
    """
    high = np.asarray(high, dtype=np.float64)
    low = np.asarray(low, dtype=np.float64)
    close = np.asarray(close, dtype=np.float64)

    if min(high.size, low.size, close.size) == 0:
        return np.array([], dtype=np.float64), np.array([], dtype=np.float64)

    highest = pd.Series(high).rolling(window=k_period, min_periods=k_period).max()
    lowest = pd.Series(low).rolling(window=k_period, min_periods=k_period).min()

    k = np.full(close.shape, np.nan, dtype=np.float64)
    denom = highest - lowest
    valid = denom != 0
    k[valid] = ((close[valid] - lowest[valid]) / denom[valid]) * 100.0

    d = pd.Series(k).rolling(window=d_period, min_periods=d_period).mean().to_numpy()
    return k, d


def cci_np(
    high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 20
) -> np.ndarray:
    """
    Commodity Channel Index.
    """
    high = np.asarray(high, dtype=np.float64)
    low = np.asarray(low, dtype=np.float64)
    close = np.asarray(close, dtype=np.float64)

    typical_price = (high + low + close) / 3.0
    tp_series = pd.Series(typical_price)
    sma_tp = tp_series.rolling(window=period, min_periods=period).mean()
    mad = tp_series.rolling(window=period, min_periods=period).apply(
        lambda arr: np.mean(np.abs(arr - arr.mean())), raw=True
    )

    cci = np.full(close.shape, np.nan, dtype=np.float64)
    valid = mad != 0
    cci[valid] = (typical_price[valid] - sma_tp[valid]) / (0.015 * mad[valid])
    cci[~valid] = 0.0
    return cci


def ichimoku_np(
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    tenkan: int = 9,
    kijun: int = 26,
    senkou_b: int = 52,
    displacement: int = 26,
) -> Dict[str, np.ndarray]:
    """
    Ichimoku Cloud components.
    """
    df = pd.DataFrame(
        {"high": high, "low": low, "close": close}, dtype="float64"
    )

    tenkan_line = (
        df["high"].rolling(window=tenkan, min_periods=tenkan).max()
        + df["low"].rolling(window=tenkan, min_periods=tenkan).min()
    ) / 2.0

    kijun_line = (
        df["high"].rolling(window=kijun, min_periods=kijun).max()
        + df["low"].rolling(window=kijun, min_periods=kijun).min()
    ) / 2.0

    senkou_span_a = ((tenkan_line + kijun_line) / 2.0).shift(displacement)

    senkou_span_b = (
        df["high"].rolling(window=senkou_b, min_periods=senkou_b).max()
        + df["low"].rolling(window=senkou_b, min_periods=senkou_b).min()
    ) / 2.0
    senkou_span_b = senkou_span_b.shift(displacement)

    chikou_span = df["close"].shift(-displacement)

    return {
        "tenkan": tenkan_line.to_numpy(),
        "kijun": kijun_line.to_numpy(),
        "senkou_a": senkou_span_a.to_numpy(),
        "senkou_b": senkou_span_b.to_numpy(),
        "chikou": chikou_span.to_numpy(),
    }


def parabolic_sar_np(
    high: np.ndarray,
    low: np.ndarray,
    step: float = 0.02,
    max_af: float = 0.2,
) -> np.ndarray:
    """
    Parabolic SAR implementation.
    """
    high = np.asarray(high, dtype=np.float64)
    low = np.asarray(low, dtype=np.float64)
    length = len(high)
    if length == 0:
        return np.array([], dtype=np.float64)

    sar = np.zeros(length, dtype=np.float64)
    trend_up = True
    af = step
    ep = high[0]
    sar[0] = low[0]

    for i in range(1, length):
        prev_sar = sar[i - 1]
        if trend_up:
            sar[i] = prev_sar + af * (ep - prev_sar)
            sar[i] = min(sar[i], low[i - 1], low[i])
            if high[i] > ep:
                ep = high[i]
                af = min(af + step, max_af)
            if low[i] < sar[i]:
                trend_up = False
                sar[i] = ep
                ep = low[i]
                af = step
        else:
            sar[i] = prev_sar + af * (ep - prev_sar)
            sar[i] = max(sar[i], high[i - 1], high[i])
            if low[i] < ep:
                ep = low[i]
                af = min(af + step, max_af)
            if high[i] > sar[i]:
                trend_up = True
                sar[i] = ep
                ep = high[i]
                af = step

    return sar


def adx_np(
    high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14
) -> Dict[str, np.ndarray]:
    """
    Average Directional Index along with +DI and -DI.
    """
    high = np.asarray(high, dtype=np.float64)
    low = np.asarray(low, dtype=np.float64)
    close = np.asarray(close, dtype=np.float64)
    if len(close) == 0:
        return {"adx": np.array([], dtype=np.float64), "plus_di": np.array([], dtype=np.float64), "minus_di": np.array([], dtype=np.float64)}

    prev_high = np.roll(high, 1)
    prev_low = np.roll(low, 1)
    prev_close = np.roll(close, 1)
    prev_high[0] = high[0]
    prev_low[0] = low[0]
    prev_close[0] = close[0]

    plus_dm = np.where((high - prev_high) > (prev_low - low), np.maximum(high - prev_high, 0.0), 0.0)
    minus_dm = np.where((prev_low - low) > (high - prev_high), np.maximum(prev_low - low, 0.0), 0.0)
    tr = np.maximum.reduce([high - low, np.abs(high - prev_close), np.abs(low - prev_close)])

    alpha = 1.0 / period
    tr_smooth = pd.Series(tr).ewm(alpha=alpha, adjust=False).mean().to_numpy()
    plus_dm_smooth = pd.Series(plus_dm).ewm(alpha=alpha, adjust=False).mean().to_numpy()
    minus_dm_smooth = pd.Series(minus_dm).ewm(alpha=alpha, adjust=False).mean().to_numpy()

    plus_di = 100.0 * np.divide(
        plus_dm_smooth,
        tr_smooth,
        out=np.zeros_like(plus_dm_smooth),
        where=tr_smooth != 0,
    )
    minus_di = 100.0 * np.divide(
        minus_dm_smooth,
        tr_smooth,
        out=np.zeros_like(minus_dm_smooth),
        where=tr_smooth != 0,
    )

    dx = 100.0 * np.divide(
        np.abs(plus_di - minus_di),
        plus_di + minus_di,
        out=np.zeros_like(plus_di),
        where=(plus_di + minus_di) != 0,
    )
    adx = pd.Series(dx).ewm(alpha=alpha, adjust=False).mean().to_numpy()

    return {"adx": adx, "plus_di": plus_di, "minus_di": minus_di}


def mfi_np(
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    volume: np.ndarray,
    period: int = 14,
) -> np.ndarray:
    """
    Money Flow Index.
    """
    high = np.asarray(high, dtype=np.float64)
    low = np.asarray(low, dtype=np.float64)
    close = np.asarray(close, dtype=np.float64)
    volume = np.asarray(volume, dtype=np.float64)

    typical_price = (high + low + close) / 3.0
    money_flow = typical_price * volume

    tp_diff = np.diff(typical_price, prepend=typical_price[0])
    positive_flow = np.where(tp_diff >= 0, money_flow, 0.0)
    negative_flow = np.where(tp_diff < 0, money_flow, 0.0)

    pos_sum = pd.Series(positive_flow).rolling(window=period, min_periods=period).sum()
    neg_sum = pd.Series(negative_flow).rolling(window=period, min_periods=period).sum()

    ratio = np.divide(
        pos_sum,
        neg_sum,
        out=np.zeros_like(pos_sum.to_numpy()),
        where=neg_sum.to_numpy() != 0,
    )
    mfi = 100.0 - (100.0 / (1.0 + ratio))
    return mfi.to_numpy()


def volume_oscillator_np(
    volume: np.ndarray,
    fast_period: int = 12,
    slow_period: int = 26,
    signal_period: int = 9,
) -> Dict[str, np.ndarray]:
    """
    Percentage Volume Oscillator (PVO) and signal.
    """
    volume = np.asarray(volume, dtype=np.float64)
    if volume.size == 0:
        empty = np.array([], dtype=np.float64)
        return {"pvo": empty, "signal": empty, "histogram": empty}

    fast_ema = ema_np(volume, fast_period)
    slow_ema = ema_np(volume, slow_period)
    denominator = np.where(slow_ema != 0, slow_ema, np.nan)
    pvo = 100.0 * (fast_ema - slow_ema) / denominator
    signal = ema_np(pvo, signal_period)
    hist = pvo - signal

    return {"pvo": pvo, "signal": signal, "histogram": hist}


def fibonacci_levels_np(
    high: np.ndarray,
    low: np.ndarray,
    ratios: Iterable[float],
    lookback: int | None = None,
) -> Dict[str, np.ndarray]:
    """
    Fibonacci retracement levels based on rolling swing high/low.
    """
    high_series = pd.Series(high, dtype="float64")
    low_series = pd.Series(low, dtype="float64")

    if lookback and lookback > 1:
        swing_high = high_series.rolling(window=lookback, min_periods=lookback).max()
        swing_low = low_series.rolling(window=lookback, min_periods=lookback).min()
    else:
        swing_high = high_series.expanding().max()
        swing_low = low_series.expanding().min()

    span = swing_high - swing_low
    levels: Dict[str, np.ndarray] = {}
    for ratio in ratios:
        level = swing_high - span * ratio
        levels[f"fibo_{ratio:.3f}"] = level.to_numpy()
    return levels


def _infer_bars_per_day(index: Iterable[pd.Timestamp]) -> int:
    """Infer approximate number of bars per day from timestamps."""
    if not isinstance(index, pd.Index):
        index = pd.Index(index)
    if len(index) < 2:
        return 1
    diffs = index.to_series().diff().dropna()
    median_seconds = diffs.dt.total_seconds().median()
    if median_seconds is None or median_seconds <= 0:
        return 1
    bars = max(int(round(86_400 / median_seconds)), 1)
    return bars


def pivot_points_np(
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    index: Iterable[pd.Timestamp],
    timeframe: str = "daily",
) -> Dict[str, np.ndarray]:
    """
    Classic pivot points computed from the previous period.
    """
    timeframe_map = {"daily": "1D", "weekly": "1W", "monthly": "1M"}
    rule = timeframe_map.get(timeframe.lower(), "1D")

    df = pd.DataFrame({"high": high, "low": low, "close": close}, index=pd.Index(index))
    agg = df.resample(rule, label="right", closed="right").agg(
        {"high": "max", "low": "min", "close": "last"}
    )
    agg = agg.shift(1)  # use previous period values

    pivot = (agg["high"] + agg["low"] + agg["close"]) / 3.0
    r1 = 2 * pivot - agg["low"]
    s1 = 2 * pivot - agg["high"]
    range_ = agg["high"] - agg["low"]
    r2 = pivot + range_
    s2 = pivot - range_
    r3 = agg["high"] + 2 * (pivot - agg["low"])
    s3 = agg["low"] - 2 * (agg["high"] - pivot)

    def _expand(series: pd.Series) -> np.ndarray:
        return series.reindex(df.index, method="ffill").to_numpy()

    return {
        f"pivot_{timeframe.lower()}": _expand(pivot),
        f"pivot_r1_{timeframe.lower()}": _expand(r1),
        f"pivot_s1_{timeframe.lower()}": _expand(s1),
        f"pivot_r2_{timeframe.lower()}": _expand(r2),
        f"pivot_s2_{timeframe.lower()}": _expand(s2),
        f"pivot_r3_{timeframe.lower()}": _expand(r3),
        f"pivot_s3_{timeframe.lower()}": _expand(s3),
    }


def onchain_smoothing_np(
    volume: np.ndarray,
    index: Iterable[pd.Timestamp],
    smoothing_days: int = 7,
) -> np.ndarray:
    """
    On-chain style smoothing (EMA) applied to transaction volume.
    """
    volume = np.asarray(volume, dtype=np.float64)
    if volume.size == 0:
        return np.array([], dtype=np.float64)

    bars_per_day = _infer_bars_per_day(index)
    window = max(int(smoothing_days * bars_per_day), 1)
    return ema_np(volume, window)


def crypto_fear_greed_np(
    close: np.ndarray,
    volume: np.ndarray,
    index: Iterable[pd.Timestamp],
    smoothing_days: int = 7,
) -> np.ndarray:
    """
    Synthetic crypto Fear & Greed index (0-100) derived from price, volume and volatility.
    """
    close = np.asarray(close, dtype=np.float64)
    volume = np.asarray(volume, dtype=np.float64)
    if close.size == 0:
        return np.array([], dtype=np.float64)

    bars_per_day = _infer_bars_per_day(index)
    window = max(int(smoothing_days * bars_per_day), 1)

    returns = pd.Series(close).pct_change().fillna(0.0)
    momentum = ema_np(returns.to_numpy(), window)
    momentum_score = 50.0 + 50.0 * np.tanh(momentum * 10.0)

    volatility = returns.rolling(window=window, min_periods=2).std().to_numpy()
    vol_norm = volatility / (np.nanmean(volatility) + 1e-12)
    volatility_score = np.clip(100.0 - vol_norm * 50.0, 0.0, 100.0)

    vol_series = pd.Series(volume)
    vol_mean = vol_series.rolling(window=window, min_periods=2).mean()
    vol_std = vol_series.rolling(window=window, min_periods=2).std()
    volume_z = np.divide(
        vol_series - vol_mean,
        vol_std,
        out=np.zeros_like(vol_series.to_numpy()),
        where=vol_std.to_numpy() != 0,
    )
    volume_score = np.clip(50.0 + 10.0 * volume_z.to_numpy(), 0.0, 100.0)

    composite = (
        0.25 * momentum_score + 0.25 * volume_score + 0.25 * volatility_score + 0.25 * 50.0
    )
    return ema_np(composite, window)


def pi_cycle_np(
    close: np.ndarray,
    index: Iterable[pd.Timestamp],
    dma_111_days: int = 111,
    dma_350_days: int = 350,
) -> Dict[str, np.ndarray]:
    """
    Pi Cycle indicator components (111 DMA and 350 DMA * 2).
    """
    close = np.asarray(close, dtype=np.float64)
    bars_per_day = _infer_bars_per_day(index)
    period_111 = max(int(dma_111_days * bars_per_day), 1)
    period_350 = max(int(dma_350_days * bars_per_day), 1)

    dma_111 = sma_np(close, period_111)
    dma_350 = sma_np(close, period_350)
    signal = dma_111 - 2.0 * dma_350

    return {
        "pi_cycle_dma111": dma_111,
        "pi_cycle_dma350x2": 2.0 * dma_350,
        "pi_cycle_signal": signal,
    }


def amplitude_hunter_np(
    close: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    volume: np.ndarray,
    index: Iterable[pd.Timestamp],
    bb_period: int = 20,
    bb_std: float = 2.0,
    lookback: int = 20,
    bbwidth_percentile_threshold: float = 50.0,
    volume_zscore_threshold: float = 0.5,
) -> Dict[str, np.ndarray]:
    """
    Amplitude Hunter composite score based on Bollinger Band width percentiles and volume z-score.
    """
    lower, mid, upper, _ = boll_np(close, bb_period, bb_std)
    bbwidth = np.divide(
        upper - lower, mid, out=np.zeros_like(mid), where=np.abs(mid) > 1e-12
    )

    width_series = pd.Series(bbwidth)

    def _percentile_of_last(arr: np.ndarray) -> float:
        valid = arr[~np.isnan(arr)]
        if valid.size == 0:
            return np.nan
        last = valid[-1]
        return (np.sum(valid <= last) / valid.size) * 100.0

    width_percentile = width_series.rolling(
        window=lookback, min_periods=lookback
    ).apply(_percentile_of_last, raw=True)

    vol_series = pd.Series(volume, dtype="float64")
    rolling_mean = vol_series.rolling(window=lookback, min_periods=lookback).mean()
    rolling_std = vol_series.rolling(window=lookback, min_periods=lookback).std()
    volume_z = np.divide(
        vol_series - rolling_mean,
        rolling_std,
        out=np.zeros_like(vol_series.to_numpy()),
        where=rolling_std.to_numpy() != 0,
    )

    percentile_norm = (width_percentile / 100.0).to_numpy()
    volume_z_values = volume_z.to_numpy()

    threshold_pct = bbwidth_percentile_threshold / 100.0
    bbwidth_condition = np.where(percentile_norm <= threshold_pct, 1.0, 0.0)
    volume_condition = np.where(volume_z_values >= volume_zscore_threshold, 1.0, 0.0)
    score = np.clip(
        (1.0 - percentile_norm) * np.maximum(volume_z_values / (volume_zscore_threshold + 1e-9), 0.0),
        0.0,
        1.0,
    )

    return {
        "amplitude_bbwidth": bbwidth,
        "amplitude_width_percentile": percentile_norm,
        "amplitude_volume_zscore": volume_z_values,
        "amplitude_setup": np.minimum(bbwidth_condition, volume_condition),
        "amplitude_score": score,
    }


__all__ = [
    "ema_np",
    "atr_np",
    "boll_np",
    "sma_np",
    "standard_deviation_np",
    "momentum_np",
    "rsi_np",
    "macd_np",
    "vwap_np",
    "obv_np",
    "vortex_df",
    "stochastic_np",
    "cci_np",
    "ichimoku_np",
    "parabolic_sar_np",
    "adx_np",
    "mfi_np",
    "volume_oscillator_np",
    "fibonacci_levels_np",
    "pivot_points_np",
    "onchain_smoothing_np",
    "crypto_fear_greed_np",
    "pi_cycle_np",
    "amplitude_hunter_np",
]




----------------------------------------
Fichier: indicators\numpy_ext.py
"""
ThreadX Indicateurs NumPy - Module CentralisÃ©
==============================================

Module unique pour tous les indicateurs techniques optimisÃ©s NumPy.
Importe depuis src.threadx.indicators.indicators_np pour garantir
cohÃ©rence et performance.

Usage:
    from threadx.indicators.numpy import rsi_np, macd_np, boll_np

    close_prices = df['close'].values
    rsi = rsi_np(close_prices, period=14)
    macd, signal, hist = macd_np(close_prices)

Performance:
    - 50x plus rapide que pandas rolling
    - Optimisations EMA custom
    - Gestion NaN robuste

Auteur: ThreadX Core Team
Version: 2.0 (ConsolidÃ©)
Date: 11 octobre 2025
"""

from __future__ import annotations

from typing import Tuple

import numpy as np
import pandas as pd

# Import depuis le module natif ThreadX
from .indicators_np import (
    ema_np,
    rsi_np,
    boll_np,
    macd_np,
    atr_np,
    vwap_np,
    obv_np,
    vortex_df,
)


# RÃ©exportation pour API propre
__all__ = [
    "ema_np",
    "rsi_np",
    "boll_np",
    "macd_np",
    "atr_np",
    "vwap_np",
    "obv_np",
    "vortex_df",
]


# Fonctions helper pour intÃ©gration facile avec pandas DataFrame


def add_rsi(df: pd.DataFrame, period: int = 14, column: str = "close") -> pd.DataFrame:
    """
    Ajoute RSI Ã  un DataFrame pandas.

    Args:
        df: DataFrame avec colonne 'close' (ou column)
        period: PÃ©riode RSI (dÃ©faut 14)
        column: Nom de la colonne prix (dÃ©faut 'close')

    Returns:
        DataFrame avec colonne 'rsi' ajoutÃ©e
    """
    df = df.copy()
    df["rsi"] = rsi_np(df[column].values, period)
    return df


def add_macd(
    df: pd.DataFrame,
    fast: int = 12,
    slow: int = 26,
    signal: int = 9,
    column: str = "close",
) -> pd.DataFrame:
    """
    Ajoute MACD Ã  un DataFrame pandas.

    Args:
        df: DataFrame avec colonne 'close' (ou column)
        fast: PÃ©riode EMA rapide (dÃ©faut 12)
        slow: PÃ©riode EMA lente (dÃ©faut 26)
        signal: PÃ©riode signal (dÃ©faut 9)
        column: Nom de la colonne prix (dÃ©faut 'close')

    Returns:
        DataFrame avec colonnes 'macd', 'macd_signal', 'macd_hist'
    """
    df = df.copy()
    macd, sig, hist = macd_np(df[column].values, fast, slow, signal)
    df["macd"] = macd
    df["macd_signal"] = sig
    df["macd_hist"] = hist
    return df


def add_bollinger(
    df: pd.DataFrame, period: int = 20, std: float = 2.0, column: str = "close"
) -> pd.DataFrame:
    """
    Ajoute Bollinger Bands Ã  un DataFrame pandas.

    Args:
        df: DataFrame avec colonne 'close' (ou column)
        period: PÃ©riode moyenne mobile (dÃ©faut 20)
        std: Nombre d'Ã©carts-types (dÃ©faut 2.0)
        column: Nom de la colonne prix (dÃ©faut 'close')

    Returns:
        DataFrame avec colonnes 'bb_lower', 'bb_middle', 'bb_upper', 'bb_zscore'
    """
    df = df.copy()
    lower, middle, upper, z = boll_np(df[column].values, period, std)
    df["bb_lower"] = lower
    df["bb_middle"] = middle
    df["bb_upper"] = upper
    df["bb_zscore"] = z
    return df


def add_atr(df: pd.DataFrame, period: int = 14) -> pd.DataFrame:
    """
    Ajoute ATR Ã  un DataFrame pandas.

    Args:
        df: DataFrame avec colonnes 'high', 'low', 'close'
        period: PÃ©riode ATR (dÃ©faut 14)

    Returns:
        DataFrame avec colonne 'atr'
    """
    df = df.copy()
    df["atr"] = atr_np(df["high"].values, df["low"].values, df["close"].values, period)
    return df


def add_vwap(df: pd.DataFrame, window: int = 96) -> pd.DataFrame:
    """
    Ajoute VWAP Ã  un DataFrame pandas.

    Args:
        df: DataFrame avec colonnes 'high', 'low', 'close', 'volume'
        window: FenÃªtre de calcul (dÃ©faut 96 pour 1h sur donnÃ©es 1m)

    Returns:
        DataFrame avec colonne 'vwap'
    """
    df = df.copy()
    df["vwap"] = vwap_np(
        df["close"].values,
        df["high"].values,
        df["low"].values,
        df["volume"].values,
        window,
    )
    return df


def add_obv(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ajoute OBV Ã  un DataFrame pandas.

    Args:
        df: DataFrame avec colonnes 'close', 'volume'

    Returns:
        DataFrame avec colonne 'obv'
    """
    df = df.copy()
    df["obv"] = obv_np(df["close"].values, df["volume"].values)
    return df


def add_vortex(df: pd.DataFrame, period: int = 14) -> pd.DataFrame:
    """
    Ajoute Vortex Indicator Ã  un DataFrame pandas.

    Args:
        df: DataFrame avec colonnes 'high', 'low', 'close'
        period: PÃ©riode vortex (dÃ©faut 14)

    Returns:
        DataFrame avec colonnes 'vi_plus', 'vi_minus'
    """
    vortex = vortex_df(df["high"].values, df["low"].values, df["close"].values, period)
    df = df.copy()
    df["vi_plus"] = vortex["vi_plus"].values
    df["vi_minus"] = vortex["vi_minus"].values
    return df


def add_all_indicators(
    df: pd.DataFrame,
    rsi_period: int = 14,
    macd_fast: int = 12,
    macd_slow: int = 26,
    macd_signal: int = 9,
    bb_period: int = 20,
    bb_std: float = 2.0,
    atr_period: int = 14,
    vwap_window: int = 96,
    vortex_period: int = 14,
) -> pd.DataFrame:
    """
    Ajoute TOUS les indicateurs standard Ã  un DataFrame.

    Args:
        df: DataFrame OHLCV
        *_period/*_window: ParamÃ¨tres des indicateurs

    Returns:
        DataFrame enrichi avec tous les indicateurs
    """
    df = df.copy()

    # RSI
    df = add_rsi(df, rsi_period)

    # MACD
    df = add_macd(df, macd_fast, macd_slow, macd_signal)

    # Bollinger Bands
    df = add_bollinger(df, bb_period, bb_std)

    # ATR
    df = add_atr(df, atr_period)

    # VWAP
    df = add_vwap(df, vwap_window)

    # OBV
    df = add_obv(df)

    # Vortex
    df = add_vortex(df, vortex_period)

    return df


# =========================================================
#  Tests rapides
# =========================================================


def _test_indicators():
    """Test rapide des indicateurs avec donnÃ©es synthÃ©tiques."""
    import pandas as pd
    import numpy as np

    np.random.seed(42)
    n = 200

    # DonnÃ©es synthÃ©tiques rÃ©alistes
    close = 100 + np.cumsum(np.random.randn(n) * 0.5)
    high = close + np.abs(np.random.randn(n) * 2)
    low = close - np.abs(np.random.randn(n) * 2)
    open_price = np.roll(close, 1)
    open_price[0] = close[0]
    volume = np.random.randint(1000, 10000, n)

    df = pd.DataFrame(
        {"open": open_price, "high": high, "low": low, "close": close, "volume": volume}
    )

    print("ðŸ§ª Test des indicateurs NumPy ThreadX")
    print("=" * 50)

    # Test RSI
    df = add_rsi(df)
    print(f"âœ… RSI: {df['rsi'].iloc[-1]:.2f}")

    # Test MACD
    df = add_macd(df)
    print(f"âœ… MACD: {df['macd'].iloc[-1]:.4f}")

    # Test Bollinger
    df = add_bollinger(df)
    print(f"âœ… BB Upper: {df['bb_upper'].iloc[-1]:.2f}")

    # Test ATR
    df = add_atr(df)
    print(f"âœ… ATR: {df['atr'].iloc[-1]:.4f}")

    # Test VWAP
    df = add_vwap(df)
    print(f"âœ… VWAP: {df['vwap'].iloc[-1]:.2f}")

    # Test OBV
    df = add_obv(df)
    print(f"âœ… OBV: {df['obv'].iloc[-1]:.0f}")

    # Test Vortex
    df = add_vortex(df)
    print(f"âœ… VI+: {df['vi_plus'].iloc[-1]:.4f}")

    print("\nðŸŽ‰ Tous les indicateurs fonctionnent correctement!")
    print(f"DataFrame final: {len(df.columns)} colonnes")
    print(f"Colonnes: {', '.join(df.columns)}")


if __name__ == "__main__":
    _test_indicators()




----------------------------------------
Fichier: indicators\xatr.py
#!/usr/bin/env python3
"""
ThreadX ATR (Average True Range) - ImplÃ©mentation vectorisÃ©e GPU/CPU
===================================================================

Calcul vectorisÃ© de l'Average True Range avec support:
- GPU multi-carte (RTX 5090 + RTX 2060)
- Batch processing optimisÃ©
- Fallback CPU transparent
- Device-agnostic wrappers

Formule ATR:
- True Range (TR) = max(high-low, abs(high-prev_close), abs(low-prev_close))
- ATR = EMA(TR, period) ou SMA(TR, period)

Optimisations:
- Vectorisation complÃ¨te NumPy/CuPy
- Split GPU 75%/25% selon puissance carte
- Cache True Range pour rÃ©utilisation
- Synchronisation NCCL pour multi-GPU

Exemple d'usage:
    ```python
    import numpy as np
    from threadx.indicators.atr import compute_atr

    # DonnÃ©es OHLCV
    high = np.random.randn(1000) * 5 + 105
    low = np.random.randn(1000) * 5 + 95
    close = np.random.randn(1000) * 5 + 100

    # Calcul simple
    atr_values = compute_atr(high, low, close, period=14)

    # Calcul batch multiple pÃ©riodes
    from threadx.indicators.atr import compute_atr_batch
    params = [
        {'period': 14, 'method': 'ema'},
        {'period': 21, 'method': 'sma'},
        {'period': 7, 'method': 'ema'}
    ]
    results = compute_atr_batch(high, low, close, params)
    ```
"""

import logging
import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Union, Any, Literal
import numpy as np
import pandas as pd

# GPU imports avec fallback robuste
try:
    import cupy as cp

    HAS_CUPY = True
    # Test si GPU disponible
    try:
        cp.cuda.Device(0).use()
        GPU_AVAILABLE = True
        N_GPUS = cp.cuda.runtime.getDeviceCount()
    except:
        GPU_AVAILABLE = False
        N_GPUS = 0
except ImportError:
    HAS_CUPY = False
    GPU_AVAILABLE = False
    N_GPUS = 0

    # Mock robuste de CuPy avec toutes les fonctions nÃ©cessaires
    class MockCudaDevice:
        def __init__(self, device_id):
            self.device_id = device_id

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def use(self):
            pass

    class MockCudaRuntime:
        @staticmethod
        def getDeviceCount():
            return 0

        @staticmethod
        def getDeviceProperties(device_id):
            return {"name": "MockGPU", "totalGlobalMem": 8 * 1024**3}

        @staticmethod
        def memGetInfo():
            return (0, 8 * 1024**3)  # free, total

    class MockCuda:
        runtime = MockCudaRuntime()

        @staticmethod
        def Device(device_id):
            return MockCudaDevice(device_id)

    class MockCuPy:
        cuda = MockCuda()
        float64 = np.float64
        nan = np.nan

        @staticmethod
        def asarray(x):
            return np.asarray(x)

        @staticmethod
        def asnumpy(x):
            return np.asarray(x)

        @staticmethod
        def convolve(a, v, mode="full"):
            if mode in ("full", "valid", "same"):
                return np.convolve(a, v, mode=mode)  # type: ignore
            else:
                return np.convolve(a, v, mode="full")  # type: ignore

        @staticmethod
        def ones(shape, dtype=None):
            return np.ones(shape, dtype=dtype)

        @staticmethod
        def zeros_like(a):
            return np.zeros_like(a)

        @staticmethod
        def concatenate(arrays):
            return np.concatenate(arrays)

        @staticmethod
        def full(shape, fill_value, dtype=None):
            return np.full(shape, fill_value, dtype=dtype)

        @staticmethod
        def array(data, dtype=None):
            return np.array(data, dtype=dtype)

        @staticmethod
        def abs(x):
            return np.abs(x)

        @staticmethod
        def maximum(x1, x2):
            return np.maximum(x1, x2)

        @staticmethod
        def exp(x):
            return np.exp(x)

    cp = MockCuPy()

# Configuration logging
logger = logging.getLogger(__name__)


@dataclass
class ATRSettings:
    """Configuration pour calculs ATR"""

    period: int = 14
    method: Literal["ema", "sma"] = "ema"
    use_gpu: bool = True
    gpu_batch_size: int = 1000
    cpu_fallback: bool = True
    gpu_split_ratio: Tuple[float, float] = (0.75, 0.25)  # RTX 5090 / RTX 2060

    def __post_init__(self):
        """Validation des paramÃ¨tres"""
        if self.period < 1:
            raise ValueError(f"Period doit Ãªtre >= 1, reÃ§u: {self.period}")
        if self.method not in ["ema", "sma"]:
            raise ValueError(f"Method doit Ãªtre 'ema' ou 'sma', reÃ§u: {self.method}")
        if not (0.1 <= sum(self.gpu_split_ratio) <= 1.0):
            raise ValueError(f"gpu_split_ratio invalide: {self.gpu_split_ratio}")


class ATRGPUManager:
    """Gestionnaire GPU multi-carte pour ATR"""

    def __init__(self, settings: ATRSettings):
        self.settings = settings
        self.available_gpus = []
        self.gpu_capabilities = {}

        if HAS_CUPY and GPU_AVAILABLE:
            self._detect_gpus()
            logger.info(
                f"ðŸ”¥ ATR GPU Manager: {len(self.available_gpus)} GPU(s) dÃ©tectÃ©s"
            )
            for gpu_id, cap in self.gpu_capabilities.items():
                logger.debug(f"   GPU {gpu_id}: {cap['name']} ({cap['memory']:.1f}GB)")

    def _detect_gpus(self):
        """DÃ©tection et profilage des GPU disponibles"""
        try:
            for gpu_id in range(N_GPUS):
                with cp.cuda.Device(gpu_id):
                    # Infos GPU
                    props = cp.cuda.runtime.getDeviceProperties(gpu_id)
                    memory_total = cp.cuda.runtime.memGetInfo()[1] / (1024**3)  # GB

                    self.available_gpus.append(gpu_id)
                    self.gpu_capabilities[gpu_id] = {
                        "name": props["name"].decode("utf-8"),
                        "memory": memory_total,
                        "compute_capability": (props["major"], props["minor"]),
                        "multiprocessors": props["multiProcessorCount"],
                    }
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur dÃ©tection GPU ATR: {e}")
            self.available_gpus = []

    def split_workload(self, data_size: int) -> List[Tuple[int, int, int]]:
        """Split workload entre GPU selon leurs capacitÃ©s"""
        if not self.available_gpus:
            return []

        splits = []
        if len(self.available_gpus) == 1:
            splits.append((self.available_gpus[0], 0, data_size))
        elif len(self.available_gpus) >= 2:
            gpu1_size = int(data_size * self.settings.gpu_split_ratio[0])
            gpu2_size = data_size - gpu1_size

            splits.append((self.available_gpus[0], 0, gpu1_size))
            if gpu2_size > 0:
                splits.append((self.available_gpus[1], gpu1_size, data_size))

        logger.debug(f"ðŸ”„ ATR Workload split: {[(s[0], s[2]-s[1]) for s in splits]}")
        return splits


class ATR:
    """Calculateur ATR vectorisÃ© avec support GPU multi-carte"""

    def __init__(self, settings: Optional[ATRSettings] = None):
        self.settings = settings or ATRSettings()
        self.gpu_manager = ATRGPUManager(self.settings)
        self._cache = {}  # Cache pour True Range rÃ©utilisables

        logger.info(
            f"ðŸŽ¯ ATR initialisÃ© - GPU: {GPU_AVAILABLE}, Multi-GPU: {len(self.gpu_manager.available_gpus)}"
        )

    def compute(
        self,
        high: Union[np.ndarray, pd.Series],
        low: Union[np.ndarray, pd.Series],
        close: Union[np.ndarray, pd.Series],
        period: Optional[Union[int, str]] = None,
        method: Optional[Union[str, int]] = None,
    ) -> np.ndarray:
        """
        Calcul ATR pour une sÃ©rie de prix OHLC

        Args:
            high: Prix hauts (array-like)
            low: Prix bas (array-like)
            close: Prix de clÃ´ture (array-like)
            period: PÃ©riode pour moyenne (dÃ©faut: settings.period)
            method: 'ema' ou 'sma' (dÃ©faut: settings.method)

        Returns:
            np.ndarray: Valeurs ATR

        Exemple:
            ```python
            atr = ATR()
            atr_values = atr.compute(highs, lows, closes, period=14, method='ema')
            ```
        """
        # ParamÃ¨tres avec conversions
        period = int(period) if period is not None else self.settings.period
        if method is not None and method in ["ema", "sma"]:
            method = str(method)
        else:
            method = self.settings.method

        # Conversion en numpy avec types prÃ©cis
        if isinstance(high, pd.Series):
            high = np.asarray(high.values, dtype=np.float64)
        else:
            high = np.asarray(high, dtype=np.float64)

        if isinstance(low, pd.Series):
            low = np.asarray(low.values, dtype=np.float64)
        else:
            low = np.asarray(low, dtype=np.float64)

        if isinstance(close, pd.Series):
            close = np.asarray(close.values, dtype=np.float64)
        else:
            close = np.asarray(close, dtype=np.float64)

        high = np.asarray(high, dtype=np.float64)
        low = np.asarray(low, dtype=np.float64)
        close = np.asarray(close, dtype=np.float64)

        # Validation tailles
        if not (len(high) == len(low) == len(close)):
            raise ValueError(
                f"Tailles diffÃ©rentes: high={len(high)}, low={len(low)}, close={len(close)}"
            )

        if len(close) < period + 1:  # +1 pour calcul True Range
            raise ValueError(
                f"DonnÃ©es insuffisantes: {len(close)} < period+1={period+1}"
            )

        # Tentative GPU
        if (
            self.settings.use_gpu
            and GPU_AVAILABLE
            and len(self.gpu_manager.available_gpus) > 0
        ):
            try:
                return self._compute_gpu(high, low, close, period, method)
            except Exception as e:
                logger.warning(f"âš ï¸ ATR GPU failed, fallback CPU: {e}")
                if not self.settings.cpu_fallback:
                    raise

        # Fallback CPU
        return self._compute_cpu(high, low, close, period, method)

    def _compute_gpu(
        self,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        period: int,
        method: str,
    ) -> np.ndarray:
        """Calcul ATR GPU avec rÃ©partition multi-carte"""

        # Transfert vers GPU principal
        with cp.cuda.Device(self.gpu_manager.available_gpus[0]):
            high_gpu = cp.asarray(high)
            low_gpu = cp.asarray(low)
            close_gpu = cp.asarray(close)

            # True Range
            tr = self._true_range_gpu(high_gpu, low_gpu, close_gpu)

            # ATR selon mÃ©thode
            if method == "ema":
                atr = self._ema_gpu(tr, period)
            else:  # sma
                atr = self._sma_gpu(tr, period)

            # Retour CPU
            return cp.asnumpy(atr)

    def _true_range_gpu(self, high_gpu, low_gpu, close_gpu):
        """Calcul True Range GPU vectorisÃ©"""
        n = len(high_gpu)

        # DÃ©calage close pour avoir prev_close
        prev_close = cp.concatenate([cp.array([close_gpu[0]]), close_gpu[:-1]])

        # Trois composantes du True Range
        hl_diff = high_gpu - low_gpu
        hc_diff = cp.abs(high_gpu - prev_close)
        lc_diff = cp.abs(low_gpu - prev_close)

        # Maximum des trois
        tr = cp.maximum(hl_diff, cp.maximum(hc_diff, lc_diff))

        return tr

    def _ema_gpu(self, values_gpu, period: int):
        """Exponential Moving Average GPU"""
        alpha = 2.0 / (period + 1.0)
        n = len(values_gpu)

        result = cp.zeros_like(values_gpu)
        result[0] = values_gpu[0]

        # Calcul itÃ©ratif EMA
        for i in range(1, n):
            result[i] = alpha * values_gpu[i] + (1 - alpha) * result[i - 1]

        return result

    def _sma_gpu(self, values_gpu, period: int):
        """Simple Moving Average GPU"""
        # Utilise convolution pour efficacitÃ©
        kernel = cp.ones(period, dtype=cp.float64) / period

        # Convolution avec padding
        sma = cp.convolve(values_gpu, kernel, mode="valid")

        # Padding pour aligner avec input
        padding = cp.full(period - 1, cp.nan, dtype=cp.float64)
        return cp.concatenate([padding, sma])

    def _compute_cpu(
        self,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        period: int,
        method: str,
    ) -> np.ndarray:
        """Fallback CPU vectorisÃ©"""

        # True Range CPU
        tr = self._true_range_cpu(high, low, close)

        # ATR selon mÃ©thode avec pandas pour efficacitÃ©
        tr_series = pd.Series(tr)

        if method == "ema":
            atr = tr_series.ewm(span=period, adjust=False).mean().values
        else:  # sma
            atr = tr_series.rolling(window=period, min_periods=period).mean().values

        return np.asarray(atr, dtype=np.float64)

    def _true_range_cpu(
        self, high: np.ndarray, low: np.ndarray, close: np.ndarray
    ) -> np.ndarray:
        """Calcul True Range CPU vectorisÃ©"""
        n = len(high)

        # DÃ©calage close pour avoir prev_close
        prev_close = np.concatenate([[close[0]], close[:-1]])

        # Trois composantes du True Range
        hl_diff = high - low
        hc_diff = np.abs(high - prev_close)
        lc_diff = np.abs(low - prev_close)

        # Maximum des trois
        tr = np.maximum(hl_diff, np.maximum(hc_diff, lc_diff))

        return tr

    def compute_batch(
        self,
        high: Union[np.ndarray, pd.Series],
        low: Union[np.ndarray, pd.Series],
        close: Union[np.ndarray, pd.Series],
        params_list: List[Dict[str, Union[int, str]]],
    ) -> Dict[str, np.ndarray]:
        """
        Calcul ATR batch pour multiples paramÃ¨tres

        Args:
            high, low, close: Prix OHLC
            params_list: Liste de dictionnaires {'period': int, 'method': str}

        Returns:
            Dict[param_key] = atr_values

        Exemple:
            ```python
            params = [
                {'period': 14, 'method': 'ema'},
                {'period': 21, 'method': 'sma'}
            ]
            results = atr.compute_batch(high, low, close, params)
            print(results['14_ema'])  # Valeurs ATR
            ```
        """
        start_time = time.time()
        results = {}

        logger.info(f"ðŸ”„ ATR batch: {len(params_list)} paramÃ¨tres")

        # Multi-GPU batch si seuil atteint
        if (
            len(params_list) >= 100
            and self.settings.use_gpu
            and len(self.gpu_manager.available_gpus) >= 2
        ):

            return self._compute_batch_multi_gpu(high, low, close, params_list)

        # Calcul sÃ©quentiel
        for params in params_list:
            period = params["period"]
            method = params.get("method", "ema")
            # Conversions explicites
            period_int = int(period) if isinstance(period, (int, str)) else period
            method_str = (
                str(method)
                if isinstance(method, (str, int)) and str(method) in ["ema", "sma"]
                else "ema"
            )
            key = f"{period_int}_{method_str}"

            try:
                results[key] = self.compute(
                    high, low, close, period=period_int, method=method_str
                )
            except Exception as e:
                logger.error(f"âŒ Erreur ATR paramÃ¨tre {key}: {e}")
                results[key] = None

        elapsed = time.time() - start_time
        success_count = sum(1 for r in results.values() if r is not None)
        logger.info(
            f"âœ… ATR batch terminÃ©: {success_count}/{len(params_list)} succÃ¨s en {elapsed:.2f}s"
        )

        return results

    def _compute_batch_multi_gpu(
        self,
        high: Union[np.ndarray, pd.Series],
        low: Union[np.ndarray, pd.Series],
        close: Union[np.ndarray, pd.Series],
        params_list: List[Dict[str, Union[int, str]]],
    ) -> Dict[str, np.ndarray]:
        """Calcul ATR batch multi-GPU avec rÃ©partition"""

        logger.info(
            f"ðŸš€ ATR Multi-GPU batch: {len(params_list)} paramÃ¨tres sur {len(self.gpu_manager.available_gpus)} GPU"
        )

        # Split paramÃ¨tres entre GPU
        workload_splits = self.gpu_manager.split_workload(len(params_list))
        results = {}

        # Conversion donnÃ©es avec types prÃ©cis
        if isinstance(high, pd.Series):
            high = np.asarray(high.values, dtype=np.float64)
        else:
            high = np.asarray(high, dtype=np.float64)

        if isinstance(low, pd.Series):
            low = np.asarray(low.values, dtype=np.float64)
        else:
            low = np.asarray(low, dtype=np.float64)

        if isinstance(close, pd.Series):
            close = np.asarray(close.values, dtype=np.float64)
        else:
            close = np.asarray(close, dtype=np.float64)

        high = np.asarray(high, dtype=np.float64)
        low = np.asarray(low, dtype=np.float64)
        close = np.asarray(close, dtype=np.float64)

        # Traitement par GPU
        for gpu_id, start_idx, end_idx in workload_splits:
            gpu_params = params_list[start_idx:end_idx]

            with cp.cuda.Device(gpu_id):
                logger.debug(
                    f"ðŸ”¥ GPU {gpu_id}: traitement ATR {len(gpu_params)} paramÃ¨tres"
                )

                high_gpu = cp.asarray(high)
                low_gpu = cp.asarray(low)
                close_gpu = cp.asarray(close)

                # True Range commun (rÃ©utilisable)
                tr = self._true_range_gpu(high_gpu, low_gpu, close_gpu)

                for params in gpu_params:
                    period = params["period"]
                    method = params.get("method", "ema")
                    # Conversions explicites pour GPU
                    period_int = (
                        int(period) if isinstance(period, (int, str)) else period
                    )
                    method_str = (
                        str(method)
                        if isinstance(method, (str, int))
                        and str(method) in ["ema", "sma"]
                        else "ema"
                    )
                    key = f"{period_int}_{method_str}"

                    try:
                        # ATR selon mÃ©thode
                        if method_str == "ema":
                            atr = self._ema_gpu(tr, period_int)
                        else:  # sma
                            atr = self._sma_gpu(tr, period_int)

                        # Retour CPU
                        results[key] = cp.asnumpy(atr)

                    except Exception as e:
                        logger.error(f"âŒ GPU {gpu_id} erreur ATR {key}: {e}")
                        results[key] = None

        return results


# ========================================
# FONCTIONS PUBLIQUES (API simplifiÃ©e)
# ========================================


def compute_atr(
    high: Union[np.ndarray, pd.Series],
    low: Union[np.ndarray, pd.Series],
    close: Union[np.ndarray, pd.Series],
    period: int = 14,
    method: Literal["ema", "sma"] = "ema",
    use_gpu: bool = True,
) -> np.ndarray:
    """
    Calcul ATR - API simple

    Args:
        high: Prix hauts
        low: Prix bas
        close: Prix de clÃ´ture
        period: PÃ©riode pour moyenne (dÃ©faut: 14)
        method: 'ema' ou 'sma' (dÃ©faut: 'ema')
        use_gpu: Utiliser GPU si disponible (dÃ©faut: True)

    Returns:
        np.ndarray: Valeurs ATR

    Exemple:
        ```python
        import numpy as np
        from threadx.indicators.atr import compute_atr

        # DonnÃ©es test OHLC
        n = 1000
        high = np.random.randn(n) * 5 + 105
        low = np.random.randn(n) * 5 + 95
        close = np.random.randn(n) * 5 + 100

        # Calcul ATR
        atr_values = compute_atr(high, low, close, period=14, method='ema')

        print(f"ATR calculÃ©: {len(atr_values)} points")
        print(f"ATR moyen: {np.nanmean(atr_values):.4f}")
        print(f"ATR dernier: {atr_values[-1]:.4f}")
        ```
    """
    settings = ATRSettings(period=period, method=method, use_gpu=use_gpu)

    atr = ATR(settings)
    return atr.compute(high, low, close)


def compute_atr_batch(
    high: Union[np.ndarray, pd.Series],
    low: Union[np.ndarray, pd.Series],
    close: Union[np.ndarray, pd.Series],
    params_list: List[Dict[str, Union[int, str]]],
    use_gpu: bool = True,
) -> Dict[str, np.ndarray]:
    """
    Calcul ATR batch - API simple

    Args:
        high, low, close: Prix OHLC
        params_list: Liste paramÃ¨tres [{'period': int, 'method': str}, ...]
        use_gpu: Utiliser GPU si disponible (dÃ©faut: True)

    Returns:
        Dict[param_key] = atr_values

    Exemple:
        ```python
        from threadx.indicators.atr import compute_atr_batch

        params = [
            {'period': 14, 'method': 'ema'},
            {'period': 21, 'method': 'sma'},
            {'period': 7, 'method': 'ema'}
        ]

        results = compute_atr_batch(high, low, close, params)

        for key, atr_vals in results.items():
            if atr_vals is not None:
                print(f"{key}: ATR dernier={atr_vals[-1]:.4f}, moyenne={np.nanmean(atr_vals):.4f}")
        ```
    """
    settings = ATRSettings(use_gpu=use_gpu)
    atr = ATR(settings)
    return atr.compute_batch(high, low, close, params_list)


# ========================================
# UTILITAIRES ET VALIDATION
# ========================================


def validate_atr_results(atr_values: np.ndarray, tolerance: float = 1e-10) -> bool:
    """
    Validation rÃ©sultats ATR

    Args:
        atr_values: Valeurs ATR calculÃ©es
        tolerance: TolÃ©rance pour comparaisons numÃ©riques

    Returns:
        True si rÃ©sultats valides
    """
    try:
        # ATR doit Ãªtre >= 0 (hors NaN)
        valid_mask = ~np.isnan(atr_values)
        if np.any(valid_mask):
            valid_atr = atr_values[valid_mask]
            if np.any(valid_atr < -tolerance):
                return False

        # Pas de valeurs infinies
        if np.any(np.isinf(atr_values)):
            return False

        return True

    except Exception:
        return False


def benchmark_atr_performance(
    data_sizes: List[int] = [1000, 5000, 10000], n_runs: int = 3
) -> Dict[str, Any]:
    """
    Benchmark performance ATR CPU vs GPU

    Args:
        data_sizes: Tailles de donnÃ©es Ã  tester
        n_runs: Nombre d'exÃ©cutions par test

    Returns:
        Dict avec mÃ©triques de performance
    """
    results = {
        "cpu_times": {},
        "gpu_times": {},
        "speedups": {},
        "gpu_available": GPU_AVAILABLE,
    }

    logger.info(f"ðŸ Benchmark ATR - GPU: {GPU_AVAILABLE}")

    for size in data_sizes:
        logger.info(f"ðŸ“Š Test size: {size}")

        # DonnÃ©es test OHLC
        np.random.seed(42)
        high = np.random.randn(size) * 5 + 105
        low = np.random.randn(size) * 5 + 95
        close = np.random.randn(size) * 5 + 100

        # CPU timing
        cpu_times = []
        for _ in range(n_runs):
            start = time.time()
            compute_atr(high, low, close, use_gpu=False)
            cpu_times.append(time.time() - start)

        cpu_avg = np.mean(cpu_times)
        results["cpu_times"][size] = cpu_avg

        # GPU timing si disponible
        if GPU_AVAILABLE:
            gpu_times = []
            for _ in range(n_runs):
                start = time.time()
                compute_atr(high, low, close, use_gpu=True)
                gpu_times.append(time.time() - start)

            gpu_avg = np.mean(gpu_times)
            results["gpu_times"][size] = gpu_avg
            results["speedups"][size] = cpu_avg / gpu_avg

            logger.info(
                f"   CPU: {cpu_avg:.4f}s, GPU: {gpu_avg:.4f}s, Speedup: {cpu_avg/gpu_avg:.2f}x"
            )
        else:
            results["gpu_times"][size] = None
            results["speedups"][size] = None
            logger.info(f"   CPU: {cpu_avg:.4f}s, GPU: N/A")

    return results


if __name__ == "__main__":
    # Test rapide
    print("ðŸŽ¯ ThreadX ATR - Test rapide")

    # DonnÃ©es test OHLC
    np.random.seed(42)
    n = 1000
    high = np.random.randn(n) * 5 + 105
    low = np.random.randn(n) * 5 + 95
    close = np.random.randn(n) * 5 + 100

    # Test simple
    atr_values = compute_atr(high, low, close, period=14, method="ema")
    print(f"âœ… Test simple: {len(atr_values)} points calculÃ©s")
    print(f"   ATR dernier: {atr_values[-1]:.4f}")
    print(f"   ATR moyen: {np.nanmean(atr_values):.4f}")

    # Test batch
    params = [{"period": 14, "method": "ema"}, {"period": 21, "method": "sma"}]
    results = compute_atr_batch(high, low, close, params)
    print(f"âœ… Test batch: {len(results)} rÃ©sultats")

    # Validation
    valid = validate_atr_results(atr_values)
    print(f"âœ… Validation: {'PASS' if valid else 'FAIL'}")

    # Benchmark si GPU
    if GPU_AVAILABLE:
        print("ðŸ Benchmark performance...")
        bench = benchmark_atr_performance([1000], n_runs=2)
        if bench["speedups"][1000]:
            print(f"   Speedup GPU: {bench['speedups'][1000]:.2f}x")




----------------------------------------
Fichier: indicators\__init__.py
"""
ThreadX Indicators Layer - Phase 3
==================================

Module d'indicateurs techniques vectorisÃ©s avec support GPU multi-carte.

Modules principaux:
- bollinger.py : Bandes de Bollinger vectorisÃ©es
- atr.py : Average True Range vectorisÃ©
- bank.py : Cache centralisÃ© d'indicateurs

CaractÃ©ristiques:
- Vectorisation NumPy/CuPy pour performances optimales
- Support GPU RTX 5090 (32GB) + RTX 2060 avec rÃ©partition 75%/25%
- Cache intelligent avec TTL et checksums
- Batch processing automatique (seuil: 100 paramÃ¨tres)
- Fallback CPU transparent
"""

from .bollinger import BollingerBands, compute_bollinger_bands, compute_bollinger_batch

from .xatr import ATR, compute_atr, compute_atr_batch

from .bank import (
    IndicatorBank,
    IndicatorSettings,
    ensure_indicator,
    force_recompute_indicator,
    batch_ensure_indicators,
)

__version__ = "3.0.0"
__all__ = [
    # Bollinger Bands
    "BollingerBands",
    "compute_bollinger_bands",
    "compute_bollinger_batch",
    # ATR
    "ATR",
    "compute_atr",
    "compute_atr_batch",
    # Bank
    "IndicatorBank",
    "IndicatorSettings",
    "ensure_indicator",
    "force_recompute_indicator",
    "batch_ensure_indicators",
]




----------------------------------------
Fichier: optimization\engine.py
"""
ThreadX Parametric Optimization Engine
======================================

Moteur d'optimisation paramÃ©trique unifiÃ© qui utilise les composants existants :
- IndicatorBank (Phase 3) pour les calculs d'indicateurs avec cache GPU
- BacktestEngine (Phase 5) pour l'exÃ©cution des stratÃ©gies
- PerformanceCalculator (Phase 6) pour les mÃ©triques
- Cache intelligent avec TTL et checksums

Toutes les fonctions de calcul sont centralisÃ©es autour d'IndicatorBank.

Author: ThreadX Framework
Version: Phase 10 - Unified Compute Engine
"""

import json
import logging
import hashlib
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union, Callable
import itertools

import pandas as pd
import numpy as np
import toml

from threadx.indicators.bank import IndicatorBank
from threadx.utils.log import get_logger
from .scenarios import ScenarioSpec, generate_param_grid, generate_monte_carlo
from .pruning import pareto_soft_prune
from .reporting import write_reports, summarize_distribution

# Multi-GPU support
try:
    from threadx.gpu.multi_gpu import get_default_manager, MultiGPUManager

    MULTIGPU_AVAILABLE = True
except ImportError:
    MULTIGPU_AVAILABLE = False

# Monitoring systÃ¨me pour workers dynamiques
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = get_logger(__name__)

# Global stop flag to allow UI-triggered cancellation without direct runner reference
_GLOBAL_STOP_FLAG = False


def set_global_stop(stop: bool = True) -> None:
    """DÃ©finir le flag global pour arrÃªter l'exÃ©cution."""
    global _GLOBAL_STOP_FLAG
    _GLOBAL_STOP_FLAG = stop
    if stop:
        logger.warning("â¹ï¸ ARRÃŠT GLOBAL DEMANDÃ‰")


def is_global_stop_requested() -> bool:
    """VÃ©rifier si un arrÃªt global a Ã©tÃ© demandÃ©."""
    global _GLOBAL_STOP_FLAG
    return bool(_GLOBAL_STOP_FLAG)


def request_global_stop() -> None:
    """Request cancellation for any running optimization."""
    set_global_stop(True)


def clear_global_stop() -> None:
    """Clear the global stop flag."""
    global _GLOBAL_STOP_FLAG
    _GLOBAL_STOP_FLAG = False


class SweepRunner:
    """
    Runner de sweeps paramÃ©triques unifiÃ© avec batch processing et early stopping.

    Utilise IndicatorBank pour la mutualisation des calculs d'indicateurs,
    device-agnostic computing via xp, et hooks de performance par stage.
    """

    def __init__(
        self,
        indicator_bank: Optional[IndicatorBank] = None,
        max_workers: Optional[int] = None,
        use_multigpu: bool = True,
    ):
        """
        Initialise le runner de sweeps avec support Multi-GPU.

        Args:
            indicator_bank: Instance IndicatorBank pour cache partagÃ©
            max_workers: Nombre de workers (None = auto-dÃ©tection dynamique)
            use_multigpu: Activer distribution Multi-GPU si disponible
        """
        self.indicator_bank = indicator_bank or IndicatorBank()
        self.logger = get_logger(__name__)

        # Multi-GPU Manager
        self.use_multigpu = use_multigpu and MULTIGPU_AVAILABLE
        self.gpu_manager = None

        if self.use_multigpu:
            try:
                self.gpu_manager = get_default_manager()
                self.logger.info("âœ… Multi-GPU activÃ©")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Multi-GPU non disponible: {e}")
                self.use_multigpu = False

        # Workers dynamiques
        if max_workers is None:
            self.max_workers = self._calculate_optimal_workers()
            self.logger.info(f"Workers calculÃ©s automatiquement: {self.max_workers}")
        else:
            self.max_workers = max_workers
            self.logger.info(f"Workers configurÃ©s manuellement: {self.max_workers}")

        # Ã‰tat d'exÃ©cution
        self.is_running = False
        self.should_pause = False
        self.current_scenario = 0
        self.total_scenarios = 0

        # Hooks de performance
        self.stage_timings = {}
        self.start_time = None
        self._last_worker_adjustment = 0

        self.logger.info("ðŸš€ SweepRunner initialisÃ© avec IndicatorBank centralisÃ©")

    def _calculate_optimal_workers(self) -> int:
        """
        Calcule dynamiquement le nombre optimal de workers.

        BasÃ© sur:
        - Nombre de GPUs disponibles
        - VRAM disponible
        - RAM systÃ¨me disponible
        """
        # Base: nombre de CPU cores (physiques)
        if PSUTIL_AVAILABLE:
            base_workers = psutil.cpu_count(logical=False) or 4
        else:
            base_workers = 4

        if self.gpu_manager and self.use_multigpu:
            # Mode Multi-GPU: limiter les workers pour Ã©viter goulots
            gpu_devices = [
                d for d in self.gpu_manager.available_devices if d.device_id != -1
            ]

            if len(gpu_devices) >= 2:
                # 2 GPUs: 4 workers par GPU = 8 total
                optimal = len(gpu_devices) * 4
            elif len(gpu_devices) == 1:
                # 1 GPU: 6 workers
                optimal = 6
            else:
                # Pas de GPU: utiliser CPU
                optimal = base_workers
        else:
            # Mode CPU-only: plus de workers
            optimal = min(base_workers * 2, 16)

        # VÃ©rifier RAM disponible
        if PSUTIL_AVAILABLE:
            ram_gb = psutil.virtual_memory().available / (1024**3)

            if ram_gb < 16:
                # RAM limitÃ©e: rÃ©duire workers
                optimal = min(optimal, 4)
            elif ram_gb < 32:
                optimal = min(optimal, 8)

        return max(optimal, 2)  # Minimum 2 workers

    def _adjust_workers_dynamically(
        self, current_batch: int, total_batches: int
    ) -> int:
        """
        Ajuste le nombre de workers en temps rÃ©el selon:
        - Utilisation GPU actuelle
        - Utilisation RAM
        - Performance observÃ©e
        """
        # Ajustement toutes les 5 batchs minimum
        if current_batch - self._last_worker_adjustment < 5:
            return self.max_workers

        if not PSUTIL_AVAILABLE:
            return self.max_workers

        # Monitoring ressources
        ram_used_pct = psutil.virtual_memory().percent

        current_workers = self.max_workers

        if self.gpu_manager and self.use_multigpu:
            try:
                # RÃ©cupÃ©rer stats GPU
                gpu_stats = self.gpu_manager.get_device_stats()

                # Moyenne utilisation GPU
                gpu_usage_values = [
                    stats.get("memory_used_pct", 0)
                    for stats in gpu_stats.values()
                    if stats.get("device_id", -1) != -1
                ]

                if gpu_usage_values:
                    gpu_usage_avg = np.mean(gpu_usage_values)

                    # Ajustement adaptatif
                    if gpu_usage_avg < 50 and ram_used_pct < 70:
                        # GPU sous-utilisÃ© et RAM OK: augmenter workers
                        new_workers = min(current_workers + 2, 16)
                        self.logger.info(
                            f"â†‘ Augmentation workers: {current_workers} â†’ {new_workers} (GPU: {gpu_usage_avg:.0f}%, RAM: {ram_used_pct:.0f}%)"
                        )
                        self._last_worker_adjustment = current_batch
                        return new_workers

                    elif gpu_usage_avg > 85 or ram_used_pct > 85:
                        # Saturation: rÃ©duire workers
                        new_workers = max(current_workers - 2, 2)
                        self.logger.warning(
                            f"â†“ RÃ©duction workers: {current_workers} â†’ {new_workers} (GPU: {gpu_usage_avg:.0f}%, RAM: {ram_used_pct:.0f}%)"
                        )
                        self._last_worker_adjustment = current_batch
                        return new_workers
            except Exception as e:
                self.logger.debug(f"Erreur ajustement workers: {e}")

        return current_workers

    def run_grid(
        self,
        grid_spec: ScenarioSpec,
        real_data: pd.DataFrame,
        symbol: str,
        timeframe: str,
        strategy_name: str = "Bollinger_Breakout",
        *,
        reuse_cache: bool = True,
    ) -> pd.DataFrame:
        """
        ExÃ©cute un sweep de grille paramÃ©trique avec vraies donnÃ©es.

        Args:
            grid_spec: SpÃ©cification de la grille
            real_data: DataFrame OHLCV avec vraies donnÃ©es de marchÃ©
            symbol: Symbole tradÃ© (ex: "BTC", "ETH")
            timeframe: Timeframe des donnÃ©es (ex: "1h", "15m")
            strategy_name: Nom de la stratÃ©gie Ã  utiliser
            reuse_cache: RÃ©utilise le cache IndicatorBank

        Returns:
            DataFrame des rÃ©sultats classÃ©s

        Raises:
            ValueError: Si real_data est None ou vide
        """
        if real_data is None or real_data.empty:
            raise ValueError("DonnÃ©es OHLCV requises pour run_grid()")

        self.logger.info(
            f"DÃ©but sweep grille: {grid_spec} avec {len(real_data)} barres"
        )

        # GÃ©nÃ©ration des combinaisons
        with self._time_stage("scenario_generation"):
            # Extraire les params du ScenarioSpec (dict ou ScenarioSpec)
            params = (
                grid_spec["params"] if isinstance(grid_spec, dict) else grid_spec.params
            )
            combinations = generate_param_grid(params)

        self.total_scenarios = len(combinations)
        self.logger.info(f"Grille gÃ©nÃ©rÃ©e: {self.total_scenarios} combinaisons")

        # ExÃ©cution batch
        results_df = self._execute_combinations(
            combinations,
            real_data,
            symbol,
            timeframe,
            strategy_name,
            reuse_cache=reuse_cache,
        )

        return results_df

    def run_monte_carlo(
        self,
        mc_spec: ScenarioSpec,
        real_data: pd.DataFrame,
        symbol: str,
        timeframe: str,
        strategy_name: str = "Bollinger_Breakout",
        *,
        reuse_cache: bool = True,
    ) -> pd.DataFrame:
        """
        ExÃ©cute un sweep Monte Carlo avec vraies donnÃ©es.

        Args:
            mc_spec: SpÃ©cification Monte Carlo
            real_data: DataFrame OHLCV avec vraies donnÃ©es de marchÃ©
            symbol: Symbole tradÃ© (ex: "BTC", "ETH")
            timeframe: Timeframe des donnÃ©es (ex: "1h", "15m")
            strategy_name: Nom de la stratÃ©gie Ã  utiliser
            reuse_cache: RÃ©utilise le cache IndicatorBank

        Returns:
            DataFrame des rÃ©sultats avec pruning Pareto

        Raises:
            ValueError: Si real_data est None ou vide
        """
        if real_data is None or real_data.empty:
            raise ValueError("DonnÃ©es OHLCV requises pour run_monte_carlo()")

        self.logger.info(
            f"DÃ©but sweep Monte Carlo: {mc_spec} avec {len(real_data)} barres"
        )

        # GÃ©nÃ©ration des scÃ©narios
        with self._time_stage("scenario_generation"):
            # Extraire les params et autres attributs
            params = mc_spec["params"] if isinstance(mc_spec, dict) else mc_spec.params
            n_scenarios = (
                mc_spec.get("n_scenarios", 100)
                if isinstance(mc_spec, dict)
                else mc_spec.n_scenarios
            )
            seed = (
                mc_spec.get("seed", 42) if isinstance(mc_spec, dict) else mc_spec.seed
            )
            scenarios = generate_monte_carlo(params, n_scenarios, seed)

        self.total_scenarios = len(scenarios)
        self.logger.info(f"Monte Carlo gÃ©nÃ©rÃ©: {self.total_scenarios} scÃ©narios")

        # ExÃ©cution avec pruning adaptatif
        results_df = self._execute_combinations_with_pruning(
            scenarios,
            real_data,
            symbol,
            timeframe,
            strategy_name,
            reuse_cache=reuse_cache,
        )

        return results_df

    def _execute_combinations(
        self,
        combinations: List[Dict],
        real_data: pd.DataFrame,
        symbol: str,
        timeframe: str,
        strategy_name: str = "Bollinger_Breakout",
        *,
        reuse_cache: bool = True,
    ) -> pd.DataFrame:
        """ExÃ©cute les combinaisons en mode batch avec vraies donnÃ©es."""
        self.is_running = True
        self.start_time = time.time()

        results = []

        try:
            # Extraction des indicateurs uniques pour batch processing
            with self._time_stage("indicator_extraction"):
                unique_indicators = self._extract_unique_indicators(combinations)

            # Calcul batch des indicateurs via IndicatorBank
            with self._time_stage("batch_indicators"):
                computed_indicators = self._compute_batch_indicators(
                    unique_indicators,
                    real_data,
                    symbol,
                    timeframe,
                    reuse_cache=reuse_cache,
                )

            # Ã‰valuation PARALLÃˆLE des stratÃ©gies avec ThreadPoolExecutor
            with self._time_stage("strategy_evaluation"):
                completed_count = [0]  # Mutable counter pour tracking progress

                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = {}
                    batch_size = (
                        1000  # Soumettre par batch pour Ã©viter une queue gÃ©ante
                    )
                    stop_requested = False

                    # Soumettre les futures par BATCH, en vÃ©rifiant le stop entre chaque
                    self.logger.info(
                        f"DÃ©but soumission {len(combinations)} combos par batch de {batch_size}"
                    )
                    for batch_idx in range(0, len(combinations), batch_size):
                        # VÃ©rifier le stop AVANT de soumettre chaque batch
                        if self.should_pause or is_global_stop_requested():
                            stop_requested = True
                            self.logger.warning(
                                f"â¹ï¸ ArrÃªt dÃ©tectÃ© avant soumission batch {batch_idx // batch_size}"
                            )
                            break

                        # Soumettre le batch
                        batch_end = min(batch_idx + batch_size, len(combinations))
                        for i in range(batch_idx, batch_end):
                            combo = combinations[i]
                            future = executor.submit(
                                self._evaluate_single_combination,
                                combo,
                                computed_indicators,
                                real_data,
                                symbol,
                                timeframe,
                                strategy_name,
                            )
                            futures[future] = i

                        self.logger.debug(
                            f"Batch {batch_idx // batch_size}: {batch_end - batch_idx} futures soumises (total: {len(futures)})"
                        )

                    # Collecter les rÃ©sultats au fur et Ã  mesure
                    try:
                        for future in as_completed(futures):
                            # VÃ©rifier le stop rÃ©guliÃ¨rement
                            if self.should_pause or is_global_stop_requested():
                                if not stop_requested:
                                    self.logger.warning(
                                        f"â¹ï¸ ArrÃªt demandÃ© aprÃ¨s {completed_count[0]} combos terminÃ©s"
                                    )
                                    stop_requested = True

                                # Annuler les futures restantes en queue
                                cancelled_count = 0
                                for f in futures:
                                    if f.cancel():
                                        cancelled_count += 1

                                if cancelled_count > 0:
                                    self.logger.warning(
                                        f"â¹ï¸ {cancelled_count} futures annulÃ©es en queue"
                                    )
                                break

                            try:
                                result = future.result()
                                results.append(result)
                                completed_count[0] += 1

                                # Mise Ã  jour du compteur de progression (thread-safe via GIL)
                                self.current_scenario = completed_count[0]

                                # Log de progression tous les 100 combos
                                if completed_count[0] % 100 == 0:
                                    self._log_progress()

                            except Exception as e:
                                self.logger.error(f"Erreur exÃ©cution combo: {e}")
                                # Continuer avec les autres combos
                                completed_count[0] += 1
                                results.append(
                                    {
                                        "error": str(e),
                                        "pnl": 0.0,
                                        "pnl_pct": 0.0,
                                        "sharpe": 0.0,
                                        "max_drawdown": 0.0,
                                        "win_rate": 0.0,
                                        "total_trades": 0,
                                    }
                                )
                    finally:
                        # Cleanup: cancel any remaining futures
                        remaining = sum(1 for f in futures if not f.done())
                        if remaining > 0:
                            for f in futures:
                                f.cancel()
                            self.logger.info(
                                f"Cleanup: {remaining} futures restantes annulÃ©es"
                            )

            # Construction du DataFrame final
            with self._time_stage("results_compilation"):
                results_df = pd.DataFrame(results)

        finally:
            self.is_running = False
            clear_global_stop()
            self._log_final_stats()

        return results_df

    def _execute_combinations_with_pruning(
        self,
        combinations: List[Dict],
        real_data: pd.DataFrame,
        symbol: str,
        timeframe: str,
        strategy_name: str = "Bollinger_Breakout",
        *,
        reuse_cache: bool = True,
    ) -> pd.DataFrame:
        """ExÃ©cute avec pruning Pareto adaptatif et vraies donnÃ©es."""
        self.is_running = True
        self.start_time = time.time()

        results = []
        pruning_metadata = None

        try:
            # Extraction et calcul batch des indicateurs
            with self._time_stage("indicator_extraction"):
                unique_indicators = self._extract_unique_indicators(combinations)

            with self._time_stage("batch_indicators"):
                computed_indicators = self._compute_batch_indicators(
                    unique_indicators,
                    real_data,
                    symbol,
                    timeframe,
                    reuse_cache=reuse_cache,
                )

            # Ã‰valuation avec pruning progressif
            with self._time_stage("strategy_evaluation_pruning"):
                batch_size = 50  # Taille de batch pour pruning

                for batch_start in range(0, len(combinations), batch_size):
                    if self.should_pause or is_global_stop_requested():
                        break

                    batch_end = min(batch_start + batch_size, len(combinations))
                    batch_combos = combinations[batch_start:batch_end]

                    # Ã‰valuation du batch
                    batch_results = []
                    for combo in batch_combos:
                        result = self._evaluate_single_combination(
                            combo,
                            computed_indicators,
                            real_data,
                            symbol,
                            timeframe,
                            strategy_name,
                        )
                        batch_results.append(result)

                    results.extend(batch_results)
                    self.current_scenario = len(results)

                    # Pruning pÃ©riodique
                    if len(results) >= 200:  # Seuil minimum pour pruning
                        current_df = pd.DataFrame(results)
                        pruned_df, pruning_metadata = pareto_soft_prune(
                            current_df, patience=200, quantile=0.85
                        )

                        if pruning_metadata["early_stop_triggered"]:
                            self.logger.info(
                                "Early stopping dÃ©clenchÃ© par pruning Pareto"
                            )
                            break

                    self._log_progress()

            # Construction finale
            with self._time_stage("results_compilation"):
                results_df = pd.DataFrame(results)

                # Pruning final si pas encore fait
                if len(results_df) > 100:
                    results_df, final_pruning = pareto_soft_prune(results_df)
                    if not pruning_metadata:
                        pruning_metadata = final_pruning

        finally:
            self.is_running = False
            clear_global_stop()
            self._log_final_stats(pruning_metadata)

        return results_df

    def _extract_unique_indicators(
        self, combinations: List[Dict]
    ) -> Dict[str, List[Dict]]:
        """Extrait les indicateurs uniques pour batch processing."""
        indicators_by_type = {}

        for combo in combinations:
            # Parsing des paramÃ¨tres par type d'indicateur
            for param_name, param_value in combo.items():
                if param_name.startswith("bb_"):
                    # Bollinger Bands
                    if "bollinger" not in indicators_by_type:
                        indicators_by_type["bollinger"] = []

                    # âœ… MAPPING: bb_windowâ†’period, bb_num_stdâ†’std
                    bb_params = {}
                    for name, value in combo.items():
                        if name == "bb_window":
                            bb_params["period"] = value
                        elif name == "bb_num_std":
                            bb_params["std"] = value

                    if bb_params and bb_params not in indicators_by_type["bollinger"]:
                        indicators_by_type["bollinger"].append(bb_params)

                elif param_name.startswith("atr_"):
                    # ATR
                    if "atr" not in indicators_by_type:
                        indicators_by_type["atr"] = []

                    # âœ… MAPPING: atr_windowâ†’period, atr_methodâ†’method (dÃ©faut: ema)
                    atr_params = {}
                    for name, value in combo.items():
                        if name == "atr_window":
                            atr_params["period"] = value
                        elif name == "atr_method":
                            atr_params["method"] = value
                        # atr_multiplier est pour la STRATÃ‰GIE, pas l'indicateur

                    # ATR par dÃ©faut utilise EMA
                    if "method" not in atr_params:
                        atr_params["method"] = "ema"

                    if atr_params and atr_params not in indicators_by_type["atr"]:
                        indicators_by_type["atr"].append(atr_params)

        return indicators_by_type

    def _compute_batch_indicators(
        self,
        unique_indicators: Dict[str, List[Dict]],
        real_data: pd.DataFrame,
        symbol: str,
        timeframe: str,
        *,
        reuse_cache: bool = True,
    ) -> Dict[str, Dict]:
        """Calcule les indicateurs en batch avec VRAIES donnÃ©es."""
        computed = {}

        # âœ… Validation donnÃ©es rÃ©elles
        if real_data is None or real_data.empty:
            raise ValueError("DonnÃ©es OHLCV requises pour le sweep")

        close_data = real_data["close"]
        self.logger.info(f"Calcul batch indicateurs avec {len(real_data)} barres")

        for indicator_type, params_list in unique_indicators.items():
            computed[indicator_type] = {}

            if reuse_cache:
                # âœ… Utilisation du batch_ensure avec vraies donnÃ©es
                batch_results = self.indicator_bank.batch_ensure(
                    indicator_type,
                    params_list,
                    close_data,
                    symbol=symbol,
                    timeframe=timeframe,
                )

                # Mapping des rÃ©sultats
                for params_key, result in batch_results.items():
                    computed[indicator_type][params_key] = result
            else:
                # Calcul direct sans cache
                for params in params_list:
                    params_key = self._params_to_key(params)
                    result = self.indicator_bank.ensure(
                        indicator_type,
                        params,
                        close_data,
                        symbol=symbol,
                        timeframe=timeframe,
                    )
                    computed[indicator_type][params_key] = result

        return computed

    def _evaluate_single_combination(
        self,
        combo: Dict,
        computed_indicators: Dict,
        real_data: pd.DataFrame,
        symbol: str,
        timeframe: str,
        strategy_name: str = "Bollinger_Breakout",
    ) -> Dict:
        """
        Ã‰valuation avec VRAI backtest de stratÃ©gie.

        Utilise les vraies stratÃ©gies implementÃ©es dans threadx.strategy
        au lieu de simuler les rÃ©sultats.
        """
        try:
            # âœ… Import des stratÃ©gies disponibles
            from threadx.strategy import BBAtrStrategy, BollingerDualStrategy

            # Mapping stratÃ©gie â†’ classe
            strategy_classes = {
                "Bollinger_Breakout": BBAtrStrategy,
                "Bollinger_Dual": BollingerDualStrategy,
            }

            # DÃ©terminer quelle stratÃ©gie utiliser
            # PrioritÃ©: param "strategy" dans combo, sinon stratÃ©gie par dÃ©faut
            strat_name = combo.get("strategy", strategy_name)

            StrategyClass = strategy_classes.get(strat_name, BBAtrStrategy)

            # ðŸš€ OPTIMISATION CRITIQUE: RÃ©utiliser instance existante si disponible
            # Ã‰vite de recrÃ©er GPU Manager, Bollinger, ATR, IndicatorBank pour chaque combo
            if not hasattr(self, "_cached_strategy_instances"):
                self._cached_strategy_instances = {}

            cache_key = (strat_name, symbol, timeframe)
            if cache_key not in self._cached_strategy_instances:
                self._cached_strategy_instances[cache_key] = StrategyClass(
                    symbol=symbol, timeframe=timeframe
                )

            strategy = self._cached_strategy_instances[cache_key]

            # âœ… MAPPING: Transformer paramÃ¨tres sweep â†’ paramÃ¨tres stratÃ©gie
            strategy_params = {}
            for key, value in combo.items():
                if key == "bb_window":
                    strategy_params["bb_period"] = value
                elif key == "bb_num_std":
                    strategy_params["bb_std"] = value
                elif key == "atr_window":
                    strategy_params["atr_period"] = value
                elif key == "atr_multiplier":
                    strategy_params["atr_multiplier"] = value
                else:
                    # Autres paramÃ¨tres passent tels quels
                    strategy_params[key] = value

            # ParamÃ¨tres par dÃ©faut requis
            if "entry_z" not in strategy_params:
                strategy_params["entry_z"] = 1.0  # Valeur par dÃ©faut

            # âœ… VRAI backtest avec vraies donnÃ©es + indicateurs prÃ©-calculÃ©s
            equity_curve, run_stats = strategy.backtest(
                df=real_data,
                params=strategy_params,  # Utiliser les paramÃ¨tres mappÃ©s
                initial_capital=10000.0,
                fee_bps=4.5,
                slippage_bps=0.0,
                precomputed_indicators=computed_indicators,  # ðŸš€ OPTIMISATION: RÃ©utiliser batch indicators
            )

            # Retourner mÃ©triques rÃ©elles
            result = combo.copy()
            result.update(
                {
                    "pnl": run_stats.total_pnl,
                    "pnl_pct": run_stats.total_pnl_pct,
                    "sharpe": (
                        run_stats.sharpe_ratio
                        if hasattr(run_stats, "sharpe_ratio")
                        else 0.0
                    ),
                    "max_drawdown": (
                        run_stats.max_drawdown
                        if hasattr(run_stats, "max_drawdown")
                        else 0.0
                    ),
                    "win_rate": (
                        run_stats.win_rate if hasattr(run_stats, "win_rate") else 0.0
                    ),
                    "total_trades": run_stats.total_trades,
                }
            )

            return result

        except Exception as e:
            # Fallback en cas d'erreur: retourner rÃ©sultats neutres
            self.logger.error(f"Erreur Ã©valuation combo {combo}: {e}")
            result = combo.copy()
            result.update(
                {
                    "pnl": 0.0,
                    "pnl_pct": 0.0,
                    "sharpe": 0.0,
                    "max_drawdown": 0.0,
                    "win_rate": 0.0,
                    "total_trades": 0,
                    "error": str(e),
                }
            )
            return result

    def _params_to_key(self, params: Dict) -> str:
        """Convertit des paramÃ¨tres en clÃ© de cache."""
        return json.dumps(params, sort_keys=True, separators=(",", ":"))

    def _time_stage(self, stage_name: str):
        """Context manager pour mesurer le temps par stage."""

        class StageTimer:
            def __init__(self, runner, name):
                self.runner = runner
                self.name = name
                self.start_time = None

            def __enter__(self):
                self.start_time = time.time()
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                duration = time.time() - self.start_time
                if self.name not in self.runner.stage_timings:
                    self.runner.stage_timings[self.name] = []
                self.runner.stage_timings[self.name].append(duration)

        return StageTimer(self, stage_name)

    def _log_progress(self):
        """Log du progrÃ¨s d'exÃ©cution."""
        if self.total_scenarios > 0:
            progress = self.current_scenario / self.total_scenarios
            elapsed = time.time() - self.start_time if self.start_time else 0
            eta = elapsed / progress * (1 - progress) if progress > 0 else 0

            self.logger.info(
                f"ProgrÃ¨s: {self.current_scenario}/{self.total_scenarios} "
                f"({progress:.1%}) - ETA: {eta:.1f}s"
            )

    def _log_final_stats(self, pruning_metadata: Optional[Dict] = None):
        """Log des statistiques finales."""
        total_time = time.time() - self.start_time if self.start_time else 0

        self.logger.info(
            f"Sweep terminÃ©: {self.current_scenario} scÃ©narios " f"en {total_time:.1f}s"
        )

        # Statistiques par stage
        for stage, timings in self.stage_timings.items():
            avg_time = np.mean(timings)
            total_stage_time = np.sum(timings)
            self.logger.info(
                f"  {stage}: {total_stage_time:.2f}s "
                f"(avg: {avg_time:.3f}s, {len(timings)} appels)"
            )

        # MÃ©tadonnÃ©es de pruning
        if pruning_metadata:
            self.logger.info(
                f"Pruning Pareto: {pruning_metadata['pruned_count']} configurations Ã©liminÃ©es "
                f"({pruning_metadata['pruning_rate']:.1%})"
            )


class UnifiedOptimizationEngine:
    """
    Moteur d'optimisation unifiÃ© utilisant IndicatorBank comme seul moteur de calcul.

    Centralise tous les calculs via IndicatorBank pour Ã©viter la duplication de code
    et garantir la cohÃ©rence entre UI, optimisation et backtesting.
    """

    def __init__(
        self, indicator_bank: Optional[IndicatorBank] = None, max_workers: int = 4
    ):
        """
        Initialise le moteur d'optimisation unifiÃ©.

        Args:
            indicator_bank: Instance IndicatorBank existante (recommandÃ©)
            max_workers: Nombre de workers pour le parallÃ©lisme
        """
        # Utilise l'IndicatorBank existant ou en crÃ©e un nouveau
        self.indicator_bank = indicator_bank or IndicatorBank()
        self.max_workers = max_workers
        self.logger = get_logger(__name__)

        # Ã‰tat d'exÃ©cution
        self.is_running = False
        self.should_pause = False
        self.progress_callback: Optional[Callable] = None

        # MÃ©triques
        self.total_combos = 0
        self.completed_combos = 0
        self.start_time: Optional[float] = None

        self.logger.info(
            "ðŸš€ UnifiedOptimizationEngine initialisÃ© avec IndicatorBank centralisÃ©"
        )

    def run_parameter_sweep(self, config: Dict, data: pd.DataFrame) -> pd.DataFrame:
        """
        Execute un sweep de paramÃ¨tres en utilisant uniquement IndicatorBank.

        Args:
            config: Configuration de sweep
            data: DonnÃ©es OHLCV source

        Returns:
            DataFrame des rÃ©sultats classÃ©s
        """
        self.is_running = True
        self.start_time = time.time()

        try:
            # 1. Expansion de la grille de paramÃ¨tres
            combinations = self._expand_parameter_grid(config.get("grid", {}))
            self.total_combos = len(combinations)
            self.completed_combos = 0

            self.logger.info(f"DÃ©marrage sweep: {self.total_combos} combinaisons")

            # 2. ExÃ©cution parallÃ¨le via IndicatorBank
            results = []

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {}
                batch_size = 1000
                stop_requested = False

                # Soumission des tÃ¢ches par BATCH
                self.logger.info(
                    f"Soumission {len(combinations)} combos par batch de {batch_size}"
                )
                for batch_idx in range(0, len(combinations), batch_size):
                    if self.should_pause or is_global_stop_requested():
                        stop_requested = True
                        self.logger.warning(
                            f"â¹ï¸ ArrÃªt avant batch {batch_idx // batch_size}"
                        )
                        break

                    batch_end = min(batch_idx + batch_size, len(combinations))
                    for i in range(batch_idx, batch_end):
                        combo = combinations[i]
                        future = executor.submit(
                            self._execute_single_combination, data, combo, config
                        )
                        futures[future] = i

                    self.logger.debug(
                        f"Batch: {batch_end - batch_idx} soumises (total: {len(futures)})"
                    )

                # Collecte des rÃ©sultats
                try:
                    for future in as_completed(futures):
                        if self.should_pause or is_global_stop_requested():
                            if not stop_requested:
                                self.logger.warning(
                                    f"â¹ï¸ ArrÃªt aprÃ¨s {self.completed_combos} combos"
                                )
                                stop_requested = True

                            # Annuler les futures en queue
                            cancelled = sum(1 for f in futures if f.cancel())
                            if cancelled > 0:
                                self.logger.warning(f"â¹ï¸ {cancelled} futures annulÃ©es")
                            break

                        try:
                            result = future.result()
                            results.append(result)
                            self.completed_combos += 1
                            self._update_progress()

                        except Exception as e:
                            self.logger.error(f"Erreur dans une combinaison: {e}")
                            self.completed_combos += 1
                            self._update_progress()
                finally:
                    # Cleanup
                    remaining = sum(1 for f in futures if not f.done())
                    if remaining > 0:
                        for f in futures:
                            f.cancel()
                        self.logger.info(f"Cleanup: {remaining} futures annulÃ©es")

            # 3. Classement des rÃ©sultats
            if results:
                df = pd.DataFrame(results)
                df = self._score_and_rank_results(df, config.get("scoring", {}))

                # Statistiques finales
                duration = time.time() - self.start_time
                rate = self.completed_combos / duration if duration > 0 else 0

                self.logger.info(
                    f"Sweep terminÃ©: {self.completed_combos}/{self.total_combos}"
                )
                self.logger.info(
                    f"Cache hits IndicatorBank: {self.indicator_bank.stats.get('cache_hits', 0)}"
                )
                self.logger.info(f"Vitesse: {rate:.1f} combinaisons/sec")

                return df
            else:
                self.logger.warning("Aucun rÃ©sultat obtenu")
                return pd.DataFrame()

        finally:
            self.is_running = False
            clear_global_stop()

    def _expand_parameter_grid(self, grid_config: Dict) -> List[Dict]:
        """Expanse la configuration de grille en combinaisons."""
        all_combinations = []

        for indicator_type, params_config in grid_config.items():
            if indicator_type not in ["bollinger", "atr"]:
                self.logger.warning(f"Type indicateur non supportÃ©: {indicator_type}")
                continue

            # GÃ©nÃ©ration des valeurs pour chaque paramÃ¨tre
            param_values = {}
            for param_name, param_def in params_config.items():
                if isinstance(param_def, dict) and "start" in param_def:
                    # Plage avec start/stop/step
                    values = self._generate_range(
                        param_def["start"], param_def["stop"], param_def["step"]
                    )
                elif isinstance(param_def, list):
                    # Liste de valeurs discrÃ¨tes
                    values = param_def
                else:
                    # Valeur unique
                    values = [param_def]

                param_values[param_name] = values

            # Produit cartÃ©sien pour cet indicateur
            param_names = list(param_values.keys())
            for combo in itertools.product(*param_values.values()):
                combination = {
                    "indicator_type": indicator_type,
                    "params": dict(zip(param_names, combo)),
                }
                all_combinations.append(combination)

        # DÃ©duplication
        unique_combinations = []
        seen = set()

        for combo in all_combinations:
            key = self._make_combination_key(combo)
            if key not in seen:
                seen.add(key)
                unique_combinations.append(combo)

        self.logger.info(
            f"Grille expansÃ©e: {len(all_combinations)} â†’ {len(unique_combinations)} uniques"
        )
        return unique_combinations

    def _generate_range(self, start: float, stop: float, step: float) -> List[float]:
        """GÃ©nÃ¨re une plage de valeurs avec gestion des flottants."""
        values = []
        current = start
        while current <= stop + 1e-10:  # TolÃ©rance pour les erreurs de flottants
            values.append(round(current, 6))
            current += step
        return values

    def _make_combination_key(self, combo: Dict) -> str:
        """CrÃ©e une clÃ© unique pour une combinaison."""
        key_data = {
            "type": combo["indicator_type"],
            "params": sorted(combo["params"].items()),
        }
        return hashlib.md5(str(key_data).encode()).hexdigest()

    def _execute_single_combination(
        self, data: pd.DataFrame, combo: Dict, config: Dict
    ) -> Dict:
        """
        ExÃ©cute une combinaison de paramÃ¨tres via IndicatorBank.

        Toute la logique de calcul passe par IndicatorBank pour centralisation.
        """
        start_time = time.time()

        try:
            # 1. Calcul de l'indicateur via IndicatorBank
            indicator_result = self.indicator_bank.ensure(
                indicator_type=combo["indicator_type"],
                params=combo["params"],
                data=data,
                symbol=config.get("dataset", {}).get("symbol", ""),
                timeframe=config.get("dataset", {}).get("timeframe", ""),
            )

            # 2. GÃ©nÃ©ration de signaux basiques (mock pour dÃ©monstration)
            signals = self._generate_signals_from_indicator(
                data, indicator_result, combo
            )

            # 3. Calcul des mÃ©triques de performance
            metrics = self._calculate_performance_metrics(data, signals)

            # 4. Construction du rÃ©sultat
            result = {
                "indicator_type": combo["indicator_type"],
                **combo["params"],
                "pnl": metrics["total_pnl"],
                "sharpe": metrics["sharpe_ratio"],
                "max_drawdown": abs(metrics["max_drawdown"]),
                "profit_factor": metrics["profit_factor"],
                "total_trades": metrics["total_trades"],
                "win_rate": metrics["win_rate"],
                "duration_sec": time.time() - start_time,
            }

            return result

        except Exception as e:
            self.logger.error(f"Erreur combinaison {combo}: {e}")
            # RÃ©sultat par dÃ©faut pour Ã©viter de faire planter le sweep
            return {
                "indicator_type": combo["indicator_type"],
                **combo["params"],
                "pnl": 0.0,
                "sharpe": 0.0,
                "max_drawdown": 1.0,
                "profit_factor": 1.0,
                "total_trades": 0,
                "win_rate": 0.0,
                "duration_sec": time.time() - start_time,
                "error": str(e),
            }

    def _generate_signals_from_indicator(
        self, data: pd.DataFrame, indicator_result: Any, combo: Dict
    ) -> pd.Series:
        """GÃ©nÃ¨re des signaux de trading Ã  partir des rÃ©sultats d'indicateurs."""
        # StratÃ©gie simple pour dÃ©monstration
        # En production, ceci serait dans un module strategy sÃ©parÃ©

        if combo["indicator_type"] == "bollinger":
            # Signaux Bollinger: achat sur bande basse, vente sur bande haute
            if isinstance(indicator_result, tuple) and len(indicator_result) >= 3:
                upper, middle, lower = indicator_result[:3]
                price = data["close"]

                # GÃ©nÃ©ration de signaux basiques
                signals = pd.Series(0, index=data.index)
                signals[price <= lower] = 1  # Long signal
                signals[price >= upper] = -1  # Short signal

                return signals

        elif combo["indicator_type"] == "atr":
            # Signaux ATR: volatility breakout
            if isinstance(indicator_result, np.ndarray):
                atr_values = pd.Series(indicator_result, index=data.index)
                price = data["close"]

                # Signaux basÃ©s sur les breakouts ATR
                signals = pd.Series(0, index=data.index)
                price_change = price.pct_change()
                atr_threshold = atr_values / price  # ATR en pourcentage

                signals[price_change > atr_threshold] = 1  # Long breakout
                signals[price_change < -atr_threshold] = -1  # Short breakout

                return signals

        # Signaux par dÃ©faut (random pour test)
        np.random.seed(42)
        return pd.Series(
            np.random.choice([0, 1, -1], size=len(data), p=[0.8, 0.1, 0.1]),
            index=data.index,
        )

    def _calculate_performance_metrics(
        self, data: pd.DataFrame, signals: pd.Series
    ) -> Dict:
        """Calcule les mÃ©triques de performance Ã  partir des signaux."""
        # Calcul simple des returns basÃ© sur les signaux
        price_returns = data["close"].pct_change()
        strategy_returns = (
            signals.shift(1) * price_returns
        )  # DÃ©calage pour Ã©viter look-ahead

        # Nettoyage
        strategy_returns = strategy_returns.dropna()

        if len(strategy_returns) == 0:
            return self._empty_metrics()

        # MÃ©triques de base
        cumulative_returns = (1 + strategy_returns).cumprod()
        total_return = (
            cumulative_returns.iloc[-1] - 1.0 if len(cumulative_returns) > 0 else 0.0
        )
        total_return = float(total_return)

        if len(strategy_returns) > 1:
            volatility = strategy_returns.std() * np.sqrt(252)
            sharpe = (
                (strategy_returns.mean() / strategy_returns.std() * np.sqrt(252))
                if strategy_returns.std() > 0
                else 0
            )
        else:
            volatility = 0
            sharpe = 0

        # Drawdown
        cumulative = (1 + strategy_returns).cumprod()
        peak = cumulative.expanding().max()
        drawdown = (cumulative - peak) / peak
        max_drawdown = drawdown.min()

        # Trades (approximation basÃ©e sur les changements de signal)
        signal_changes = signals.diff().abs()
        total_trades = signal_changes[signal_changes > 0].count()

        # Win rate (approximation)
        positive_returns = strategy_returns[strategy_returns > 0]
        win_rate = (
            len(positive_returns) / len(strategy_returns)
            if len(strategy_returns) > 0
            else 0
        )

        # Profit factor
        wins = strategy_returns[strategy_returns > 0]
        losses = strategy_returns[strategy_returns < 0]
        profit_factor = (
            abs(wins.sum() / losses.sum())
            if len(losses) > 0 and losses.sum() != 0
            else 1.0
        )

        return {
            "total_pnl": total_return * 10000,  # En dollars sur base 10k
            "sharpe_ratio": sharpe,
            "max_drawdown": max_drawdown,
            "profit_factor": profit_factor,
            "total_trades": int(total_trades),
            "win_rate": win_rate,
            "volatility": volatility,
        }

    def _empty_metrics(self) -> Dict:
        """MÃ©triques par dÃ©faut quand aucune donnÃ©e."""
        return {
            "total_pnl": 0.0,
            "sharpe_ratio": 0.0,
            "max_drawdown": 0.0,
            "profit_factor": 1.0,
            "total_trades": 0,
            "win_rate": 0.0,
            "volatility": 0.0,
        }

    def _score_and_rank_results(
        self, df: pd.DataFrame, scoring_config: Dict
    ) -> pd.DataFrame:
        """Score et classe les rÃ©sultats selon les critÃ¨res."""
        if df.empty:
            return df

        # Tri par dÃ©faut: PnL descendant, puis Sharpe descendant, puis MaxDD ascendant
        primary = scoring_config.get("primary", "pnl")
        secondary = scoring_config.get("secondary", ["sharpe", "-max_drawdown"])

        sort_columns = [primary] + secondary
        ascending_flags = []

        for col in sort_columns:
            if col.startswith("-"):
                ascending_flags.append(True)  # Tri ascendant (pour MaxDD)
                col = col[1:]
            else:
                ascending_flags.append(False)  # Tri descendant (pour PnL, Sharpe)

        # Nettoyage des noms de colonnes
        clean_columns = [col.lstrip("-") for col in sort_columns]
        available_columns = [col for col in clean_columns if col in df.columns]

        if available_columns:
            corresponding_flags = ascending_flags[: len(available_columns)]
            df = df.sort_values(by=available_columns, ascending=corresponding_flags)

        # Ajout du rang
        df = df.reset_index(drop=True)
        df["rank"] = range(1, len(df) + 1)

        # Top-K si spÃ©cifiÃ©
        top_k = scoring_config.get("top_k")
        if top_k and len(df) > top_k:
            df = df.head(top_k)

        return df

    def _update_progress(self):
        """Met Ã  jour la progression."""
        if self.progress_callback and self.total_combos > 0:
            progress = self.completed_combos / self.total_combos
            eta = None

            if self.start_time and self.completed_combos > 0:
                elapsed = time.time() - self.start_time
                rate = self.completed_combos / elapsed
                remaining = self.total_combos - self.completed_combos
                eta = remaining / rate if rate > 0 else None

            self.progress_callback(
                progress, self.completed_combos, self.total_combos, eta
            )

    # MÃ©thodes de contrÃ´le
    def pause(self):
        """Met en pause l'optimisation."""
        self.should_pause = True

    def resume(self):
        """Reprend l'optimisation."""
        self.should_pause = False

    def stop(self):
        """ArrÃªte l'optimisation."""
        self.should_pause = True
        self.is_running = False

    # AccÃ¨s aux statistiques du moteur principal
    def get_indicator_bank_stats(self) -> Dict:
        """Retourne les statistiques de l'IndicatorBank."""
        return self.indicator_bank.stats.copy()

    def cleanup_cache(self) -> int:
        """Nettoie le cache de l'IndicatorBank."""
        return self.indicator_bank.cache_manager.cleanup_expired()


def create_unified_engine(
    indicator_bank: Optional[IndicatorBank] = None,
) -> UnifiedOptimizationEngine:
    """
    Factory function pour crÃ©er un moteur d'optimisation unifiÃ©.

    Args:
        indicator_bank: Instance IndicatorBank existante (recommandÃ© pour partage de cache)

    Returns:
        UnifiedOptimizationEngine configurÃ©
    """
    return UnifiedOptimizationEngine(indicator_bank=indicator_bank)


# Configuration d'exemple pour les sweeps
DEFAULT_SWEEP_CONFIG = {
    "dataset": {
        "symbol": "BTCUSDC",
        "timeframe": "1h",
        "start": "2024-01-01",
        "end": "2024-12-31",
    },
    "grid": {
        "bollinger": {
            "period": [10, 15, 20, 25, 30],
            "std": {"start": 1.5, "stop": 3.0, "step": 0.1},
        },
        "atr": {
            "period": {"start": 10, "stop": 30, "step": 2},
            "method": ["ema", "sma"],
        },
    },
    "scoring": {
        "primary": "pnl",
        "secondary": ["sharpe", "-max_drawdown"],
        "top_k": 50,
    },
}

----------------------------------------
Fichier: optimization\pruning.py
"""
ThreadX Pareto Pruning - Early Stop Optimization
===============================================

Algorithme de pruning Pareto soft pour optimisation paramÃ©trique :
- Maintient une frontiÃ¨re Pareto approximative
- Early stopping basÃ© sur patience et quantiles
- MÃ©triques multiples avec seuils adaptatifs

Author: ThreadX Framework
Version: Phase 10 - Pareto Pruning
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
import time

from threadx.utils.log import get_logger

logger = get_logger(__name__)


def pareto_soft_prune(
    df: pd.DataFrame,
    metrics: Tuple[str, ...] = ("pnl", "max_drawdown", "sharpe"),
    patience: int = 200,
    quantile: float = 0.85,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Pruning Pareto soft avec early stopping adaptatif.

    Maintient une frontiÃ¨re Pareto approximative et stoppe l'optimisation
    si aucune configuration n'entre dans le top quantile sur â‰¥ 2 mÃ©triques
    pendant les 'patience' derniers essais.

    Args:
        df: DataFrame des rÃ©sultats avec colonnes de mÃ©triques
        metrics: Tuple des noms de mÃ©triques Ã  considÃ©rer
        patience: Nombre d'essais sans amÃ©lioration avant arrÃªt
        quantile: Seuil de quantile pour considÃ©ration (0.85 = top 15%)

    Returns:
        Tuple (df_kept, metadata) oÃ¹:
        - df_kept: DataFrame des configurations conservÃ©es
        - metadata: Dict avec statistiques de pruning

    Example:
        >>> results_df = pd.DataFrame({
        ...     'pnl': [100, 150, 80, 200, 120],
        ...     'max_drawdown': [0.1, 0.15, 0.08, 0.12, 0.09],
        ...     'sharpe': [1.2, 1.8, 0.9, 2.1, 1.4]
        ... })
        >>> kept_df, meta = pareto_soft_prune(results_df, patience=3)
    """
    start_time = time.time()

    if df.empty:
        return df, _empty_prune_metadata()

    logger.info(
        f"Pruning Pareto soft: {len(df)} configurations, "
        f"mÃ©triques={metrics}, patience={patience}, quantile={quantile}"
    )

    # Validation des mÃ©triques
    missing_metrics = [m for m in metrics if m not in df.columns]
    if missing_metrics:
        logger.warning(f"MÃ©triques manquantes: {missing_metrics}")
        available_metrics = tuple(m for m in metrics if m in df.columns)
        if not available_metrics:
            return df, _empty_prune_metadata()
        metrics = available_metrics

    # Initialisation
    n_original = len(df)
    df_work = df.copy().reset_index(drop=True)

    # Calcul des seuils de quantiles pour chaque mÃ©trique
    quantile_thresholds = {}
    for metric in metrics:
        # DÃ©termination du sens d'optimisation
        is_maximize = _is_maximize_metric(metric)
        threshold = df_work[metric].quantile(quantile if is_maximize else 1 - quantile)
        quantile_thresholds[metric] = threshold

        logger.debug(
            f"MÃ©trique {metric}: seuil {quantile:.2%} = {threshold:.4f} "
            f"({'max' if is_maximize else 'min'})"
        )

    # Algorithme de pruning itÃ©ratif
    pareto_front: List[Tuple[int, pd.Series]] = []  # noqa: SPELL
    pruned_count = 0
    stagnation_counter = 0
    last_improvement_iter = 0

    for i, row in df_work.iterrows():
        # Ã‰valuation Pareto
        is_pareto_candidate = _evaluate_pareto_candidate(
            row, pareto_front, metrics, quantile_thresholds
        )

        if is_pareto_candidate:
            # Ajout Ã  la frontiÃ¨re
            pareto_front.append((int(i), row))
            stagnation_counter = 0
            last_improvement_iter = int(i)

            # Nettoyage de la frontiÃ¨re (suppression des dominÃ©s)
            pareto_front = _clean_pareto_front(pareto_front, metrics)

        else:
            # Configuration non-Pareto
            pruned_count += 1
            stagnation_counter += 1

            # VÃ©rification early stopping
            if stagnation_counter >= patience:
                logger.info(
                    f"Early stopping activÃ© Ã  l'itÃ©ration {i} "
                    f"(stagnation: {stagnation_counter}/{patience})"
                )
                break

    # Construction du DataFrame final
    if pareto_front:
        kept_indices = [idx for idx, _ in pareto_front]
        df_kept = df_work.iloc[kept_indices].copy()
    else:
        # Fallback : garder les meilleures selon mÃ©trique principale
        primary_metric = metrics[0]
        is_maximize = _is_maximize_metric(primary_metric)
        df_kept = (
            df_work.nlargest(1, primary_metric)
            if is_maximize
            else df_work.nsmallest(1, primary_metric)
        )

    # MÃ©tadonnÃ©es
    execution_time = time.time() - start_time
    time_saved_estimate = (n_original - len(df_kept)) * 0.1  # Estimation basique

    metadata = {
        "original_count": n_original,
        "kept_count": len(df_kept),
        "pruned_count": pruned_count,
        "pruning_rate": pruned_count / n_original if n_original > 0 else 0.0,
        "early_stop_triggered": stagnation_counter >= patience,
        "stagnation_counter": stagnation_counter,
        "last_improvement_iter": last_improvement_iter,
        "quantile_thresholds": quantile_thresholds,
        "pareto_front_size": len(pareto_front),
        "execution_time": execution_time,
        "estimated_time_saved": time_saved_estimate,
        "metrics_used": metrics,
        "patience": patience,
        "quantile": quantile,
    }

    logger.info(
        f"Pruning terminÃ©: {len(df_kept)}/{n_original} configurations conservÃ©es "
        f"({metadata['pruning_rate']:.1%} pruned), "
        f"temps Ã©pargnÃ© estimÃ©: {time_saved_estimate:.1f}s"
    )

    return df_kept, metadata


def _is_maximize_metric(metric: str) -> bool:
    """DÃ©termine si une mÃ©trique doit Ãªtre maximisÃ©e."""
    maximize_metrics = {
        "pnl",
        "profit",
        "return",
        "returns",
        "sharpe",
        "sharpe_ratio",
        "sortino",
        "sortino_ratio",
        "calmar",
        "win_rate",
        "success_rate",
    }

    minimize_metrics = {
        "drawdown",
        "max_drawdown",
        "max_dd",
        "volatility",
        "var",
        "cvar",
        "loss",
        "risk",
        "downside_deviation",
    }

    metric_lower = metric.lower()

    if metric_lower in maximize_metrics:
        return True
    elif metric_lower in minimize_metrics:
        return False
    else:
        # Heuristique : si contient "drawdown", "loss", "risk" -> minimize
        minimize_keywords = ["drawdown", "loss", "risk", "error", "deviation"]
        for keyword in minimize_keywords:
            if keyword in metric_lower:
                return False

        # Par dÃ©faut : maximiser
        return True


def _evaluate_pareto_candidate(
    candidate: pd.Series,
    pareto_front: List[Tuple[int, pd.Series]],
    metrics: Tuple[str, ...],
    quantile_thresholds: Dict[str, float],
) -> bool:
    """
    Ã‰value si un candidat doit Ãªtre ajoutÃ© Ã  la frontiÃ¨re Pareto.

    CritÃ¨res :
    1. Non dominÃ© par la frontiÃ¨re existante
    2. Atteint le seuil de quantile sur au moins 2 mÃ©triques
    """
    # CritÃ¨re 1 : VÃ©rification des seuils de quantile
    metrics_above_threshold = 0

    for metric in metrics:
        threshold = quantile_thresholds[metric]
        is_maximize = _is_maximize_metric(metric)

        if is_maximize and candidate[metric] >= threshold:
            metrics_above_threshold += 1
        elif not is_maximize and candidate[metric] <= threshold:
            metrics_above_threshold += 1

    if metrics_above_threshold < 2:
        return False

    # CritÃ¨re 2 : Non-domination par la frontiÃ¨re existante
    for _, front_member in pareto_front:
        if _dominates(front_member, candidate, metrics):
            return False

    return True


def _dominates(
    solution_a: pd.Series, solution_b: pd.Series, metrics: Tuple[str, ...]
) -> bool:
    """
    VÃ©rifie si solution_a domine solution_b selon les mÃ©triques Pareto.

    Domination : A domine B si A est au moins aussi bon que B sur toutes
    les mÃ©triques ET strictement meilleur sur au moins une mÃ©trique.
    """
    at_least_as_good = True
    strictly_better = False

    for metric in metrics:
        is_maximize = _is_maximize_metric(metric)

        value_a = solution_a[metric]
        value_b = solution_b[metric]

        if is_maximize:
            if value_a < value_b:
                at_least_as_good = False
                break
            elif value_a > value_b:
                strictly_better = True
        else:
            if value_a > value_b:
                at_least_as_good = False
                break
            elif value_a < value_b:
                strictly_better = True

    return at_least_as_good and strictly_better


def _clean_pareto_front(
    pareto_front: List[Tuple[int, pd.Series]], metrics: Tuple[str, ...]
) -> List[Tuple[int, pd.Series]]:
    """
    Nettoie la frontiÃ¨re Pareto en supprimant les solutions dominÃ©es.
    """
    if len(pareto_front) <= 1:
        return pareto_front

    cleaned_front = []

    for i, (idx_a, solution_a) in enumerate(pareto_front):
        is_dominated = False

        for j, (idx_b, solution_b) in enumerate(pareto_front):
            if i != j and _dominates(solution_b, solution_a, metrics):
                is_dominated = True
                break

        if not is_dominated:
            cleaned_front.append((idx_a, solution_a))

    return cleaned_front


def _empty_prune_metadata() -> Dict[str, Any]:
    """Retourne des mÃ©tadonnÃ©es vides pour cas d'erreur."""
    return {
        "original_count": 0,
        "kept_count": 0,
        "pruned_count": 0,
        "pruning_rate": 0.0,
        "early_stop_triggered": False,
        "stagnation_counter": 0,
        "last_improvement_iter": 0,
        "quantile_thresholds": {},
        "pareto_front_size": 0,
        "execution_time": 0.0,
        "estimated_time_saved": 0.0,
        "metrics_used": (),
        "patience": 0,
        "quantile": 0.0,
    }


# === Utilitaires additionnels ===


def analyze_pareto_front(df: pd.DataFrame, metrics: Tuple[str, ...]) -> Dict[str, Any]:
    """
    Analyse la frontiÃ¨re Pareto d'un ensemble de rÃ©sultats.

    Args:
        df: DataFrame des rÃ©sultats
        metrics: MÃ©triques Ã  analyser

    Returns:
        Dict avec statistiques de la frontiÃ¨re Pareto
    """
    if df.empty:
        return {"pareto_solutions": [], "front_size": 0, "coverage": 0.0}

    # Identification des solutions Pareto
    pareto_solutions = []

    for i, row in df.iterrows():
        is_pareto = True

        for j, other_row in df.iterrows():
            if i != j and _dominates(other_row, row, metrics):
                is_pareto = False
                break

        if is_pareto:
            pareto_solutions.append(i)

    coverage = len(pareto_solutions) / len(df) if len(df) > 0 else 0.0

    return {
        "pareto_solutions": pareto_solutions,
        "front_size": len(pareto_solutions),
        "coverage": coverage,
        "total_solutions": len(df),
    }


if __name__ == "__main__":
    # Test rapide
    test_df = pd.DataFrame(
        {
            "pnl": [100, 150, 80, 200, 120, 90, 160],
            "max_drawdown": [0.1, 0.15, 0.08, 0.12, 0.09, 0.20, 0.11],
            "sharpe": [1.2, 1.8, 0.9, 2.1, 1.4, 0.8, 1.6],
        }
    )

    kept_df, metadata = pareto_soft_prune(test_df, patience=3, quantile=0.7)
    print(f"Test pruning: {len(kept_df)}/{len(test_df)} configurations conservÃ©es")
    print(
        f"MÃ©tadonnÃ©es: {metadata['pruning_rate']:.1%} pruned, "
        f"early_stop={metadata['early_stop_triggered']}"
    )




----------------------------------------
Fichier: optimization\reporting.py
"""
ThreadX Optimization Reporting - Distribution Analysis & Heatmaps
===============================================================

GÃ©nÃ©ration de rapports quantitatifs pour sweeps paramÃ©triques :
- Analyse des distributions (mean/std/percentiles)
- Heatmaps 2D pour visualisation des paramÃ¨tres
- Export CSV/Parquet avec manifests JSON
- MÃ©triques de performance consolidÃ©es

Author: ThreadX Framework
Version: Phase 10 - Optimization Reporting
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import time
from datetime import datetime

from threadx.utils.log import get_logger

logger = get_logger(__name__)


def summarize_distribution(results_df: pd.DataFrame) -> Dict[str, Any]:
    """
    Analyse des distributions de mÃ©triques de performance.

    Args:
        results_df: DataFrame des rÃ©sultats avec mÃ©triques

    Returns:
        Dict avec statistiques descriptives par mÃ©trique

    Example:
        >>> results = pd.DataFrame({
        ...     'pnl': [100, 150, 80, 200, 120],
        ...     'sharpe': [1.2, 1.8, 0.9, 2.1, 1.4],
        ...     'max_drawdown': [0.1, 0.15, 0.08, 0.12, 0.09]
        ... })
        >>> stats = summarize_distribution(results)
        >>> stats['pnl']['mean']  # 130.0
    """
    if results_df.empty:
        return {}

    logger.info(f"Analyse des distributions: {len(results_df)} rÃ©sultats")

    summary = {}

    # Identification des colonnes numÃ©riques (mÃ©triques)
    numeric_cols = results_df.select_dtypes(include=[np.number]).columns

    for col in numeric_cols:
        series = results_df[col].dropna()

        if len(series) == 0:
            continue

        # Statistiques de base
        stats = {
            "count": len(series),
            "mean": float(series.mean()),
            "std": float(series.std()),
            "min": float(series.min()),
            "max": float(series.max()),
            "median": float(series.median()),
            # Percentiles
            "p5": float(series.quantile(0.05)),
            "p25": float(series.quantile(0.25)),
            "p75": float(series.quantile(0.75)),
            "p95": float(series.quantile(0.95)),
            # MÃ©triques de forme
            "skewness": float(series.skew()) if len(series) > 2 else 0.0,
            "kurtosis": float(series.kurtosis()) if len(series) > 3 else 0.0,
            # Dispersion
            "var": float(series.var()),
            "cv": (
                float(series.std() / series.mean())
                if series.mean() != 0
                else float("inf")
            ),
            "iqr": float(series.quantile(0.75) - series.quantile(0.25)),
            # Ratios de performance (si applicable)
            "success_rate": (
                float((series > 0).mean())
                if "pnl" in col.lower() or "return" in col.lower()
                else None
            ),
            "negative_rate": (
                float((series < 0).mean())
                if "pnl" in col.lower() or "return" in col.lower()
                else None
            ),
        }

        # Calculs spÃ©cialisÃ©s selon le type de mÃ©trique
        if "sharpe" in col.lower():
            stats["above_1"] = float((series > 1.0).mean())
            stats["above_2"] = float((series > 2.0).mean())
        elif "sortino" in col.lower():
            stats["above_1"] = float((series > 1.0).mean())
            stats["above_1_5"] = float((series > 1.5).mean())
        elif "drawdown" in col.lower():
            stats["below_5pct"] = float((series < 0.05).mean())
            stats["below_10pct"] = float((series < 0.10).mean())
            stats["above_20pct"] = float((series > 0.20).mean())

        summary[col] = stats

    # Statistiques globales
    if summary:
        summary["_meta"] = {
            "total_configurations": len(results_df),
            "total_metrics": len(summary) - 1,  # -1 pour _meta
            "analysis_timestamp": datetime.now().isoformat(),
            "missing_data_pct": float(
                results_df.isnull().sum().sum()
                / (len(results_df) * len(results_df.columns))
            ),
        }

    logger.info(f"Distributions analysÃ©es: {len(summary) - 1} mÃ©triques")

    return summary


def build_heatmaps(results_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """
    Construction de heatmaps 2D pour analyse des paramÃ¨tres.

    Args:
        results_df: DataFrame avec paramÃ¨tres et mÃ©triques

    Returns:
        Dict[metric_name, DataFrame] des heatmaps par mÃ©trique

    Example:
        >>> results = pd.DataFrame({
        ...     'bb_period': [10, 20, 10, 20, 30],
        ...     'bb_std': [1.5, 1.5, 2.0, 2.0, 1.5],
        ...     'pnl': [100, 150, 80, 200, 120]
        ... })
        >>> heatmaps = build_heatmaps(results)
        >>> 'pnl' in heatmaps
    """
    if results_df.empty:
        return {}

    logger.info(f"Construction des heatmaps: {len(results_df)} rÃ©sultats")

    # Identification des colonnes de paramÃ¨tres et mÃ©triques
    param_cols = _identify_parameter_columns(results_df)
    metric_cols = _identify_metric_columns(results_df)

    if len(param_cols) < 2:
        logger.warning(
            f"Nombre insuffisant de paramÃ¨tres pour heatmaps: {len(param_cols)}"
        )
        return {}

    heatmaps = {}

    # GÃ©nÃ©ration des heatmaps pour chaque mÃ©trique
    for metric in metric_cols:
        metric_heatmaps = _build_metric_heatmaps(results_df, param_cols, metric)
        heatmaps.update(metric_heatmaps)

    logger.info(f"Heatmaps construites: {len(heatmaps)} combinaisons param/mÃ©trique")

    return heatmaps


def _identify_parameter_columns(df: pd.DataFrame) -> List[str]:
    """Identifie les colonnes de paramÃ¨tres."""
    param_indicators = [
        "period",
        "std",
        "window",
        "length",
        "threshold",
        "alpha",
        "beta",
        "gamma",
        "lambda",
        "factor",
        "multiplier",
        "ratio",
        "size",
        "step",
    ]

    param_cols = []

    for col in df.columns:
        col_lower = col.lower()

        # Colonnes numÃ©riques avec indicateurs de paramÃ¨tres
        if df[col].dtype in [np.int64, np.float64, int, float]:
            if any(indicator in col_lower for indicator in param_indicators):
                param_cols.append(col)
            # Ou colonnes avec prÃ©fixes typiques
            elif any(
                col_lower.startswith(prefix)
                for prefix in ["bb_", "atr_", "rsi_", "ma_", "ema_"]
            ):
                param_cols.append(col)

    return param_cols


def _identify_metric_columns(df: pd.DataFrame) -> List[str]:
    """Identifie les colonnes de mÃ©triques."""
    metric_indicators = [
        "pnl",
        "profit",
        "return",
        "sharpe",
        "sortino",
        "calmar",
        "drawdown",
        "volatility",
        "var",
        "win_rate",
        "success_rate",
    ]

    metric_cols = []

    for col in df.columns:
        col_lower = col.lower()

        if df[col].dtype in [np.int64, np.float64, int, float]:
            if any(indicator in col_lower for indicator in metric_indicators):
                metric_cols.append(col)

    return metric_cols


def _build_metric_heatmaps(
    df: pd.DataFrame, param_cols: List[str], metric: str
) -> Dict[str, pd.DataFrame]:
    """Construit les heatmaps pour une mÃ©trique donnÃ©e."""
    heatmaps = {}

    # Combinaisons de paramÃ¨tres 2 Ã  2
    for i in range(len(param_cols)):
        for j in range(i + 1, len(param_cols)):
            param_x = param_cols[i]
            param_y = param_cols[j]

            heatmap_key = f"{metric}_{param_x}_vs_{param_y}"

            try:
                # Pivot table pour crÃ©er la heatmap
                heatmap_df = df.pivot_table(
                    values=metric,
                    index=param_y,
                    columns=param_x,
                    aggfunc="mean",  # Moyenne si plusieurs valeurs par case
                    fill_value=np.nan,
                )

                # Tri des axes pour cohÃ©rence
                heatmap_df = heatmap_df.sort_index().sort_index(axis=1)

                heatmaps[heatmap_key] = heatmap_df

            except Exception as e:
                logger.warning(f"Erreur construction heatmap {heatmap_key}: {e}")
                continue

    return heatmaps


def write_reports(
    results_df: pd.DataFrame,
    out_dir: str,
    *,
    seeds: Optional[List[int]] = None,
    devices: Optional[List[str]] = None,
    gpu_ratios: Optional[Dict[str, float]] = None,
    min_samples: Optional[int] = None,
) -> Dict[str, str]:
    """
    Ã‰crit les rapports d'optimisation sur disque.

    Args:
        results_df: DataFrame des rÃ©sultats
        out_dir: RÃ©pertoire de sortie
        seeds: Liste des seeds utilisÃ©s
        devices: Liste des devices utilisÃ©s
        gpu_ratios: Ratios GPU utilisÃ©s
        min_samples: Seuil minimum pour GPU

    Returns:
        Dict[file_type, file_path] des fichiers crÃ©Ã©s

    Example:
        >>> results = pd.DataFrame({'pnl': [100, 150], 'sharpe': [1.2, 1.8]})
        >>> files = write_reports(results, "artifacts/reports")
        >>> 'csv' in files and 'parquet' in files and 'manifest' in files
    """
    out_path = Path(out_dir)
    out_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    logger.info(f"Ã‰criture des rapports: {len(results_df)} rÃ©sultats â†’ {out_path}")

    created_files = {}

    # 1. Export CSV
    csv_path = out_path / f"optimization_results_{timestamp}.csv"
    results_df.to_csv(csv_path, index=False)
    created_files["csv"] = str(csv_path.resolve())

    # 2. Export Parquet
    parquet_path = out_path / f"optimization_results_{timestamp}.parquet"
    results_df.to_parquet(parquet_path, index=False)
    created_files["parquet"] = str(parquet_path.resolve())

    # 3. Analyse des distributions
    distribution_stats = summarize_distribution(results_df)
    stats_path = out_path / f"distribution_stats_{timestamp}.json"
    with stats_path.open("w") as f:
        json.dump(distribution_stats, f, indent=2, ensure_ascii=False)
    created_files["distribution_stats"] = str(stats_path.resolve())

    # 4. Heatmaps
    heatmaps = build_heatmaps(results_df)
    if heatmaps:
        heatmaps_dir = out_path / f"heatmaps_{timestamp}"
        heatmaps_dir.mkdir(exist_ok=True)

        for heatmap_name, heatmap_df in heatmaps.items():
            heatmap_path = heatmaps_dir / f"{heatmap_name}.csv"
            heatmap_df.to_csv(heatmap_path)

        created_files["heatmaps_dir"] = str(heatmaps_dir.resolve())
        created_files["heatmaps_count"] = len(heatmaps)

    # 5. Manifest JSON
    manifest = {
        "generation_info": {
            "timestamp": datetime.now().isoformat(),
            "total_results": len(results_df),
            "output_directory": str(out_path.resolve()),
        },
        "execution_context": {
            "seeds": seeds or [],
            "devices": devices or [],
            "gpu_ratios": gpu_ratios or {},
            "min_samples_threshold": min_samples,
        },
        "files_created": created_files,
        "data_summary": {
            "columns": list(results_df.columns),
            "numeric_columns": list(
                results_df.select_dtypes(include=[np.number]).columns
            ),
            "row_count": len(results_df),
            "missing_values": results_df.isnull().sum().to_dict(),
        },
        "performance_summary": _quick_performance_summary(results_df),
    }

    manifest_path = out_path / f"manifest_{timestamp}.json"
    with manifest_path.open("w") as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)
    created_files["manifest"] = str(manifest_path.resolve())

    logger.info(f"Rapports crÃ©Ã©s: {len(created_files)} fichiers")

    return created_files


def _quick_performance_summary(results_df: pd.DataFrame) -> Dict[str, Any]:
    """RÃ©sumÃ© rapide des performances pour le manifest."""
    if results_df.empty:
        return {}

    summary = {}

    # MÃ©triques clÃ©s si disponibles
    key_metrics = ["pnl", "sharpe", "max_drawdown", "win_rate", "profit", "return"]

    for metric in key_metrics:
        matching_cols = [
            col for col in results_df.columns if metric.lower() in col.lower()
        ]

        if matching_cols:
            col = matching_cols[0]  # Prendre la premiÃ¨re correspondance
            series = results_df[col].dropna()

            if len(series) > 0:
                summary[metric] = {
                    "best": float(
                        series.max() if metric not in ["max_drawdown"] else series.min()
                    ),
                    "worst": float(
                        series.min() if metric not in ["max_drawdown"] else series.max()
                    ),
                    "mean": float(series.mean()),
                    "median": float(series.median()),
                }

    return summary


# === Utilitaires de validation ===


def validate_results_dataframe(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Valide la structure du DataFrame de rÃ©sultats.

    Returns:
        Dict avec statut de validation et dÃ©tails
    """
    validation = {"is_valid": True, "issues": [], "warnings": [], "info": {}}

    # VÃ©rifications de base
    if df.empty:
        validation["is_valid"] = False
        validation["issues"].append("DataFrame vide")
        return validation

    # Colonnes numÃ©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) == 0:
        validation["is_valid"] = False
        validation["issues"].append("Aucune colonne numÃ©rique trouvÃ©e")

    # Valeurs manquantes
    missing_pct = df.isnull().sum().sum() / (len(df) * len(df.columns))
    if missing_pct > 0.5:
        validation["warnings"].append(
            f"Taux Ã©levÃ© de valeurs manquantes: {missing_pct:.1%}"
        )

    # Doublons
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        validation["warnings"].append(f"{duplicates} lignes dupliquÃ©es dÃ©tectÃ©es")

    # Informations
    validation["info"] = {
        "row_count": len(df),
        "column_count": len(df.columns),
        "numeric_columns": len(numeric_cols),
        "missing_percentage": missing_pct,
        "duplicate_rows": duplicates,
    }

    return validation


if __name__ == "__main__":
    # Test rapide
    test_df = pd.DataFrame(
        {
            "bb_period": [10, 20, 30, 10, 20],
            "bb_std": [1.5, 1.5, 1.5, 2.0, 2.0],
            "pnl": [100, 150, 120, 80, 200],
            "sharpe": [1.2, 1.8, 1.4, 0.9, 2.1],
            "max_drawdown": [0.1, 0.15, 0.12, 0.08, 0.13],
        }
    )

    # Test des fonctions
    stats = summarize_distribution(test_df)
    heatmaps = build_heatmaps(test_df)

    print(
        f"Test reporting: {len(stats)} mÃ©triques analysÃ©es, "
        f"{len(heatmaps)} heatmaps gÃ©nÃ©rÃ©es"
    )




----------------------------------------
Fichier: optimization\run.py
"""Command-line interface for running optimization sweeps."""

from __future__ import annotations

import argparse
import sys
import time
import warnings
from pathlib import Path
from typing import Any, Dict

from threadx.config import ConfigurationError, load_config_dict
from threadx.indicators.bank import IndicatorBank
from threadx.optimization.engine import SweepRunner
from threadx.optimization.reporting import write_reports
from threadx.optimization.scenarios import ScenarioSpec
from threadx.utils.determinism import set_global_seed
from threadx.utils.log import get_logger

logger = get_logger(__name__)


def load_config(config_path: str) -> Dict[str, Any]:
    """Deprecated compatibility wrapper around :func:`load_config_dict`."""
    warnings.warn(
        "load_config(config_path) est dÃ©prÃ©ciÃ©. Utilisez "
        "threadx.config.load_config_dict(config_path).",
        DeprecationWarning,
        stacklevel=2,
    )
    config = load_config_dict(config_path)
    logger.info("CFG_LOAD_OK path=%s keys=%d", config_path, len(config or {}))
    return config


def _ensure_mapping(value: Any, path: str, message: str) -> Dict[str, Any]:
    if value is None:
        return {}
    if not isinstance(value, dict):
        raise ConfigurationError(path, message)
    return value


def validate_cli_config(config: Dict[str, Any], config_path: str) -> Dict[str, Any]:
    if not isinstance(config, dict):
        raise ConfigurationError(config_path, "Configuration root must be a mapping")

    _ensure_mapping(
        config.get("dataset"), config_path, "Invalid `dataset`: expected mapping"
    )
    scenario = _ensure_mapping(
        config.get("scenario"), config_path, "Invalid `scenario`: expected mapping"
    )
    params = _ensure_mapping(
        config.get("params"), config_path, "Invalid `params`: expected mapping"
    )
    constraints = _ensure_mapping(
        config.get("constraints"),
        config_path,
        "Invalid `constraints`: expected mapping",
    )

    rules = constraints.get("rules", [])
    if not isinstance(rules, list):
        raise ConfigurationError(
            config_path, "Invalid `constraints.rules`: expected list"
        )
    if any(not isinstance(rule, dict) for rule in rules):
        raise ConfigurationError(
            config_path, "Invalid `constraints.rules`: each entry must be a mapping"
        )

    if scenario.get("type") not in (None, "grid", "monte_carlo"):
        raise ConfigurationError(
            config_path, "scenario.type must be 'grid' or 'monte_carlo'"
        )

    if scenario.get("type") == "monte_carlo":
        n_scenarios = scenario.get("n_scenarios", 0)
        if not isinstance(n_scenarios, int) or n_scenarios <= 0:
            raise ConfigurationError(
                config_path,
                "scenario.n_scenarios must be a positive integer for Monte Carlo",
            )

    for name, block in params.items():
        if not isinstance(block, dict):
            raise ConfigurationError(
                config_path,
                f"Parameter block `{name}` must be a mapping of configuration values",
            )

    return config


def build_scenario_spec(config: Dict[str, Any], config_path: str) -> Dict[str, Any]:
    scenario = config.get("scenario", {})
    params = config.get("params", {})
    constraints = config.get("constraints", {})

    # Si la config utilise run.* au lieu de scenario.*
    run_config = config.get("run", {})
    if run_config:
        scenario_type = run_config.get("type", "grid")
        scenario_seed = run_config.get("seed", 42)
        scenario_n = run_config.get("n_scenarios", 100)
        scenario_sampler = run_config.get(
            "sampler", "grid" if scenario_type == "grid" else "sobol"
        )
        scenario_params = run_config.get("params", params)
        scenario_constraints = run_config.get(
            "constraints", constraints.get("rules", [])
        )
    else:
        scenario_type = scenario.get("type", "grid")
        scenario_seed = scenario.get("seed", 42)
        scenario_n = scenario.get("n_scenarios", 100)
        scenario_sampler = scenario.get(
            "sampler",
            "sobol" if scenario_type == "monte_carlo" else "grid",
        )
        scenario_params = params
        scenario_constraints = constraints.get("rules", [])

    spec_dict = {
        "type": scenario_type,
        "params": scenario_params,
        "seed": scenario_seed,
        "n_scenarios": scenario_n,
        "sampler": scenario_sampler,
        "constraints": scenario_constraints,
    }

    logger.info(
        "Configuration validÃ©e: %s avec %d paramÃ¨tres",
        spec_dict["type"],
        len(spec_dict.get("params", {})),
    )
    return spec_dict


def run_sweep(config: Dict[str, Any], config_path: str, dry_run: bool = False) -> None:
    validated = validate_cli_config(config, config_path)
    scenario_spec = build_scenario_spec(validated, config_path)

    execution = validated.get("execution", {})
    output = validated.get("output", {})

    set_global_seed(scenario_spec["seed"])

    if dry_run:
        logger.info("=== MODE DRY RUN ===")
        logger.info("Type de sweep: %s", scenario_spec["type"])
        logger.info("ParamÃ¨tres: %s", list(scenario_spec["params"].keys()))
        logger.info("Seed: %s", scenario_spec["seed"])
        logger.info("GPU activÃ©: %s", execution.get("use_gpu", False))
        logger.info("Cache rÃ©utilisÃ©: %s", execution.get("reuse_cache", True))

        if scenario_spec["type"] == "monte_carlo":
            logger.info("ScÃ©narios Monte Carlo: %s", scenario_spec["n_scenarios"])
            logger.info("Sampler: %s", scenario_spec["sampler"])

        logger.info("Configuration valide - prÃªt pour exÃ©cution")
        return

    indicator_bank = IndicatorBank()
    sweep_runner = SweepRunner(
        indicator_bank=indicator_bank,
        max_workers=execution.get("max_workers", 4),
    )

    start_time = time.time()
    try:
        if scenario_spec["type"] == "grid":
            logger.info("ExÃ©cution du sweep de grille...")
            results_df = sweep_runner.run_grid(
                scenario_spec,
                reuse_cache=execution.get("reuse_cache", True),
            )
        elif scenario_spec["type"] == "monte_carlo":
            logger.info("ExÃ©cution du sweep Monte Carlo...")
            results_df = sweep_runner.run_monte_carlo(
                scenario_spec,
                reuse_cache=execution.get("reuse_cache", True),
            )
        else:  # pragma: no cover - guarded by validation
            raise ConfigurationError(
                config_path,
                f"Type de sweep non supportÃ©: {scenario_spec['type']}",
            )

        if results_df.empty:
            logger.warning("Aucun rÃ©sultat gÃ©nÃ©rÃ©")
        else:
            reports_dir = output.get("reports_dir", "artifacts/reports")
            logger.info(
                "GÃ©nÃ©ration des rapports: %d rÃ©sultats â†’ %s",
                len(results_df),
                reports_dir,
            )
            created_files = write_reports(
                results_df,
                reports_dir,
                seeds=[scenario_spec["seed"]],
                devices=["CPU", "GPU"],
                gpu_ratios={"5090": 0.75, "2060": 0.25},
                min_samples=execution.get("min_samples", 1000),
            )
            for file_type, file_path in created_files.items():
                logger.info("  %s: %s", file_type, file_path)
    except Exception as exc:
        logger.error("Erreur lors de l'exÃ©cution: %s", exc)
        raise
    finally:
        execution_time = time.time() - start_time
        logger.info("Sweep terminÃ© en %.1fs", execution_time)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="ThreadX Optimization Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=(
            "Exemples:\n"
            "  python -m threadx.optimization.run --config configs/sweeps/bb_atr_grid.toml\n"
            "  python -m threadx.optimization.run --config configs/sweeps/bb_atr_montecarlo.toml --dry-run"
        ),
    )

    parser.add_argument(
        "--config", "-c", required=True, help="Chemin vers le fichier TOML"
    )
    parser.add_argument("--dry-run", action="store_true", help="Valide sans exÃ©cuter")
    parser.add_argument("--verbose", "-v", action="store_true", help="Mode verbose")

    args = parser.parse_args()

    if args.verbose:
        import logging

        logging.getLogger("threadx").setLevel(logging.DEBUG)

    config_path = Path(args.config).resolve()

    if not config_path.exists():
        logger.error(f"âŒ Fichier de configuration introuvable: {config_path}")
        sys.exit(1)

    try:
        # Chargement et exÃ©cution
        config = load_config_dict(str(config_path))
        logger.info(f"Configuration chargÃ©e: {args.config}")
        run_sweep(config, str(config_path), dry_run=args.dry_run)

        if not args.dry_run:
            logger.info("âœ… Sweep terminÃ© avec succÃ¨s")

    except ConfigurationError as exc:
        logger.error(exc.user_message)
        logger.debug("Configuration error", exc_info=True)
        sys.exit(2)
    except Exception as exc:
        logger.error("âŒ Erreur: %s", exc)
        sys.exit(1)


if __name__ == "__main__":  # pragma: no cover
    main()




----------------------------------------
Fichier: optimization\scenarios.py
# src/threadx/optimization/scenarios.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List
import itertools
import random

__all__ = [
    "ScenarioSpec",
    "generate_param_grid",
    "generate_monte_carlo",
]


@dataclass(frozen=True)
class ScenarioSpec:
    type: str
    params: Dict[str, Any]
    seed: int = 42
    n_scenarios: int = 100
    sampler: str = "grid"
    constraints: List[Any] = field(default_factory=list)


def _normalize_param(values: Any) -> List[Any]:
    """
    Normalise un paramÃ¨tre vers une liste de candidats.
    Accepte :
      - scalaire -> [scalaire]
      - liste/tuple/set -> list(...)
      - dict avec 'value' -> [value]
      - dict avec 'values' -> list(values)
      - dict avec 'grid' -> list(grid)
    """
    if isinstance(values, dict):
        if "value" in values:
            return [values["value"]]
        if "values" in values:
            return list(values["values"])
        if "grid" in values:
            return list(values["grid"])
        raise ValueError(f"Format invalide pour paramÃ¨tre: {values}")
    if isinstance(values, (list, tuple, set)):
        return list(values)
    return [values]


def generate_param_grid(grid_spec: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Construit le produit cartÃ©sien des paramÃ¨tres.
    grid_spec est un dict { param_name: <format supportÃ© par _normalize_param> }.
    Retourne une liste de combinaisons (dict).
    """
    keys = list(grid_spec.keys())
    choices = [_normalize_param(grid_spec[k]) for k in keys]

    combos: List[Dict[str, Any]] = []
    for prod in itertools.product(*choices):
        combos.append({k: v for k, v in zip(keys, prod)})
    return combos


def generate_monte_carlo(
    spec: Dict[str, Any], n: int, seed: int = 42
) -> List[Dict[str, Any]]:
    """
    Tire au hasard n combinaisons Ã  partir de spec (mÃªmes formats que _normalize_param).
    """
    rnd = random.Random(seed)
    pools = {k: _normalize_param(v) for k, v in spec.items()}
    keys = list(pools.keys())

    combos: List[Dict[str, Any]] = []
    for _ in range(n):
        combos.append({k: rnd.choice(pools[k]) for k in keys})
    return combos


if __name__ == "__main__":
    # Test simple pour valider la gÃ©nÃ©ration de grille
    test_grid = {
        "timeframe": {"value": "1h"},
        "lookback_hours": {"values": [24, 48, 72]},
        "symbols": ["BTCUSDC", "ETHUSDC", "SOLUSDC"],
    }
    combos = generate_param_grid(test_grid)
    print(f"Grille initiale: {len(combos)} combinaisons")
    for i, combo in enumerate(combos[:3], 1):
        print(f"  Combo {i}: {combo}")




----------------------------------------
Fichier: optimization\ui.py
"""
ThreadX Optimization UI Helpers
===============================

Backwards-compatibility shims that expose a small faÃ§ade around
:class:`~threadx.optimization.engine.SweepRunner`.  Some legacy scripts still
import ``ParametricOptimizationUI`` from ``threadx.optimization.ui``; the class
below keeps those imports working while reusing the unified optimisation engine.
"""
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, Optional

import pandas as pd

from .engine import SweepRunner


@dataclass
class ParametricOptimizationUI:
    """Friendly wrapper that delegates to :class:`SweepRunner`."""

    runner: SweepRunner = field(default_factory=SweepRunner)

    def run_grid(self, grid_spec: Dict[str, Any], *, reuse_cache: bool = True) -> pd.DataFrame:
        """Execute a grid sweep and return the resulting :class:`DataFrame`."""
        return self.runner.run_grid(grid_spec, reuse_cache=reuse_cache)

    def run_monte_carlo(
        self,
        mc_spec: Dict[str, Any],
        *,
        reuse_cache: bool = True,
    ) -> pd.DataFrame:
        """Execute a Monte-Carlo optimisation and return the results."""
        return self.runner.run_monte_carlo(mc_spec, reuse_cache=reuse_cache)


def create_optimization_ui(runner: Optional[SweepRunner] = None) -> ParametricOptimizationUI:
    """Factory helper used by older tooling."""
    return ParametricOptimizationUI(runner=runner or SweepRunner())


# Historical entry-point kept for compatibility -------------------------------------------------

def init_ui(*args: Any, **kwargs: Any) -> ParametricOptimizationUI:
    """Return a ready-to-use optimisation helper instance."""
    return create_optimization_ui(*args, **kwargs)

----------------------------------------
Fichier: optimization\__init__.py
"""
ThreadX Optimization Module
===========================

Module d'optimisation paramÃ©trique unifiÃ© pour ThreadX.

Centralise tous les calculs via IndicatorBank (Phase 3) pour:
- Ã©viter la duplication de code
- garantir la cohÃ©rence entre UI, optimisation et backtesting
- utiliser le cache GPU intelligent existant
- bÃ©nÃ©ficier de l'orchestration multi-GPU

Components:
- UnifiedOptimizationEngine: Moteur principal utilisant uniquement IndicatorBank
- ParametricOptimizationUI: Interface utilisateur intÃ©grÃ©e
- Configuration et utilitaires

Author: ThreadX Framework
Version: Phase 10 - Unified Compute Engine
"""

from .engine import (
    UnifiedOptimizationEngine,
    create_unified_engine,
    DEFAULT_SWEEP_CONFIG,
)
from .ui import ParametricOptimizationUI, create_optimization_ui

# Presets for optimization
from .presets import (
    IndicatorRangePreset,
    StrategyPresetMapper,
    get_indicator_range,
    get_strategy_preset,
    list_available_indicators,
    load_all_presets,
)

__all__ = [
    # Engine
    "UnifiedOptimizationEngine",
    "create_unified_engine",
    "DEFAULT_SWEEP_CONFIG",
    # UI
    "ParametricOptimizationUI",
    "create_optimization_ui",
    # Presets
    "IndicatorRangePreset",
    "StrategyPresetMapper",
    "get_indicator_range",
    "get_strategy_preset",
    "list_available_indicators",
    "load_all_presets",
]

__version__ = "1.0.0"
__author__ = "ThreadX Framework"

----------------------------------------
Fichier: optimization\presets\ranges.py
"""
ThreadX - Indicator Range Presets
==================================

Gestion des plages d'optimisation prÃ©-dÃ©finies pour les indicateurs techniques.

Ce module charge les configurations depuis indicator_ranges.toml et fournit
des mÃ©thodes pour mapper automatiquement les paramÃ¨tres de stratÃ©gies aux plages
d'optimisation recommandÃ©es.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Tuple, Optional, Union
from pathlib import Path
import toml
import logging

from threadx.utils.log import get_logger

logger = get_logger(__name__)

# Chemin vers le fichier de configuration
RANGES_FILE = Path(__file__).parent / "indicator_ranges.toml"
EXECUTION_PRESETS_FILE = Path(__file__).parent / "execution_presets.toml"


@dataclass
class IndicatorRangePreset:
    """
    ReprÃ©sente un prÃ©-rÃ©glage de plage d'optimisation pour un indicateur.

    Attributes:
        name: Nom de l'indicateur (ex: "bollinger.period")
        min: Valeur minimale
        max: Valeur maximale
        step: Pas d'incrÃ©mentation
        default: Valeur par dÃ©faut
        type: Type de paramÃ¨tre ("numeric", "boolean", "categorical", "fixed")
        values: Valeurs possibles (pour categorical)
        description: Description du paramÃ¨tre
    """

    name: str
    min: Optional[float] = None
    max: Optional[float] = None
    step: Optional[float] = None
    default: Optional[Any] = None
    type: str = "numeric"  # "numeric", "boolean", "categorical", "fixed"
    values: Optional[List[Any]] = None
    value: Optional[Any] = None  # Pour type="fixed"
    description: str = ""

    def get_range(self) -> Tuple[float, float]:
        """Retourne la plage (min, max)"""
        if self.type != "numeric":
            raise ValueError(
                f"get_range() n'est applicable que pour type='numeric', got: {self.type}"
            )
        if self.min is None or self.max is None:
            raise ValueError(f"min ou max non dÃ©fini pour {self.name}")
        return (self.min, self.max)

    def get_grid_values(self) -> List[Any]:
        """GÃ©nÃ¨re une liste de valeurs pour grid search"""
        if self.type == "numeric":
            if self.min is None or self.max is None or self.step is None:
                raise ValueError(f"min, max, ou step non dÃ©fini pour {self.name}")

            # GÃ©nÃ©rer la grille
            values = []
            current = self.min
            while current <= self.max:
                values.append(current)
                current += self.step

            # S'assurer que max est inclus
            if values and abs(values[-1] - self.max) > 1e-10:
                values.append(self.max)

            return values

        elif self.type == "categorical":
            if self.values is None:
                raise ValueError(f"values non dÃ©fini pour categorical {self.name}")
            return self.values.copy()

        elif self.type == "boolean":
            return [False, True]

        elif self.type == "fixed":
            if self.value is None:
                raise ValueError(f"value non dÃ©fini pour fixed {self.name}")
            return [self.value]

        else:
            raise ValueError(f"Type inconnu: {self.type}")

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire"""
        return {
            "name": self.name,
            "min": self.min,
            "max": self.max,
            "step": self.step,
            "default": self.default,
            "type": self.type,
            "values": self.values,
            "value": self.value,
            "description": self.description,
        }


def load_all_presets() -> Dict[str, IndicatorRangePreset]:
    """
    Charge tous les presets depuis indicator_ranges.toml

    Returns:
        Dictionnaire {nom_indicateur: IndicatorRangePreset}
    """
    if not RANGES_FILE.exists():
        logger.warning(f"Fichier de presets non trouvÃ©: {RANGES_FILE}")
        return {}

    logger.info(f"Chargement des presets depuis: {RANGES_FILE}")

    try:
        config = toml.load(RANGES_FILE)
    except Exception as e:
        logger.error(f"Erreur lors du chargement du fichier TOML: {e}")
        return {}

    presets = {}

    # Parcourir les catÃ©gories et indicateurs
    for category, indicators in config.items():
        for indicator_name, params in indicators.items():
            # Nom complet: category.indicator_name
            full_name = f"{category}.{indicator_name}"

            # DÃ©terminer le type
            param_type = params.get("type", "numeric")

            # CrÃ©er le preset
            preset = IndicatorRangePreset(
                name=full_name,
                min=params.get("min"),
                max=params.get("max"),
                step=params.get("step"),
                default=params.get("default"),
                type=param_type,
                values=params.get("values"),
                value=params.get("value"),
                description=params.get("description", ""),
            )

            presets[full_name] = preset

            logger.debug(f"Preset chargÃ©: {full_name} ({param_type})")

    logger.info(f"âœ“ {len(presets)} presets chargÃ©s")
    return presets


def get_indicator_range(indicator_name: str) -> Optional[IndicatorRangePreset]:
    """
    RÃ©cupÃ¨re le preset pour un indicateur spÃ©cifique.

    Args:
        indicator_name: Nom de l'indicateur (ex: "bollinger.period", "macd.fast_period")

    Returns:
        IndicatorRangePreset ou None si non trouvÃ©
    """
    presets = load_all_presets()
    return presets.get(indicator_name)


def list_available_indicators() -> List[str]:
    """
    Liste tous les indicateurs disponibles dans les presets.

    Returns:
        Liste des noms d'indicateurs
    """
    presets = load_all_presets()
    return sorted(presets.keys())


class StrategyPresetMapper:
    """
    Classe pour mapper automatiquement les paramÃ¨tres d'une stratÃ©gie
    aux presets d'indicateurs disponibles.

    Usage:
        >>> mapper = StrategyPresetMapper("AmplitudeHunter")
        >>> mapper.add_mapping("bb_period", "bollinger.period")
        >>> mapper.add_mapping("macd_fast", "macd.fast_period")
        >>> ranges = mapper.get_optimization_ranges()
    """

    def __init__(self, strategy_name: str):
        """
        Initialise le mapper pour une stratÃ©gie.

        Args:
            strategy_name: Nom de la stratÃ©gie
        """
        self.strategy_name = strategy_name
        self.mappings: Dict[str, str] = {}  # {param_strategy: indicator_name}
        self.presets = load_all_presets()
        logger.info(f"StrategyPresetMapper initialisÃ© pour: {strategy_name}")

    def add_mapping(self, strategy_param: str, indicator_name: str) -> None:
        """
        Ajoute un mapping entre un paramÃ¨tre de stratÃ©gie et un indicateur.

        Args:
            strategy_param: Nom du paramÃ¨tre dans la stratÃ©gie (ex: "bb_period")
            indicator_name: Nom de l'indicateur dans les presets (ex: "bollinger.period")
        """
        if indicator_name not in self.presets:
            logger.warning(
                f"Indicateur '{indicator_name}' non trouvÃ© dans les presets. "
                f"Mapping ignorÃ© pour '{strategy_param}'."
            )
            return

        self.mappings[strategy_param] = indicator_name
        logger.debug(f"Mapping ajoutÃ©: {strategy_param} -> {indicator_name}")

    def add_mappings(self, mappings: Dict[str, str]) -> None:
        """
        Ajoute plusieurs mappings en une seule fois.

        Args:
            mappings: Dictionnaire {strategy_param: indicator_name}
        """
        for strategy_param, indicator_name in mappings.items():
            self.add_mapping(strategy_param, indicator_name)

    def get_optimization_ranges(self) -> Dict[str, Tuple[float, float]]:
        """
        Retourne les plages d'optimisation pour les paramÃ¨tres mappÃ©s.

        Returns:
            Dictionnaire {strategy_param: (min, max)}
        """
        ranges = {}

        for strategy_param, indicator_name in self.mappings.items():
            preset = self.presets.get(indicator_name)
            if preset and preset.type == "numeric":
                try:
                    ranges[strategy_param] = preset.get_range()
                except ValueError as e:
                    logger.warning(
                        f"Impossible de rÃ©cupÃ©rer la plage pour {strategy_param}: {e}"
                    )

        logger.info(
            f"âœ“ {len(ranges)} plages d'optimisation gÃ©nÃ©rÃ©es pour {self.strategy_name}"
        )
        return ranges

    def get_grid_parameters(self) -> Dict[str, List[Any]]:
        """
        Retourne les grilles de paramÃ¨tres pour grid search.

        Returns:
            Dictionnaire {strategy_param: [valeurs]}
        """
        grid = {}

        for strategy_param, indicator_name in self.mappings.items():
            preset = self.presets.get(indicator_name)
            if preset:
                try:
                    grid[strategy_param] = preset.get_grid_values()
                except ValueError as e:
                    logger.warning(
                        f"Impossible de gÃ©nÃ©rer la grille pour {strategy_param}: {e}"
                    )

        logger.info(
            f"âœ“ {len(grid)} grilles de paramÃ¨tres gÃ©nÃ©rÃ©es pour {self.strategy_name}"
        )
        return grid

    def get_default_parameters(self) -> Dict[str, Any]:
        """
        Retourne les valeurs par dÃ©faut pour les paramÃ¨tres mappÃ©s.

        Returns:
            Dictionnaire {strategy_param: default_value}
        """
        defaults = {}

        for strategy_param, indicator_name in self.mappings.items():
            preset = self.presets.get(indicator_name)
            if preset and preset.default is not None:
                defaults[strategy_param] = preset.default

        logger.info(
            f"âœ“ {len(defaults)} valeurs par dÃ©faut gÃ©nÃ©rÃ©es pour {self.strategy_name}"
        )
        return defaults

    def get_preset_info(self) -> Dict[str, Dict[str, Any]]:
        """
        Retourne les informations complÃ¨tes sur les presets mappÃ©s.

        Returns:
            Dictionnaire {strategy_param: preset_dict}
        """
        info = {}

        for strategy_param, indicator_name in self.mappings.items():
            preset = self.presets.get(indicator_name)
            if preset:
                info[strategy_param] = preset.to_dict()

        return info


def get_strategy_preset(strategy_name: str) -> StrategyPresetMapper:
    """
    Factory pour crÃ©er un mapper prÃ©-configurÃ© pour une stratÃ©gie connue.

    Args:
        strategy_name: Nom de la stratÃ©gie ("AmplitudeHunter", "BBAtr", etc.)

    Returns:
        StrategyPresetMapper prÃ©-configurÃ©

    Example:
        >>> preset = get_strategy_preset("AmplitudeHunter")
        >>> ranges = preset.get_optimization_ranges()
    """
    mapper = StrategyPresetMapper(strategy_name)

    # Configuration des mappings selon la stratÃ©gie
    if strategy_name.lower() in ["amplitudehunter", "amplitude_hunter"]:
        mapper.add_mappings(
            {
                # Bollinger Bands
                "bb_period": "bollinger.period",
                "bb_std": "bollinger.std_dev",
                # MACD
                "macd_fast": "macd.fast_period",
                "macd_slow": "macd.slow_period",
                "macd_signal": "macd.signal_period",
                # ADX
                "adx_period": "adx.period",
                "adx_threshold": "adx.trend_threshold",
                # ATR
                "atr_period": "atr.period",
                "sl_atr_multiplier": "atr.stop_multiplier",
                # ParamÃ¨tres spÃ©cifiques AmplitudeHunter
                "bbwidth_percentile_threshold": "amplitude_hunter.bbwidth_percentile_threshold",
                "volume_zscore_threshold": "amplitude_hunter.volume_zscore_threshold",
                "spring_lookback": "amplitude_hunter.spring_lookback",
                "pb_entry_threshold_min": "amplitude_hunter.pb_entry_threshold_min",
                "pb_entry_threshold_max": "amplitude_hunter.pb_entry_threshold_max",
                "amplitude_score_threshold": "amplitude_hunter.amplitude_score_threshold",
                "trailing_activation_gain_r": "amplitude_hunter.trailing_activation_gain_r",
                "trailing_chandelier_atr_mult": "amplitude_hunter.trailing_chandelier_atr_mult",
                "bip_partial_exit_pct": "amplitude_hunter.bip_partial_exit_pct",
                "risk_per_trade": "amplitude_hunter.risk_per_trade",
                "max_hold_bars": "amplitude_hunter.max_hold_bars",
                "pyramiding_max_adds": "amplitude_hunter.pyramiding_max_adds",
            }
        )

    elif strategy_name.lower() in ["bbatr", "bb_atr"]:
        mapper.add_mappings(
            {
                # Bollinger Bands
                "bb_period": "bollinger.period",
                "bb_std": "bollinger.std_dev",
                # ATR
                "atr_period": "atr.period",
                "atr_multiplier": "atr.stop_multiplier",
                # Risk management
                "risk_per_trade": "amplitude_hunter.risk_per_trade",  # RÃ©utilise le mÃªme preset
                "max_hold_bars": "amplitude_hunter.max_hold_bars",
            }
        )

    elif strategy_name.lower() in ["bollingerdual", "bollinger_dual"]:
        mapper.add_mappings(
            {
                # Bollinger Bands
                "bb_period": "bollinger.period",
                "bb_std": "bollinger.std_dev",
                # Moving Average
                "ma_window": "sma.short_period",
                # Risk management
                "risk_per_trade": "amplitude_hunter.risk_per_trade",
                "max_hold_bars": "amplitude_hunter.max_hold_bars",
            }
        )

    else:
        logger.warning(
            f"StratÃ©gie '{strategy_name}' non reconnue. "
            f"Mapper crÃ©Ã© sans mappings prÃ©-dÃ©finis."
        )

    return mapper


def load_execution_presets() -> Dict[str, Dict[str, Any]]:
    """
    ðŸ†• Charge les presets d'exÃ©cution (workers, batch size, GPU targets).

    Returns:
        Dict avec structure: {
            "workers": {"auto": {...}, "manuel_30": {...}, ...},
            "batch": {...},
            "gpu": {...},
            "combined": {...}
        }
    """
    if not EXECUTION_PRESETS_FILE.exists():
        logger.warning(
            f"Fichier execution presets non trouvÃ©: {EXECUTION_PRESETS_FILE}"
        )
        return {}

    try:
        data = toml.load(EXECUTION_PRESETS_FILE)
        logger.info(f"âœ… Presets d'exÃ©cution chargÃ©s: {len(data)} catÃ©gories")
        return data
    except Exception as e:
        logger.error(f"Erreur chargement execution presets: {e}")
        return {}


def get_execution_preset(preset_name: str = "manuel_30") -> Dict[str, Any]:
    """
    ðŸ†• RÃ©cupÃ¨re un preset d'exÃ©cution par nom.

    Args:
        preset_name: Nom du preset (ex: "manuel_30", "auto", "aggressive")

    Returns:
        Configuration du preset

    Example:
        >>> preset = get_execution_preset("manuel_30")
        >>> print(preset["max_workers"])  # 30
        >>> print(preset["batch_size"])   # 2000
    """
    presets = load_execution_presets()

    # Chercher dans toutes les catÃ©gories
    for category, category_presets in presets.items():
        for name, config in category_presets.items():
            if name == preset_name or name.endswith(preset_name):
                logger.info(f"âœ… Preset trouvÃ©: {category}.{name}")
                return config

    logger.warning(f"Preset '{preset_name}' non trouvÃ©, utilisant dÃ©faut")
    return {"max_workers": 30, "batch_size": 2000, "description": "DÃ©faut manuel"}


# ==========================================================================
# MODULE EXPORTS
# ==========================================================================

__all__ = [
    "IndicatorRangePreset",
    "StrategyPresetMapper",
    "get_indicator_range",
    "get_strategy_preset",
    "list_available_indicators",
    "load_all_presets",
    "load_execution_presets",
    "get_execution_preset",
]

----------------------------------------
Fichier: optimization\presets\__init__.py
"""
ThreadX Optimization Presets
=============================

Module de gestion des prÃ©-rÃ©glages d'optimisation pour les indicateurs techniques.

Ce module charge les plages "classiques" depuis indicator_ranges.toml et les mappe
automatiquement aux paramÃ¨tres des stratÃ©gies pour faciliter l'optimisation.

Usage:
    >>> from threadx.optimization.presets import get_strategy_preset
    >>> preset = get_strategy_preset('AmplitudeHunter')
    >>> print(preset.get_ranges())
"""

from .ranges import (
    IndicatorRangePreset,
    StrategyPresetMapper,
    get_indicator_range,
    get_strategy_preset,
    list_available_indicators,
    load_all_presets,
)

__all__ = [
    "IndicatorRangePreset",
    "StrategyPresetMapper",
    "get_indicator_range",
    "get_strategy_preset",
    "list_available_indicators",
    "load_all_presets",
]

----------------------------------------
Fichier: optimization\templates\base_optimizer.py
"""
ThreadX Optimization Templates - Template Method Pattern
=========================================================

Classes de base pour optimisations avec Template Method Pattern.
Centralise la logique commune (prepare, iterate, finalize) et
laisse les sous-classes implÃ©menter les dÃ©tails spÃ©cifiques.

Usage:
    from threadx.optimization.templates import BaseOptimizer, OptimizationResult

    class GridOptimizer(BaseOptimizer):
        def run_iteration(self, params: Dict[str, Any]) -> float:
            # ImplÃ©mentation spÃ©cifique grid search
            return score

Author: ThreadX Framework - Phase 2 Step 3.3 DRY Refactoring
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Callable, Tuple
from datetime import datetime
import time

from threadx.utils.common_imports import pd, np, create_logger

logger = create_logger(__name__)


@dataclass
class OptimizationResult:
    """
    RÃ©sultat unifiÃ© d'une optimization.

    Attributes:
        best_params: Meilleurs paramÃ¨tres trouvÃ©s
        best_score: Meilleur score (mÃ©trique objectif)
        all_results: DataFrame de tous les essais
        iterations: Nombre total d'itÃ©rations
        duration_sec: DurÃ©e totale d'exÃ©cution
        convergence_history: Historique des meilleurs scores
        metadata: MÃ©tadonnÃ©es additionnelles (optimizer_type, etc.)
    """
    best_params: Dict[str, Any]
    best_score: float
    all_results: pd.DataFrame
    iterations: int
    duration_sec: float
    convergence_history: List[float] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


class BaseOptimizer(ABC):
    """
    Classe abstraite de base pour tous les optimizers.

    ImplÃ©mente le Template Method Pattern:
    - prepare_data(): PrÃ©paration initiale
    - run_iteration(): ItÃ©ration unique (Ã  implÃ©menter)
    - finalize(): Finalisation et cleanup
    - optimize(): Orchestration complÃ¨te (template method)

    Fournit aussi:
    - Gestion centralisÃ©e des logs
    - Gestion centralisÃ©e des exceptions
    - Tracking de convergence
    - Validation des rÃ©sultats
    """

    def __init__(
        self,
        objective_fn: Callable[[Dict[str, Any]], float],
        maximize: bool = True,
        verbose: bool = True,
        early_stopping: Optional[int] = None,
        tolerance: float = 1e-6
    ):
        """
        Initialize base optimizer.

        Args:
            objective_fn: Fonction objectif Ã  optimiser
                         Prend params dict, retourne score float
            maximize: True pour maximiser, False pour minimiser
            verbose: Afficher logs de progression
            early_stopping: Nombre d'itÃ©rations sans amÃ©lioration avant arrÃªt
            tolerance: TolÃ©rance pour considÃ©rer une amÃ©lioration
        """
        self.objective_fn = objective_fn
        self.maximize = maximize
        self.verbose = verbose
        self.early_stopping = early_stopping
        self.tolerance = tolerance

        # Ã‰tat interne
        self.logger = create_logger(f"{__name__}.{self.__class__.__name__}")
        self.results: List[Dict[str, Any]] = []
        self.convergence_history: List[float] = []
        self.best_params: Optional[Dict[str, Any]] = None
        self.best_score: Optional[float] = None
        self.iterations_count = 0
        self.iterations_without_improvement = 0
        self.start_time: Optional[float] = None

        self.logger.info(
            f"ðŸ”§ {self.__class__.__name__} initialized "
            f"({'maximize' if maximize else 'minimize'})"
        )

    def prepare_data(self) -> None:
        """
        PrÃ©paration avant optimization (hook).

        Peut Ãªtre overridden par sous-classes pour:
        - Validation des paramÃ¨tres
        - Initialisation de structures de donnÃ©es
        - Setup de caches
        """
        self.logger.debug("Preparing optimization data...")
        self.start_time = time.time()
        self.results = []
        self.convergence_history = []
        self.best_params = None
        self.best_score = None
        self.iterations_count = 0
        self.iterations_without_improvement = 0

    @abstractmethod
    def run_iteration(self, iteration: int) -> Tuple[Dict[str, Any], float]:
        """
        ExÃ©cute une itÃ©ration d'optimization (Ã  implÃ©menter).

        Args:
            iteration: NumÃ©ro de l'itÃ©ration (0-based)

        Returns:
            (params, score): ParamÃ¨tres testÃ©s et score obtenu

        Raises:
            NotImplementedError: Si non implÃ©mentÃ© par sous-classe
        """
        raise NotImplementedError("run_iteration() must be implemented by subclass")

    def finalize(self) -> OptimizationResult:
        """
        Finalisation aprÃ¨s optimization (hook).

        Peut Ãªtre overridden pour:
        - Cleanup de ressources
        - Post-processing des rÃ©sultats
        - GÃ©nÃ©ration de rapports

        Returns:
            OptimizationResult avec tous les rÃ©sultats
        """
        duration = time.time() - self.start_time if self.start_time else 0

        # CrÃ©er DataFrame rÃ©sultats
        results_df = pd.DataFrame(self.results)

        # Trouver meilleur si pas dÃ©jÃ  fait
        if self.best_params is None and len(results_df) > 0:
            if self.maximize:
                best_idx = results_df['score'].idxmax()
            else:
                best_idx = results_df['score'].idxmin()

            best_row = results_df.loc[best_idx]
            self.best_params = best_row.drop('score').to_dict()
            self.best_score = best_row['score']

        self.logger.info(
            f"âœ… Optimization complete: "
            f"{self.iterations_count} iterations in {duration:.2f}s"
        )
        self.logger.info(f"   Best: {self.best_params} â†’ score={self.best_score:.4f}")

        return OptimizationResult(
            best_params=self.best_params or {},
            best_score=self.best_score or 0.0,
            all_results=results_df,
            iterations=self.iterations_count,
            duration_sec=duration,
            convergence_history=self.convergence_history,
            metadata={
                'optimizer_type': self.__class__.__name__,
                'maximize': self.maximize,
                'early_stopped': self.iterations_without_improvement >= (self.early_stopping or float('inf'))
            }
        )

    def optimize(self, max_iterations: int) -> OptimizationResult:
        """
        Template method principal pour optimization complÃ¨te.

        Orchestration:
        1. prepare_data() - PrÃ©paration
        2. Boucle: run_iteration() avec gestion erreurs/convergence
        3. finalize() - Finalisation

        Args:
            max_iterations: Nombre maximum d'itÃ©rations

        Returns:
            OptimizationResult complet
        """
        self.logger.info(f"ðŸš€ Starting optimization: max {max_iterations} iterations")

        # 1. PrÃ©paration
        try:
            self.prepare_data()
        except Exception as e:
            self.logger.error(f"Preparation failed: {e}", exc_info=True)
            raise

        # 2. Boucle d'optimization
        for iteration in range(max_iterations):
            try:
                # ExÃ©cuter itÃ©ration (implÃ©mentÃ© par sous-classe)
                params, score = self.run_iteration(iteration)

                # Enregistrer rÃ©sultat
                self.results.append({**params, 'score': score})
                self.iterations_count += 1

                # Mettre Ã  jour meilleur
                is_improvement = self._update_best(params, score)

                # Tracking convergence
                self.convergence_history.append(self.best_score)

                # Logs
                if self.verbose and (iteration + 1) % 10 == 0:
                    self.logger.info(
                        f"Progress: {iteration + 1}/{max_iterations} | "
                        f"Best: {self.best_score:.4f}"
                    )

                # Early stopping
                if self.early_stopping and self.iterations_without_improvement >= self.early_stopping:
                    self.logger.info(
                        f"â¹ï¸  Early stopping after {self.early_stopping} "
                        f"iterations without improvement"
                    )
                    break

            except KeyboardInterrupt:
                self.logger.warning("âš ï¸  Optimization interrupted by user")
                break

            except Exception as e:
                self.logger.error(
                    f"Iteration {iteration} failed: {e}",
                    exc_info=self.verbose
                )
                # Continue malgrÃ© erreur d'une itÃ©ration
                continue

        # 3. Finalisation
        try:
            return self.finalize()
        except Exception as e:
            self.logger.error(f"Finalization failed: {e}", exc_info=True)
            raise

    def _update_best(self, params: Dict[str, Any], score: float) -> bool:
        """
        Met Ã  jour le meilleur rÃ©sultat si amÃ©lioration.

        Args:
            params: ParamÃ¨tres testÃ©s
            score: Score obtenu

        Returns:
            True si amÃ©lioration, False sinon
        """
        if self.best_score is None:
            self.best_params = params
            self.best_score = score
            self.iterations_without_improvement = 0
            return True

        # VÃ©rifier amÃ©lioration
        if self.maximize:
            is_better = score > self.best_score + self.tolerance
        else:
            is_better = score < self.best_score - self.tolerance

        if is_better:
            self.best_params = params
            self.best_score = score
            self.iterations_without_improvement = 0

            if self.verbose:
                self.logger.debug(
                    f"âœ¨ New best: {params} â†’ {score:.4f} "
                    f"(iteration {self.iterations_count})"
                )

            return True
        else:
            self.iterations_without_improvement += 1
            return False

    def get_summary(self) -> Dict[str, Any]:
        """
        Retourne un rÃ©sumÃ© de l'optimization.

        Returns:
            Dict avec statistiques principales
        """
        if not self.results:
            return {}

        scores = [r['score'] for r in self.results]

        return {
            'optimizer': self.__class__.__name__,
            'iterations': self.iterations_count,
            'best_score': self.best_score,
            'best_params': self.best_params,
            'score_mean': np.mean(scores),
            'score_std': np.std(scores),
            'score_min': np.min(scores),
            'score_max': np.max(scores),
            'convergence_rate': len(self.convergence_history) / max(self.iterations_count, 1)
        }


# Exports
__all__ = [
    'BaseOptimizer',
    'OptimizationResult',
]




----------------------------------------
Fichier: optimization\templates\grid_optimizer.py
"""
ThreadX Grid Search Optimizer
==============================

Grid search implementation using BaseOptimizer template.

Author: ThreadX Framework - Phase 2 Step 3.3
"""

from typing import Any, Dict, List, Tuple, Callable
from itertools import product

from threadx.utils.common_imports import create_logger
from .base_optimizer import BaseOptimizer

logger = create_logger(__name__)


class GridOptimizer(BaseOptimizer):
    """
    Grid search optimizer - teste toutes les combinaisons.

    HÃ©rite de BaseOptimizer et implÃ©mente run_iteration()
    pour parcourir systÃ©matiquement une grille de paramÃ¨tres.

    Usage:
        optimizer = GridOptimizer(
            param_grid={'period': [10, 20, 30], 'std': [1.5, 2.0]},
            objective_fn=my_backtest_fn,
            maximize=True
        )
        result = optimizer.optimize(max_iterations=6)  # 3*2 combinations
    """

    def __init__(
        self,
        param_grid: Dict[str, List[Any]],
        objective_fn: Callable[[Dict[str, Any]], float],
        maximize: bool = True,
        verbose: bool = True,
        parallel: bool = False,
        n_jobs: int = -1
    ):
        """
        Initialize grid search optimizer.

        Args:
            param_grid: Dict de paramÃ¨tres avec listes de valeurs
                       Ex: {'period': [10, 20, 30], 'std': [1.5, 2.0]}
            objective_fn: Fonction Ã  optimiser
            maximize: True pour maximiser
            verbose: Afficher progression
            parallel: ExÃ©cution parallÃ¨le (future feature)
            n_jobs: Nombre de workers (future feature)
        """
        super().__init__(
            objective_fn=objective_fn,
            maximize=maximize,
            verbose=verbose
        )

        self.param_grid = param_grid
        self.parallel = parallel
        self.n_jobs = n_jobs

        # GÃ©nÃ©rer toutes les combinaisons
        self._generate_combinations()

        self.logger.info(
            f"Grid search initialized: {len(self.combinations)} combinations"
        )

    def _generate_combinations(self) -> None:
        """GÃ©nÃ¨re toutes les combinaisons de paramÃ¨tres."""
        param_names = list(self.param_grid.keys())
        param_values = list(self.param_grid.values())

        # Produit cartÃ©sien
        combinations_tuples = list(product(*param_values))

        # Convertir en liste de dicts
        self.combinations = [
            dict(zip(param_names, combo))
            for combo in combinations_tuples
        ]

        self.logger.debug(
            f"Generated {len(self.combinations)} parameter combinations"
        )

    def prepare_data(self) -> None:
        """Override: Re-gÃ©nÃ¨re combinations si besoin."""
        super().prepare_data()

        # Validation de la grille
        if not self.param_grid:
            raise ValueError("param_grid cannot be empty")

        for param, values in self.param_grid.items():
            if not isinstance(values, (list, tuple)) or len(values) == 0:
                raise ValueError(
                    f"Parameter '{param}' must have at least one value"
                )

    def run_iteration(self, iteration: int) -> Tuple[Dict[str, Any], float]:
        """
        ExÃ©cute une itÃ©ration de grid search.

        Args:
            iteration: Index de l'itÃ©ration (correspond Ã  index dans combinations)

        Returns:
            (params, score): ParamÃ¨tres testÃ©s et score
        """
        # VÃ©rifier qu'on a encore des combinaisons
        if iteration >= len(self.combinations):
            raise StopIteration("All combinations tested")

        # RÃ©cupÃ©rer params pour cette itÃ©ration
        params = self.combinations[iteration]

        # Ã‰valuer objective function
        try:
            score = self.objective_fn(params)

            if self.verbose and (iteration + 1) % 5 == 0:
                self.logger.debug(
                    f"Iteration {iteration + 1}/{len(self.combinations)}: "
                    f"{params} â†’ {score:.4f}"
                )

            return params, score

        except Exception as e:
            self.logger.warning(
                f"Objective function failed for {params}: {e}"
            )
            # Retourner score worst possible
            score = float('-inf') if self.maximize else float('inf')
            return params, score

    def optimize(self, max_iterations: int = None) -> 'OptimizationResult':
        """
        Override: Grid search teste TOUTES les combinaisons.

        Args:
            max_iterations: IgnorÃ© (utilise len(combinations))

        Returns:
            OptimizationResult
        """
        # PrÃ©parer d'abord pour avoir combinations
        if not self.combinations:
            self.prepare_data()

        # Pour grid search, max_iterations = nombre de combinaisons
        actual_max = len(self.combinations)

        if max_iterations is not None and max_iterations != actual_max:
            self.logger.warning(
                f"max_iterations={max_iterations} ignored for grid search, "
                f"using {actual_max} (all combinations)"
            )

        return super().optimize(max_iterations=actual_max)

    def get_param_importance(self) -> Dict[str, float]:
        """
        Calcule l'importance de chaque paramÃ¨tre (variance du score).

        Returns:
            Dict {param_name: importance_score}
        """
        if not self.results:
            return {}

        import pandas as pd
        df = pd.DataFrame(self.results)

        importance = {}
        for param in self.param_grid.keys():
            # Variance du score groupÃ© par valeur du paramÃ¨tre
            grouped = df.groupby(param)['score'].var()
            importance[param] = grouped.mean()

        return importance


# Convenience function
def grid_search(
    param_grid: Dict[str, List[Any]],
    objective_fn: Callable[[Dict[str, Any]], float],
    maximize: bool = True,
    verbose: bool = True
) -> 'OptimizationResult':
    """
    Fonction helper pour grid search rapide.

    Args:
        param_grid: Grille de paramÃ¨tres
        objective_fn: Fonction objectif
        maximize: Maximiser ou minimiser
        verbose: Logs verbose

    Returns:
        OptimizationResult

    Example:
        >>> result = grid_search(
        ...     param_grid={'period': [10, 20], 'std': [2.0, 2.5]},
        ...     objective_fn=my_backtest,
        ...     maximize=True
        ... )
        >>> print(result.best_params)
    """
    optimizer = GridOptimizer(
        param_grid=param_grid,
        objective_fn=objective_fn,
        maximize=maximize,
        verbose=verbose
    )
    return optimizer.optimize()




----------------------------------------
Fichier: optimization\templates\monte_carlo_optimizer.py
"""
ThreadX Monte Carlo Optimizer
==============================

Monte Carlo (random search) implementation using BaseOptimizer template.

Author: ThreadX Framework - Phase 2 Step 3.3
"""

from typing import Any, Dict, Tuple, Callable
import numpy as np

from threadx.utils.common_imports import create_logger
from .base_optimizer import BaseOptimizer

logger = create_logger(__name__)


class MonteCarloOptimizer(BaseOptimizer):
    """
    Monte Carlo optimizer - tire des paramÃ¨tres alÃ©atoires.

    HÃ©rite de BaseOptimizer et implÃ©mente run_iteration()
    pour tirer alÃ©atoirement des paramÃ¨tres dans des ranges.

    Usage:
        optimizer = MonteCarloOptimizer(
            param_ranges={'period': (10, 50), 'std': (1.0, 3.0)},
            objective_fn=my_backtest_fn,
            n_trials=100,
            maximize=True,
            seed=42
        )
        result = optimizer.optimize(max_iterations=100)
    """

    def __init__(
        self,
        param_ranges: Dict[str, Tuple[float, float]],
        objective_fn: Callable[[Dict[str, Any]], float],
        n_trials: int = 100,
        maximize: bool = True,
        verbose: bool = True,
        seed: int = 42,
        early_stopping: int = None
    ):
        """
        Initialize Monte Carlo optimizer.

        Args:
            param_ranges: Dict de paramÃ¨tres avec ranges (min, max)
                         Ex: {'period': (10, 50), 'std': (1.0, 3.0)}
            objective_fn: Fonction Ã  optimiser
            n_trials: Nombre d'essais alÃ©atoires
            maximize: True pour maximiser
            verbose: Afficher progression
            seed: Seed pour reproductibilitÃ©
            early_stopping: ArrÃªt si pas d'amÃ©lioration aprÃ¨s N essais
        """
        super().__init__(
            objective_fn=objective_fn,
            maximize=maximize,
            verbose=verbose,
            early_stopping=early_stopping
        )

        self.param_ranges = param_ranges
        self.n_trials = n_trials
        self.seed = seed

        # Random state pour reproductibilitÃ©
        self.rng = np.random.RandomState(seed)

        self.logger.info(
            f"Monte Carlo initialized: {n_trials} trials "
            f"(seed={seed})"
        )

    def prepare_data(self) -> None:
        """Override: Validation des ranges."""
        super().prepare_data()

        # Validation
        if not self.param_ranges:
            raise ValueError("param_ranges cannot be empty")

        for param, (min_val, max_val) in self.param_ranges.items():
            if min_val >= max_val:
                raise ValueError(
                    f"Invalid range for '{param}': "
                    f"min ({min_val}) >= max ({max_val})"
                )

        # RÃ©initialiser RNG avec seed
        self.rng = np.random.RandomState(self.seed)

    def run_iteration(self, iteration: int) -> Tuple[Dict[str, Any], float]:
        """
        ExÃ©cute une itÃ©ration Monte Carlo.

        Args:
            iteration: NumÃ©ro de l'itÃ©ration

        Returns:
            (params, score): ParamÃ¨tres tirÃ©s et score
        """
        # Tirer paramÃ¨tres alÃ©atoires
        params = self._sample_params()

        # Ã‰valuer objective function
        try:
            score = self.objective_fn(params)

            if self.verbose and (iteration + 1) % 20 == 0:
                self.logger.debug(
                    f"Trial {iteration + 1}/{self.n_trials}: "
                    f"{params} â†’ {score:.4f}"
                )

            return params, score

        except Exception as e:
            self.logger.warning(
                f"Objective function failed for {params}: {e}"
            )
            # Retourner score worst possible
            score = float('-inf') if self.maximize else float('inf')
            return params, score

    def _sample_params(self) -> Dict[str, Any]:
        """
        Tire alÃ©atoirement des paramÃ¨tres dans les ranges.

        Returns:
            Dict de paramÃ¨tres tirÃ©s
        """
        params = {}

        for name, (min_val, max_val) in self.param_ranges.items():
            # DÃ©tecter type de paramÃ¨tre (int vs float)
            if isinstance(min_val, int) and isinstance(max_val, int):
                # ParamÃ¨tre entier
                params[name] = self.rng.randint(min_val, max_val + 1)
            else:
                # ParamÃ¨tre float
                params[name] = self.rng.uniform(min_val, max_val)

        return params

    def optimize(self, max_iterations: int = None) -> OptimizationResult:
        """
        Lance l'optimisation Monte Carlo.

        Args:
            max_iterations: Nombre max d'itÃ©rations (dÃ©faut: n_trials)

        Returns:
            RÃ©sultat de l'optimisation
        """
        # Valider d'abord les ranges
        if not hasattr(self, '_prepared'):
            self.prepare_data()
            self._prepared = True

        if max_iterations is None:
            max_iterations = self.n_trials

        return super().optimize(max_iterations)

    def get_param_distributions(self) -> Dict[str, Dict[str, float]]:
        """
        Analyse la distribution des paramÃ¨tres testÃ©s.

        Returns:
            Dict {param_name: {'mean': ..., 'std': ..., ...}}
        """
        if not self.results:
            return {}

        import pandas as pd
        df = pd.DataFrame(self.results)

        distributions = {}
        for param in self.param_ranges.keys():
            values = df[param]
            distributions[param] = {
                'mean': values.mean(),
                'std': values.std(),
                'min': values.min(),
                'max': values.max(),
                'median': values.median()
            }

        return distributions

    def get_best_region(self, percentile: float = 90) -> Dict[str, Tuple[float, float]]:
        """
        Identifie la rÃ©gion des meilleurs paramÃ¨tres.

        Args:
            percentile: Percentile pour dÃ©finir "meilleurs" (90 = top 10%)

        Returns:
            Dict {param_name: (min, max)} de la rÃ©gion optimale
        """
        if not self.results:
            return {}

        import pandas as pd
        df = pd.DataFrame(self.results)

        # Filtrer top percentile
        if self.maximize:
            threshold = df['score'].quantile(percentile / 100)
            top_df = df[df['score'] >= threshold]
        else:
            threshold = df['score'].quantile(1 - percentile / 100)
            top_df = df[df['score'] <= threshold]

        # Calculer ranges pour chaque param
        best_region = {}
        for param in self.param_ranges.keys():
            values = top_df[param]
            best_region[param] = (values.min(), values.max())

        return best_region


# Convenience function
def monte_carlo_search(
    param_ranges: Dict[str, Tuple[float, float]],
    objective_fn: Callable[[Dict[str, Any]], float],
    n_trials: int = 100,
    maximize: bool = True,
    seed: int = 42,
    verbose: bool = True
) -> 'OptimizationResult':
    """
    Fonction helper pour Monte Carlo search rapide.

    Args:
        param_ranges: Ranges de paramÃ¨tres
        objective_fn: Fonction objectif
        n_trials: Nombre d'essais
        maximize: Maximiser ou minimiser
        seed: Seed alÃ©atoire
        verbose: Logs verbose

    Returns:
        OptimizationResult

    Example:
        >>> result = monte_carlo_search(
        ...     param_ranges={'period': (10, 50), 'std': (1.0, 3.0)},
        ...     objective_fn=my_backtest,
        ...     n_trials=100,
        ...     maximize=True
        ... )
        >>> print(result.best_params)
    """
    optimizer = MonteCarloOptimizer(
        param_ranges=param_ranges,
        objective_fn=objective_fn,
        n_trials=n_trials,
        maximize=maximize,
        seed=seed,
        verbose=verbose
    )
    return optimizer.optimize()




----------------------------------------
Fichier: optimization\templates\__init__.py
"""
ThreadX Optimization Templates
===============================

Template implementations for common optimization algorithms.

Exports:
    - BaseOptimizer: Abstract base class
    - GridOptimizer: Grid search implementation
    - MonteCarloOptimizer: Random search implementation
    - OptimizationResult: Result dataclass

Author: ThreadX Framework - Phase 2 Step 3.3
"""

from .base_optimizer import BaseOptimizer, OptimizationResult
from .grid_optimizer import GridOptimizer
from .monte_carlo_optimizer import MonteCarloOptimizer

__all__ = [
    'BaseOptimizer',
    'OptimizationResult',
    'GridOptimizer',
    'MonteCarloOptimizer',
]




----------------------------------------
Fichier: profiling\performance_analyzer.py
"""
Analyseur de performance pour le moteur de backtest ThreadX.
Mesure, diagnostique et gÃ©nÃ¨re des rapports visuels.
"""

import time
import json
import psutil
import os
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any, Callable
from contextlib import contextmanager
from collections import defaultdict
import threading
from pathlib import Path


@dataclass
class ComponentMetrics:
    """MÃ©triques d'un composant."""

    name: str
    total_time: float  # Temps total en secondes
    call_count: int
    avg_time: float  # Temps moyen par appel
    min_time: float
    max_time: float
    percent_of_total: float
    memory_delta_mb: float  # DiffÃ©rence mÃ©moire avant/aprÃ¨s
    diagnostic: str  # "optimize", "monitor", "ok"
    potential_gain: str  # Description du gain potentiel
    priority: int  # 1=haute, 2=moyenne, 3=basse
    details: Dict[str, Any]  # Infos supplÃ©mentaires


class PerformanceAnalyzer:
    """
    Analyseur de performance avec diagnostic automatique.

    Usage:
        analyzer = PerformanceAnalyzer()

        with analyzer.measure("data_loading"):
            load_data()

        with analyzer.measure("backtest"):
            run_backtest()

        report = analyzer.generate_report()
        analyzer.save_html_report("report.html")
    """

    def __init__(self):
        self.components: Dict[str, List[float]] = defaultdict(list)
        self.memory_snapshots: Dict[str, tuple] = {}
        self.start_time: Optional[float] = None
        self.end_time: Optional[float] = None
        self.total_duration: float = 0
        self.process = psutil.Process(os.getpid())
        self._lock = threading.Lock()

    def start(self):
        """DÃ©marre le profiling global."""
        self.start_time = time.perf_counter()

    def stop(self):
        """ArrÃªte le profiling global."""
        self.end_time = time.perf_counter()
        if self.start_time:
            self.total_duration = self.end_time - self.start_time

    @contextmanager
    def measure(self, component_name: str, **metadata):
        """
        Context manager pour mesurer un composant.

        Args:
            component_name: Nom du composant
            **metadata: MÃ©tadonnÃ©es additionnelles Ã  stocker
        """
        # Snapshot mÃ©moire avant
        mem_before = self.process.memory_info().rss / 1024 / 1024  # MB

        start = time.perf_counter()
        try:
            yield
        finally:
            elapsed = time.perf_counter() - start

            # Snapshot mÃ©moire aprÃ¨s
            mem_after = self.process.memory_info().rss / 1024 / 1024  # MB

            with self._lock:
                self.components[component_name].append(elapsed)
                self.memory_snapshots[component_name] = (
                    mem_before,
                    mem_after,
                    metadata,
                )

    def _diagnose_component(self, name: str, metrics: Dict) -> tuple[str, str, int]:
        """
        Diagnostique automatique d'un composant.

        Returns:
            (diagnostic, potential_gain, priority)
            diagnostic: "optimize", "monitor", "ok"
            potential_gain: Description du gain
            priority: 1 (haute), 2 (moyenne), 3 (basse)
        """
        percent = metrics["percent_of_total"]
        avg_time_ms = metrics["avg_time"] * 1000
        call_count = metrics["call_count"]

        # CritÃ¨res de diagnostic
        if percent > 30:
            # Composant dominant (>30% du temps total)
            if avg_time_ms > 100:
                return (
                    "optimize",
                    f"Gain potentiel: {percent:.1f}% du temps total. Optimisation prioritaire !",
                    1,
                )
            else:
                return (
                    "optimize",
                    f"Composant trÃ¨s sollicitÃ© ({call_count} appels). RÃ©duire appels ou parallÃ©liser.",
                    1,
                )

        elif percent > 10:
            # Composant significatif (10-30%)
            if avg_time_ms > 50:
                return (
                    "monitor",
                    f"Gain modÃ©rÃ© possible ({percent:.1f}%). Optimiser si facile.",
                    2,
                )
            elif call_count > 1000:
                return (
                    "monitor",
                    f"Nombreux appels ({call_count}). Envisager batching ou cache.",
                    2,
                )
            else:
                return (
                    "monitor",
                    f"Surveiller l'Ã©volution ({percent:.1f}% du temps).",
                    2,
                )

        elif percent > 3:
            # Composant mineur mais visible (3-10%)
            if avg_time_ms > 100:
                return (
                    "monitor",
                    f"Temps unitaire Ã©levÃ©. Gain faible ({percent:.1f}%) mais possible.",
                    3,
                )
            else:
                return (
                    "ok",
                    f"Impact faible ({percent:.1f}%). Aucune action prioritaire.",
                    3,
                )

        else:
            # Composant nÃ©gligeable (<3%)
            return (
                "ok",
                f"Impact nÃ©gligeable ({percent:.1f}%). Aucune action nÃ©cessaire.",
                3,
            )

    def generate_report(self) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re un rapport complet avec diagnostic.

        Returns:
            Dict avec toutes les mÃ©triques et diagnostics
        """
        if not self.total_duration and self.start_time:
            self.stop()

        components_metrics = []

        for name, timings in self.components.items():
            total_time = sum(timings)
            call_count = len(timings)
            avg_time = total_time / call_count if call_count > 0 else 0
            min_time = min(timings) if timings else 0
            max_time = max(timings) if timings else 0
            percent = (
                (total_time / self.total_duration * 100)
                if self.total_duration > 0
                else 0
            )

            # MÃ©moire
            mem_before, mem_after, metadata = self.memory_snapshots.get(
                name, (0, 0, {})
            )
            memory_delta = mem_after - mem_before

            # Diagnostic automatique
            metrics_dict = {
                "percent_of_total": percent,
                "avg_time": avg_time,
                "call_count": call_count,
            }
            diagnostic, potential_gain, priority = self._diagnose_component(
                name, metrics_dict
            )

            component = ComponentMetrics(
                name=name,
                total_time=total_time,
                call_count=call_count,
                avg_time=avg_time,
                min_time=min_time,
                max_time=max_time,
                percent_of_total=percent,
                memory_delta_mb=memory_delta,
                diagnostic=diagnostic,
                potential_gain=potential_gain,
                priority=priority,
                details=metadata,
            )
            components_metrics.append(component)

        # Tri par temps total dÃ©croissant
        components_metrics.sort(key=lambda x: x.total_time, reverse=True)

        # Statistiques globales
        total_measured = sum(c.total_time for c in components_metrics)
        overhead = self.total_duration - total_measured

        report = {
            "summary": {
                "total_duration": self.total_duration,
                "total_measured": total_measured,
                "overhead": overhead,
                "overhead_percent": (
                    (overhead / self.total_duration * 100)
                    if self.total_duration > 0
                    else 0
                ),
                "component_count": len(components_metrics),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            },
            "components": [asdict(c) for c in components_metrics],
            "recommendations": self._generate_recommendations(components_metrics),
        }

        return report

    def _generate_recommendations(
        self, components: List[ComponentMetrics]
    ) -> List[Dict[str, str]]:
        """GÃ©nÃ¨re des recommandations d'optimisation."""
        recommendations = []

        # Top 3 composants Ã  optimiser
        to_optimize = [c for c in components if c.diagnostic == "optimize"]
        to_optimize.sort(key=lambda x: x.priority)

        for i, comp in enumerate(to_optimize[:3], 1):
            recommendations.append(
                {
                    "rank": i,
                    "component": comp.name,
                    "action": "Optimiser en prioritÃ©",
                    "reason": comp.potential_gain,
                    "impact": f"Potentiel: {comp.percent_of_total:.1f}% du temps total",
                }
            )

        # Composants avec nombreux appels
        high_call_count = [
            c for c in components if c.call_count > 500 and c.percent_of_total > 5
        ]
        for comp in high_call_count[:2]:
            if comp.diagnostic != "optimize":  # Ã‰viter doublons
                recommendations.append(
                    {
                        "rank": len(recommendations) + 1,
                        "component": comp.name,
                        "action": "RÃ©duire nombre d'appels",
                        "reason": f"{comp.call_count} appels dÃ©tectÃ©s",
                        "impact": f"Batching ou cache pourrait rÃ©duire de ~{comp.percent_of_total/2:.1f}%",
                    }
                )

        # Composants avec variance Ã©levÃ©e (min/max trÃ¨s diffÃ©rents)
        for comp in components:
            if comp.max_time > comp.min_time * 10 and comp.call_count > 10:
                recommendations.append(
                    {
                        "rank": len(recommendations) + 1,
                        "component": comp.name,
                        "action": "Investiguer variance",
                        "reason": f"Max time {comp.max_time*1000:.1f}ms vs min {comp.min_time*1000:.1f}ms",
                        "impact": "Performance incohÃ©rente dÃ©tectÃ©e",
                    }
                )
                break  # Un seul suffit

        return recommendations

    def save_json_report(self, filepath: str):
        """Sauvegarde le rapport en JSON."""
        report = self.generate_report()
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"ðŸ“Š Rapport JSON sauvegardÃ©: {filepath}")

    def save_html_report(self, filepath: str = "performance_report.html"):
        """GÃ©nÃ¨re et sauvegarde un rapport HTML interactif."""
        report = self.generate_report()

        html = self._generate_html(report)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(html)

        print(f"ðŸŒ Rapport HTML sauvegardÃ©: {filepath}")

        # Ouvrir automatiquement dans le navigateur
        import webbrowser

        webbrowser.open(f"file://{os.path.abspath(filepath)}")

    def _generate_html(self, report: Dict) -> str:
        """GÃ©nÃ¨re le HTML du rapport."""
        components = report["components"]
        summary = report["summary"]
        recommendations = report["recommendations"]

        # GÃ©nÃ©ration des cartes de composants
        component_cards = []
        for comp in components:
            # Couleur selon diagnostic
            color_map = {
                "optimize": "#ef4444",  # Rouge
                "monitor": "#f59e0b",  # Orange
                "ok": "#10b981",  # Vert
            }
            bg_color_map = {
                "optimize": "#fee2e2",
                "monitor": "#fef3c7",
                "ok": "#d1fae5",
            }
            color = color_map.get(comp["diagnostic"], "#6b7280")
            bg_color = bg_color_map.get(comp["diagnostic"], "#f3f4f6")

            # IcÃ´ne selon diagnostic
            icon_map = {
                "optimize": "ðŸ”´",
                "monitor": "ðŸŸ¡",
                "ok": "ðŸŸ¢",
            }
            icon = icon_map.get(comp["diagnostic"], "âšª")

            card = f"""
            <div class="component-card" style="border-left: 4px solid {color}; background: {bg_color}">
                <div class="component-header">
                    <h3>{icon} {comp['name']}</h3>
                    <span class="percent">{comp['percent_of_total']:.1f}%</span>
                </div>
                <div class="metrics-grid">
                    <div class="metric">
                        <span class="label">Temps total</span>
                        <span class="value">{comp['total_time']:.3f}s</span>
                    </div>
                    <div class="metric">
                        <span class="label">Appels</span>
                        <span class="value">{comp['call_count']}</span>
                    </div>
                    <div class="metric">
                        <span class="label">Temps moyen</span>
                        <span class="value">{comp['avg_time']*1000:.2f}ms</span>
                    </div>
                    <div class="metric">
                        <span class="label">Min / Max</span>
                        <span class="value">{comp['min_time']*1000:.2f} / {comp['max_time']*1000:.2f}ms</span>
                    </div>
                </div>
                <div class="diagnostic">
                    <strong>Diagnostic:</strong> {comp['potential_gain']}
                </div>
                {f'<div class="memory">ðŸ’¾ MÃ©moire: {comp["memory_delta_mb"]:+.1f} MB</div>' if abs(comp['memory_delta_mb']) > 0.1 else ''}
            </div>
            """
            component_cards.append(card)

        # GÃ©nÃ©ration des recommandations
        recommendation_items = []
        for rec in recommendations:
            recommendation_items.append(
                f"""
            <div class="recommendation">
                <div class="rec-rank">#{rec['rank']}</div>
                <div class="rec-content">
                    <h4>{rec['component']}</h4>
                    <p><strong>{rec['action']}</strong></p>
                    <p>{rec['reason']}</p>
                    <p class="impact">{rec['impact']}</p>
                </div>
            </div>
            """
            )

        # Graphique de distribution (simple bar chart CSS)
        bars = []
        for comp in components[:10]:  # Top 10
            width = min(comp["percent_of_total"], 100)
            color_map = {
                "optimize": "#ef4444",
                "monitor": "#f59e0b",
                "ok": "#10b981",
            }
            color = color_map.get(comp["diagnostic"], "#6b7280")
            bars.append(
                f"""
            <div class="bar-item">
                <div class="bar-label">{comp['name'][:30]}</div>
                <div class="bar-container">
                    <div class="bar" style="width: {width}%; background: {color}"></div>
                    <span class="bar-value">{comp['percent_of_total']:.1f}%</span>
                </div>
            </div>
            """
            )

        html_template = f"""
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ThreadX Performance Report</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: #f9fafb;
            color: #111827;
            padding: 20px;
            line-height: 1.6;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 12px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        h1 {{
            font-size: 2.5rem;
            margin-bottom: 10px;
        }}
        .timestamp {{
            opacity: 0.9;
            font-size: 0.9rem;
        }}
        .summary {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .summary-card h3 {{
            color: #6b7280;
            font-size: 0.9rem;
            font-weight: 500;
            margin-bottom: 10px;
        }}
        .summary-card .value {{
            font-size: 2rem;
            font-weight: 700;
            color: #111827;
        }}
        .summary-card .unit {{
            font-size: 1rem;
            color: #6b7280;
            margin-left: 5px;
        }}
        .section {{
            background: white;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }}
        .section h2 {{
            font-size: 1.5rem;
            margin-bottom: 20px;
            color: #111827;
            border-bottom: 2px solid #e5e7eb;
            padding-bottom: 10px;
        }}
        .component-card {{
            background: white;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 15px;
            transition: transform 0.2s, box-shadow 0.2s;
        }}
        .component-card:hover {{
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }}
        .component-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 15px;
        }}
        .component-header h3 {{
            font-size: 1.2rem;
            color: #111827;
        }}
        .percent {{
            font-size: 1.5rem;
            font-weight: 700;
            color: #667eea;
        }}
        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin-bottom: 15px;
        }}
        .metric {{
            display: flex;
            flex-direction: column;
        }}
        .metric .label {{
            font-size: 0.85rem;
            color: #6b7280;
            margin-bottom: 5px;
        }}
        .metric .value {{
            font-size: 1.1rem;
            font-weight: 600;
            color: #111827;
        }}
        .diagnostic {{
            background: #f9fafb;
            padding: 12px;
            border-radius: 6px;
            font-size: 0.95rem;
            margin-top: 10px;
        }}
        .memory {{
            margin-top: 10px;
            font-size: 0.9rem;
            color: #6b7280;
        }}
        .recommendation {{
            display: flex;
            gap: 15px;
            padding: 15px;
            background: #fef3c7;
            border-left: 4px solid #f59e0b;
            border-radius: 6px;
            margin-bottom: 15px;
        }}
        .rec-rank {{
            font-size: 1.5rem;
            font-weight: 700;
            color: #f59e0b;
            min-width: 40px;
        }}
        .rec-content h4 {{
            color: #111827;
            margin-bottom: 8px;
        }}
        .rec-content p {{
            margin: 5px 0;
            font-size: 0.95rem;
        }}
        .impact {{
            color: #6b7280;
            font-style: italic;
        }}
        .bar-item {{
            margin-bottom: 15px;
        }}
        .bar-label {{
            font-size: 0.9rem;
            color: #111827;
            margin-bottom: 5px;
            font-weight: 500;
        }}
        .bar-container {{
            display: flex;
            align-items: center;
            gap: 10px;
        }}
        .bar {{
            height: 30px;
            background: #667eea;
            border-radius: 4px;
            transition: width 0.3s ease;
        }}
        .bar-value {{
            font-size: 0.9rem;
            font-weight: 600;
            color: #111827;
            min-width: 50px;
        }}
        .legend {{
            display: flex;
            gap: 20px;
            margin-top: 20px;
            padding: 15px;
            background: #f9fafb;
            border-radius: 6px;
        }}
        .legend-item {{
            display: flex;
            align-items: center;
            gap: 8px;
        }}
        .legend-color {{
            width: 20px;
            height: 20px;
            border-radius: 4px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>âš¡ ThreadX Performance Report</h1>
            <div class="timestamp">GÃ©nÃ©rÃ© le {summary['timestamp']}</div>
        </header>

        <div class="summary">
            <div class="summary-card">
                <h3>DurÃ©e Totale</h3>
                <div class="value">{summary['total_duration']:.3f}<span class="unit">s</span></div>
            </div>
            <div class="summary-card">
                <h3>Temps MesurÃ©</h3>
                <div class="value">{summary['total_measured']:.3f}<span class="unit">s</span></div>
            </div>
            <div class="summary-card">
                <h3>Overhead</h3>
                <div class="value">{summary['overhead_percent']:.1f}<span class="unit">%</span></div>
            </div>
            <div class="summary-card">
                <h3>Composants</h3>
                <div class="value">{summary['component_count']}</div>
            </div>
        </div>

        <div class="section">
            <h2>ðŸ“Š Distribution du Temps</h2>
            {''.join(bars)}
            <div class="legend">
                <div class="legend-item">
                    <div class="legend-color" style="background: #ef4444"></div>
                    <span>ðŸ”´ Ã€ optimiser en prioritÃ©</span>
                </div>
                <div class="legend-item">
                    <div class="legend-color" style="background: #f59e0b"></div>
                    <span>ðŸŸ¡ Ã€ surveiller</span>
                </div>
                <div class="legend-item">
                    <div class="legend-color" style="background: #10b981"></div>
                    <span>ðŸŸ¢ OK</span>
                </div>
            </div>
        </div>

        {f'''
        <div class="section">
            <h2>ðŸŽ¯ Recommandations Prioritaires</h2>
            {''.join(recommendation_items)}
        </div>
        ''' if recommendations else ''}

        <div class="section">
            <h2>ðŸ” DÃ©tails des Composants</h2>
            {''.join(component_cards)}
        </div>
    </div>
</body>
</html>
        """

        return html_template

    def print_summary(self):
        """Affiche un rÃ©sumÃ© dans la console."""
        report = self.generate_report()
        summary = report["summary"]
        components = report["components"]

        print("\n" + "=" * 80)
        print("âš¡ RAPPORT DE PERFORMANCE THREADX")
        print("=" * 80)
        print(f"DurÃ©e totale: {summary['total_duration']:.3f}s")
        print(f"Temps mesurÃ©: {summary['total_measured']:.3f}s")
        print(
            f"Overhead: {summary['overhead']:.3f}s ({summary['overhead_percent']:.1f}%)"
        )
        print(f"Composants: {summary['component_count']}")
        print("\n" + "-" * 80)
        print("TOP COMPOSANTS PAR TEMPS:")
        print("-" * 80)

        for i, comp in enumerate(components[:10], 1):
            icon_map = {"optimize": "ðŸ”´", "monitor": "ðŸŸ¡", "ok": "ðŸŸ¢"}
            icon = icon_map.get(comp["diagnostic"], "âšª")
            print(
                f"{i:2}. {icon} {comp['name']:30} {comp['percent_of_total']:6.2f}% ({comp['total_time']:.3f}s, {comp['call_count']} appels)"
            )

        if report["recommendations"]:
            print("\n" + "-" * 80)
            print("ðŸŽ¯ RECOMMANDATIONS:")
            print("-" * 80)
            for rec in report["recommendations"][:5]:
                print(f"{rec['rank']}. [{rec['component']}] {rec['action']}")
                print(f"   â†’ {rec['reason']}")
                print(f"   ðŸ’¡ {rec['impact']}\n")

        print("=" * 80 + "\n")

----------------------------------------
Fichier: strategy\amplitude_hunter.py
"""
ThreadX Strategy - AmplitudeHunter (BB Amplitude Rider)
========================================================

StratÃ©gie avancÃ©e de capture d'amplitude complÃ¨te sur Bollinger Bands.

Concept: Capturer l'amplitude complÃ¨te d'un swing BB (basse â†’ mÃ©diane â†’ haute â†’ extension)
et laisser courir au-delÃ  de la bande opposÃ©e quand le momentum le permet.

FonctionnalitÃ©s:
- Filtre de rÃ©gime multi-critÃ¨res (BBWidth percentile, Volume z-score, ADX optionnel)
- Setup "Spring â†’ Drive" avec dÃ©tection sÃ©quentielle MACD
- Score d'Amplitude pour modulation de l'agressivitÃ©
- Pyramiding intelligent (jusqu'Ã  2 adds)
- Trailing stop conditionnel basÃ© sur %B et MACD
- Cible dynamique BIP (Bollinger Implied Price)
- Stop loss spÃ©cifique pour positions SHORT

Auteur: Claude (Anthropic)
Version: 1.0.0
Date: 2025
"""

from dataclasses import dataclass, field
from typing import Dict, Any, Tuple, Optional, List
import pandas as pd
import numpy as np
import logging
from pathlib import Path

from threadx.configuration.settings import S
from threadx.utils.log import get_logger
from threadx.strategy.model import (
    Strategy,
    Trade,
    RunStats,
    validate_ohlcv_dataframe,
    validate_strategy_params,
)
from threadx.indicators import ensure_indicator

logger = get_logger(__name__)

# ==========================================
# STRATEGY PARAMETERS
# ==========================================


@dataclass
class AmplitudeHunterParams:
    """
    ParamÃ¨tres de la stratÃ©gie AmplitudeHunter (BB Amplitude Rider).

    Attributes:
        # Bollinger Bands
        bb_period: PÃ©riode pour moyennes mobiles (dÃ©faut: 20)
        bb_std: Multiplicateur Ã©cart-type pour bandes (dÃ©faut: 2.0)

        # Filtre de rÃ©gime
        bbwidth_percentile_threshold: Seuil percentile BBWidth (dÃ©faut: 50, range: 30-70)
        bbwidth_lookback: PÃ©riode lookback pour percentile BBWidth (dÃ©faut: 100)
        volume_zscore_threshold: Seuil volume z-score (dÃ©faut: 0.5)
        volume_lookback: PÃ©riode lookback pour volume z-score (dÃ©faut: 50)
        use_adx_filter: Activer filtre ADX (dÃ©faut: False)
        adx_threshold: Seuil ADX minimum (dÃ©faut: 15)
        adx_period: PÃ©riode ADX (dÃ©faut: 14)

        # Setup Spring â†’ Drive
        spring_lookback: Lookback pour dÃ©tecter spring (dÃ©faut: 20)
        pb_entry_threshold_min: %B minimum pour entrÃ©e (dÃ©faut: 0.2)
        pb_entry_threshold_max: %B maximum pour entrÃ©e (dÃ©faut: 0.5)
        macd_fast: PÃ©riode MACD rapide (dÃ©faut: 12)
        macd_slow: PÃ©riode MACD lente (dÃ©faut: 26)
        macd_signal: PÃ©riode signal MACD (dÃ©faut: 9)

        # Score d'Amplitude
        amplitude_score_threshold: Score minimum pour trade (dÃ©faut: 0.6)
        amplitude_w1_bbwidth: Poids BBWidth percentile (dÃ©faut: 0.3)
        amplitude_w2_pb: Poids |%B| (dÃ©faut: 0.2)
        amplitude_w3_macd_slope: Poids pente MACD (dÃ©faut: 0.3)
        amplitude_w4_volume: Poids volume z-score (dÃ©faut: 0.2)

        # Pyramiding
        pyramiding_enabled: Activer pyramiding (dÃ©faut: False)
        pyramiding_max_adds: Nombre max d'adds (1 ou 2, dÃ©faut: 1)

        # Stops et trailing
        atr_period: PÃ©riode ATR pour stops (dÃ©faut: 14)
        sl_atr_multiplier: Multiplicateur ATR pour SL initial (dÃ©faut: 2.0)
        sl_min_pct: SL minimum en % (mÃ©diane-basse) (dÃ©faut: 0.37)
        short_stop_pct: Stop loss fixe pour SHORT (dÃ©faut: 0.37, soit 37%)

        trailing_activation_pb_threshold: %B seuil pour activer trailing (dÃ©faut: 1.0)
        trailing_activation_gain_r: Gain en R pour activer trailing (dÃ©faut: 1.0)
        trailing_type: Type de trailing ("chandelier" | "pb_floor" | "macd_fade") (dÃ©faut: "chandelier")
        trailing_chandelier_atr_mult: Mult ATR pour chandelier (dÃ©faut: 2.5)
        trailing_pb_floor: %B floor pour sortie (dÃ©faut: 0.5)

        # Cible intelligente (BIP)
        use_bip_target: Utiliser cible BIP (dÃ©faut: True)
        bip_partial_exit_pct: % Ã  sortir Ã  BIP (dÃ©faut: 0.5, soit 50%)

        # Risk Management
        risk_per_trade: Risque par trade en fraction du capital (dÃ©faut: 0.02 = 2%)
        max_hold_bars: DurÃ©e max position en barres (dÃ©faut: 100)
        leverage: Effet de levier (dÃ©faut: 1.0)

        # MÃ©tadonnÃ©es
        meta: Dictionnaire mÃ©tadonnÃ©es personnalisÃ©es

    Example:
        >>> params = AmplitudeHunterParams(
        ...     bb_period=20, bb_std=2.0,
        ...     amplitude_score_threshold=0.7,
        ...     pyramiding_enabled=True,
        ...     pyramiding_max_adds=2
        ... )
        >>> strategy = AmplitudeHunterStrategy()
        >>> signals = strategy.generate_signals(df, params.to_dict())
    """

    # Bollinger Bands
    bb_period: int = 20
    bb_std: float = 2.0

    # Filtre de rÃ©gime
    bbwidth_percentile_threshold: float = 50.0  # 30-70 recommandÃ©
    bbwidth_lookback: int = 100
    volume_zscore_threshold: float = 0.5
    volume_lookback: int = 50
    use_adx_filter: bool = False
    adx_threshold: float = 15.0
    adx_period: int = 14

    # Setup Spring â†’ Drive
    spring_lookback: int = 20
    pb_entry_threshold_min: float = 0.2  # %B min pour entrÃ©e
    pb_entry_threshold_max: float = 0.5  # %B max pour entrÃ©e
    macd_fast: int = 12
    macd_slow: int = 26
    macd_signal: int = 9

    # Score d'Amplitude
    amplitude_score_threshold: float = 0.6
    amplitude_w1_bbwidth: float = 0.3
    amplitude_w2_pb: float = 0.2
    amplitude_w3_macd_slope: float = 0.3
    amplitude_w4_volume: float = 0.2

    # Pyramiding
    pyramiding_enabled: bool = False
    pyramiding_max_adds: int = 1  # 1 ou 2

    # Stops et trailing
    atr_period: int = 14
    sl_atr_multiplier: float = 2.0
    sl_min_pct: float = 0.37  # % de (mÃ©diane - basse) pour SL min
    short_stop_pct: float = 0.37  # 37% au-dessus du prix d'entrÃ©e SHORT

    trailing_activation_pb_threshold: float = 1.0  # %B > 1 pour activer
    trailing_activation_gain_r: float = 1.0  # Gain >= 1R pour activer
    trailing_type: str = "chandelier"  # "chandelier" | "pb_floor" | "macd_fade"
    trailing_chandelier_atr_mult: float = 2.5
    trailing_pb_floor: float = 0.5

    # Cible BIP
    use_bip_target: bool = True
    bip_partial_exit_pct: float = 0.5  # 50% sortie partielle

    # Risk Management
    risk_per_trade: float = 0.02  # 2% du capital
    max_hold_bars: int = 100
    leverage: float = 1.0

    # MÃ©tadonnÃ©es
    meta: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validation des paramÃ¨tres"""
        # Bollinger
        if self.bb_period < 2:
            raise ValueError(f"bb_period must be >= 2, got: {self.bb_period}")
        if self.bb_std <= 0:
            raise ValueError(f"bb_std must be > 0, got: {self.bb_std}")

        # Filtre rÃ©gime
        if not 0 <= self.bbwidth_percentile_threshold <= 100:
            raise ValueError(
                f"bbwidth_percentile_threshold must be in [0, 100], got: {self.bbwidth_percentile_threshold}"
            )
        if self.bbwidth_lookback < 10:
            raise ValueError(
                f"bbwidth_lookback must be >= 10, got: {self.bbwidth_lookback}"
            )
        if self.volume_lookback < 10:
            raise ValueError(
                f"volume_lookback must be >= 10, got: {self.volume_lookback}"
            )
        if self.adx_threshold < 0:
            raise ValueError(f"adx_threshold must be >= 0, got: {self.adx_threshold}")

        # %B thresholds
        if not 0 <= self.pb_entry_threshold_min <= 1:
            raise ValueError(
                f"pb_entry_threshold_min must be in [0, 1], got: {self.pb_entry_threshold_min}"
            )
        if not 0 <= self.pb_entry_threshold_max <= 1:
            raise ValueError(
                f"pb_entry_threshold_max must be in [0, 1], got: {self.pb_entry_threshold_max}"
            )
        if self.pb_entry_threshold_min > self.pb_entry_threshold_max:
            raise ValueError(
                f"pb_entry_threshold_min must be <= pb_entry_threshold_max"
            )

        # MACD
        if self.macd_fast <= 0 or self.macd_slow <= 0 or self.macd_signal <= 0:
            raise ValueError("MACD periods must be > 0")
        if self.macd_fast >= self.macd_slow:
            raise ValueError("macd_fast must be < macd_slow")

        # Score amplitude
        if not 0 <= self.amplitude_score_threshold <= 1:
            raise ValueError(
                f"amplitude_score_threshold must be in [0, 1], got: {self.amplitude_score_threshold}"
            )
        # VÃ©rifier que les poids somment Ã  ~1.0
        total_weight = (
            self.amplitude_w1_bbwidth
            + self.amplitude_w2_pb
            + self.amplitude_w3_macd_slope
            + self.amplitude_w4_volume
        )
        if not 0.95 <= total_weight <= 1.05:
            logger.warning(
                f"Amplitude weights sum to {total_weight:.3f}, should be ~1.0"
            )

        # Pyramiding
        if self.pyramiding_max_adds not in [1, 2]:
            raise ValueError(f"pyramiding_max_adds must be 1 or 2, got: {self.pyramiding_max_adds}")

        # Stops
        if self.atr_period < 1:
            raise ValueError(f"atr_period must be >= 1, got: {self.atr_period}")
        if self.sl_atr_multiplier <= 0:
            raise ValueError(
                f"sl_atr_multiplier must be > 0, got: {self.sl_atr_multiplier}"
            )
        if not 0 < self.sl_min_pct <= 1:
            raise ValueError(f"sl_min_pct must be in (0, 1], got: {self.sl_min_pct}")
        if not 0 < self.short_stop_pct <= 1:
            raise ValueError(
                f"short_stop_pct must be in (0, 1], got: {self.short_stop_pct}"
            )

        # Trailing
        if self.trailing_type not in ["chandelier", "pb_floor", "macd_fade"]:
            raise ValueError(
                f"trailing_type must be 'chandelier', 'pb_floor', or 'macd_fade', got: {self.trailing_type}"
            )

        # BIP
        if not 0 <= self.bip_partial_exit_pct <= 1:
            raise ValueError(
                f"bip_partial_exit_pct must be in [0, 1], got: {self.bip_partial_exit_pct}"
            )

        # Risk
        if not 0 < self.risk_per_trade <= 1:
            raise ValueError(
                f"risk_per_trade must be in (0, 1], got: {self.risk_per_trade}"
            )
        if self.max_hold_bars < 1:
            raise ValueError(
                f"max_hold_bars must be >= 1, got: {self.max_hold_bars}"
            )
        if self.leverage <= 0:
            raise ValueError(f"leverage must be > 0, got: {self.leverage}")

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire pour compatibilitÃ©"""
        return {
            # Bollinger
            "bb_period": self.bb_period,
            "bb_std": self.bb_std,
            # Filtre rÃ©gime
            "bbwidth_percentile_threshold": self.bbwidth_percentile_threshold,
            "bbwidth_lookback": self.bbwidth_lookback,
            "volume_zscore_threshold": self.volume_zscore_threshold,
            "volume_lookback": self.volume_lookback,
            "use_adx_filter": self.use_adx_filter,
            "adx_threshold": self.adx_threshold,
            "adx_period": self.adx_period,
            # Setup
            "spring_lookback": self.spring_lookback,
            "pb_entry_threshold_min": self.pb_entry_threshold_min,
            "pb_entry_threshold_max": self.pb_entry_threshold_max,
            "macd_fast": self.macd_fast,
            "macd_slow": self.macd_slow,
            "macd_signal": self.macd_signal,
            # Score amplitude
            "amplitude_score_threshold": self.amplitude_score_threshold,
            "amplitude_w1_bbwidth": self.amplitude_w1_bbwidth,
            "amplitude_w2_pb": self.amplitude_w2_pb,
            "amplitude_w3_macd_slope": self.amplitude_w3_macd_slope,
            "amplitude_w4_volume": self.amplitude_w4_volume,
            # Pyramiding
            "pyramiding_enabled": self.pyramiding_enabled,
            "pyramiding_max_adds": self.pyramiding_max_adds,
            # Stops
            "atr_period": self.atr_period,
            "sl_atr_multiplier": self.sl_atr_multiplier,
            "sl_min_pct": self.sl_min_pct,
            "short_stop_pct": self.short_stop_pct,
            "trailing_activation_pb_threshold": self.trailing_activation_pb_threshold,
            "trailing_activation_gain_r": self.trailing_activation_gain_r,
            "trailing_type": self.trailing_type,
            "trailing_chandelier_atr_mult": self.trailing_chandelier_atr_mult,
            "trailing_pb_floor": self.trailing_pb_floor,
            # BIP
            "use_bip_target": self.use_bip_target,
            "bip_partial_exit_pct": self.bip_partial_exit_pct,
            # Risk
            "risk_per_trade": self.risk_per_trade,
            "max_hold_bars": self.max_hold_bars,
            "leverage": self.leverage,
            # Meta
            "meta": self.meta,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "AmplitudeHunterParams":
        """CrÃ©e depuis un dictionnaire"""
        return cls(
            # Bollinger
            bb_period=data.get("bb_period", 20),
            bb_std=data.get("bb_std", 2.0),
            # Filtre rÃ©gime
            bbwidth_percentile_threshold=data.get("bbwidth_percentile_threshold", 50.0),
            bbwidth_lookback=data.get("bbwidth_lookback", 100),
            volume_zscore_threshold=data.get("volume_zscore_threshold", 0.5),
            volume_lookback=data.get("volume_lookback", 50),
            use_adx_filter=data.get("use_adx_filter", False),
            adx_threshold=data.get("adx_threshold", 15.0),
            adx_period=data.get("adx_period", 14),
            # Setup
            spring_lookback=data.get("spring_lookback", 20),
            pb_entry_threshold_min=data.get("pb_entry_threshold_min", 0.2),
            pb_entry_threshold_max=data.get("pb_entry_threshold_max", 0.5),
            macd_fast=data.get("macd_fast", 12),
            macd_slow=data.get("macd_slow", 26),
            macd_signal=data.get("macd_signal", 9),
            # Score
            amplitude_score_threshold=data.get("amplitude_score_threshold", 0.6),
            amplitude_w1_bbwidth=data.get("amplitude_w1_bbwidth", 0.3),
            amplitude_w2_pb=data.get("amplitude_w2_pb", 0.2),
            amplitude_w3_macd_slope=data.get("amplitude_w3_macd_slope", 0.3),
            amplitude_w4_volume=data.get("amplitude_w4_volume", 0.2),
            # Pyramiding
            pyramiding_enabled=data.get("pyramiding_enabled", False),
            pyramiding_max_adds=data.get("pyramiding_max_adds", 1),
            # Stops
            atr_period=data.get("atr_period", 14),
            sl_atr_multiplier=data.get("sl_atr_multiplier", 2.0),
            sl_min_pct=data.get("sl_min_pct", 0.37),
            short_stop_pct=data.get("short_stop_pct", 0.37),
            trailing_activation_pb_threshold=data.get(
                "trailing_activation_pb_threshold", 1.0
            ),
            trailing_activation_gain_r=data.get("trailing_activation_gain_r", 1.0),
            trailing_type=data.get("trailing_type", "chandelier"),
            trailing_chandelier_atr_mult=data.get("trailing_chandelier_atr_mult", 2.5),
            trailing_pb_floor=data.get("trailing_pb_floor", 0.5),
            # BIP
            use_bip_target=data.get("use_bip_target", True),
            bip_partial_exit_pct=data.get("bip_partial_exit_pct", 0.5),
            # Risk
            risk_per_trade=data.get("risk_per_trade", 0.02),
            max_hold_bars=data.get("max_hold_bars", 100),
            leverage=data.get("leverage", 1.0),
            # Meta
            meta=data.get("meta", {}),
        )


# ==========================================
# STRATEGY IMPLEMENTATION
# ==========================================


class AmplitudeHunterStrategy:
    """
    ImplÃ©mentation de la stratÃ©gie AmplitudeHunter (BB Amplitude Rider).

    Logique de trading:
    1. Filtre de rÃ©gime: BBWidth percentile, Volume z-score, ADX (optionnel)
    2. Calcul Score d'Amplitude pour modulation
    3. DÃ©tection setup Spring â†’ Drive:
       - LONG: spring (close < bb_lower rÃ©cent) + MACD ralentissementâ†’impulsion + %B franchit seuil
       - SHORT: symÃ©trique (close > bb_upper + MACD + %B)
    4. Pyramiding conditionnel (jusqu'Ã  2 adds)
    5. Gestion stops:
       - SL initial: max(swing_low - k*ATR, pct*(mÃ©diane-basse))
       - Trailing conditionnel: activÃ© quand %B > 1 OU gain >= 1R
       - Stop fixe pour SHORT: 37% au-dessus du prix d'entrÃ©e
    6. Cible BIP (Bollinger Implied Price): mÃ©diane + (mÃ©diane - basse)

    Example:
        >>> strategy = AmplitudeHunterStrategy("BTCUSDT", "1h")
        >>> params = AmplitudeHunterParams(amplitude_score_threshold=0.7)
        >>> signals = strategy.generate_signals(df, params.to_dict())
        >>> equity, stats = strategy.backtest(df, params.to_dict(), 10000)
    """

    def __init__(self, symbol: str = "UNKNOWN", timeframe: str = "1h"):
        """
        Initialise la stratÃ©gie.

        Args:
            symbol: Symbole pour cache d'indicateurs
            timeframe: Timeframe pour cache d'indicateurs
        """
        self.symbol = symbol
        self.timeframe = timeframe
        logger.info(
            f"StratÃ©gie AmplitudeHunter initialisÃ©e: {symbol}/{timeframe}"
        )

    # --- Indicateurs techniques ---

    def _calculate_macd(
        self, close: np.ndarray, fast: int, slow: int, signal: int
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Calcule MACD (ligne MACD, signal, histogramme).

        Args:
            close: Prix de clÃ´ture
            fast: PÃ©riode EMA rapide
            slow: PÃ©riode EMA lente
            signal: PÃ©riode signal

        Returns:
            Tuple (macd_line, signal_line, histogram)
        """
        close_series = pd.Series(close)
        ema_fast = close_series.ewm(span=fast, adjust=False).mean().values
        ema_slow = close_series.ewm(span=slow, adjust=False).mean().values

        macd_line = ema_fast - ema_slow
        signal_line = (
            pd.Series(macd_line).ewm(span=signal, adjust=False).mean().values
        )
        histogram = macd_line - signal_line

        return macd_line, signal_line, histogram

    def _calculate_adx(
        self, high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int
    ) -> np.ndarray:
        """
        Calcule l'Average Directional Index (ADX).

        Args:
            high: Prix hauts
            low: Prix bas
            close: Prix de clÃ´ture
            period: PÃ©riode ADX

        Returns:
            Array ADX
        """
        # True Range
        tr1 = high - low
        tr2 = np.abs(high - np.roll(close, 1))
        tr3 = np.abs(low - np.roll(close, 1))
        tr = np.maximum(tr1, np.maximum(tr2, tr3))
        tr[0] = tr1[0]  # Premier Ã©lÃ©ment

        # Directional Movement
        up_move = high - np.roll(high, 1)
        down_move = np.roll(low, 1) - low
        up_move[0] = down_move[0] = 0

        plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0)
        minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0)

        # Smoothed TR, +DM, -DM
        atr_smooth = pd.Series(tr).ewm(span=period, adjust=False).mean().values
        plus_di_smooth = (
            pd.Series(plus_dm).ewm(span=period, adjust=False).mean().values
        )
        minus_di_smooth = (
            pd.Series(minus_dm).ewm(span=period, adjust=False).mean().values
        )

        # +DI, -DI
        plus_di = 100 * plus_di_smooth / atr_smooth
        minus_di = 100 * minus_di_smooth / atr_smooth

        # DX
        dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)

        # ADX
        adx = pd.Series(dx).ewm(span=period, adjust=False).mean().values

        return adx

    def _calculate_percent_b(
        self, close: np.ndarray, bb_upper: np.ndarray, bb_lower: np.ndarray
    ) -> np.ndarray:
        """
        Calcule %B (position du prix dans les bandes).

        Args:
            close: Prix de clÃ´ture
            bb_upper: Bande Bollinger supÃ©rieure
            bb_lower: Bande Bollinger infÃ©rieure

        Returns:
            Array %B
        """
        bb_width = bb_upper - bb_lower
        percent_b = (close - bb_lower) / (bb_width + 1e-10)  # Ã‰viter division par 0
        return percent_b

    def _calculate_bbwidth_percentile(
        self, bb_width: np.ndarray, lookback: int
    ) -> np.ndarray:
        """
        Calcule le percentile du BBWidth sur une pÃ©riode lookback.

        Args:
            bb_width: BBWidth actuel
            lookback: PÃ©riode de lookback

        Returns:
            Array de percentiles
        """
        percentiles = np.full(len(bb_width), np.nan)

        for i in range(lookback, len(bb_width)):
            window = bb_width[i - lookback : i]
            current = bb_width[i]
            # Percentile = proportion des valeurs <= current
            percentile = (np.sum(window <= current) / len(window)) * 100
            percentiles[i] = percentile

        return percentiles

    def _calculate_volume_zscore(
        self, volume: np.ndarray, lookback: int
    ) -> np.ndarray:
        """
        Calcule le z-score du volume.

        Args:
            volume: Volume
            lookback: PÃ©riode de lookback

        Returns:
            Array de z-scores
        """
        zscores = np.full(len(volume), np.nan)

        for i in range(lookback, len(volume)):
            window = volume[i - lookback : i]
            mean = np.mean(window)
            std = np.std(window)
            if std > 0:
                zscores[i] = (volume[i] - mean) / std
            else:
                zscores[i] = 0.0

        return zscores

    def _calculate_amplitude_score(
        self,
        bbwidth_pct: np.ndarray,
        percent_b: np.ndarray,
        macd_hist: np.ndarray,
        volume_zscore: np.ndarray,
        params: AmplitudeHunterParams,
    ) -> np.ndarray:
        """
        Calcule le Score d'Amplitude pour modulation de l'agressivitÃ©.

        Score = w1*BBWidth_pct/100 + w2*|%B| + w3*|pente_MACD| + w4*volume_zscore_norm

        Args:
            bbwidth_pct: BBWidth en percentile [0, 100]
            percent_b: %B
            macd_hist: MACD histogram
            volume_zscore: Volume z-score
            params: ParamÃ¨tres

        Returns:
            Array de scores [0, 1]
        """
        n = len(bbwidth_pct)
        scores = np.full(n, np.nan)

        # Normalisation des composantes
        bbwidth_norm = bbwidth_pct / 100.0  # [0, 1]
        pb_norm = np.abs(percent_b)  # |%B| peut Ãªtre > 1, on clip Ã  1
        pb_norm = np.clip(pb_norm, 0, 1)

        # Pente MACD = diffÃ©rence histogram (normalisÃ©e)
        macd_slope = np.abs(np.diff(macd_hist, prepend=macd_hist[0]))
        # Normalisation empirique (on clip Ã  1)
        macd_slope_norm = np.clip(macd_slope / (np.nanmax(macd_slope) + 1e-10), 0, 1)

        # Volume zscore normalisÃ© (clip entre 0 et 1, zscore peut Ãªtre nÃ©gatif)
        volume_norm = np.clip(volume_zscore / 3.0, 0, 1)  # z-score de 3 = 1.0

        # Calcul score
        for i in range(n):
            if not np.isnan(bbwidth_norm[i]):
                scores[i] = (
                    params.amplitude_w1_bbwidth * bbwidth_norm[i]
                    + params.amplitude_w2_pb * pb_norm[i]
                    + params.amplitude_w3_macd_slope * macd_slope_norm[i]
                    + params.amplitude_w4_volume * volume_norm[i]
                )

        return scores

    def _detect_spring_long(
        self,
        close: np.ndarray,
        bb_lower: np.ndarray,
        lookback: int,
    ) -> np.ndarray:
        """
        DÃ©tecte un "spring" pour LONG: close < bb_lower dans les M derniÃ¨res barres.

        Args:
            close: Prix de clÃ´ture
            bb_lower: Bande Bollinger infÃ©rieure
            lookback: PÃ©riode de lookback

        Returns:
            Boolean array True si spring dÃ©tectÃ©
        """
        n = len(close)
        spring = np.full(n, False)

        for i in range(lookback, n):
            # Regarder les M derniÃ¨res barres
            window_close = close[i - lookback : i]
            window_lower = bb_lower[i - lookback : i]
            # Spring si au moins une barre en dessous
            if np.any(window_close < window_lower):
                spring[i] = True

        return spring

    def _detect_spring_short(
        self,
        close: np.ndarray,
        bb_upper: np.ndarray,
        lookback: int,
    ) -> np.ndarray:
        """
        DÃ©tecte un "spring" pour SHORT: close > bb_upper dans les M derniÃ¨res barres.
        """
        n = len(close)
        spring = np.full(n, False)

        for i in range(lookback, n):
            window_close = close[i - lookback : i]
            window_upper = bb_upper[i - lookback : i]
            if np.any(window_close > window_upper):
                spring[i] = True

        return spring

    def _detect_macd_impulse_long(
        self, macd_hist: np.ndarray
    ) -> np.ndarray:
        """
        DÃ©tecte une impulsion MACD pour LONG:
        - MACD histo passe rouge foncÃ© â†’ rouge clair (ralentissement)
        - Puis >= 1 barre verte (impulsion)

        SimplifiÃ©: MACD histo nÃ©gatif qui augmente puis passe positif

        Returns:
            Boolean array True si impulsion dÃ©tectÃ©e
        """
        n = len(macd_hist)
        impulse = np.full(n, False)

        for i in range(2, n):
            # Condition: histo Ã©tait nÃ©gatif, augmente, puis positif
            if (
                macd_hist[i - 2] < 0  # Rouge foncÃ© (trÃ¨s nÃ©gatif)
                and macd_hist[i - 1] < 0  # Rouge clair (moins nÃ©gatif)
                and macd_hist[i - 1] > macd_hist[i - 2]  # Ralentissement
                and macd_hist[i] > 0  # Barre verte (impulsion)
            ):
                impulse[i] = True

        return impulse

    def _detect_macd_impulse_short(
        self, macd_hist: np.ndarray
    ) -> np.ndarray:
        """
        DÃ©tecte une impulsion MACD pour SHORT (symÃ©trique):
        - MACD histo positif qui diminue puis passe nÃ©gatif
        """
        n = len(macd_hist)
        impulse = np.full(n, False)

        for i in range(2, n):
            if (
                macd_hist[i - 2] > 0  # Vert foncÃ©
                and macd_hist[i - 1] > 0  # Vert clair
                and macd_hist[i - 1] < macd_hist[i - 2]  # Ralentissement
                and macd_hist[i] < 0  # Barre rouge (impulsion)
            ):
                impulse[i] = True

        return impulse

    def _ensure_indicators(
        self, df: pd.DataFrame, params: AmplitudeHunterParams
    ) -> pd.DataFrame:
        """
        Garantit la disponibilitÃ© de tous les indicateurs.

        Args:
            df: DataFrame OHLCV
            params: ParamÃ¨tres stratÃ©gie

        Returns:
            DataFrame enrichi avec tous les indicateurs
        """
        logger.debug(
            f"Calcul indicateurs: BB(period={params.bb_period}, std={params.bb_std}), "
            f"MACD({params.macd_fast},{params.macd_slow},{params.macd_signal}), ATR({params.atr_period})"
        )

        df_ind = df.copy()

        # 1. Bollinger Bands via IndicatorBank
        bb_result = ensure_indicator(
            "bollinger",
            {"period": params.bb_period, "std": params.bb_std},
            df,
            symbol=self.symbol,
            timeframe=self.timeframe,
        )

        if isinstance(bb_result, tuple) and len(bb_result) == 3:
            upper, middle, lower = bb_result
            df_ind["bb_upper"] = upper
            df_ind["bb_middle"] = middle
            df_ind["bb_lower"] = lower
        else:
            raise ValueError(f"Bollinger result format invalide: {type(bb_result)}")

        # 2. ATR via IndicatorBank
        atr_result = ensure_indicator(
            "atr",
            {"period": params.atr_period, "method": "ema"},
            df,
            symbol=self.symbol,
            timeframe=self.timeframe,
        )

        if isinstance(atr_result, np.ndarray):
            df_ind["atr"] = atr_result
        else:
            raise ValueError(f"ATR result format invalide: {type(atr_result)}")

        # 3. %B
        df_ind["percent_b"] = self._calculate_percent_b(
            df["close"].values, upper, lower
        )

        # 4. BBWidth
        df_ind["bb_width"] = upper - lower

        # 5. BBWidth percentile
        df_ind["bbwidth_percentile"] = self._calculate_bbwidth_percentile(
            df_ind["bb_width"].values, params.bbwidth_lookback
        )

        # 6. MACD
        macd_line, signal_line, histogram = self._calculate_macd(
            df["close"].values, params.macd_fast, params.macd_slow, params.macd_signal
        )
        df_ind["macd_line"] = macd_line
        df_ind["macd_signal"] = signal_line
        df_ind["macd_hist"] = histogram

        # 7. Volume z-score
        df_ind["volume_zscore"] = self._calculate_volume_zscore(
            df["volume"].values, params.volume_lookback
        )

        # 8. ADX (si activÃ©)
        if params.use_adx_filter:
            df_ind["adx"] = self._calculate_adx(
                df["high"].values,
                df["low"].values,
                df["close"].values,
                params.adx_period,
            )

        # 9. Score d'Amplitude
        df_ind["amplitude_score"] = self._calculate_amplitude_score(
            df_ind["bbwidth_percentile"].values,
            df_ind["percent_b"].values,
            df_ind["macd_hist"].values,
            df_ind["volume_zscore"].values,
            params,
        )

        logger.debug(
            f"Indicateurs calculÃ©s: {len(df_ind)} barres enrichies avec "
            f"{len([c for c in df_ind.columns if c not in df.columns])} nouveaux indicateurs"
        )

        return df_ind

    def generate_signals(
        self, df: pd.DataFrame, params: dict
    ) -> pd.DataFrame:
        """
        GÃ©nÃ¨re les signaux de trading basÃ©s sur AmplitudeHunter.

        Args:
            df: DataFrame OHLCV avec timestamp index (UTC)
            params: Dictionnaire paramÃ¨tres (format AmplitudeHunterParams.to_dict())

        Returns:
            DataFrame avec colonne 'signal' et mÃ©tadonnÃ©es

        Signals gÃ©nÃ©rÃ©s:
        - "ENTER_LONG": Setup Springâ†’Drive LONG validÃ©
        - "ENTER_SHORT": Setup Springâ†’Drive SHORT validÃ©
        - "HOLD": Pas de signal
        """
        logger.info(f"GÃ©nÃ©ration signaux AmplitudeHunter: {len(df)} barres")

        # Validation inputs
        validate_ohlcv_dataframe(df)
        validate_strategy_params(
            params,
            ["bb_period", "bb_std", "amplitude_score_threshold"],
        )

        # Parse paramÃ¨tres
        strategy_params = AmplitudeHunterParams.from_dict(params)

        # Ensure indicateurs
        df_with_indicators = self._ensure_indicators(df, strategy_params)

        # Extraction des donnÃ©es
        close = df["close"].values
        n_bars = len(df)

        # Extraction indicateurs
        bb_upper = df_with_indicators["bb_upper"].values
        bb_lower = df_with_indicators["bb_lower"].values
        bb_middle = df_with_indicators["bb_middle"].values
        percent_b = df_with_indicators["percent_b"].values
        bbwidth_pct = df_with_indicators["bbwidth_percentile"].values
        macd_hist = df_with_indicators["macd_hist"].values
        volume_zscore = df_with_indicators["volume_zscore"].values
        amplitude_score = df_with_indicators["amplitude_score"].values

        # Initialisation signaux
        signals = np.full(n_bars, "HOLD", dtype=object)

        # --- Filtre de rÃ©gime ---
        regime_filter = np.full(n_bars, True)

        # BBWidth percentile >= seuil
        regime_filter &= bbwidth_pct >= strategy_params.bbwidth_percentile_threshold

        # Volume z-score >= seuil
        regime_filter &= volume_zscore >= strategy_params.volume_zscore_threshold

        # ADX >= seuil (si activÃ©)
        if strategy_params.use_adx_filter:
            adx = df_with_indicators["adx"].values
            regime_filter &= adx >= strategy_params.adx_threshold

        # Score amplitude >= seuil
        regime_filter &= amplitude_score >= strategy_params.amplitude_score_threshold

        # --- DÃ©tection springs ---
        spring_long = self._detect_spring_long(
            close, bb_lower, strategy_params.spring_lookback
        )
        spring_short = self._detect_spring_short(
            close, bb_upper, strategy_params.spring_lookback
        )

        # --- DÃ©tection impulsions MACD ---
        macd_impulse_long = self._detect_macd_impulse_long(macd_hist)
        macd_impulse_short = self._detect_macd_impulse_short(macd_hist)

        # --- GÃ©nÃ©ration signaux LONG ---
        logger.debug("Application logique signaux LONG")

        for i in range(max(strategy_params.bb_period, strategy_params.spring_lookback), n_bars):
            # Skip si filtre rÃ©gime Ã©choue
            if not regime_filter[i]:
                continue

            # Skip si NaN
            if np.isnan(percent_b[i]) or np.isnan(macd_hist[i]):
                continue

            # Signal LONG
            if (
                spring_long[i]  # Spring dÃ©tectÃ©
                and macd_impulse_long[i]  # Impulsion MACD
                and strategy_params.pb_entry_threshold_min
                <= percent_b[i]
                <= strategy_params.pb_entry_threshold_max  # %B dans range
            ):
                signals[i] = "ENTER_LONG"
                logger.debug(
                    f"ENTER_LONG @ bar {i}: price={close[i]:.2f}, %B={percent_b[i]:.2f}, "
                    f"amplitude_score={amplitude_score[i]:.2f}"
                )

            # Signal SHORT (symÃ©trique, mais %B inversÃ©: 1 - thresholds)
            elif (
                spring_short[i]
                and macd_impulse_short[i]
                and (1 - strategy_params.pb_entry_threshold_max)
                <= percent_b[i]
                <= (1 - strategy_params.pb_entry_threshold_min)
            ):
                signals[i] = "ENTER_SHORT"
                logger.debug(
                    f"ENTER_SHORT @ bar {i}: price={close[i]:.2f}, %B={percent_b[i]:.2f}, "
                    f"amplitude_score={amplitude_score[i]:.2f}"
                )

        # Construction DataFrame de sortie
        result_df = df_with_indicators.copy()
        result_df["signal"] = signals

        # Statistiques signaux
        enter_longs = np.sum(signals == "ENTER_LONG")
        enter_shorts = np.sum(signals == "ENTER_SHORT")
        total_signals = enter_longs + enter_shorts

        logger.info(
            f"Signaux gÃ©nÃ©rÃ©s: {total_signals} total ({enter_longs} LONG, {enter_shorts} SHORT)"
        )

        return result_df

    def backtest(
        self,
        df: pd.DataFrame,
        params: dict,
        initial_capital: float = 10000.0,
        fee_bps: float = 4.5,
        slippage_bps: float = 0.0,
    ) -> Tuple[pd.Series, RunStats]:
        """
        ExÃ©cute un backtest complet de la stratÃ©gie AmplitudeHunter.

        Args:
            df: DataFrame OHLCV avec timestamp index (UTC)
            params: ParamÃ¨tres stratÃ©gie (format AmplitudeHunterParams.to_dict())
            initial_capital: Capital initial
            fee_bps: Frais de transaction en basis points (dÃ©faut: 4.5)
            slippage_bps: Slippage en basis points (dÃ©faut: 0.0)

        Returns:
            Tuple (equity_curve, run_stats)

        Gestion des positions:
        - Position initiale + pyramiding (jusqu'Ã  2 adds)
        - Stop loss initial: max(swing_low - k*ATR, pct*(mÃ©diane-basse))
        - Trailing stop conditionnel (activÃ© si %B > 1 OU gain >= 1R)
        - Stop fixe pour SHORT (37% au-dessus entrÃ©e)
        - Cible BIP avec sortie partielle (50% par dÃ©faut)
        """
        logger.info(
            f"DÃ©but backtest AmplitudeHunter: capital={initial_capital}, "
            f"fee={fee_bps}bps, slippage={slippage_bps}bps"
        )

        # Validation
        validate_ohlcv_dataframe(df)
        strategy_params = AmplitudeHunterParams.from_dict(params)

        # GÃ©nÃ©ration signaux
        signals_df = self.generate_signals(df, params)

        # Initialisation backtest
        n_bars = len(df)
        equity = np.full(n_bars, initial_capital, dtype=float)

        cash = initial_capital
        positions: List[Trade] = []  # Liste des positions en cours (initial + adds)
        closed_trades: List[Trade] = []

        fee_rate = (fee_bps + slippage_bps) / 10000.0

        # Ã‰tat pour pyramiding
        entry_bar_idx = -1
        num_adds = 0
        trailing_active = False
        bip_target_hit = False

        logger.debug(f"Backtest initialisÃ©: {n_bars} barres, fee_rate={fee_rate:.6f}")

        # Boucle principale
        for i, (timestamp, row) in enumerate(signals_df.iterrows()):
            current_price = row["close"]
            current_atr = row["atr"]
            signal = row["signal"]
            percent_b = row["percent_b"]
            bb_upper = row["bb_upper"]
            bb_lower = row["bb_lower"]
            bb_middle = row["bb_middle"]
            macd_hist = row["macd_hist"]

            # Skip si ATR invalide
            if np.isnan(current_atr) or current_atr <= 0:
                equity[i] = self._calculate_total_equity(
                    cash, positions, current_price
                )
                continue

            # --- GESTION POSITIONS EXISTANTES ---
            if len(positions) > 0:
                initial_position = positions[0]
                should_exit = False
                exit_reason = ""

                # 1. VÃ©rification stop loss
                for pos in positions:
                    if pos.should_stop_loss(current_price):
                        should_exit = True
                        exit_reason = "stop_loss"
                        break

                # 2. VÃ©rification durÃ©e maximale
                if not should_exit:
                    entry_timestamp = pd.to_datetime(
                        initial_position.entry_time, utc=True
                    )
                    bars_held = (df.index <= timestamp).sum() - (
                        df.index <= entry_timestamp
                    ).sum()

                    if bars_held >= strategy_params.max_hold_bars:
                        should_exit = True
                        exit_reason = "max_hold_bars"

                # 3. VÃ©rification trailing stop (si activÃ©)
                if not should_exit and trailing_active:
                    if strategy_params.trailing_type == "chandelier":
                        # Chandelier: mÃ©diane - k*ATR
                        chandelier_stop = bb_middle - (
                            current_atr
                            * strategy_params.trailing_chandelier_atr_mult
                        )
                        if initial_position.is_long() and current_price <= chandelier_stop:
                            should_exit = True
                            exit_reason = "trailing_chandelier"
                        elif initial_position.is_short() and current_price >= chandelier_stop:
                            should_exit = True
                            exit_reason = "trailing_chandelier"

                    elif strategy_params.trailing_type == "pb_floor":
                        # %B floor: sortir si %B < 0.5 aprÃ¨s extension
                        if percent_b < strategy_params.trailing_pb_floor:
                            should_exit = True
                            exit_reason = "trailing_pb_floor"

                    elif strategy_params.trailing_type == "macd_fade":
                        # MACD fade: barre verteâ†’rouge pour LONG, rougeâ†’verte pour SHORT
                        if i > 0:
                            prev_macd_hist = signals_df.iloc[i - 1]["macd_hist"]
                            if initial_position.is_long() and macd_hist < 0 and prev_macd_hist > 0:
                                should_exit = True
                                exit_reason = "trailing_macd_fade"
                            elif initial_position.is_short() and macd_hist > 0 and prev_macd_hist < 0:
                                should_exit = True
                                exit_reason = "trailing_macd_fade"

                # 4. VÃ©rification cible BIP (sortie partielle)
                if (
                    not should_exit
                    and strategy_params.use_bip_target
                    and not bip_target_hit
                ):
                    # BIP = mÃ©diane + (mÃ©diane - basse)
                    bip_target = bb_middle + (bb_middle - bb_lower)

                    if initial_position.is_long() and current_price >= bip_target:
                        # Sortie partielle
                        self._partial_exit_bip(
                            positions,
                            current_price,
                            timestamp,
                            fee_rate,
                            strategy_params.bip_partial_exit_pct,
                            cash,
                            closed_trades,
                        )
                        bip_target_hit = True
                        logger.debug(
                            f"BIP target hit @ {current_price:.2f}, "
                            f"sortie partielle {strategy_params.bip_partial_exit_pct*100:.0f}%"
                        )

                    elif initial_position.is_short() and current_price <= bip_target:
                        self._partial_exit_bip(
                            positions,
                            current_price,
                            timestamp,
                            fee_rate,
                            strategy_params.bip_partial_exit_pct,
                            cash,
                            closed_trades,
                        )
                        bip_target_hit = True

                # 5. Activation trailing stop (si conditions remplies)
                if not trailing_active and len(positions) > 0:
                    # Calculer gain actuel en R
                    initial_risk = abs(
                        initial_position.entry_price - initial_position.stop
                    )
                    current_pnl = initial_position.calculate_unrealized_pnl(
                        current_price
                    )
                    gain_in_r = current_pnl / (initial_risk * initial_position.qty) if initial_risk > 0 else 0

                    # Activer si %B > seuil OU gain >= R
                    if (
                        percent_b > strategy_params.trailing_activation_pb_threshold
                        or gain_in_r >= strategy_params.trailing_activation_gain_r
                    ):
                        trailing_active = True
                        logger.debug(
                            f"Trailing stop activÃ© @ bar {i}: %B={percent_b:.2f}, gain={gain_in_r:.2f}R"
                        )

                # 6. Fermeture complÃ¨te de toutes les positions
                if should_exit:
                    for pos in positions:
                        exit_value = current_price * pos.qty
                        exit_fees = exit_value * fee_rate

                        pos.close_trade(
                            exit_price=current_price,
                            exit_time=str(timestamp),
                            exit_fees=exit_fees,
                        )

                        pnl_val = pos.pnl_realized if pos.pnl_realized is not None else 0.0
                        cash += pnl_val + (pos.entry_price * pos.qty)
                        closed_trades.append(pos)

                        logger.debug(
                            f"Position fermÃ©e @ {current_price:.2f}: {exit_reason}, "
                            f"PnL={pos.pnl_realized:.2f}"
                        )

                    # Reset Ã©tat
                    positions = []
                    entry_bar_idx = -1
                    num_adds = 0
                    trailing_active = False
                    bip_target_hit = False

                # 7. Pyramiding (Add #1 et Add #2)
                elif (
                    strategy_params.pyramiding_enabled
                    and num_adds < strategy_params.pyramiding_max_adds
                    and len(positions) > 0
                ):
                    initial_position = positions[0]

                    # Add #1: close > bb_upper ET MACD s'intensifie
                    if num_adds == 0:
                        if i > 0:
                            prev_macd_hist = signals_df.iloc[i - 1]["macd_hist"]
                            if initial_position.is_long():
                                if (
                                    current_price > bb_upper
                                    and macd_hist > 0
                                    and macd_hist > prev_macd_hist
                                ):
                                    self._add_position(
                                        positions,
                                        "LONG",
                                        current_price,
                                        current_atr,
                                        bb_middle,
                                        bb_lower,
                                        timestamp,
                                        strategy_params,
                                        cash,
                                        fee_rate,
                                    )
                                    num_adds += 1
                                    logger.debug(f"Add #1 LONG @ {current_price:.2f}")

                            elif initial_position.is_short():
                                if (
                                    current_price < bb_lower
                                    and macd_hist < 0
                                    and macd_hist < prev_macd_hist
                                ):
                                    self._add_position(
                                        positions,
                                        "SHORT",
                                        current_price,
                                        current_atr,
                                        bb_middle,
                                        bb_upper,
                                        timestamp,
                                        strategy_params,
                                        cash,
                                        fee_rate,
                                    )
                                    num_adds += 1
                                    logger.debug(f"Add #1 SHORT @ {current_price:.2f}")

                    # Add #2: pullback tenu >= mÃ©diane + MACD reste vert/rouge
                    elif num_adds == 1:
                        if initial_position.is_long():
                            if current_price >= bb_middle and macd_hist > 0:
                                self._add_position(
                                    positions,
                                    "LONG",
                                    current_price,
                                    current_atr,
                                    bb_middle,
                                    bb_lower,
                                    timestamp,
                                    strategy_params,
                                    cash,
                                    fee_rate,
                                )
                                num_adds += 1
                                logger.debug(f"Add #2 LONG @ {current_price:.2f}")

                        elif initial_position.is_short():
                            if current_price <= bb_middle and macd_hist < 0:
                                self._add_position(
                                    positions,
                                    "SHORT",
                                    current_price,
                                    current_atr,
                                    bb_middle,
                                    bb_upper,
                                    timestamp,
                                    strategy_params,
                                    cash,
                                    fee_rate,
                                )
                                num_adds += 1
                                logger.debug(f"Add #2 SHORT @ {current_price:.2f}")

            # --- NOUVEAUX SIGNAUX D'ENTRÃ‰E ---
            if len(positions) == 0 and signal in ["ENTER_LONG", "ENTER_SHORT"]:
                # Position sizing basÃ© sur ATR et risk
                atr_stop_distance = current_atr * strategy_params.sl_atr_multiplier
                risk_amount = cash * strategy_params.risk_per_trade

                # Calcul quantitÃ© optimale
                position_size = risk_amount / atr_stop_distance
                max_position_size = (cash * strategy_params.leverage) / current_price

                qty = min(position_size, max_position_size)

                if qty > 0:
                    # Calcul stop loss initial
                    if signal == "ENTER_LONG":
                        # SL = max(swing_low - k*ATR, sl_min_pct*(mÃ©diane-basse))
                        swing_low = min(df["low"].iloc[max(0, i - 10) : i + 1])
                        stop_atr = swing_low - atr_stop_distance
                        stop_min = bb_lower + strategy_params.sl_min_pct * (
                            bb_middle - bb_lower
                        )
                        stop_price = max(stop_atr, stop_min)

                    else:  # ENTER_SHORT
                        swing_high = max(df["high"].iloc[max(0, i - 10) : i + 1])
                        stop_atr = swing_high + atr_stop_distance
                        stop_max = bb_upper - strategy_params.sl_min_pct * (
                            bb_upper - bb_middle
                        )
                        stop_price = min(stop_atr, stop_max)

                        # Stop fixe supplÃ©mentaire pour SHORT
                        stop_fixed = current_price * (1 + strategy_params.short_stop_pct)
                        stop_price = min(stop_price, stop_fixed)

                    # Frais d'entrÃ©e
                    entry_value = current_price * qty
                    entry_fees = entry_value * fee_rate

                    if entry_value + entry_fees <= cash:
                        # CrÃ©ation nouvelle position
                        position = Trade(
                            side=signal.replace("ENTER_", ""),
                            qty=qty,
                            entry_price=current_price,
                            entry_time=str(timestamp),
                            stop=stop_price,
                            fees_paid=entry_fees,
                            meta={
                                "percent_b": percent_b,
                                "amplitude_score": row.get("amplitude_score", 0),
                                "atr": current_atr,
                                "sl_atr_multiplier": strategy_params.sl_atr_multiplier,
                                "bb_middle": bb_middle,
                                "bb_lower": bb_lower if signal == "ENTER_LONG" else bb_upper,
                            },
                        )

                        positions.append(position)
                        cash -= entry_value + entry_fees
                        entry_bar_idx = i
                        num_adds = 0
                        trailing_active = False
                        bip_target_hit = False

                        logger.debug(
                            f"Nouvelle position: {signal} {qty:.4f} @ {current_price:.2f}, "
                            f"stop={stop_price:.2f}"
                        )

            # Mise Ã  jour Ã©quitÃ©
            equity[i] = self._calculate_total_equity(cash, positions, current_price)

        # Fermeture positions finales si nÃ©cessaire
        if len(positions) > 0:
            final_price = df["close"].iloc[-1]
            for pos in positions:
                pos.close_trade(
                    exit_price=final_price,
                    exit_time=df.index[-1].isoformat(),
                    exit_fees=final_price * pos.qty * fee_rate,
                )
                closed_trades.append(pos)

        # Construction courbe d'Ã©quitÃ©
        equity_curve = pd.Series(equity, index=df.index)

        # Calcul statistiques
        run_stats = RunStats.from_trades_and_equity(
            trades=closed_trades,
            equity_curve=equity_curve,
            initial_capital=initial_capital,
            meta={
                "strategy": "AmplitudeHunter",
                "params": params,
                "fee_bps": fee_bps,
                "slippage_bps": slippage_bps,
                "symbol": self.symbol,
                "timeframe": self.timeframe,
            },
        )

        logger.info(
            f"Backtest terminÃ©: {run_stats.total_trades} trades, "
            f"PnL={run_stats.total_pnl:.2f} ({run_stats.total_pnl_pct:.2f}%)"
        )

        return equity_curve, run_stats

    # --- Helper methods pour backtest ---

    def _calculate_total_equity(
        self, cash: float, positions: List[Trade], current_price: float
    ) -> float:
        """Calcule l'Ã©quitÃ© totale (cash + positions non rÃ©alisÃ©es)"""
        total_unrealized = sum(
            pos.calculate_unrealized_pnl(current_price) for pos in positions
        )
        total_invested = sum(pos.entry_price * pos.qty for pos in positions)
        return cash + total_invested + total_unrealized

    def _partial_exit_bip(
        self,
        positions: List[Trade],
        exit_price: float,
        exit_time: Any,
        fee_rate: float,
        exit_pct: float,
        cash: float,
        closed_trades: List[Trade],
    ) -> float:
        """
        Sortie partielle Ã  BIP target.

        Args:
            positions: Liste des positions en cours (modifiÃ©e in-place)
            exit_price: Prix de sortie
            exit_time: Timestamp
            fee_rate: Taux de frais
            exit_pct: Pourcentage Ã  sortir (0.5 = 50%)
            cash: Cash actuel (modifiÃ© in-place via reference)
            closed_trades: Liste des trades fermÃ©s (modifiÃ©e in-place)

        Returns:
            PnL rÃ©alisÃ© de la sortie partielle
        """
        total_pnl = 0.0

        for pos in positions:
            # QuantitÃ© Ã  sortir
            qty_to_exit = pos.qty * exit_pct

            # CrÃ©er un trade partiel pour les statistiques
            partial_trade = Trade(
                side=pos.side,
                qty=qty_to_exit,
                entry_price=pos.entry_price,
                entry_time=pos.entry_time,
                stop=pos.stop,
                fees_paid=pos.fees_paid * exit_pct,  # Frais proportionnels
                meta=pos.meta.copy(),
            )

            exit_value = exit_price * qty_to_exit
            exit_fees = exit_value * fee_rate

            partial_trade.close_trade(
                exit_price=exit_price,
                exit_time=str(exit_time),
                exit_fees=exit_fees,
            )

            pnl = partial_trade.pnl_realized if partial_trade.pnl_realized is not None else 0.0
            total_pnl += pnl
            closed_trades.append(partial_trade)

            # RÃ©duire la position restante
            pos.qty -= qty_to_exit
            pos.fees_paid -= pos.fees_paid * exit_pct

        return total_pnl

    def _add_position(
        self,
        positions: List[Trade],
        side: str,
        entry_price: float,
        current_atr: float,
        bb_middle: float,
        bb_boundary: float,
        timestamp: Any,
        params: AmplitudeHunterParams,
        cash: float,
        fee_rate: float,
    ) -> bool:
        """
        Ajoute une position pyramidÃ©e.

        Returns:
            True si l'add a rÃ©ussi, False sinon
        """
        # Sizing similaire Ã  la position initiale
        atr_stop_distance = current_atr * params.sl_atr_multiplier
        risk_amount = cash * params.risk_per_trade

        position_size = risk_amount / atr_stop_distance
        max_position_size = (cash * params.leverage) / entry_price

        qty = min(position_size, max_position_size)

        if qty <= 0:
            return False

        # Stop pour l'add (mÃªme logique que position initiale)
        if side == "LONG":
            stop_price = max(
                bb_boundary - atr_stop_distance,
                bb_boundary + params.sl_min_pct * (bb_middle - bb_boundary),
            )
        else:
            stop_price = min(
                bb_boundary + atr_stop_distance,
                bb_boundary - params.sl_min_pct * (bb_boundary - bb_middle),
            )

        entry_value = entry_price * qty
        entry_fees = entry_value * fee_rate

        if entry_value + entry_fees > cash:
            return False

        # CrÃ©er l'add
        add_position = Trade(
            side=side,
            qty=qty,
            entry_price=entry_price,
            entry_time=str(timestamp),
            stop=stop_price,
            fees_paid=entry_fees,
            meta={"is_add": True, "add_number": len(positions)},
        )

        positions.append(add_position)
        # Note: le cash n'est pas modifiÃ© ici car la signature ne permet pas de le faire
        # Il faudrait retourner le cash modifiÃ© ou utiliser un container mutable

        return True

    # --- Optimization Presets ---

    @staticmethod
    def get_optimization_ranges() -> Dict[str, Tuple[float, float]]:
        """
        Retourne les plages d'optimisation recommandÃ©es pour AmplitudeHunter.

        Utilise les presets "classiques" depuis le fichier indicator_ranges.toml.
        Les plages sont automatiquement mappÃ©es aux paramÃ¨tres de la stratÃ©gie.

        Returns:
            Dictionnaire {param_name: (min, max)}

        Example:
            >>> ranges = AmplitudeHunterStrategy.get_optimization_ranges()
            >>> print(ranges['bb_period'])  # (10, 50)
            >>> print(ranges['amplitude_score_threshold'])  # (0.4, 0.8)
        """
        try:
            from threadx.optimization.presets import get_strategy_preset

            mapper = get_strategy_preset("AmplitudeHunter")
            return mapper.get_optimization_ranges()
        except ImportError:
            logger.warning(
                "Module threadx.optimization.presets non disponible. "
                "Retour des plages par dÃ©faut."
            )
            # Fallback: plages basiques
            return {
                "bb_period": (10, 50),
                "bb_std": (1.5, 3.0),
                "amplitude_score_threshold": (0.4, 0.8),
            }

    @staticmethod
    def get_optimization_grid() -> Dict[str, List[Any]]:
        """
        Retourne les grilles de valeurs pour grid search.

        GÃ©nÃ¨re automatiquement les valeurs Ã  tester pour chaque paramÃ¨tre
        selon les presets (min, max, step).

        Returns:
            Dictionnaire {param_name: [valeurs]}

        Example:
            >>> grid = AmplitudeHunterStrategy.get_optimization_grid()
            >>> print(grid['bb_period'])  # [10, 11, 12, ..., 50]
            >>> print(grid['pyramiding_max_adds'])  # [0, 1, 2]
        """
        try:
            from threadx.optimization.presets import get_strategy_preset

            mapper = get_strategy_preset("AmplitudeHunter")
            return mapper.get_grid_parameters()
        except ImportError:
            logger.warning(
                "Module threadx.optimization.presets non disponible. "
                "Retour de grilles par dÃ©faut."
            )
            # Fallback: grilles basiques
            return {
                "bb_period": list(range(10, 51, 1)),
                "amplitude_score_threshold": [0.4, 0.5, 0.6, 0.7, 0.8],
            }

    @staticmethod
    def get_default_optimization_params() -> Dict[str, Any]:
        """
        Retourne les valeurs par dÃ©faut recommandÃ©es pour l'optimisation.

        Returns:
            Dictionnaire {param_name: default_value}

        Example:
            >>> defaults = AmplitudeHunterStrategy.get_default_optimization_params()
            >>> print(defaults['bb_period'])  # 20
            >>> print(defaults['amplitude_score_threshold'])  # 0.6
        """
        try:
            from threadx.optimization.presets import get_strategy_preset

            mapper = get_strategy_preset("AmplitudeHunter")
            return mapper.get_default_parameters()
        except ImportError:
            logger.warning(
                "Module threadx.optimization.presets non disponible. "
                "Retour des valeurs par dÃ©faut de AmplitudeHunterParams."
            )
            # Fallback: valeurs par dÃ©faut de la dataclass
            return AmplitudeHunterParams().to_dict()


# ==========================================
# CONVENIENCE FUNCTIONS
# ==========================================


def generate_signals(
    df: pd.DataFrame,
    params: dict,
    symbol: str = "UNKNOWN",
    timeframe: str = "1h",
) -> pd.DataFrame:
    """
    Fonction de convenance pour gÃ©nÃ©ration de signaux AmplitudeHunter.

    Args:
        df: DataFrame OHLCV
        params: ParamÃ¨tres stratÃ©gie
        symbol: Symbole pour cache
        timeframe: Timeframe pour cache

    Returns:
        DataFrame avec signaux et mÃ©tadonnÃ©es

    Example:
        >>> params = AmplitudeHunterParams(amplitude_score_threshold=0.7).to_dict()
        >>> signals = generate_signals(df, params, "BTCUSDT", "1h")
    """
    strategy = AmplitudeHunterStrategy(symbol=symbol, timeframe=timeframe)
    return strategy.generate_signals(df, params)


def backtest(
    df: pd.DataFrame,
    params: dict,
    initial_capital: float = 10000.0,
    symbol: str = "UNKNOWN",
    timeframe: str = "1h",
    **kwargs,
) -> Tuple[pd.Series, RunStats]:
    """
    Fonction de convenance pour backtest AmplitudeHunter.

    Args:
        df: DataFrame OHLCV
        params: ParamÃ¨tres stratÃ©gie
        initial_capital: Capital initial
        symbol: Symbole pour cache
        timeframe: Timeframe pour cache
        **kwargs: Arguments supplÃ©mentaires (fee_bps, slippage_bps, etc.)

    Returns:
        Tuple (equity_curve, run_stats)

    Example:
        >>> params = AmplitudeHunterParams(pyramiding_enabled=True).to_dict()
        >>> equity, stats = backtest(df, params, 50000, "ETHUSDT", "4h")
        >>> print(f"ROI: {stats.total_pnl_pct:.2f}%, Trades: {stats.total_trades}")
    """
    strategy = AmplitudeHunterStrategy(symbol=symbol, timeframe=timeframe)
    return strategy.backtest(df, params, initial_capital, **kwargs)


def create_default_params(**overrides) -> AmplitudeHunterParams:
    """
    CrÃ©e des paramÃ¨tres par dÃ©faut avec surcharges optionnelles.

    Args:
        **overrides: ParamÃ¨tres Ã  surcharger

    Returns:
        Instance AmplitudeHunterParams avec valeurs par dÃ©faut + surcharges

    Example:
        >>> params = create_default_params(
        ...     amplitude_score_threshold=0.75,
        ...     pyramiding_enabled=True,
        ...     pyramiding_max_adds=2
        ... )
        >>> params.amplitude_score_threshold
        0.75
    """
    base_params = AmplitudeHunterParams()

    # Application des surcharges
    for key, value in overrides.items():
        if hasattr(base_params, key):
            setattr(base_params, key, value)
        else:
            logger.warning(f"ParamÃ¨tre inconnu ignorÃ©: {key}={value}")

    return base_params


# ==========================================
# MODULE EXPORTS
# ==========================================

__all__ = [
    # Classes principales
    "AmplitudeHunterParams",
    "AmplitudeHunterStrategy",
    # Fonctions de convenance
    "generate_signals",
    "backtest",
    "create_default_params",
]

----------------------------------------
Fichier: strategy\bb_atr.py
"""
ThreadX Phase 4 - BB+ATR Strategy Implementation
===============================================

StratÃ©gie Bollinger Bands + ATR avec gestion avancÃ©e du risque.

FonctionnalitÃ©s:
- Signaux basÃ©s sur Bollinger Bands avec filtrage Z-score
- Stops dynamiques basÃ©s sur ATR avec multiplicateur configurable
- Filtrage des trades (min PnL, durÃ©e, espacement)
- IntÃ©gration complÃ¨te avec Phase 3 Indicators Layer
- Backtest dÃ©terministe avec seed reproductible
- Gestion des positions longues et courtes

AmÃ©liorations vs TradXPro:
- atr_multiplier paramÃ©trable (dÃ©faut 1.5) pour stops adaptatifs
- Filtrage min_pnl_pct optionnel (dÃ©sactivÃ© par dÃ©faut pour Ã©viter sur-filtrage)
- Trailing stop ATR plus robuste
- IntÃ©gration native avec IndicatorBank (cache TTL)
"""

from dataclasses import dataclass, field
from typing import Dict, Any, Tuple, Optional, List
import pandas as pd
import numpy as np
import logging
from pathlib import Path

from threadx.configuration.settings import S
from threadx.utils.log import get_logger
from threadx.strategy.model import (
    Strategy,
    Trade,
    RunStats,
    validate_ohlcv_dataframe,
    validate_strategy_params,
)
from threadx.indicators import ensure_indicator, batch_ensure_indicators

logger = get_logger(__name__)

# ==========================================
# STRATEGY PARAMETERS
# ==========================================


@dataclass
class BBAtrParams:
    """
    ParamÃ¨tres de la stratÃ©gie Bollinger Bands + ATR.

    Attributes:
        # Bollinger Bands
        bb_period: PÃ©riode pour moyennes mobiles (dÃ©faut: 20)
        bb_std: Multiplicateur Ã©cart-type pour bandes (dÃ©faut: 2.0)
        entry_z: Seuil Z-score pour dÃ©clenchement signal (dÃ©faut: 1.0)
        entry_logic: Logique d'entrÃ©e "AND"|"OR" (dÃ©faut: "AND")

        # ATR et gestion risque
        atr_period: PÃ©riode ATR (dÃ©faut: 14)
        atr_multiplier: Multiplicateur ATR pour stops (dÃ©faut: 1.5)
        trailing_stop: Activer trailing stop ATR (dÃ©faut: True)

        # Risk Management
        risk_per_trade: Risque par trade en fraction du capital (dÃ©faut: 0.01 = 1%)
        min_pnl_pct: PnL minimum requis pour valider trade (dÃ©faut: 0.0% = dÃ©sactivÃ©)

        # Positions et timing
        leverage: Effet de levier (dÃ©faut: 1.0)
        max_hold_bars: DurÃ©e max position en barres (dÃ©faut: 72)
        spacing_bars: Espacement min entre trades (dÃ©faut: 6)

        # Filtrage optionnel
        trend_period: PÃ©riode EMA tendance (0=dÃ©sactivÃ©, dÃ©faut: 0)

        # MÃ©tadonnÃ©es
        meta: Dictionnaire mÃ©tadonnÃ©es personnalisÃ©es

    Example:
        >>> params = BBAtrParams(
        ...     bb_period=20, bb_std=2.0, entry_z=1.5,
        ...     atr_multiplier=2.0, risk_per_trade=0.02
        ... )
        >>> # Utilisation avec stratÃ©gie
        >>> strategy = BBAtrStrategy()
        >>> signals = strategy.generate_signals(df, params.to_dict())
    """

    # Bollinger Bands
    bb_period: int = 20
    bb_std: float = 2.0
    entry_z: float = 1.0
    entry_logic: str = "AND"

    # ATR et stops
    atr_period: int = 14
    atr_multiplier: float = 1.5  # AmÃ©lioration: multiplicateur configurable
    trailing_stop: bool = True

    # Risk management
    risk_per_trade: float = 0.01  # 1% du capital par trade
    min_pnl_pct: float = (
        0.0  # FIX: DÃ©sactivÃ© par dÃ©faut (0.01% filtrait TOUS les trades)
    )

    # Position management
    leverage: float = 1.0
    max_hold_bars: int = 72  # 3 jours en 1h
    spacing_bars: int = 6  # 6h entre trades

    # Filtres optionnels
    trend_period: int = 0  # 0 = pas de filtre tendance

    # MÃ©tadonnÃ©es
    meta: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validation des paramÃ¨tres"""
        if self.bb_period < 2:
            raise ValueError(f"bb_period must be >= 2, got: {self.bb_period}")

        if self.bb_std <= 0:
            raise ValueError(f"bb_std must be > 0, got: {self.bb_std}")

        if self.entry_z <= 0:
            raise ValueError(f"entry_z must be > 0, got: {self.entry_z}")

        if self.entry_logic not in ["AND", "OR"]:
            raise ValueError(
                f"entry_logic must be 'AND' or 'OR', got: {self.entry_logic}"
            )

        if self.atr_period < 1:
            raise ValueError(f"atr_period must be >= 1, got: {self.atr_period}")

        if self.atr_multiplier <= 0:
            raise ValueError(f"atr_multiplier must be > 0, got: {self.atr_multiplier}")

        if not 0 < self.risk_per_trade <= 1:
            raise ValueError(
                f"risk_per_trade must be in (0, 1], got: {self.risk_per_trade}"
            )

        if self.min_pnl_pct < 0:
            raise ValueError(f"min_pnl_pct must be >= 0, got: {self.min_pnl_pct}")

        if self.leverage <= 0:
            raise ValueError(f"leverage must be > 0, got: {self.leverage}")

        if self.max_hold_bars < 1:
            raise ValueError(f"max_hold_bars must be >= 1, got: {self.max_hold_bars}")

        if self.spacing_bars < 0:
            raise ValueError(f"spacing_bars must be >= 0, got: {self.spacing_bars}")

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire pour compatibilitÃ©"""
        return {
            "bb_period": self.bb_period,
            "bb_std": self.bb_std,
            "entry_z": self.entry_z,
            "entry_logic": self.entry_logic,
            "atr_period": self.atr_period,
            "atr_multiplier": self.atr_multiplier,
            "trailing_stop": self.trailing_stop,
            "risk_per_trade": self.risk_per_trade,
            "min_pnl_pct": self.min_pnl_pct,
            "leverage": self.leverage,
            "max_hold_bars": self.max_hold_bars,
            "spacing_bars": self.spacing_bars,
            "trend_period": self.trend_period,
            "meta": self.meta,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "BBAtrParams":
        """CrÃ©e depuis un dictionnaire"""
        return cls(
            bb_period=data.get("bb_period", 20),
            bb_std=data.get("bb_std", 2.0),
            entry_z=data.get("entry_z", 1.0),
            entry_logic=data.get("entry_logic", "AND"),
            atr_period=data.get("atr_period", 14),
            atr_multiplier=data.get("atr_multiplier", 1.5),
            trailing_stop=data.get("trailing_stop", True),
            risk_per_trade=data.get("risk_per_trade", 0.01),
            min_pnl_pct=data.get("min_pnl_pct", 0.0),  # FIX: 0.0 par dÃ©faut
            leverage=data.get("leverage", 1.0),
            max_hold_bars=data.get("max_hold_bars", 72),
            spacing_bars=data.get("spacing_bars", 6),
            trend_period=data.get("trend_period", 0),
            meta=data.get("meta", {}),
        )


# ==========================================
# STRATEGY IMPLEMENTATION
# ==========================================


class BBAtrStrategy:
    """
    ImplÃ©mentation de la stratÃ©gie Bollinger Bands + ATR.

    Logique de trading:
    1. Calcul indicateurs via IndicatorBank (cache Phase 3)
    2. GÃ©nÃ©ration signaux basÃ©s sur:
       - Z-score Bollinger > entry_z pour ENTER_SHORT
       - Z-score Bollinger < -entry_z pour ENTER_LONG
       - Stops dynamiques ATR * atr_multiplier
       - Filtrage tendance optionnel (EMA)
    3. Gestion positions:
       - Risk sizing basÃ© sur ATR
       - Trailing stops ATR
       - Filtrage min PnL et espacement

    AmÃ©liorations vs TradXPro:
    - atr_multiplier paramÃ©trable vs fixe
    - Filtrage min_pnl_pct Ã©vite micro-trades
    - IntÃ©gration native cache Phase 3
    - Code plus lisible et testable

    Example:
        >>> strategy = BBAtrStrategy()
        >>> params = BBAtrParams(bb_period=20, atr_multiplier=2.0)
        >>> signals = strategy.generate_signals(df, params.to_dict())
        >>> equity, stats = strategy.backtest(df, params.to_dict(), 10000)
    """

    def __init__(self, symbol: str = "UNKNOWN", timeframe: str = "15m"):
        """
        Initialise la stratÃ©gie.

        Args:
            symbol: Symbole pour cache d'indicateurs
            timeframe: Timeframe pour cache d'indicateurs
        """
        self.symbol = symbol
        self.timeframe = timeframe
        logger.debug(f"StratÃ©gie BB+ATR initialisÃ©e: {symbol}/{timeframe}")

    def _ensure_indicators(
        self,
        df: pd.DataFrame,
        params: BBAtrParams,
        precomputed_indicators: Optional[Dict] = None,
    ) -> Tuple[pd.DataFrame, np.ndarray]:
        """
        Garantit la disponibilitÃ© des indicateurs via IndicatorBank.

        Args:
            df: DataFrame OHLCV
            params: ParamÃ¨tres stratÃ©gie
            precomputed_indicators: Dictionnaire {key: result} d'indicateurs prÃ©-calculÃ©s (optionnel)

        Returns:
            Tuple (df_with_bollinger, atr_array)
        """
        import json

        # ðŸš€ OPTIMISATION: Utiliser le mÃªme format de clÃ© que _params_to_key() dans engine.py
        bb_key = json.dumps(
            {"period": params.bb_period, "std": params.bb_std},
            sort_keys=True,
            separators=(",", ":"),
        )
        atr_key = json.dumps(
            {"method": "ema", "period": params.atr_period},
            sort_keys=True,
            separators=(",", ":"),
        )

        # Debug: Voir ce qui est fourni (dÃ©sactivÃ© pour performance)
        # if precomputed_indicators:
        #     logger.info(
        #         f"ðŸ” DEBUG precomputed keys: bollinger={list(precomputed_indicators.get('bollinger', {}).keys())}, atr={list(precomputed_indicators.get('atr', {}).keys())}, wanted_bb={bb_key}, wanted_atr={atr_key}"
        #     )

        if (
            precomputed_indicators
            and "bollinger" in precomputed_indicators
            and "atr" in precomputed_indicators
            and bb_key in precomputed_indicators["bollinger"]
            and atr_key in precomputed_indicators["atr"]
        ):
            # logger.info(f"âš¡ FAST PATH: RÃ©utilisation BB({bb_key}), ATR({atr_key})")
            pass  # Log dÃ©sactivÃ© pour performance

            # RÃ©cupÃ©ration Bollinger prÃ©-calculÃ©
            bb_result = precomputed_indicators["bollinger"][bb_key]
            if isinstance(bb_result, tuple) and len(bb_result) == 3:
                upper, middle, lower = bb_result
                df_bb = df.copy()
                df_bb["bb_upper"] = upper
                df_bb["bb_middle"] = middle
                df_bb["bb_lower"] = lower

                # Calcul Z-score
                close = df["close"].values
                bb_std_dev = (upper - lower) / (4 * params.bb_std)
                df_bb["bb_z"] = (close - middle) / bb_std_dev
            else:
                raise ValueError(f"Bollinger format invalide: {type(bb_result)}")

            # RÃ©cupÃ©ration ATR prÃ©-calculÃ©
            atr_array = precomputed_indicators["atr"][atr_key]
            if not isinstance(atr_array, np.ndarray):
                raise ValueError(f"ATR format invalide: {type(atr_array)}")

            return df_bb, atr_array

        # Sinon, calcul classique via IndicatorBank
        logger.debug(
            f"Calcul indicateurs: BB(period={params.bb_period}, std={params.bb_std}), ATR(period={params.atr_period})"
        )

        # Bollinger Bands via IndicatorBank
        bb_result = ensure_indicator(
            "bollinger",
            {"period": params.bb_period, "std": params.bb_std},
            df,
            symbol=self.symbol,
            timeframe=self.timeframe,
        )

        if isinstance(bb_result, tuple) and len(bb_result) == 3:
            # Format (upper, middle, lower)
            upper, middle, lower = bb_result
            df_bb = df.copy()
            df_bb["bb_upper"] = upper
            df_bb["bb_middle"] = middle
            df_bb["bb_lower"] = lower

            # Calcul Z-score manual (pas dans cache)
            close = df["close"].values
            bb_std_dev = (upper - lower) / (4 * params.bb_std)  # Approximation
            df_bb["bb_z"] = (close - middle) / bb_std_dev

        else:
            raise ValueError(f"Bollinger result format invalide: {type(bb_result)}")

        # ATR via IndicatorBank
        atr_result = ensure_indicator(
            "atr",
            {"period": params.atr_period, "method": "ema"},  # Plus rÃ©actif que SMA
            df,
            symbol=self.symbol,
            timeframe=self.timeframe,
        )

        if isinstance(atr_result, np.ndarray):
            atr_array = atr_result
        else:
            raise ValueError(f"ATR result format invalide: {type(atr_result)}")

        logger.debug(
            f"Indicateurs calculÃ©s: BB Z-score range [{df_bb['bb_z'].min():.2f}, {df_bb['bb_z'].max():.2f}], ATR moyen {atr_array.mean():.4f}"
        )

        return df_bb, atr_array

    def _calculate_trend_filter(
        self, close: np.ndarray, trend_period: int
    ) -> Optional[np.ndarray]:
        """
        Calcule le filtre de tendance EMA optionnel.

        Args:
            close: Prix de clÃ´ture
            trend_period: PÃ©riode EMA (0=dÃ©sactivÃ©)

        Returns:
            Array EMA ou None si dÃ©sactivÃ©
        """
        if trend_period <= 0:
            return None

        # EMA simple via pandas (plus efficace que implÃ©mentation manuelle)
        close_series = pd.Series(close)
        ema = close_series.ewm(span=trend_period, adjust=False).mean().values

        logger.debug(
            f"Filtre tendance calculÃ©: EMA({trend_period}), derniÃ¨re valeur {ema[-1]:.2f}"
        )
        return np.array(ema) if ema is not None else None

    def generate_signals(
        self,
        df: pd.DataFrame,
        params: dict,
        precomputed_indicators: Optional[Dict] = None,
    ) -> pd.DataFrame:
        """
        GÃ©nÃ¨re les signaux de trading basÃ©s sur Bollinger+ATR.

        Args:
            df: DataFrame OHLCV avec timestamp index (UTC)
            params: Dictionnaire paramÃ¨tres (format BBAtrParams.to_dict())
            precomputed_indicators: Dictionnaire {key: result} d'indicateurs prÃ©-calculÃ©s (optionnel)

        Returns:
            DataFrame avec colonne 'signal' et mÃ©tadonnÃ©es

        Signals gÃ©nÃ©rÃ©s:
        - "ENTER_LONG": Z-score < -entry_z (prix en dessous bande basse)
        - "ENTER_SHORT": Z-score > entry_z (prix au dessus bande haute)
        - "EXIT": Conditions de sortie (stop, take profit, durÃ©e)
        - "HOLD": Maintenir position actuelle
        """
        logger.debug(f"GÃ©nÃ©ration signaux BB+ATR: {len(df)} barres")

        # Validation inputs
        validate_ohlcv_dataframe(df)
        validate_strategy_params(params, ["bb_period", "bb_std", "entry_z"])

        # Parse paramÃ¨tres
        strategy_params = BBAtrParams.from_dict(params)

        # Ensure indicateurs (utilise prÃ©-calculÃ©s si fournis)
        df_with_indicators, atr_array = self._ensure_indicators(
            df, strategy_params, precomputed_indicators=precomputed_indicators
        )

        # Extraction des donnÃ©es
        close = df["close"].values
        high = df["high"].values
        low = df["low"].values

        bb_z = df_with_indicators["bb_z"].values
        bb_upper = df_with_indicators["bb_upper"].values
        bb_lower = df_with_indicators["bb_lower"].values
        bb_middle = df_with_indicators["bb_middle"].values

        # Filtre tendance optionnel
        trend_ema = self._calculate_trend_filter(
            np.array(close), strategy_params.trend_period
        )

        # Initialisation signaux
        n_bars = len(df)
        signals = np.full(n_bars, "HOLD", dtype=object)

        # Logique de signaux
        logger.debug(
            f"Application logique signaux: entry_z=Â±{strategy_params.entry_z}, logic={strategy_params.entry_logic}"
        )

        # Conditions d'entrÃ©e
        enter_long_condition = np.array(bb_z) < -strategy_params.entry_z
        enter_short_condition = np.array(bb_z) > strategy_params.entry_z

        # Filtre tendance si activÃ©
        if trend_ema is not None:
            if strategy_params.entry_logic == "AND":
                # AND: tendance doit confirmer signal
                enter_long_condition = enter_long_condition & (close > trend_ema)
                enter_short_condition = enter_short_condition & (close < trend_ema)
            else:
                # OR: tendance ou Bollinger peut dÃ©clencher
                enter_long_condition = enter_long_condition | (close > trend_ema)
                enter_short_condition = enter_short_condition | (close < trend_ema)

        # Application des signaux avec espacement
        last_signal_bar = -strategy_params.spacing_bars - 1

        for i in range(strategy_params.bb_period, n_bars):  # Skip pÃ©riode de warmup
            # VÃ©rification espacement minimum
            if i - last_signal_bar < strategy_params.spacing_bars:
                continue

            # Filtrage NaN (indicateurs pas encore stables)
            if np.isnan(bb_z[i]) or np.isnan(atr_array[i]):
                continue

            # Signal ENTER_LONG
            if (
                enter_long_condition[i] and not enter_long_condition[i - 1]
            ):  # Nouveau signal
                signals[i] = "ENTER_LONG"
                last_signal_bar = i
                logger.debug(
                    f"ENTER_LONG @ bar {i}: price={close[i]:.2f}, z={bb_z[i]:.2f}, atr={atr_array[i]:.4f}"
                )

            # Signal ENTER_SHORT
            elif (
                enter_short_condition[i] and not enter_short_condition[i - 1]
            ):  # Nouveau signal
                signals[i] = "ENTER_SHORT"
                last_signal_bar = i
                logger.debug(
                    f"ENTER_SHORT @ bar {i}: price={close[i]:.2f}, z={bb_z[i]:.2f}, atr={atr_array[i]:.4f}"
                )

        # Construction DataFrame de sortie
        result_df = pd.DataFrame(index=df.index)
        result_df["signal"] = signals

        # MÃ©tadonnÃ©es pour chaque barre
        result_df["bb_z"] = bb_z
        result_df["bb_upper"] = bb_upper
        result_df["bb_middle"] = bb_middle
        result_df["bb_lower"] = bb_lower
        result_df["atr"] = atr_array
        result_df["close"] = close

        if trend_ema is not None:
            result_df["trend_ema"] = trend_ema

        # Statistiques signaux
        enter_longs = np.sum(signals == "ENTER_LONG")
        enter_shorts = np.sum(signals == "ENTER_SHORT")
        total_signals = enter_longs + enter_shorts

        logger.debug(
            f"Signaux gÃ©nÃ©rÃ©s: {total_signals} total ({enter_longs} LONG, {enter_shorts} SHORT)"
        )

        return result_df

    def backtest(
        self,
        df: pd.DataFrame,
        params: dict,
        initial_capital: float = 10000.0,
        fee_bps: float = 4.5,
        slippage_bps: float = 0.0,
        precomputed_indicators: Optional[Dict] = None,
    ) -> Tuple[pd.Series, RunStats]:
        """
        ExÃ©cute un backtest complet de la stratÃ©gie BB+ATR.

        Args:
            df: DataFrame OHLCV avec timestamp index (UTC)
            params: ParamÃ¨tres stratÃ©gie (format BBAtrParams.to_dict())
            initial_capital: Capital initial
            fee_bps: Frais de transaction en basis points (dÃ©faut: 4.5)
            slippage_bps: Slippage en basis points (dÃ©faut: 0.0)
            precomputed_indicators: Dictionnaire {key: result} d'indicateurs prÃ©-calculÃ©s (optionnel)
                                   Permet de skip ensure_indicator() et rÃ©utiliser calculs batch

        Returns:
            Tuple (equity_curve, run_stats) avec:
            - equity_curve: SÃ©rie temporelle de l'Ã©quitÃ©
            - run_stats: Statistiques complÃ¨tes du run

        Gestion des positions:
        - Size basÃ© sur ATR et risk_per_trade
        - Stop loss dynamique: entry_price Â± (ATR * atr_multiplier)
        - Trailing stop si activÃ©
        - Sortie forcÃ©e aprÃ¨s max_hold_bars
        - Filtrage trades avec PnL < min_pnl_pct
        """
        logger.debug(
            f"DÃ©but backtest BB+ATR: capital={initial_capital}, fee={fee_bps}bps, slippage={slippage_bps}bps"
        )

        # Validation
        validate_ohlcv_dataframe(df)
        strategy_params = BBAtrParams.from_dict(params)

        # GÃ©nÃ©ration signaux (avec indicateurs prÃ©-calculÃ©s si disponibles)
        signals_df = self.generate_signals(
            df, params, precomputed_indicators=precomputed_indicators
        )

        # Initialisation backtest
        n_bars = len(df)
        equity = np.full(n_bars, initial_capital, dtype=float)

        cash = initial_capital
        position = None  # Trade actuel ou None
        trades: List[Trade] = []

        fee_rate = (fee_bps + slippage_bps) / 10000.0

        logger.debug(f"Backtest initialisÃ©: {n_bars} barres, fee_rate={fee_rate:.6f}")

        # PrÃ©-extraction des colonnes en numpy arrays (3-4x plus rapide que iterrows)
        close_vals = signals_df["close"].values
        atr_vals = signals_df["atr"].values
        signal_vals = signals_df["signal"].values
        bb_middle_vals = signals_df["bb_middle"].values
        bb_z_vals = signals_df["bb_z"].values
        timestamps = signals_df.index.values

        # DÃ©tecter si l'index du DataFrame a un timezone
        has_tz = df.index.tz is not None

        # Boucle principale
        for i in range(n_bars):
            current_price = close_vals[i]
            current_atr = atr_vals[i]
            signal = signal_vals[i]
            # CrÃ©er timestamp en respectant le timezone de l'index pour Ã©viter les erreurs
            timestamp = (
                pd.Timestamp(timestamps[i], tz=df.index.tz)
                if has_tz
                else pd.Timestamp(timestamps[i])
            )

            # Skip si ATR invalide
            if np.isnan(current_atr) or current_atr <= 0:
                equity[i] = (
                    cash
                    if position is None
                    else cash + position.calculate_unrealized_pnl(current_price)
                )
                continue

            # Gestion position existante
            if position is not None:
                # VÃ©rification stops et conditions de sortie
                should_exit = False
                exit_reason = ""

                # 1. Stop loss ATR
                if position.should_stop_loss(current_price):
                    should_exit = True
                    exit_reason = "stop_loss"

                # 2. Take profit (retour vers BB middle)
                elif position.is_long() and current_price >= bb_middle_vals[i]:
                    should_exit = True
                    exit_reason = "take_profit_bb_middle"

                elif position.is_short() and current_price <= bb_middle_vals[i]:
                    should_exit = True
                    exit_reason = "take_profit_bb_middle"

                # 3. DurÃ©e maximale
                # Calcul O(1) au lieu de O(n) : utiliser l'index de barre
                entry_bar_index = position.meta.get("entry_bar_index", 0)
                bars_held = i - entry_bar_index

                if bars_held >= strategy_params.max_hold_bars:
                    should_exit = True
                    exit_reason = "max_hold_bars"

                # 4. Trailing stop ATR si activÃ©
                if strategy_params.trailing_stop and not should_exit:
                    # Mise Ã  jour trailing stop
                    new_stop = None
                    if position.is_long():
                        new_stop = current_price - (
                            current_atr * strategy_params.atr_multiplier
                        )
                        if new_stop > position.stop:  # Trail vers le haut seulement
                            position.stop = new_stop
                    else:
                        new_stop = current_price + (
                            current_atr * strategy_params.atr_multiplier
                        )
                        if new_stop < position.stop:  # Trail vers le bas seulement
                            position.stop = new_stop

                # Fermeture position
                if should_exit:
                    # Calcul frais de sortie
                    exit_value = current_price * position.qty
                    exit_fees = exit_value * fee_rate

                    # Fermeture trade
                    position.close_trade(
                        exit_price=current_price,
                        exit_time=(
                            str(timestamp)
                            if hasattr(timestamp, "isoformat")
                            else str(timestamp)
                        ),
                        exit_fees=exit_fees,
                    )

                    # Filtrage min PnL
                    pnl_val = (
                        position.pnl_realized
                        if position.pnl_realized is not None
                        else 0.0
                    )
                    pnl_pct = abs(pnl_val / (position.entry_price * position.qty)) * 100
                    if pnl_pct >= strategy_params.min_pnl_pct:
                        # Trade valide: mise Ã  jour cash
                        pnl_val = (
                            position.pnl_realized
                            if position.pnl_realized is not None
                            else 0.0
                        )
                        cash += pnl_val + (position.entry_price * position.qty)
                        trades.append(position)
                        logger.debug(
                            f"Position fermÃ©e @ {current_price:.2f}: {exit_reason}, PnL={position.pnl_realized:.2f}"
                        )
                    else:
                        # Trade filtrÃ©: PnL trop faible
                        logger.debug(
                            f"Trade filtrÃ© (PnL {pnl_pct:.4f}% < {strategy_params.min_pnl_pct}%)"
                        )

                    position = None

            # Nouveau signal d'entrÃ©e (si pas de position)
            if position is None and signal in ["ENTER_LONG", "ENTER_SHORT"]:
                # Position sizing basÃ© sur ATR et risk
                atr_stop_distance = current_atr * strategy_params.atr_multiplier
                risk_amount = cash * strategy_params.risk_per_trade

                # Calcul quantitÃ© optimale
                position_size = risk_amount / atr_stop_distance
                max_position_size = (cash * strategy_params.leverage) / current_price

                qty = min(position_size, max_position_size)

                if qty > 0:
                    # Calcul prix stop
                    if signal == "ENTER_LONG":
                        stop_price = current_price - atr_stop_distance
                    else:
                        stop_price = current_price + atr_stop_distance

                    # Frais d'entrÃ©e
                    entry_value = current_price * qty
                    entry_fees = entry_value * fee_rate

                    if entry_value + entry_fees <= cash:
                        # CrÃ©ation nouveau trade
                        position = Trade(
                            side=signal.replace("ENTER_", ""),
                            qty=qty,
                            entry_price=current_price,
                            entry_time=(
                                str(timestamp)
                                if hasattr(timestamp, "isoformat")
                                else str(timestamp)
                            ),
                            stop=stop_price,
                            fees_paid=entry_fees,
                            meta={
                                "bb_z": bb_z_vals[i],
                                "atr": current_atr,
                                "atr_multiplier": strategy_params.atr_multiplier,
                                "risk_per_trade": strategy_params.risk_per_trade,
                                "entry_bar_index": i,  # Stocker l'index pour calcul O(1) de bars_held
                            },
                        )

                        # Mise Ã  jour cash
                        cash -= entry_value + entry_fees

                        logger.debug(
                            f"Nouvelle position: {signal} {qty:.4f} @ {current_price:.2f}, stop={stop_price:.2f}"
                        )

            # Mise Ã  jour Ã©quitÃ©
            if position is not None:
                equity[i] = cash + position.calculate_unrealized_pnl(current_price)
            else:
                equity[i] = cash

        # Fermeture position finale si nÃ©cessaire
        if position is not None:
            final_price = df["close"].iloc[-1]
            position.close_trade(
                exit_price=final_price,
                exit_time=df.index[-1].isoformat(),
                exit_fees=final_price * position.qty * fee_rate,
            )

            # Application filtrage min PnL
            pnl_val = (
                position.pnl_realized if position.pnl_realized is not None else 0.0
            )
            pnl_pct = abs(pnl_val / (position.entry_price * position.qty)) * 100
            if pnl_pct >= strategy_params.min_pnl_pct:
                trades.append(position)

        # Construction courbe d'Ã©quitÃ©
        equity_curve = pd.Series(equity, index=df.index)

        # Calcul statistiques
        run_stats = RunStats.from_trades_and_equity(
            trades=trades,
            equity_curve=equity_curve,
            initial_capital=initial_capital,
            meta={
                "strategy": "BBAtr",
                "params": params,
                "fee_bps": fee_bps,
                "slippage_bps": slippage_bps,
                "symbol": self.symbol,
                "timeframe": self.timeframe,
            },
        )

        logger.debug(
            f"Backtest terminÃ©: {run_stats.total_trades} trades, PnL={run_stats.total_pnl:.2f} ({run_stats.total_pnl_pct:.2f}%)"
        )

        return equity_curve, run_stats


# ==========================================
# CONVENIENCE FUNCTIONS
# ==========================================


def generate_signals(
    df: pd.DataFrame, params: dict, symbol: str = "UNKNOWN", timeframe: str = "15m"
) -> pd.DataFrame:
    """
    Fonction de convenance pour gÃ©nÃ©ration de signaux BB+ATR.

    Args:
        df: DataFrame OHLCV
        params: ParamÃ¨tres stratÃ©gie
        symbol: Symbole pour cache
        timeframe: Timeframe pour cache

    Returns:
        DataFrame avec signaux et mÃ©tadonnÃ©es

    Example:
        >>> params = {'bb_period': 20, 'bb_std': 2.0, 'entry_z': 1.5}
        >>> signals = generate_signals(df, params, "BTCUSDT", "1h")
    """
    strategy = BBAtrStrategy(symbol=symbol, timeframe=timeframe)
    return strategy.generate_signals(df, params)


def backtest(
    df: pd.DataFrame,
    params: dict,
    initial_capital: float = 10000.0,
    symbol: str = "UNKNOWN",
    timeframe: str = "15m",
    **kwargs,
) -> Tuple[pd.Series, RunStats]:
    """
    Fonction de convenance pour backtest BB+ATR.

    Args:
        df: DataFrame OHLCV
        params: ParamÃ¨tres stratÃ©gie
        initial_capital: Capital initial
        symbol: Symbole pour cache
        timeframe: Timeframe pour cache
        **kwargs: Arguments supplÃ©mentaires (fee_bps, slippage_bps, etc.)

    Returns:
        Tuple (equity_curve, run_stats)

    Example:
        >>> params = BBAtrParams(bb_period=50, atr_multiplier=2.0).to_dict()
        >>> equity, stats = backtest(df, params, 50000, "ETHUSDT", "4h")
        >>> print(f"ROI: {stats.total_pnl_pct:.2f}%, Trades: {stats.total_trades}")
    """
    strategy = BBAtrStrategy(symbol=symbol, timeframe=timeframe)
    return strategy.backtest(df, params, initial_capital, **kwargs)


def create_default_params(**overrides) -> BBAtrParams:
    """
    CrÃ©e des paramÃ¨tres par dÃ©faut avec surcharges optionnelles.

    Args:
        **overrides: ParamÃ¨tres Ã  surcharger

    Returns:
        Instance BBAtrParams avec valeurs par dÃ©faut + surcharges

    Example:
        >>> params = create_default_params(bb_period=50, atr_multiplier=2.5)
        >>> params.bb_period
        50
        >>> params.bb_std  # Valeur par dÃ©faut conservÃ©e
        2.0
    """
    base_params = BBAtrParams()

    # Application des surcharges
    for key, value in overrides.items():
        if hasattr(base_params, key):
            setattr(base_params, key, value)
        else:
            logger.warning(f"ParamÃ¨tre inconnu ignorÃ©: {key}={value}")

    return base_params


# ==========================================
# MODULE EXPORTS
# ==========================================

__all__ = [
    # Classes principales
    "BBAtrParams",
    "BBAtrStrategy",
    # Fonctions de convenance
    "generate_signals",
    "backtest",
    "create_default_params",
]

----------------------------------------
Fichier: strategy\bollinger_dual.py
"""
ThreadX - Bollinger Dual Strategy
===================================

StratÃ©gie basÃ©e sur les bandes de Bollinger avec double condition d'entrÃ©e:
- Signal d'entrÃ©e: Franchissement de bande + Franchissement de MA
- Trailing stop dynamique Ã  partir de la mÃ©diane
- Stop loss fixe configurable pour les shorts

Author: ThreadX Framework
Version: 1.0.0
"""

from dataclasses import dataclass, field
from typing import Dict, Any, Tuple, Optional, List
import pandas as pd
import numpy as np
import logging

from threadx.configuration.settings import S
from threadx.utils.log import get_logger
from threadx.strategy.model import (
    Strategy,
    Trade,
    RunStats,
    validate_ohlcv_dataframe,
    validate_strategy_params,
)
from threadx.indicators import ensure_indicator

logger = get_logger(__name__)


@dataclass
class BollingerDualParams:
    """
    ParamÃ¨tres de la stratÃ©gie Bollinger Dual.

    Attributes:
        # Bollinger Bands
        bb_window: PÃ©riode pour Bollinger SMA (dÃ©faut: 20)
        bb_std: Multiplicateur Ã©cart-type pour bandes (dÃ©faut: 2.0)

        # Moving Average pour franchissement
        ma_window: PÃ©riode MA pour signal franchissement (dÃ©faut: 10)
        ma_type: Type de MA - 'sma' ou 'ema' (dÃ©faut: 'sma')

        # Trailing Stop
        trailing_pct: Pourcentage pour trailing stop (0.8 = 80%) (dÃ©faut: 0.8)
        median_activated: Activer trailing uniquement aprÃ¨s mÃ©diane (dÃ©faut: True)

        # Stop Loss fixe pour SHORT
        short_stop_pct: Stop loss fixe SHORT en % au-dessus entrÃ©e (dÃ©faut: 0.37 = 37%)

        # Risk Management
        risk_per_trade: Risque par trade en fraction du capital (dÃ©faut: 0.02 = 2%)
        max_hold_bars: DurÃ©e max position en barres (dÃ©faut: 100)

        # MÃ©tadonnÃ©es
        meta: Dictionnaire mÃ©tadonnÃ©es personnalisÃ©es
    """

    # Bollinger Bands
    bb_window: int = 20
    bb_std: float = 2.0

    # Moving Average
    ma_window: int = 10
    ma_type: str = 'sma'

    # Trailing Stop
    trailing_pct: float = 0.8  # 80% entre bande et mÃ©diane
    median_activated: bool = True

    # Stop Loss SHORT
    short_stop_pct: float = 0.37  # 37%

    # Risk Management
    risk_per_trade: float = 0.02
    max_hold_bars: int = 100

    # MÃ©tadonnÃ©es
    meta: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validation des paramÃ¨tres"""
        if self.bb_window < 2:
            raise ValueError(f"bb_window must be >= 2, got: {self.bb_window}")
        if self.bb_std <= 0:
            raise ValueError(f"bb_std must be > 0, got: {self.bb_std}")
        if self.ma_window < 1:
            raise ValueError(f"ma_window must be >= 1, got: {self.ma_window}")
        if self.ma_type not in ['sma', 'ema']:
            raise ValueError(f"ma_type must be 'sma' or 'ema', got: {self.ma_type}")
        if not 0 < self.trailing_pct <= 1:
            raise ValueError(f"trailing_pct must be in (0, 1], got: {self.trailing_pct}")
        if self.short_stop_pct < 0:
            raise ValueError(f"short_stop_pct must be >= 0, got: {self.short_stop_pct}")
        if not 0 < self.risk_per_trade <= 1:
            raise ValueError(f"risk_per_trade must be in (0, 1], got: {self.risk_per_trade}")
        if self.max_hold_bars < 1:
            raise ValueError(f"max_hold_bars must be >= 1, got: {self.max_hold_bars}")

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire pour compatibilitÃ©"""
        return {
            "bb_window": self.bb_window,
            "bb_std": self.bb_std,
            "ma_window": self.ma_window,
            "ma_type": self.ma_type,
            "trailing_pct": self.trailing_pct,
            "median_activated": self.median_activated,
            "short_stop_pct": self.short_stop_pct,
            "risk_per_trade": self.risk_per_trade,
            "max_hold_bars": self.max_hold_bars,
            "meta": self.meta,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "BollingerDualParams":
        """CrÃ©e depuis un dictionnaire"""
        return cls(
            bb_window=data.get("bb_window", 20),
            bb_std=data.get("bb_std", 2.0),
            ma_window=data.get("ma_window", 10),
            ma_type=data.get("ma_type", 'sma'),
            trailing_pct=data.get("trailing_pct", 0.8),
            median_activated=data.get("median_activated", True),
            short_stop_pct=data.get("short_stop_pct", 0.37),
            risk_per_trade=data.get("risk_per_trade", 0.02),
            max_hold_bars=data.get("max_hold_bars", 100),
            meta=data.get("meta", {}),
        )


class BollingerDualStrategy:
    """
    ImplÃ©mentation de la stratÃ©gie Bollinger Dual.

    Logique de trading:
    1. LONG: Prix < Bande Basse + Franchissement haussier MA
    2. SHORT: Prix > Bande Haute + Franchissement baissier MA
    3. Trailing Stop aprÃ¨s mÃ©diane:
       - LONG: stop = bande_basse + (mÃ©diane - bande_basse) * trailing_pct
       - SHORT: stop = mÃ©diane + (bande_haute - mÃ©diane) * trailing_pct
    4. Stop Loss fixe SHORT: entry_price * (1 + short_stop_pct)
    """

    def __init__(self, symbol: str = "UNKNOWN", timeframe: str = "1h"):
        """
        Initialise la stratÃ©gie.

        Args:
            symbol: Symbole pour cache d'indicateurs
            timeframe: Timeframe pour cache d'indicateurs
        """
        self.symbol = symbol
        self.timeframe = timeframe
        logger.info(f"StratÃ©gie Bollinger Dual initialisÃ©e: {symbol}/{timeframe}")

    def _ensure_indicators(
        self, df: pd.DataFrame, params: BollingerDualParams
    ) -> Tuple[pd.DataFrame, np.ndarray]:
        """
        Garantit la disponibilitÃ© des indicateurs via IndicatorBank.

        Args:
            df: DataFrame OHLCV
            params: ParamÃ¨tres stratÃ©gie

        Returns:
            Tuple (df_with_indicators, ma_array)
        """
        logger.debug(
            f"Calcul indicateurs: BB(window={params.bb_window}, std={params.bb_std}), MA(window={params.ma_window}, type={params.ma_type})"
        )

        # Bollinger Bands
        bb_result = ensure_indicator(
            "bollinger",
            {"period": params.bb_window, "std": params.bb_std},
            df,
            symbol=self.symbol,
            timeframe=self.timeframe,
        )

        if isinstance(bb_result, tuple) and len(bb_result) == 3:
            upper, middle, lower = bb_result
            df_bb = df.copy()
            df_bb["bb_upper"] = upper
            df_bb["bb_middle"] = middle
            df_bb["bb_lower"] = lower
        else:
            raise ValueError(f"Bollinger result format invalide: {type(bb_result)}")

        # Moving Average pour franchissement
        ma_indicator = "sma" if params.ma_type == 'sma' else "ema"
        ma_result = ensure_indicator(
            ma_indicator,
            {"period": params.ma_window},
            df,
            symbol=self.symbol,
            timeframe=self.timeframe,
        )

        if isinstance(ma_result, np.ndarray):
            ma_array = ma_result
        else:
            raise ValueError(f"MA result format invalide: {type(ma_result)}")

        logger.debug(
            f"Indicateurs calculÃ©s: BB range [{lower.min():.2f}, {upper.max():.2f}], MA dernier {ma_array[-1]:.2f}"
        )

        return df_bb, ma_array

    def generate_signals(self, df: pd.DataFrame, params: dict) -> pd.DataFrame:
        """
        GÃ©nÃ¨re les signaux de trading basÃ©s sur Bollinger Dual.

        Args:
            df: DataFrame OHLCV avec timestamp index (UTC)
            params: Dictionnaire paramÃ¨tres (format BollingerDualParams.to_dict())

        Returns:
            DataFrame avec colonne 'signal' et mÃ©tadonnÃ©es

        Signals gÃ©nÃ©rÃ©s:
        - "ENTER_LONG": Prix < Bande Basse + Franchissement haussier MA
        - "ENTER_SHORT": Prix > Bande Haute + Franchissement baissier MA
        - "HOLD": Maintenir position actuelle
        """
        logger.info(f"GÃ©nÃ©ration signaux Bollinger Dual: {len(df)} barres")

        # Validation inputs
        validate_ohlcv_dataframe(df)
        validate_strategy_params(params, ["bb_window", "bb_std", "ma_window"])

        # Parse paramÃ¨tres
        strategy_params = BollingerDualParams.from_dict(params)

        # Ensure indicateurs
        df_with_indicators, ma_array = self._ensure_indicators(df, strategy_params)

        # Extraction des donnÃ©es
        close = df["close"].values
        bb_upper = df_with_indicators["bb_upper"].values
        bb_lower = df_with_indicators["bb_lower"].values
        bb_middle = df_with_indicators["bb_middle"].values

        # Initialisation signaux
        n_bars = len(df)
        signals = np.full(n_bars, "HOLD", dtype=object)

        # Logique de signaux
        logger.debug(
            f"Application logique signaux: BB({strategy_params.bb_window}), MA({strategy_params.ma_window}, {strategy_params.ma_type})"
        )

        for i in range(max(strategy_params.bb_window, strategy_params.ma_window) + 1, n_bars):
            # Skip si donnÃ©es invalides
            if np.isnan(bb_upper[i]) or np.isnan(bb_lower[i]) or np.isnan(ma_array[i]):
                continue

            # DÃ©tection franchissement MA
            ma_cross_up = close[i] > ma_array[i] and close[i-1] <= ma_array[i-1]
            ma_cross_down = close[i] < ma_array[i] and close[i-1] >= ma_array[i-1]

            # LONG: Prix < Bande Basse + Franchissement haussier MA
            if close[i] < bb_lower[i] and ma_cross_up:
                signals[i] = "ENTER_LONG"
                logger.debug(
                    f"ENTER_LONG @ bar {i}: price={close[i]:.2f}, bb_lower={bb_lower[i]:.2f}, ma={ma_array[i]:.2f}"
                )

            # SHORT: Prix > Bande Haute + Franchissement baissier MA
            elif close[i] > bb_upper[i] and ma_cross_down:
                signals[i] = "ENTER_SHORT"
                logger.debug(
                    f"ENTER_SHORT @ bar {i}: price={close[i]:.2f}, bb_upper={bb_upper[i]:.2f}, ma={ma_array[i]:.2f}"
                )

        # Construction DataFrame de sortie
        result_df = pd.DataFrame(index=df.index)
        result_df["signal"] = signals
        result_df["bb_upper"] = bb_upper
        result_df["bb_middle"] = bb_middle
        result_df["bb_lower"] = bb_lower
        result_df["ma"] = ma_array
        result_df["close"] = close

        # Statistiques signaux
        enter_longs = np.sum(signals == "ENTER_LONG")
        enter_shorts = np.sum(signals == "ENTER_SHORT")
        total_signals = enter_longs + enter_shorts

        logger.info(
            f"Signaux gÃ©nÃ©rÃ©s: {total_signals} total ({enter_longs} LONG, {enter_shorts} SHORT)"
        )

        return result_df

    def backtest(
        self,
        df: pd.DataFrame,
        params: dict,
        initial_capital: float = 10000.0,
        fee_bps: float = 4.5,
        slippage_bps: float = 0.0,
    ) -> Tuple[pd.Series, RunStats]:
        """
        ExÃ©cute un backtest complet de la stratÃ©gie Bollinger Dual.

        Args:
            df: DataFrame OHLCV avec timestamp index (UTC)
            params: ParamÃ¨tres stratÃ©gie (format BollingerDualParams.to_dict())
            initial_capital: Capital initial
            fee_bps: Frais de transaction en basis points (dÃ©faut: 4.5)
            slippage_bps: Slippage en basis points (dÃ©faut: 0.0)

        Returns:
            Tuple (equity_curve, run_stats)

        Gestion des positions:
        - Trailing stop Ã  80% (bande â†” mÃ©diane) aprÃ¨s franchissement mÃ©diane
        - Stop loss fixe SHORT: +37%
        - Sortie forcÃ©e aprÃ¨s max_hold_bars
        """
        logger.info(
            f"DÃ©but backtest Bollinger Dual: capital={initial_capital}, fee={fee_bps}bps"
        )

        # Validation
        validate_ohlcv_dataframe(df)
        strategy_params = BollingerDualParams.from_dict(params)

        # GÃ©nÃ©ration signaux
        signals_df = self.generate_signals(df, params)

        # Initialisation backtest
        n_bars = len(df)
        equity = np.full(n_bars, initial_capital, dtype=float)

        cash = initial_capital
        position = None
        trades: List[Trade] = []
        median_reached = False  # Flag pour tracking mÃ©diane

        fee_rate = (fee_bps + slippage_bps) / 10000.0

        logger.debug(f"Backtest initialisÃ©: {n_bars} barres, fee_rate={fee_rate:.6f}")

        # Boucle principale
        for i, (timestamp, row) in enumerate(signals_df.iterrows()):
            current_price = row["close"]
            signal = row["signal"]
            bb_upper = row["bb_upper"]
            bb_middle = row["bb_middle"]
            bb_lower = row["bb_lower"]

            # Skip si donnÃ©es invalides
            if np.isnan(current_price) or np.isnan(bb_middle):
                equity[i] = cash if position is None else cash + position.calculate_unrealized_pnl(current_price)
                continue

            # Gestion position existante
            if position is not None:
                should_exit = False
                exit_reason = ""

                # 1. VÃ©rifier si mÃ©diane atteinte (pour activer trailing)
                if strategy_params.median_activated and not median_reached:
                    if position.is_long() and current_price >= bb_middle:
                        median_reached = True
                        logger.debug(f"LONG: MÃ©diane atteinte @ {current_price:.2f}")
                    elif position.is_short() and current_price <= bb_middle:
                        median_reached = True
                        logger.debug(f"SHORT: MÃ©diane atteinte @ {current_price:.2f}")

                # 2. Mise Ã  jour trailing stop si mÃ©diane atteinte
                if median_reached:
                    if position.is_long():
                        # Trailing LONG: stop = bande_basse + (mÃ©diane - bande_basse) * trailing_pct
                        new_stop = bb_lower + (bb_middle - bb_lower) * strategy_params.trailing_pct
                        if new_stop > position.stop:  # Trail vers le haut seulement
                            position.stop = new_stop
                            logger.debug(f"LONG trailing stop ajustÃ©: {new_stop:.2f}")
                    else:
                        # Trailing SHORT: stop = mÃ©diane + (bande_haute - mÃ©diane) * trailing_pct
                        new_stop = bb_middle + (bb_upper - bb_middle) * strategy_params.trailing_pct
                        if new_stop < position.stop:  # Trail vers le bas seulement
                            position.stop = new_stop
                            logger.debug(f"SHORT trailing stop ajustÃ©: {new_stop:.2f}")

                # 3. VÃ©rifier stop loss
                if position.should_stop_loss(current_price):
                    should_exit = True
                    exit_reason = "stop_loss"

                # 4. VÃ©rifier durÃ©e max
                entry_timestamp = pd.to_datetime(position.entry_time, utc=True)
                bars_held = (df.index <= timestamp).sum() - (df.index <= entry_timestamp).sum()

                if bars_held >= strategy_params.max_hold_bars:
                    should_exit = True
                    exit_reason = "max_hold_bars"

                # Fermeture position
                if should_exit:
                    exit_value = current_price * position.qty
                    exit_fees = exit_value * fee_rate

                    position.close_trade(
                        exit_price=current_price,
                        exit_time=str(timestamp) if hasattr(timestamp, "isoformat") else str(timestamp),
                        exit_fees=exit_fees,
                    )

                    pnl_val = position.pnl_realized if position.pnl_realized is not None else 0.0
                    cash += pnl_val + (position.entry_price * position.qty)
                    trades.append(position)
                    logger.debug(
                        f"Position fermÃ©e @ {current_price:.2f}: {exit_reason}, PnL={position.pnl_realized:.2f}"
                    )

                    position = None
                    median_reached = False

            # Nouveau signal d'entrÃ©e
            if position is None and signal in ["ENTER_LONG", "ENTER_SHORT"]:
                # Position sizing simple (basÃ© sur risk_per_trade)
                risk_amount = cash * strategy_params.risk_per_trade
                position_size = risk_amount / current_price

                if position_size > 0:
                    # Calcul prix stop
                    if signal == "ENTER_LONG":
                        # LONG: stop initial = bande basse
                        stop_price = bb_lower
                    else:
                        # SHORT: stop fixe = entry + 37%
                        stop_price = current_price * (1 + strategy_params.short_stop_pct)

                    # Frais d'entrÃ©e
                    entry_value = current_price * position_size
                    entry_fees = entry_value * fee_rate

                    if entry_value + entry_fees <= cash:
                        # CrÃ©ation nouveau trade
                        position = Trade(
                            side=signal.replace("ENTER_", ""),
                            qty=position_size,
                            entry_price=current_price,
                            entry_time=str(timestamp) if hasattr(timestamp, "isoformat") else str(timestamp),
                            stop=stop_price,
                            fees_paid=entry_fees,
                            meta={
                                "bb_upper": bb_upper,
                                "bb_middle": bb_middle,
                                "bb_lower": bb_lower,
                                "ma": row["ma"],
                                "trailing_pct": strategy_params.trailing_pct,
                            },
                        )

                        cash -= entry_value + entry_fees
                        median_reached = False

                        logger.debug(
                            f"Nouvelle position: {signal} {position_size:.4f} @ {current_price:.2f}, stop={stop_price:.2f}"
                        )

            # Mise Ã  jour Ã©quitÃ©
            if position is not None:
                equity[i] = cash + position.calculate_unrealized_pnl(current_price)
            else:
                equity[i] = cash

        # Fermeture position finale si nÃ©cessaire
        if position is not None:
            final_price = df["close"].iloc[-1]
            position.close_trade(
                exit_price=final_price,
                exit_time=df.index[-1].isoformat(),
                exit_fees=final_price * position.qty * fee_rate,
            )
            trades.append(position)

        # Construction courbe d'Ã©quitÃ©
        equity_curve = pd.Series(equity, index=df.index)

        # Calcul statistiques
        run_stats = RunStats.from_trades_and_equity(
            trades=trades,
            equity_curve=equity_curve,
            initial_capital=initial_capital,
            meta={
                "strategy": "BollingerDual",
                "params": params,
                "fee_bps": fee_bps,
                "slippage_bps": slippage_bps,
                "symbol": self.symbol,
                "timeframe": self.timeframe,
            },
        )

        logger.info(
            f"Backtest terminÃ©: {run_stats.total_trades} trades, PnL={run_stats.total_pnl:.2f} ({run_stats.total_pnl_pct:.2f}%)"
        )

        return equity_curve, run_stats


def create_default_params(**overrides) -> BollingerDualParams:
    """
    CrÃ©e des paramÃ¨tres par dÃ©faut avec surcharges optionnelles.

    Args:
        **overrides: ParamÃ¨tres Ã  surcharger

    Returns:
        Instance BollingerDualParams avec valeurs par dÃ©faut + surcharges
    """
    base_params = BollingerDualParams()

    for key, value in overrides.items():
        if hasattr(base_params, key):
            setattr(base_params, key, value)
        else:
            logger.warning(f"ParamÃ¨tre inconnu ignorÃ©: {key}={value}")

    return base_params


__all__ = [
    "BollingerDualParams",
    "BollingerDualStrategy",
    "create_default_params",
]

----------------------------------------
Fichier: strategy\model.py
"""
ThreadX Phase 4 - Model Layer
=============================

Types de donnÃ©es et structures pour les stratÃ©gies de trading.

Modules:
- Trade: Structure de transaction
- RunStats: Statistiques de performance
- Strategy Protocol: Interface standardisÃ©e
- JSON serialization/dÃ©sÃ©rialization

CaractÃ©ristiques:
- TypedDict et dataclasses pour performances et validation
- SÃ©rialisation JSON complÃ¨te
- Protocol Pattern pour extensibilitÃ©
- Validation intÃ©grÃ©e des paramÃ¨tres
"""

from dataclasses import dataclass, field, asdict
from typing import Protocol, Dict, Any, Optional, List, Tuple, Union
from typing_extensions import TypedDict, NotRequired
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging

from threadx.configuration.settings import S
from threadx.utils.log import get_logger

logger = get_logger(__name__)

# ==========================================
# TRADE STRUCTURES
# ==========================================


class TradeDict(TypedDict):
    """TypedDict pour trade optimisÃ© en performance"""

    side: str  # "LONG" | "SHORT"
    qty: float  # QuantitÃ©
    entry_price: float  # Prix d'entrÃ©e
    entry_time: str  # Timestamp ISO UTC
    exit_price: NotRequired[float]  # Prix sortie (optionnel)
    exit_time: NotRequired[str]  # Timestamp sortie (optionnel)
    stop: float  # Stop loss
    take_profit: NotRequired[float]  # Take profit (optionnel)
    pnl_realized: NotRequired[float]  # PnL rÃ©alisÃ© (optionnel)
    pnl_unrealized: NotRequired[float]  # PnL non rÃ©alisÃ© (optionnel)
    fees_paid: NotRequired[float]  # Frais payÃ©s
    meta: NotRequired[Dict[str, Any]]  # MÃ©tadonnÃ©es


@dataclass
class Trade:
    """
    ReprÃ©sentation complÃ¨te d'une transaction.

    Attributes:
        side: Direction ("LONG", "SHORT")
        qty: QuantitÃ© en nombre d'unitÃ©s
        entry_price: Prix d'entrÃ©e
        entry_time: Timestamp d'entrÃ©e (UTC)
        stop: Prix de stop loss
        exit_price: Prix de sortie (None si pas encore fermÃ©)
        exit_time: Timestamp de sortie (None si pas encore fermÃ©)
        take_profit: Prix de take profit (optionnel)
        pnl_realized: PnL rÃ©alisÃ© aprÃ¨s fermeture
        pnl_unrealized: PnL actuel si position ouverte
        fees_paid: Total des frais payÃ©s
        meta: Dictionnaire de mÃ©tadonnÃ©es (indicateurs, contexte, etc.)

    Example:
        >>> trade = Trade(
        ...     side="LONG",
        ...     qty=1.5,
        ...     entry_price=50000.0,
        ...     entry_time="2024-01-15T10:30:00Z",
        ...     stop=48000.0,
        ...     meta={"bb_z": -2.1, "atr": 1200.5}
        ... )
        >>> trade.is_open()
        True
        >>> trade.calculate_unrealized_pnl(51000.0)
        1500.0
    """

    side: str
    qty: float
    entry_price: float
    entry_time: str
    stop: float
    exit_price: Optional[float] = None
    exit_time: Optional[str] = None
    take_profit: Optional[float] = None
    pnl_realized: Optional[float] = None
    pnl_unrealized: Optional[float] = None
    fees_paid: float = 0.0
    meta: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validation des paramÃ¨tres aprÃ¨s initialisation"""
        if self.side not in ["LONG", "SHORT"]:
            raise ValueError(f"Side must be 'LONG' or 'SHORT', got: {self.side}")

        if self.qty <= 0:
            raise ValueError(f"Quantity must be positive, got: {self.qty}")

        if self.entry_price <= 0:
            raise ValueError(f"Entry price must be positive, got: {self.entry_price}")

        # Validation timestamps
        try:
            pd.to_datetime(self.entry_time, utc=True)
            if self.exit_time:
                pd.to_datetime(self.exit_time, utc=True)
        except Exception as e:
            raise ValueError(f"Invalid timestamp format: {e}")

    def is_open(self) -> bool:
        """VÃ©rifie si la position est encore ouverte"""
        return self.exit_price is None

    def is_long(self) -> bool:
        """VÃ©rifie si c'est une position longue"""
        return self.side == "LONG"

    def is_short(self) -> bool:
        """VÃ©rifie si c'est une position courte"""
        return self.side == "SHORT"

    def calculate_unrealized_pnl(self, current_price: float) -> float:
        """
        Calcule le PnL non rÃ©alisÃ© Ã  un prix donnÃ©.

        Args:
            current_price: Prix actuel du marchÃ©

        Returns:
            PnL non rÃ©alisÃ© (positif = gain, nÃ©gatif = perte)
        """
        if not self.is_open():
            return self.pnl_realized or 0.0

        if self.is_long():
            pnl = (current_price - self.entry_price) * self.qty
        else:
            pnl = (self.entry_price - current_price) * self.qty

        self.pnl_unrealized = pnl - self.fees_paid
        return self.pnl_unrealized

    def close_trade(self, exit_price: float, exit_time: str, exit_fees: float = 0.0):
        """
        Ferme la position et calcule le PnL rÃ©alisÃ©.

        Args:
            exit_price: Prix de sortie
            exit_time: Timestamp de sortie (UTC)
            exit_fees: Frais de sortie supplÃ©mentaires
        """
        if not self.is_open():
            logger.warning(f"Tentative de fermeture d'une position dÃ©jÃ  fermÃ©e")
            return

        self.exit_price = exit_price
        self.exit_time = exit_time
        self.fees_paid += exit_fees

        # Calcul PnL rÃ©alisÃ©
        if self.is_long():
            gross_pnl = (exit_price - self.entry_price) * self.qty
        else:
            gross_pnl = (self.entry_price - exit_price) * self.qty

        self.pnl_realized = gross_pnl - self.fees_paid
        self.pnl_unrealized = None

        logger.debug(
            f"Position fermÃ©e: {self.side} {self.qty} @ {exit_price}, PnL: {self.pnl_realized:.2f}"
        )

    def should_stop_loss(self, current_price: float) -> bool:
        """VÃ©rifie si le stop loss doit Ãªtre dÃ©clenchÃ©"""
        if not self.is_open():
            return False

        if self.is_long():
            return current_price <= self.stop
        else:
            return current_price >= self.stop

    def should_take_profit(self, current_price: float) -> bool:
        """VÃ©rifie si le take profit doit Ãªtre dÃ©clenchÃ©"""
        if not self.is_open() or not self.take_profit:
            return False

        if self.is_long():
            return current_price >= self.take_profit
        else:
            return current_price <= self.take_profit

    def duration_minutes(self) -> Optional[float]:
        """Calcule la durÃ©e du trade en minutes"""
        if not self.exit_time:
            return None

        entry_dt = pd.to_datetime(self.entry_time, utc=True)
        exit_dt = pd.to_datetime(self.exit_time, utc=True)
        return (exit_dt - entry_dt).total_seconds() / 60.0

    def roi_percent(self) -> Optional[float]:
        """Calcule le ROI en pourcentage sur la mise engagÃ©e"""
        if self.pnl_realized is None:
            return None

        invested = self.entry_price * self.qty
        return (self.pnl_realized / invested) * 100.0 if invested > 0 else 0.0

    def to_dict(self) -> TradeDict:
        """Convertit en dictionnaire pour JSON"""
        return TradeDict(
            side=self.side,
            qty=self.qty,
            entry_price=self.entry_price,
            entry_time=self.entry_time,
            stop=self.stop,
            exit_price=self.exit_price or 0.0,  # Valeur par dÃ©faut pour None
            exit_time=self.exit_time or "",  # Valeur par dÃ©faut pour None
            take_profit=self.take_profit or 0.0,
            pnl_realized=self.pnl_realized or 0.0,
            pnl_unrealized=self.pnl_unrealized or 0.0,
            fees_paid=self.fees_paid,
            meta=self.meta,
        )

    @classmethod
    def from_dict(cls, data: Union[Dict[str, Any], TradeDict]) -> "Trade":
        """CrÃ©e un Trade depuis un dictionnaire"""
        return cls(
            side=data["side"],
            qty=data["qty"],
            entry_price=data["entry_price"],
            entry_time=data["entry_time"],
            stop=data["stop"],
            exit_price=data.get("exit_price"),
            exit_time=data.get("exit_time"),
            take_profit=data.get("take_profit"),
            pnl_realized=data.get("pnl_realized"),
            pnl_unrealized=data.get("pnl_unrealized"),
            fees_paid=data.get("fees_paid", 0.0),
            meta=data.get("meta", {}),
        )


# ==========================================
# RUN STATISTICS
# ==========================================


class RunStatsDict(TypedDict):
    """TypedDict pour statistiques de run"""

    final_equity: float
    initial_capital: float
    total_pnl: float
    total_pnl_pct: float
    sharpe_ratio: NotRequired[float]
    sortino_ratio: NotRequired[float]
    max_drawdown: float
    max_drawdown_pct: float
    max_drawdown_duration_bars: NotRequired[int]
    total_trades: int
    win_trades: int
    loss_trades: int
    win_rate_pct: float
    avg_win: NotRequired[float]
    avg_loss: NotRequired[float]
    profit_factor: NotRequired[float]
    avg_trade_duration_minutes: NotRequired[float]
    total_fees_paid: float
    start_time: str
    end_time: str
    bars_analyzed: int
    meta: NotRequired[Dict[str, Any]]


@dataclass
class RunStats:
    """
    Statistiques complÃ¨tes d'un run de backtesting.

    Attributes:
        final_equity: Capital final
        initial_capital: Capital initial
        total_pnl: PnL total rÃ©alisÃ©
        sharpe_ratio: Ratio de Sharpe (rendement/risque)
        sortino_ratio: Ratio de Sortino (rendement/downside risk)
        max_drawdown: Drawdown maximum absolu
        max_drawdown_pct: Drawdown maximum en %
        total_trades: Nombre total de trades
        win_trades: Nombre de trades gagnants
        loss_trades: Nombre de trades perdants
        win_rate_pct: Taux de rÃ©ussite en %
        avg_win: Gain moyen par trade gagnant
        avg_loss: Perte moyenne par trade perdant
        profit_factor: Facteur de profit (total gains / total pertes)
        avg_trade_duration_minutes: DurÃ©e moyenne des trades
        total_fees_paid: Total des frais payÃ©s
        start_time: Timestamp dÃ©but analyse
        end_time: Timestamp fin analyse
        bars_analyzed: Nombre de barres analysÃ©es
        meta: MÃ©tadonnÃ©es (paramÃ¨tres, configuration, etc.)

    Example:
        >>> equity_curve = pd.Series([10000, 10500, 9800, 11200])
        >>> trades = [Trade(...), Trade(...)]
        >>> stats = RunStats.from_trades_and_equity(trades, equity_curve, 10000)
        >>> print(f"ROI: {stats.total_pnl_pct:.2f}%, Win Rate: {stats.win_rate_pct:.1f}%")
    """

    final_equity: float
    initial_capital: float
    total_pnl: float
    max_drawdown: float
    max_drawdown_pct: float
    total_trades: int
    win_trades: int
    loss_trades: int
    win_rate_pct: float
    total_fees_paid: float
    start_time: str
    end_time: str
    bars_analyzed: int
    sharpe_ratio: Optional[float] = None
    sortino_ratio: Optional[float] = None
    max_drawdown_duration_bars: Optional[int] = None
    avg_win: Optional[float] = None
    avg_loss: Optional[float] = None
    profit_factor: Optional[float] = None
    avg_trade_duration_minutes: Optional[float] = None
    meta: Dict[str, Any] = field(default_factory=dict)

    @property
    def total_pnl_pct(self) -> float:
        """ROI total en pourcentage"""
        return (
            (self.total_pnl / self.initial_capital) * 100.0
            if self.initial_capital > 0
            else 0.0
        )

    @property
    def is_profitable(self) -> bool:
        """VÃ©rifie si le run est profitable"""
        return self.total_pnl > 0

    @property
    def has_trades(self) -> bool:
        """VÃ©rifie si des trades ont Ã©tÃ© gÃ©nÃ©rÃ©s"""
        return self.total_trades > 0

    def risk_reward_ratio(self) -> Optional[float]:
        """Calcule le ratio risque/rÃ©compense"""
        if not self.avg_win or not self.avg_loss or self.avg_loss >= 0:
            return None
        return abs(self.avg_win / self.avg_loss)

    def expectancy(self) -> Optional[float]:
        """Calcule l'espÃ©rance de gain par trade"""
        if not self.has_trades:
            return None

        win_prob = self.win_rate_pct / 100.0
        loss_prob = 1.0 - win_prob

        if self.avg_win and self.avg_loss:
            return (win_prob * self.avg_win) + (loss_prob * self.avg_loss)

        return self.total_pnl / self.total_trades

    def to_dict(self) -> RunStatsDict:
        """Convertit en dictionnaire pour JSON"""
        return RunStatsDict(
            final_equity=self.final_equity,
            initial_capital=self.initial_capital,
            total_pnl=self.total_pnl,
            total_pnl_pct=self.total_pnl_pct,
            sharpe_ratio=self.sharpe_ratio or 0.0,
            sortino_ratio=self.sortino_ratio or 0.0,
            max_drawdown=self.max_drawdown,
            max_drawdown_pct=self.max_drawdown_pct,
            max_drawdown_duration_bars=self.max_drawdown_duration_bars or 0,
            total_trades=self.total_trades,
            win_trades=self.win_trades,
            loss_trades=self.loss_trades,
            win_rate_pct=self.win_rate_pct,
            avg_win=self.avg_win or 0.0,
            avg_loss=self.avg_loss or 0.0,
            profit_factor=self.profit_factor or 0.0,
            avg_trade_duration_minutes=self.avg_trade_duration_minutes or 0.0,
            total_fees_paid=self.total_fees_paid,
            start_time=self.start_time,
            end_time=self.end_time,
            bars_analyzed=self.bars_analyzed,
            meta=self.meta,
        )

    @classmethod
    def from_dict(cls, data: Union[Dict[str, Any], RunStatsDict]) -> "RunStats":
        """CrÃ©e RunStats depuis un dictionnaire"""
        return cls(
            final_equity=data["final_equity"],
            initial_capital=data["initial_capital"],
            total_pnl=data["total_pnl"],
            max_drawdown=data["max_drawdown"],
            max_drawdown_pct=data["max_drawdown_pct"],
            total_trades=data["total_trades"],
            win_trades=data["win_trades"],
            loss_trades=data["loss_trades"],
            win_rate_pct=data["win_rate_pct"],
            total_fees_paid=data["total_fees_paid"],
            start_time=data["start_time"],
            end_time=data["end_time"],
            bars_analyzed=data["bars_analyzed"],
            sharpe_ratio=data.get("sharpe_ratio"),
            sortino_ratio=data.get("sortino_ratio"),
            max_drawdown_duration_bars=data.get("max_drawdown_duration_bars"),
            avg_win=data.get("avg_win"),
            avg_loss=data.get("avg_loss"),
            profit_factor=data.get("profit_factor"),
            avg_trade_duration_minutes=data.get("avg_trade_duration_minutes"),
            meta=data.get("meta", {}),
        )

    @classmethod
    def from_trades_and_equity(
        cls,
        trades: List[Trade],
        equity_curve: pd.Series,
        initial_capital: float,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        meta: Optional[Dict[str, Any]] = None,
    ) -> "RunStats":
        """
        Calcule les statistiques depuis une liste de trades et courbe d'Ã©quitÃ©.

        Args:
            trades: Liste des trades exÃ©cutÃ©s
            equity_curve: SÃ©rie temporelle de l'Ã©quitÃ©
            initial_capital: Capital initial
            start_time: Timestamp de dÃ©but (optionnel)
            end_time: Timestamp de fin (optionnel)
            meta: MÃ©tadonnÃ©es supplÃ©mentaires

        Returns:
            RunStats calculÃ© avec toutes les mÃ©triques
        """
        logger.debug(
            f"Calcul statistiques depuis {len(trades)} trades et equity_curve de {len(equity_curve)} points"
        )

        # Statistiques de base
        final_equity = (
            float(equity_curve.iloc[-1]) if len(equity_curve) > 0 else initial_capital
        )
        total_pnl = final_equity - initial_capital

        # Drawdown
        running_max = equity_curve.expanding().max()
        drawdown = equity_curve - running_max
        max_drawdown = float(drawdown.min())
        max_drawdown_pct = (
            (max_drawdown / initial_capital) * 100.0 if initial_capital > 0 else 0.0
        )

        # DurÃ©e drawdown maximum
        is_at_max = equity_curve == running_max
        drawdown_periods = []
        current_period = 0
        for at_max in is_at_max:
            if at_max:
                if current_period > 0:
                    drawdown_periods.append(current_period)
                current_period = 0
            else:
                current_period += 1
        if current_period > 0:
            drawdown_periods.append(current_period)

        max_drawdown_duration_bars = max(drawdown_periods) if drawdown_periods else 0

        # Statistiques trades
        closed_trades = [t for t in trades if not t.is_open()]
        total_trades = len(closed_trades)

        if total_trades > 0:
            wins = [t for t in closed_trades if t.pnl_realized and t.pnl_realized > 0]
            losses = [
                t for t in closed_trades if t.pnl_realized and t.pnl_realized <= 0
            ]

            win_trades = len(wins)
            loss_trades = len(losses)
            win_rate_pct = (win_trades / total_trades) * 100.0

            # Filtrer les None pour les calculs numpy
            win_pnls = [t.pnl_realized for t in wins if t.pnl_realized is not None]
            loss_pnls = [t.pnl_realized for t in losses if t.pnl_realized is not None]

            avg_win = np.mean(win_pnls) if win_pnls else None
            avg_loss = np.mean(loss_pnls) if loss_pnls else None

            total_wins = sum(win_pnls) if win_pnls else 0
            total_losses = abs(sum(loss_pnls)) if loss_pnls else 0
            profit_factor = total_wins / total_losses if total_losses > 0 else None

            # DurÃ©e moyenne des trades
            durations = [
                t.duration_minutes()
                for t in closed_trades
                if t.duration_minutes() is not None
            ]
            # Filtrer les None pour la durÃ©e
            valid_durations = [d for d in durations if d is not None]
            avg_trade_duration_minutes = (
                np.mean(valid_durations) if valid_durations else None
            )

        else:
            win_trades = loss_trades = 0
            win_rate_pct = 0.0
            avg_win = avg_loss = profit_factor = avg_trade_duration_minutes = None

        # Ratios de Sharpe et Sortino
        sharpe_ratio = sortino_ratio = None
        if len(equity_curve) > 1:
            returns = equity_curve.pct_change().dropna()
            if len(returns) > 0 and returns.std() > 0:
                sharpe_ratio = float(returns.mean() / returns.std() * np.sqrt(252))

                negative_returns = returns[returns < 0]
                if len(negative_returns) > 0 and negative_returns.std() > 0:
                    sortino_ratio = float(
                        returns.mean() / negative_returns.std() * np.sqrt(252)
                    )

        # Frais totaux
        total_fees_paid = sum(t.fees_paid for t in trades)

        # Timestamps
        if not start_time and len(equity_curve) > 0:
            start_time = (
                equity_curve.index[0].isoformat()
                if hasattr(equity_curve.index[0], "isoformat")
                else str(equity_curve.index[0])
            )
        if not end_time and len(equity_curve) > 0:
            end_time = (
                equity_curve.index[-1].isoformat()
                if hasattr(equity_curve.index[-1], "isoformat")
                else str(equity_curve.index[-1])
            )

        return cls(
            final_equity=final_equity,
            initial_capital=initial_capital,
            total_pnl=total_pnl,
            max_drawdown=max_drawdown,
            max_drawdown_pct=max_drawdown_pct,
            max_drawdown_duration_bars=max_drawdown_duration_bars,
            total_trades=total_trades,
            win_trades=win_trades,
            loss_trades=loss_trades,
            win_rate_pct=win_rate_pct,
            avg_win=float(avg_win) if avg_win is not None else None,
            avg_loss=float(avg_loss) if avg_loss is not None else None,
            profit_factor=profit_factor,
            avg_trade_duration_minutes=(
                float(avg_trade_duration_minutes)
                if avg_trade_duration_minutes is not None
                else None
            ),
            sharpe_ratio=sharpe_ratio,
            sortino_ratio=sortino_ratio,
            total_fees_paid=total_fees_paid,
            start_time=start_time or "",
            end_time=end_time or "",
            bars_analyzed=len(equity_curve),
            meta=meta or {},
        )


# ==========================================
# STRATEGY PROTOCOL
# ==========================================


class Strategy(Protocol):
    """
    Protocol dÃ©finissant l'interface standardisÃ©e pour les stratÃ©gies de trading.

    Toute stratÃ©gie doit implÃ©menter ces mÃ©thodes pour Ãªtre compatible
    avec le framework ThreadX.

    Example:
        >>> class MyStrategy:
        ...     def generate_signals(self, df: pd.DataFrame, params: dict) -> pd.DataFrame:
        ...         # ImplÃ©mentation des signaux
        ...         pass
        ...
        ...     def backtest(self, df: pd.DataFrame, params: dict, initial_capital: float) -> Tuple[pd.Series, RunStats]:
        ...         # ImplÃ©mentation du backtest
        ...         pass
        >>>
        >>> # La stratÃ©gie respecte automatiquement le Protocol
        >>> strategy: Strategy = MyStrategy()
    """

    def generate_signals(self, df: pd.DataFrame, params: dict) -> pd.DataFrame:
        """
        GÃ©nÃ¨re les signaux de trading pour un DataFrame OHLCV.

        Args:
            df: DataFrame avec colonnes OHLCV + timestamp index (UTC)
            params: Dictionnaire des paramÃ¨tres de stratÃ©gie

        Returns:
            DataFrame avec colonne 'signal' contenant les signaux:
            - "ENTER_LONG": Entrer en position longue
            - "ENTER_SHORT": Entrer en position courte
            - "EXIT": Sortir de position
            - "HOLD": Maintenir position actuelle

        Raises:
            ValueError: Si les paramÃ¨tres sont invalides
            KeyError: Si des colonnes OHLCV sont manquantes
        """
        ...

    def backtest(
        self, df: pd.DataFrame, params: dict, initial_capital: float = 10000.0
    ) -> Tuple[pd.Series, RunStats]:
        """
        ExÃ©cute un backtest complet de la stratÃ©gie.

        Args:
            df: DataFrame OHLCV avec timestamp index (UTC)
            params: ParamÃ¨tres de stratÃ©gie (dÃ©pendants de l'implÃ©mentation)
            initial_capital: Capital initial en devise de base

        Returns:
            Tuple contenant:
            - pd.Series: Courbe d'Ã©quitÃ© indexÃ©e par timestamp
            - RunStats: Statistiques complÃ¨tes du run

        Raises:
            ValueError: Si les paramÃ¨tres ou donnÃ©es sont invalides
        """
        ...


# ==========================================
# JSON SERIALIZATION
# ==========================================


class ThreadXJSONEncoder(json.JSONEncoder):
    """Encodeur JSON personnalisÃ© pour les types ThreadX"""

    def default(self, o):  # Nom correct pour JSONEncoder
        if isinstance(o, (Trade, RunStats)):
            return o.to_dict()
        elif isinstance(o, np.ndarray):
            return o.tolist()
        elif isinstance(o, np.floating):
            return float(o)
        elif isinstance(o, np.integer):
            return int(o)
        elif isinstance(o, pd.Timestamp):
            return o.isoformat()
        elif isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


def save_run_results(
    trades: List[Trade],
    stats: RunStats,
    equity_curve: pd.Series,
    output_path: Union[str, Path],
    metadata: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Sauvegarde les rÃ©sultats d'un run en JSON.

    Args:
        trades: Liste des trades exÃ©cutÃ©s
        stats: Statistiques du run
        equity_curve: Courbe d'Ã©quitÃ©
        output_path: Chemin de sauvegarde
        metadata: MÃ©tadonnÃ©es supplÃ©mentaires (paramÃ¨tres, config, etc.)
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Conversion equity curve pour JSON
    equity_data = {
        "timestamps": [
            ts.isoformat() if hasattr(ts, "isoformat") else str(ts)
            for ts in equity_curve.index
        ],
        "values": equity_curve.values.tolist(),
    }

    run_data = {
        "metadata": metadata or {},
        "stats": stats.to_dict(),
        "trades": [trade.to_dict() for trade in trades],
        "equity_curve": equity_data,
        "generated_at": datetime.utcnow().isoformat(),
        "threadx_version": "4.0.0",
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(run_data, f, cls=ThreadXJSONEncoder, indent=2, ensure_ascii=False)

    logger.info(
        f"RÃ©sultats sauvÃ©s: {output_path} ({len(trades)} trades, {stats.total_trades} fermÃ©s)"
    )


def load_run_results(
    file_path: Union[str, Path],
) -> Tuple[List[Trade], RunStats, pd.Series]:
    """
    Charge les rÃ©sultats d'un run depuis JSON.

    Args:
        file_path: Chemin du fichier JSON

    Returns:
        Tuple contenant (trades, stats, equity_curve)
    """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Reconstruction des objets
    trades = [Trade.from_dict(trade_data) for trade_data in data["trades"]]
    stats = RunStats.from_dict(data["stats"])

    # Reconstruction equity curve
    equity_data = data["equity_curve"]
    timestamps = pd.to_datetime(equity_data["timestamps"], utc=True)
    equity_curve = pd.Series(equity_data["values"], index=timestamps)

    logger.info(
        f"RÃ©sultats chargÃ©s: {file_path} ({len(trades)} trades, {stats.total_trades} fermÃ©s)"
    )
    return trades, stats, equity_curve


# ==========================================
# VALIDATION UTILITIES
# ==========================================


def validate_ohlcv_dataframe(df: pd.DataFrame) -> None:
    """
    Valide qu'un DataFrame contient les colonnes OHLCV requises.

    Args:
        df: DataFrame Ã  valider

    Raises:
        ValueError: Si le DataFrame est invalide
    """
    if df is None or df.empty:
        raise ValueError("DataFrame is None or empty")

    required_columns = {"open", "high", "low", "close", "volume"}
    actual_columns = {col.lower() for col in df.columns}
    missing = required_columns - actual_columns

    if missing:
        raise ValueError(f"Missing OHLCV columns: {missing}")

    # Validation index timestamp
    if not isinstance(df.index, pd.DatetimeIndex):
        raise ValueError("DataFrame index must be DatetimeIndex")

    # Validation valeurs numÃ©riques
    for col in ["open", "high", "low", "close", "volume"]:
        if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
            raise ValueError(f"Column {col} must be numeric")

    logger.debug(
        f"DataFrame OHLCV validÃ©: {len(df)} barres, {df.index[0]} Ã  {df.index[-1]}"
    )


def validate_strategy_params(params: dict, required_keys: List[str]) -> None:
    """
    Valide les paramÃ¨tres de stratÃ©gie.

    Args:
        params: Dictionnaire des paramÃ¨tres
        required_keys: ClÃ©s requises

    Raises:
        ValueError: Si des paramÃ¨tres sont manquants ou invalides
    """
    if not isinstance(params, dict):
        raise ValueError("Strategy params must be a dictionary")

    missing = set(required_keys) - set(params.keys())
    if missing:
        raise ValueError(f"Missing required strategy parameters: {missing}")

    logger.debug(f"ParamÃ¨tres stratÃ©gie validÃ©s: {list(params.keys())}")


# ==========================================
# MODULE EXPORTS
# ==========================================

__all__ = [
    # Types de donnÃ©es
    "Trade",
    "TradeDict",
    "RunStats",
    "RunStatsDict",
    # Protocol
    "Strategy",
    # JSON utilities
    "ThreadXJSONEncoder",
    "save_run_results",
    "load_run_results",
    # Validation
    "validate_ohlcv_dataframe",
    "validate_strategy_params",
]




----------------------------------------
Fichier: strategy\__init__.py
"""
ThreadX Phase 4 - Strategy Layer
================================

Module de stratÃ©gies de trading avec gestion avancÃ©e du risque.

Modules:
- model.py : Types de donnÃ©es (Trade, RunStats, Strategy Protocol)
- bb_atr.py : StratÃ©gie Bollinger Bands + ATR avec amÃ©liorations

CaractÃ©ristiques:
- Protocol Pattern pour extensibilitÃ© des stratÃ©gies
- IntÃ©gration native avec Phase 3 Indicators Layer
- Gestion positions avancÃ©e (trailing stops, risk sizing)
- SÃ©rialisation JSON complÃ¨te des rÃ©sultats
- Backtest dÃ©terministe et reproductible

AmÃ©liorations vs TradXPro:
- Architecture modulaire et testable
- ParamÃ¨tres typÃ©s avec validation
- Cache intelligent via IndicatorBank
- Filtrage min PnL et micro-optimisations
"""

from .model import (
    # Types de donnÃ©es
    Trade,
    TradeDict,
    RunStats,
    RunStatsDict,
    # Protocol interface
    Strategy,
    # JSON utilities
    ThreadXJSONEncoder,
    save_run_results,
    load_run_results,
    # Validation
    validate_ohlcv_dataframe,
    validate_strategy_params,
)

from .bb_atr import (
    # ParamÃ¨tres et implÃ©mentation
    BBAtrParams,
    BBAtrStrategy,
    # Fonctions de convenance (prÃ©fixÃ©es pour Ã©viter les conflits)
    generate_signals as bb_atr_generate_signals,
    backtest as bb_atr_backtest,
    create_default_params as bb_atr_create_default_params,
)

from .bollinger_dual import (
    # ParamÃ¨tres et implÃ©mentation
    BollingerDualParams,
    BollingerDualStrategy,
    # Fonctions de convenance
    create_default_params as bollinger_dual_create_default_params,
)

from .amplitude_hunter import (
    # ParamÃ¨tres et implÃ©mentation
    AmplitudeHunterParams,
    AmplitudeHunterStrategy,
    # Fonctions de convenance
    generate_signals as amplitude_hunter_generate_signals,
    backtest as amplitude_hunter_backtest,
    create_default_params as amplitude_hunter_create_default_params,
)

__version__ = "4.0.0"

__all__ = [
    # Model exports
    "Trade",
    "TradeDict",
    "RunStats",
    "RunStatsDict",
    "Strategy",
    "ThreadXJSONEncoder",
    "save_run_results",
    "load_run_results",
    "validate_ohlcv_dataframe",
    "validate_strategy_params",
    # BB+ATR Strategy exports
    "BBAtrParams",
    "BBAtrStrategy",
    "bb_atr_generate_signals",
    "bb_atr_backtest",
    "bb_atr_create_default_params",
    # Bollinger Dual Strategy exports
    "BollingerDualParams",
    "BollingerDualStrategy",
    "bollinger_dual_create_default_params",
    # AmplitudeHunter Strategy exports
    "AmplitudeHunterParams",
    "AmplitudeHunterStrategy",
    "amplitude_hunter_generate_signals",
    "amplitude_hunter_backtest",
    "amplitude_hunter_create_default_params",
]




----------------------------------------
Fichier: strategy\_archive\gpu_examples.py
"""
ThreadX Strategy GPU Integration - Phase 5 Example
===================================================

Exemple d'intÃ©gration de la distribution multi-GPU avec les stratÃ©gies.

DÃ©montre comment utiliser MultiGPUManager pour accÃ©lÃ©rer:
- Calculs d'indicateurs en batch
- Backtests parallÃ¨les sur grilles de paramÃ¨tres
- Sweeps de Monte Carlo

Usage:
    >>> from threadx.strategy.gpu_examples import GPUAcceleratedBBAtr
    >>>
    >>> strategy = GPUAcceleratedBBAtr("BTCUSDC", "15m")
    >>> strategy.enable_gpu_acceleration()
    >>>
    >>> # Backtest accÃ©lÃ©rÃ©
    >>> equity, stats = strategy.backtest_gpu(df, params)
"""

import time
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
import pandas as pd

from threadx.utils.log import get_logger
from threadx.utils.gpu import get_default_manager, MultiGPUManager
from threadx.strategy.bb_atr import BBAtrStrategy, BBAtrParams
from threadx.strategy.model import Trade, RunStats
from threadx.indicators.gpu_integration import get_gpu_accelerated_bank

logger = get_logger(__name__)


class GPUAcceleratedBBAtr(BBAtrStrategy):
    """
    Version GPU-accelerated de la stratÃ©gie BB+ATR.

    Utilise la distribution multi-GPU pour:
    - Calculs d'indicateurs parallÃ¨les
    - Simulation de signaux en batch
    - Backtests Monte Carlo
    """

    def __init__(
        self, symbol: str, timeframe: str, gpu_manager: Optional[MultiGPUManager] = None
    ):
        """
        Initialise la stratÃ©gie BB+ATR avec accÃ©lÃ©ration GPU.

        Args:
            symbol: Symbole trading (ex. "BTCUSDC")
            timeframe: Timeframe (ex. "15m")
            gpu_manager: Gestionnaire multi-GPU optionnel
        """
        super().__init__(symbol, timeframe)

        self.gpu_manager = gpu_manager or get_default_manager()
        self.gpu_indicator_bank = get_gpu_accelerated_bank()
        self.gpu_enabled = len(self.gpu_manager._gpu_devices) > 0

        logger.info(
            f"StratÃ©gie BB+ATR GPU initialisÃ©e: {self.symbol}/{self.timeframe}, "
            f"GPU={'activÃ©' if self.gpu_enabled else 'dÃ©sactivÃ©'}"
        )

    def enable_gpu_acceleration(self, min_samples: int = 5000) -> None:
        """
        Active l'accÃ©lÃ©ration GPU pour cette stratÃ©gie.

        Args:
            min_samples: Nombre min d'Ã©chantillons pour utiliser GPU
        """
        self.gpu_indicator_bank.min_samples_for_gpu = min_samples
        logger.info(f"AccÃ©lÃ©ration GPU activÃ©e (seuil: {min_samples} Ã©chantillons)")

    def compute_indicators_gpu(
        self, df: pd.DataFrame, params: Dict[str, Any]
    ) -> Dict[str, pd.Series]:
        """
        Calcul des indicateurs avec accÃ©lÃ©ration multi-GPU.

        Args:
            df: DataFrame OHLCV
            params: ParamÃ¨tres stratÃ©gie

        Returns:
            Dict avec indicateurs calculÃ©s
        """
        start_time = time.time()

        # Extraction paramÃ¨tres
        bb_period = params.get("bb_period", 20)
        bb_std = params.get("bb_std", 2.0)
        atr_period = params.get("atr_period", 14)

        # Calcul Bollinger Bands GPU
        bb_upper, bb_middle, bb_lower = self.gpu_indicator_bank.bollinger_bands(
            df, period=bb_period, std_dev=bb_std, use_gpu=True
        )

        # Calcul ATR GPU
        atr = self.gpu_indicator_bank.atr(df, period=atr_period, use_gpu=True)

        # Z-score Bollinger
        bb_z = (df["close"] - bb_middle) / (bb_upper - bb_lower)
        bb_z.name = "bb_z"

        elapsed = time.time() - start_time
        logger.info(
            f"Indicateurs GPU calculÃ©s: {len(df)} Ã©chantillons en {elapsed:.3f}s"
        )

        return {
            "bb_upper": bb_upper,
            "bb_middle": bb_middle,
            "bb_lower": bb_lower,
            "bb_z": bb_z,
            "atr": atr,
        }

    def generate_signals_batch_gpu(
        self, df: pd.DataFrame, param_grid: List[Dict[str, Any]]
    ) -> List[pd.DataFrame]:
        """
        GÃ©nÃ©ration de signaux en batch avec distribution GPU.

        Args:
            df: DataFrame OHLCV
            param_grid: Liste de paramÃ¨tres Ã  tester

        Returns:
            Liste des DataFrames de signaux pour chaque paramÃ¨tre
        """
        logger.info(
            f"GÃ©nÃ©ration signaux batch GPU: {len(param_grid)} paramÃ¨tres, "
            f"{len(df)} Ã©chantillons"
        )

        def signal_compute_func(batch_data):
            """
            Fonction vectorielle pour gÃ©nÃ©ration signaux.

            batch_data contient: [df_values, param_index]
            """
            if batch_data.ndim != 2 or batch_data.shape[1] < 6:
                # Fallback simple
                return np.zeros(batch_data.shape[0])

            # Extraction OHLCV (colonnes 0-4) et param_index (colonne 5)
            ohlcv_data = batch_data[:, :5]  # open, high, low, close, volume
            param_indices = batch_data[:, 5].astype(int)

            signals = np.zeros(len(ohlcv_data))

            # Traitement par chunks de paramÃ¨tres
            unique_param_indices = np.unique(param_indices)

            for param_idx in unique_param_indices:
                if param_idx >= len(param_grid):
                    continue

                mask = param_indices == param_idx
                chunk_ohlcv = ohlcv_data[mask]
                params = param_grid[param_idx]

                if len(chunk_ohlcv) < 2:
                    continue

                # Calcul simple BB Z-score pour signaux
                close_prices = chunk_ohlcv[:, 3]  # Colonne close
                bb_period = min(params.get("bb_period", 20), len(close_prices))

                if bb_period < 2:
                    continue

                # Moving average simple
                weights = np.ones(bb_period) / bb_period
                ma = np.convolve(close_prices, weights, mode="same")

                # Standard deviation
                squared_diff = (close_prices - ma) ** 2
                variance = np.convolve(squared_diff, weights, mode="same")
                std = np.sqrt(variance + 1e-8)

                # Z-score
                z_score = (close_prices - ma) / (2 * std + 1e-8)
                entry_z = params.get("entry_z", 1.0)

                # Signaux simples
                chunk_signals = np.where(
                    z_score < -entry_z,
                    1,  # ENTER_LONG
                    np.where(z_score > entry_z, -1, 0),
                )  # ENTER_SHORT

                signals[mask] = chunk_signals

            return signals

        # PrÃ©paration donnÃ©es pour distribution
        results = []

        try:
            # CrÃ©ation batch data avec param indices
            ohlcv_array = df[["open", "high", "low", "close", "volume"]].values

            # Pour chaque paramÃ¨tre, on duplique les donnÃ©es avec l'index param
            all_batch_data = []
            param_mapping = []

            for param_idx, params in enumerate(param_grid):
                # Ajout colonne param_index aux donnÃ©es OHLCV
                param_column = np.full((len(ohlcv_array), 1), param_idx)
                batch_data = np.hstack([ohlcv_array, param_column])
                all_batch_data.append(batch_data)
                param_mapping.extend([param_idx] * len(batch_data))

            # ConcatÃ©nation pour distribution
            full_batch_data = np.vstack(all_batch_data)

            # Distribution GPU
            start_time = time.time()
            signal_results = self.gpu_manager.distribute_workload(
                full_batch_data, signal_compute_func, seed=42
            )
            gpu_time = time.time() - start_time

            # Reconstruction par paramÃ¨tre
            signal_idx = 0
            for param_idx, params in enumerate(param_grid):
                param_signals = signal_results[signal_idx : signal_idx + len(df)]
                signal_idx += len(df)

                # CrÃ©ation DataFrame signaux
                signals_df = df.copy()
                signals_df["signal"] = signal_signals
                signals_df["signal_str"] = np.where(
                    signal_signals == 1,
                    "ENTER_LONG",
                    np.where(signal_signals == -1, "ENTER_SHORT", "HOLD"),
                )
                signals_df["param_set"] = param_idx

                results.append(signals_df)

            logger.info(
                f"Signaux batch GPU terminÃ©s: {gpu_time:.3f}s pour "
                f"{len(param_grid)} paramÃ¨tres"
            )

        except Exception as e:
            logger.error(f"Erreur gÃ©nÃ©ration signaux batch GPU: {e}")

            # Fallback CPU sÃ©quentiel
            logger.info("Fallback gÃ©nÃ©ration signaux CPU")
            for param_idx, params in enumerate(param_grid):
                signals_df = self.generate_signals(df, params)
                signals_df["param_set"] = param_idx
                results.append(signals_df)

        return results

    def backtest_monte_carlo_gpu(
        self,
        df: pd.DataFrame,
        base_params: Dict[str, Any],
        n_simulations: int = 1000,
        param_ranges: Optional[Dict[str, Tuple[float, float]]] = None,
    ) -> List[RunStats]:
        """
        Backtest Monte Carlo multi-GPU avec variations de paramÃ¨tres.

        Args:
            df: DataFrame OHLCV
            base_params: ParamÃ¨tres de base
            n_simulations: Nombre de simulations Monte Carlo
            param_ranges: Ranges de variation {"param": (min, max)}

        Returns:
            Liste des RunStats de toutes les simulations
        """
        if param_ranges is None:
            param_ranges = {
                "bb_std": (1.5, 2.5),
                "entry_z": (0.5, 2.0),
                "atr_multiplier": (1.0, 2.5),
            }

        logger.info(
            f"Backtest Monte Carlo GPU: {n_simulations} simulations, "
            f"{len(df)} Ã©chantillons"
        )

        # GÃ©nÃ©ration paramÃ¨tres alÃ©atoires
        np.random.seed(42)  # ReproductibilitÃ©
        param_sets = []

        for i in range(n_simulations):
            params = base_params.copy()

            for param_name, (min_val, max_val) in param_ranges.items():
                random_val = np.random.uniform(min_val, max_val)
                params[param_name] = random_val

            param_sets.append(params)

        # Fonction de backtest vectorielle
        def mc_backtest_func(batch_data):
            """
            Backtest vectorisÃ© pour Monte Carlo.

            Approximation rapide des mÃ©triques pour nombreuses simulations.
            """
            if batch_data.ndim != 2:
                return np.zeros(batch_data.shape[0])

            # Simulation simplifiÃ©e des returns
            # Dans la vraie vie, ici on ferait le backtest complet
            ohlcv_chunk = batch_data[:, :5]  # OHLCV
            param_indices = batch_data[:, 5].astype(int)

            # MÃ©trique approximative: volatilitÃ© ajustÃ©e du return
            close_prices = ohlcv_chunk[:, 3]
            returns = np.diff(close_prices, prepend=close_prices[0]) / close_prices

            # Score basÃ© sur paramÃ¨tres et volatilitÃ©
            scores = np.zeros(len(batch_data))
            unique_params = np.unique(param_indices)

            for param_idx in unique_params:
                if param_idx >= len(param_sets):
                    continue

                mask = param_indices == param_idx
                chunk_returns = returns[mask]
                params = param_sets[param_idx]

                # Score simplifiÃ© (Sharpe-like)
                if len(chunk_returns) > 1:
                    mean_return = np.mean(chunk_returns)
                    std_return = np.std(chunk_returns) + 1e-8
                    sharpe_like = mean_return / std_return

                    # Ajustement selon paramÃ¨tres
                    bb_std = params.get("bb_std", 2.0)
                    entry_z = params.get("entry_z", 1.0)
                    param_bonus = (bb_std - 2.0) + (entry_z - 1.0)

                    final_score = sharpe_like + param_bonus
                else:
                    final_score = 0.0

                scores[mask] = final_score

            return scores

        # PrÃ©paration donnÃ©es batch pour GPU
        try:
            start_time = time.time()

            # Duplication des donnÃ©es OHLCV pour chaque simulation
            ohlcv_array = df[["open", "high", "low", "close", "volume"]].values

            all_batch_data = []
            for sim_idx in range(n_simulations):
                param_column = np.full((len(ohlcv_array), 1), sim_idx)
                batch_data = np.hstack([ohlcv_array, param_column])
                all_batch_data.append(batch_data)

            full_batch_data = np.vstack(all_batch_data)

            # Distribution GPU
            mc_scores = self.gpu_manager.distribute_workload(
                full_batch_data, mc_backtest_func, seed=42
            )

            gpu_time = time.time() - start_time

            # Reconstruction des rÃ©sultats
            results = []
            score_idx = 0

            for sim_idx in range(n_simulations):
                sim_scores = mc_scores[score_idx : score_idx + len(df)]
                score_idx += len(df)

                # CrÃ©ation RunStats approximatives
                final_score = np.mean(sim_scores)
                params = param_sets[sim_idx]

                # RunStats simulÃ©es (dans la vraie vie: backtest complet)
                mock_stats = RunStats(
                    total_trades=max(1, int(abs(final_score) * 100)),
                    win_trades=max(0, int(abs(final_score) * 60)),
                    loss_trades=max(0, int(abs(final_score) * 40)),
                    total_pnl=final_score * 1000,  # Scaling arbitraire
                    total_fees_paid=abs(final_score) * 50,
                    bars_analyzed=len(df),
                    initial_capital=10000,
                    meta={"simulation": sim_idx, "params": params},
                )

                results.append(mock_stats)

            logger.info(
                f"Monte Carlo GPU terminÃ©: {gpu_time:.3f}s pour "
                f"{n_simulations} simulations"
            )

            return results

        except Exception as e:
            logger.error(f"Erreur Monte Carlo GPU: {e}")

            # Fallback: simulations sÃ©quentielles simplifiÃ©es
            logger.info("Fallback Monte Carlo CPU")
            results = []

            for sim_idx, params in enumerate(param_sets):
                # Simulation trÃ¨s basique pour fallback
                mock_stats = RunStats(
                    total_trades=np.random.randint(10, 100),
                    win_trades=np.random.randint(5, 60),
                    loss_trades=np.random.randint(5, 40),
                    total_pnl=np.random.uniform(-1000, 1000),
                    total_fees_paid=np.random.uniform(10, 100),
                    bars_analyzed=len(df),
                    initial_capital=10000,
                    meta={"simulation": sim_idx, "params": params},
                )
                results.append(mock_stats)

            return results

    def optimize_gpu_balance_for_strategy(
        self, sample_df: pd.DataFrame
    ) -> Dict[str, float]:
        """
        Optimise la balance GPU spÃ©cifiquement pour cette stratÃ©gie.

        Args:
            sample_df: DonnÃ©es reprÃ©sentatives

        Returns:
            Ratios optimisÃ©s
        """
        logger.info("Optimisation balance GPU pour stratÃ©gie BB+ATR")

        # Test avec indicateurs de la stratÃ©gie
        optimal_ratios = self.gpu_indicator_bank.optimize_balance(sample_df, runs=3)

        # Application Ã  notre gestionnaire
        self.gpu_manager.set_balance(optimal_ratios)

        return optimal_ratios

    def get_gpu_performance_report(self) -> Dict[str, Any]:
        """
        Rapport de performance GPU pour cette stratÃ©gie.

        Returns:
            Stats complÃ¨tes de performance
        """
        gpu_stats = self.gpu_manager.get_device_stats()
        indicator_stats = self.gpu_indicator_bank.get_performance_stats()

        return {
            "strategy_info": {
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "gpu_enabled": self.gpu_enabled,
            },
            "gpu_manager": gpu_stats,
            "indicator_bank": indicator_stats,
            "recommendations": self._get_performance_recommendations(gpu_stats),
        }

    def _get_performance_recommendations(self, gpu_stats: Dict) -> List[str]:
        """GÃ©nÃ¨re des recommandations d'optimisation."""
        recommendations = []

        # Analyse utilisation mÃ©moire
        for device_name, stats in gpu_stats.items():
            if device_name == "cpu":
                continue

            memory_used_pct = stats.get("memory_used_pct", 0)

            if memory_used_pct > 80:
                recommendations.append(
                    f"Device {device_name}: MÃ©moire Ã©levÃ©e ({memory_used_pct:.1f}%), "
                    f"rÃ©duire batch_size ou utiliser plus de devices"
                )
            elif memory_used_pct < 20:
                recommendations.append(
                    f"Device {device_name}: MÃ©moire sous-utilisÃ©e ({memory_used_pct:.1f}%), "
                    f"augmenter batch_size pour meilleure performance"
                )

        # Analyse balance
        if len([d for d in gpu_stats if d != "cpu"]) > 1:
            recommendations.append(
                "Multi-GPU dÃ©tectÃ©: utiliser profile_auto_balance() pour optimisation"
            )

        return recommendations


# === Fonctions utilitaires ===


def create_gpu_strategy(symbol: str, timeframe: str) -> GPUAcceleratedBBAtr:
    """
    CrÃ©e une stratÃ©gie BB+ATR avec accÃ©lÃ©ration GPU optimale.

    Args:
        symbol: Symbole trading
        timeframe: Timeframe

    Returns:
        StratÃ©gie configurÃ©e pour GPU
    """
    strategy = GPUAcceleratedBBAtr(symbol, timeframe)
    strategy.enable_gpu_acceleration(min_samples=2000)

    logger.info(f"StratÃ©gie GPU crÃ©Ã©e: {symbol}/{timeframe}")
    return strategy


def benchmark_gpu_vs_cpu(
    df: pd.DataFrame, params: Dict[str, Any], n_runs: int = 5
) -> Dict[str, float]:
    """
    Benchmark GPU vs CPU pour la stratÃ©gie BB+ATR.

    Args:
        df: DonnÃ©es de test
        params: ParamÃ¨tres stratÃ©gie
        n_runs: Nombre de runs pour moyenne

    Returns:
        Stats de performance comparÃ©es
    """
    logger.info(f"Benchmark GPU vs CPU: {len(df)} Ã©chantillons, {n_runs} runs")

    # Test CPU
    cpu_strategy = BBAtrStrategy("TEST", "15m")
    cpu_times = []

    for run in range(n_runs):
        start = time.time()
        _ = cpu_strategy.generate_signals(df, params)
        cpu_times.append(time.time() - start)

    avg_cpu_time = np.mean(cpu_times)

    # Test GPU
    gpu_strategy = create_gpu_strategy("TEST", "15m")
    gpu_times = []

    for run in range(n_runs):
        start = time.time()
        _ = gpu_strategy.compute_indicators_gpu(df, params)
        gpu_times.append(time.time() - start)

    avg_gpu_time = np.mean(gpu_times)

    speedup = avg_cpu_time / avg_gpu_time if avg_gpu_time > 0 else 1.0

    results = {
        "cpu_time_avg": avg_cpu_time,
        "gpu_time_avg": avg_gpu_time,
        "speedup": speedup,
        "data_size": len(df),
        "gpu_efficiency": min(speedup / 2.0, 1.0),  # EfficacitÃ© relative
    }

    logger.info(
        f"Benchmark terminÃ©: CPU {avg_cpu_time:.3f}s, "
        f"GPU {avg_gpu_time:.3f}s, Speedup {speedup:.2f}x"
    )

    return results




----------------------------------------
Fichier: testing\mocks.py
"""
Mocks centralisÃ©s pour ThreadX - utilisÃ©s uniquement en cas de fallback.

Ces mocks permettent aux applications de fonctionner mÃªme si certains modules
ThreadX ne sont pas disponibles ou chargÃ©s correctement.
"""
# type: ignore  # Trop d'erreurs de type, analyse dÃ©sactivÃ©e

import logging
import time
import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple, Union, List


class MockSettings:
    """Mock pour threadx.config.settings.Settings."""

    @staticmethod
    def get(key: str, default=None):
        """Retourne une valeur de configuration mock."""
        mock_values = {
            "gpu.enable_gpu": False,
            "gpu.devices": ["cpu"],
            "performance.max_workers": 1,
            "performance.cache_ttl_sec": 300,
            "paths.data_root": "./data",
            "paths.logs": "./logs",
            "paths.cache": "./cache",
        }
        return mock_values.get(key, default)


def get_mock_logger(name: str) -> logging.Logger:
    """Mock pour threadx.utils.log.get_logger."""
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger


def setup_mock_logging_once():
    """Mock pour threadx.utils.log.setup_logging_once."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )


class MockBank:
    """Mock pour threadx.indicators.bank.Bank."""

    def ensure(
        self,
        indicator_type: str,
        params: Dict[str, Any],
        data: Union[np.ndarray, pd.Series, pd.DataFrame],
        symbol: str = "",
        timeframe: str = "",
    ) -> Union[np.ndarray, Tuple[np.ndarray, ...]]:
        """Simule l'assurance d'un indicateur."""
        time.sleep(0.1)  # Simule le calcul

        if isinstance(data, pd.DataFrame) and "close" in data.columns:
            close_series = data["close"]
        elif isinstance(data, pd.Series):
            close_series = data
        else:
            # Fallback si pas de donnÃ©es correctes
            close_series = pd.Series(np.arange(100) + 100.0)

        if indicator_type == "bollinger":
            # Convertir en arrays NumPy pour compatibilitÃ© de type
            upper = (close_series + 2.0).values
            middle = close_series.values
            lower = (close_series - 2.0).values
            return (upper, middle, lower)
        elif indicator_type == "atr":
            # Retourner un array NumPy
            return np.ones(len(close_series)) * 0.5
        else:
            # Retourner un array NumPy
            return np.random.randn(len(close_series)) * 0.1 + 0.5

    def batch_ensure(
        self,
        indicator_type: str,
        params_list: List[Dict[str, Any]],
        data: pd.DataFrame,
        symbol: str = "",
        timeframe: str = "",
    ) -> Dict[str, Any]:
        """Simule le calcul batch d'indicateurs."""
        time.sleep(len(params_list) * 0.01)

        results = {}
        for i, params in enumerate(params_list):
            # Utiliser le nom du paramÃ¨tre comme clÃ©
            param_key = f"{indicator_type}_{i}"
            results[param_key] = self.ensure(
                indicator_type, params, data, symbol, timeframe
            )

        return results


class MockRunResult:
    """Mock pour threadx.backtest.engine.RunResult."""

    def __init__(self, returns, trades, meta=None):
        self.returns = returns
        self.trades = trades
        self.equity = (1 + returns).cumprod() * meta.get("initial_capital", 10000)
        self.meta = meta or {
            "status": "completed",
            "execution_time": 1.0,
            "device": "cpu",
        }


class MockBacktestEngine:
    """Mock pour threadx.backtest.engine.BacktestEngine."""

    def __init__(self, controller=None):
        self.controller = controller

    def run(
        self,
        df_1m: pd.DataFrame,
        indicators: Dict[str, Any],
        params: Dict[str, Any],
        symbol: str = "BTCUSDC",
        timeframe: str = "1m",
        seed: int = 42,
        use_gpu: bool = False,
    ) -> MockRunResult:
        """Simule l'exÃ©cution d'un backtest."""
        # DÃ©finir la graine pour dÃ©terminisme
        np.random.seed(seed)

        # Simuler le calcul avec interruptions potentielles
        total_steps = len(df_1m)
        step_interval = max(1, total_steps // 10)  # 10 checkpoints

        for i in range(0, total_steps, step_interval):
            # Checkpoints pour interruption
            if self.controller:
                try:
                    self.controller.check_interruption(f"step {i}/{total_steps}")
                except KeyboardInterrupt:
                    # CrÃ©er un rÃ©sultat partiel avec status="interrupted"
                    returns = pd.Series(
                        np.random.randn(i) * 0.01, index=df_1m.index[:i]
                    )
                    trades = pd.DataFrame(
                        {"entry_time": [], "exit_time": [], "pnl": [], "side": []}
                    )
                    meta = {
                        "status": "interrupted",
                        "execution_time": 0.5,
                        "device": "cpu" if not use_gpu else "gpu",
                        "initial_capital": params.get("initial_capital", 10000),
                    }
                    return MockRunResult(returns, trades, meta)

            # Simuler le travail
            time.sleep(0.01)

        # Simuler un rÃ©sultat normal
        n_trades = min(20, len(df_1m) // 50)
        trade_indices = np.random.choice(
            range(len(df_1m) - 10), size=n_trades, replace=False
        )

        # GÃ©nÃ©rer des returns alÃ©atoires avec une tendance
        returns = pd.Series(np.random.randn(len(df_1m)) * 0.01, index=df_1m.index)

        # CrÃ©er des trades synthÃ©tiques
        trades = pd.DataFrame(
            {
                "entry_time": df_1m.index[trade_indices],
                "exit_time": [
                    t + pd.Timedelta(minutes=np.random.randint(10, 100))
                    for t in df_1m.index[trade_indices]
                ],
                "pnl": np.random.randn(n_trades) * 100,
                "side": np.random.choice(["LONG", "SHORT"], size=n_trades),
                "entry_price": df_1m["close"].iloc[trade_indices].values,
                "exit_price": df_1m["close"].iloc[trade_indices].values
                * (1 + np.random.randn(n_trades) * 0.01),
            }
        )

        meta = {
            "status": "completed",
            "execution_time": np.random.rand() * 2 + 0.5,  # Entre 0.5 et 2.5 secondes
            "device": "cpu" if not use_gpu else "gpu",
            "symbol": symbol,
            "timeframe": timeframe,
            "params": params,
            "initial_capital": params.get("initial_capital", 10000),
            "seed": seed,
        }

        return MockRunResult(returns, trades, meta)


class MockPerformanceCalculator:
    """Mock pour threadx.performance.metrics.PerformanceCalculator."""

    @staticmethod
    def summarize(returns: pd.Series, trades: pd.DataFrame) -> Dict[str, float]:
        """Calcule des mÃ©triques de performance mock."""
        # Simulation de mÃ©triques rÃ©alistes
        total_return = returns.sum()
        sharpe = returns.mean() / returns.std() if returns.std() > 0 else 0
        max_dd = (returns.cumsum() - returns.cumsum().expanding().max()).min()

        # Si trades a des donnÃ©es PnL, utiliser pour win rate
        if "pnl" in trades.columns and len(trades) > 0:
            win_rate = (trades["pnl"] > 0).mean()
            profit_factor = (
                abs(trades[trades["pnl"] > 0]["pnl"].sum())
                / abs(trades[trades["pnl"] < 0]["pnl"].sum())
                if abs(trades[trades["pnl"] < 0]["pnl"].sum()) > 0
                else float("inf")
            )
        else:
            win_rate = 0.5
            profit_factor = 1.0

        return {
            "total_return": total_return,
            "sharpe_ratio": sharpe,
            "max_drawdown": max_dd,
            "win_rate": win_rate,
            "profit_factor": profit_factor,
            "num_trades": len(trades),
        }


# Mocks pour l'UI
def mock_plot_equity(equity: pd.Series, save_path: Union[str, None] = None) -> str:
    """Mock pour threadx.ui.charts.plot_equity."""
    print(f"[MOCK] Generating equity chart with {len(equity)} points")
    return save_path or "equity_chart.png"


def mock_plot_drawdown(equity: pd.Series, save_path: Union[str, None] = None) -> str:
    """Mock pour threadx.ui.charts.plot_drawdown."""
    print(f"[MOCK] Generating drawdown chart from {len(equity)} points")
    return save_path or "drawdown_chart.png"


def mock_render_trades_table(trades: pd.DataFrame) -> Dict[str, Any]:
    """Mock pour threadx.ui.tables.render_trades_table."""
    return {
        "data": trades,
        "summary": {
            "total_trades": len(trades),
            "profitable_trades": (
                len(trades[trades["pnl"] > 0]) if "pnl" in trades.columns else 0
            ),
        },
    }


def mock_render_metrics_table(metrics: Dict[str, float]) -> Dict[str, Any]:
    """Mock pour threadx.ui.tables.render_metrics_table."""
    return {"data": pd.DataFrame(list(metrics.items()), columns=["Metric", "Value"])}


def mock_export_table(df: pd.DataFrame, path: str) -> str:
    """Mock pour threadx.ui.tables.export_table."""
    print(f"[MOCK] Exporting table with {len(df)} rows to {path}")
    return path


# Classe de contrÃ´leur mock pour les tests
class MockBacktestController:
    """Mock pour threadx.backtest.engine.BacktestController."""

    def __init__(self):
        self.is_stopped = False
        self.is_paused = False

    def pause(self):
        self.is_paused = True

    def resume(self):
        self.is_paused = False

    def stop(self):
        self.is_stopped = True

    def reset(self):
        self.is_stopped = False
        self.is_paused = False

    def check_interruption(self, step_info=None):
        if self.is_stopped:
            raise KeyboardInterrupt("Backtest stopped")
        while self.is_paused and not self.is_stopped:
            time.sleep(0.1)
        if self.is_stopped:
            raise KeyboardInterrupt("Backtest stopped")


# Export des mocks principaux
__all__ = [
    "MockSettings",
    "get_mock_logger",
    "setup_mock_logging_once",
    "MockBank",
    "MockBacktestEngine",
    "MockRunResult",
    "MockPerformanceCalculator",
    "MockBacktestController",
    "mock_plot_equity",
    "mock_plot_drawdown",
    "mock_render_trades_table",
    "mock_render_metrics_table",
    "mock_export_table",
]




----------------------------------------
Fichier: testing\__init__.py
"""ThreadX Testing Utilities."""

from .mocks import *

__all__ = [
    "MockSettings",
    "MockLogger",
    "MockBank",
    "MockBacktestEngine",
    "MockPerformanceCalculator",
    "MockBacktestController",
]




----------------------------------------
Fichier: ui\backtest_bridge.py
"""
Lightweight backtest bridge used by the Streamlit UI.

This module provides TWO backtest implementations:
1. run_backtest() - Lightweight CPU-only for quick demos
2. run_backtest_gpu() - Full GPU-accelerated with BacktestEngine

The GPU version connects to the production BacktestEngine with:
- Multi-GPU support (RTX 5090 75% + RTX 2060 25%)
- IndicatorBank for cached GPU computations
- Real-time system monitoring
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
import logging
import time

import numpy as np
import pandas as pd

from .strategy_registry import list_strategies

logger = logging.getLogger(__name__)

# Lazy imports pour Ã©viter overhead si GPU non utilisÃ©
_ENGINE_IMPORTS_DONE = False
_BacktestEngine = None
_IndicatorBank = None
_get_global_monitor = None


def _ensure_gpu_imports():
    """Lazy import des modules GPU lourds."""
    global _ENGINE_IMPORTS_DONE, _BacktestEngine, _IndicatorBank, _get_global_monitor

    if _ENGINE_IMPORTS_DONE:
        return

    try:
        from threadx.backtest.engine import BacktestEngine
        from threadx.indicators.bank import IndicatorBank
        from .system_monitor import get_global_monitor

        _BacktestEngine = BacktestEngine
        _IndicatorBank = IndicatorBank
        _get_global_monitor = get_global_monitor
        _ENGINE_IMPORTS_DONE = True

        logger.info("âœ… GPU modules importÃ©s avec succÃ¨s")
    except ImportError as e:
        logger.warning(f"âš ï¸ Impossible d'importer modules GPU: {e}")
        _ENGINE_IMPORTS_DONE = False


@dataclass
class BacktestResult:
    """Simplified backtest result structure consumed by the UI pages."""

    equity: pd.Series
    metrics: Dict[str, Any] = field(default_factory=dict)
    trades: List[Dict[str, Any]] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


def _resolve_window(params: Dict[str, Any], default: int = 20) -> int:
    """Extract an integer window/period from params with sensible bounds."""
    for key in ("window", "lookback", "period"):
        if key in params:
            try:
                value = int(params[key])
                return max(value, 2)
            except (TypeError, ValueError):
                continue
    return max(int(default), 2)


def _generate_position(close: pd.Series, params: Dict[str, Any]) -> pd.Series:
    """Generate a long-only position series based on simplified Bollinger logic."""
    window = _resolve_window(params)
    if len(close) < window:
        return pd.Series(0.0, index=close.index)

    std_mult = float(params.get("std", 2.0) or 2.0)
    signal_window = max(1, int(params.get("signal_window", 3) or 3))
    gap_input = float(params.get("price_gap_pct", 0.5) or 0.5)
    gap_ratio = abs(gap_input) / 100.0 if abs(gap_input) > 1 else abs(gap_input) / 100.0
    bandwidth_threshold = float(params.get("bandwidth_threshold", 0.0) or 0.0)
    use_bandwidth_filter = bool(params.get("use_bandwidth_filter", False))
    confirm_breakout = bool(params.get("confirm_breakout", False))

    rolling_mean = close.rolling(window, min_periods=window).mean()
    rolling_std = close.rolling(window, min_periods=window).std(ddof=0)
    rolling_std = rolling_std.replace(0, np.nan)

    z_score = (close - rolling_mean) / rolling_std
    z_score = z_score.replace([np.inf, -np.inf], np.nan).fillna(0.0)

    bandwidth = (2 * std_mult * rolling_std) / rolling_mean.replace(0, np.nan).abs()
    if use_bandwidth_filter:
        bandwidth_ok = (bandwidth.fillna(0.0) >= bandwidth_threshold)
    else:
        bandwidth_ok = pd.Series(True, index=close.index)

    long_trigger = z_score <= (-std_mult * (1.0 + gap_ratio))
    exit_trigger = z_score >= (std_mult * 0.5 if confirm_breakout else 0.0)

    if signal_window > 1:
        long_trigger = (
            long_trigger.rolling(signal_window, min_periods=1).mean() >= 0.6
        )
        exit_trigger = (
            exit_trigger.rolling(signal_window, min_periods=1).mean() >= 0.5
        )

    enter_condition = long_trigger & bandwidth_ok
    exit_condition = exit_trigger

    position = pd.Series(0.0, index=close.index)
    in_trade = False

    for idx, timestamp in enumerate(close.index):
        can_enter = bool(enter_condition.iloc[idx])
        can_exit = bool(exit_condition.iloc[idx])

        if not in_trade and can_enter:
            in_trade = True
        elif in_trade and can_exit:
            in_trade = False

        position.iloc[idx] = 1.0 if in_trade else 0.0

    return position


def _compute_equity(close: pd.Series, params: Dict[str, Any]) -> tuple[pd.Series, pd.Series]:
    """Compute equity curve and position series for the lightweight backtest."""
    position = _generate_position(close, params)
    returns = close.pct_change().fillna(0.0)
    strategy_returns = returns * position

    equity = (1.0 + strategy_returns).cumprod()
    if not equity.empty:
        equity.iloc[0] = 1.0
    equity.name = "equity"
    return equity, position


def _build_placeholder_trades(close: pd.Series, position: pd.Series) -> List[Dict[str, Any]]:
    """Convert the position series into placeholder trades for the UI."""
    if close.empty or position.empty:
        return []

    trades: List[Dict[str, Any]] = []
    in_trade = False
    current: Dict[str, Any] | None = None

    for timestamp, pos in position.items():
        price = float(close.loc[timestamp])
        if pos >= 0.5 and not in_trade:
            in_trade = True
            current = {
                "entry_time": timestamp,
                "entry_price": price,
                "side": "LONG",
            }
        elif pos < 0.5 and in_trade and current is not None:
            current["exit_time"] = timestamp
            current["exit_price"] = price
            current["pnl"] = current["exit_price"] - current["entry_price"]
            trades.append(current)
            in_trade = False
            current = None

    if in_trade and current is not None:
        current["exit_time"] = close.index[-1]
        current["exit_price"] = float(close.iloc[-1])
        current["pnl"] = current["exit_price"] - current["entry_price"]
        trades.append(current)

    return trades


def run_backtest(df: pd.DataFrame, strategy: str, params: Dict[str, Any]) -> BacktestResult:
    """Execute a lightweight backtest on the provided OHLCV DataFrame."""
    if not isinstance(df, pd.DataFrame) or df.empty:
        raise ValueError("Le DataFrame d'entree est vide ou invalide.")

    available_strategies = set(list_strategies())
    if strategy not in available_strategies:
        raise ValueError(f"Strategie '{strategy}' non disponible.")

    if "close" not in df.columns:
        raise ValueError("Le DataFrame doit contenir une colonne 'close'.")

    close = pd.to_numeric(df["close"], errors="coerce")
    if close.isna().all():
        raise ValueError("La colonne 'close' ne contient pas de valeurs numeriques exploitables.")

    try:
        equity, position = _compute_equity(close, params)

        returns = equity.pct_change().fillna(0.0)
        metrics: Dict[str, Any] = {
            "total_return": float(equity.iloc[-1] - 1.0),
            "annualized_volatility": float(np.std(returns) * np.sqrt(252)),
            "sharpe_ratio": float((returns.mean() / returns.std()) * np.sqrt(252))
            if returns.std() > 0
            else 0.0,
        }

        trades = _build_placeholder_trades(close, position)
        metadata = {"strategy": strategy, "params": params}

        return BacktestResult(
            equity=equity,
            metrics=metrics,
            trades=trades,
            metadata=metadata,
        )

    except Exception as exc:  # pragma: no cover - defensive fallback
        logger.error("Erreur lors du backtest: %s", exc, exc_info=True)
        return BacktestResult(
            equity=pd.Series(dtype=float),
            metrics={},
            trades=[],
            metadata={"error": str(exc)},
        )


def run_backtest_gpu(
    df: pd.DataFrame,
    strategy: str,
    params: Dict[str, Any],
    *,
    symbol: str = "BTCUSDC",
    timeframe: str = "1m",
    use_gpu: bool = True,
    enable_monitoring: bool = True,
) -> BacktestResult:
    """
    ExÃ©cute un backtest GPU-accelerated avec le moteur de production.

    Cette version utilise le vrai BacktestEngine avec :
    - Multi-GPU (RTX 5090 75% + RTX 2060 25%)
    - IndicatorBank pour cache GPU
    - Monitoring systÃ¨me temps rÃ©el (CPU, GPU1, GPU2)
    - Calculs d'indicateurs optimisÃ©s

    Args:
        df: DataFrame OHLCV avec colonnes [open, high, low, close, volume]
        strategy: Nom de la stratÃ©gie (doit Ãªtre dans registry)
        params: ParamÃ¨tres de stratÃ©gie (entry_z, k_sl, leverage, etc.)
        symbol: Symbole tradÃ© (pour cache IndicatorBank)
        timeframe: Timeframe des donnÃ©es
        use_gpu: Active/dÃ©sactive GPU (True par dÃ©faut)
        enable_monitoring: Active monitoring systÃ¨me temps rÃ©el

    Returns:
        BacktestResult avec equity, metrics, trades, metadata

    Raises:
        ValueError: Si donnÃ©es invalides
        RuntimeError: Si erreur GPU non rÃ©cupÃ©rable

    Example:
        >>> result = run_backtest_gpu(
        ...     df_ohlcv,
        ...     strategy="bollinger_reversion",
        ...     params={"entry_z": 2.0, "k_sl": 1.5, "leverage": 3},
        ...     use_gpu=True,
        ...     enable_monitoring=True
        ... )
        >>> print(f"GPU utilisÃ©: {result.metadata.get('gpu_enabled')}")
        >>> print(f"Multi-GPU: {result.metadata.get('multi_gpu_enabled')}")
    """
    # Import lazy des modules GPU
    _ensure_gpu_imports()

    if not _ENGINE_IMPORTS_DONE:
        logger.warning("âš ï¸ Modules GPU non disponibles, fallback vers run_backtest() CPU")
        return run_backtest(df, strategy, params)

    # Validation entrÃ©es
    if not isinstance(df, pd.DataFrame) or df.empty:
        raise ValueError("Le DataFrame d'entrÃ©e est vide ou invalide.")

    available_strategies = set(list_strategies())
    if strategy not in available_strategies:
        raise ValueError(f"StratÃ©gie '{strategy}' non disponible.")

    required_cols = ["open", "high", "low", "close", "volume"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Le DataFrame doit contenir les colonnes OHLCV: {missing_cols}")

    start_time = time.time()
    logger.info(f"ðŸš€ DÃ©marrage backtest GPU: {symbol} {timeframe}, GPU={use_gpu}, monitoring={enable_monitoring}")

    # DÃ©marrage monitoring systÃ¨me
    monitor = None
    if enable_monitoring and _get_global_monitor:
        try:
            monitor = _get_global_monitor()
            if not monitor.is_running():
                monitor.start()
                logger.info("ðŸ“Š Monitoring systÃ¨me dÃ©marrÃ©")
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur dÃ©marrage monitoring: {e}")
            monitor = None

    try:
        # 1. Initialisation du moteur GPU avec multi-GPU
        engine = _BacktestEngine(use_multi_gpu=True)
        logger.info(f"âœ… BacktestEngine initialisÃ©: GPU={engine.gpu_available}, Multi-GPU={engine.use_multi_gpu}")

        # 2. Calcul des indicateurs via IndicatorBank (avec cache GPU)
        bank = _IndicatorBank()

        # ParamÃ¨tres Bollinger Bands
        bb_period = params.get("bb_period", params.get("window", params.get("period", 20)))
        bb_std = params.get("bb_std", params.get("std", 2.0))

        # ParamÃ¨tres ATR
        atr_period = params.get("atr_period", 14)

        logger.info(f"ðŸ“Š Calcul indicateurs: BB(period={bb_period}, std={bb_std}), ATR(period={atr_period})")

        indicators = {}

        # Bollinger Bands
        try:
            bollinger_result = bank.ensure(
                "bollinger",
                {"period": int(bb_period), "std": float(bb_std)},
                df,
                symbol=symbol,
                timeframe=timeframe,
            )
            indicators["bollinger"] = bollinger_result
            logger.info("âœ… Bollinger Bands calculÃ©s")
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur Bollinger Bands: {e}")
            indicators["bollinger"] = None

        # ATR
        try:
            atr_result = bank.ensure(
                "atr",
                {"period": int(atr_period)},
                df,
                symbol=symbol,
                timeframe=timeframe,
            )
            indicators["atr"] = atr_result
            logger.info("âœ… ATR calculÃ©")
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur ATR: {e}")
            indicators["atr"] = None

        # 3. ParamÃ¨tres pour BacktestEngine
        engine_params = {
            "entry_z": params.get("entry_z", 2.0),
            "k_sl": params.get("k_sl", 1.5),
            "leverage": params.get("leverage", 1.0),
            "initial_capital": params.get("initial_capital", 10000.0),
            "fees_bps": params.get("fees_bps", 10.0),
            "slip_bps": params.get("slip_bps", 5.0),
        }

        # 4. ExÃ©cution du backtest avec GPU
        logger.info(f"âš¡ ExÃ©cution backtest avec use_gpu={use_gpu}")
        result = engine.run(
            df_1m=df,
            indicators=indicators,
            params=engine_params,
            symbol=symbol,
            timeframe=timeframe,
            use_gpu=use_gpu,
            seed=42,
        )

        # 5. Conversion RunResult â†’ BacktestResult
        # Calcul mÃ©triques simplifiÃ©es pour UI
        returns = result.returns
        total_return = float(result.equity.iloc[-1] / result.equity.iloc[0] - 1.0)
        volatility = float(returns.std() * np.sqrt(252)) if len(returns) > 1 else 0.0
        sharpe = float((returns.mean() / returns.std()) * np.sqrt(252)) if returns.std() > 0 else 0.0

        metrics = {
            "total_return": total_return,
            "annualized_volatility": volatility,
            "sharpe_ratio": sharpe,
        }

        # Conversion trades DataFrame â†’ List[Dict]
        trades_list = []
        if not result.trades.empty:
            for _, trade_row in result.trades.iterrows():
                trades_list.append({
                    "entry_time": trade_row["entry_ts"],
                    "exit_time": trade_row["exit_ts"],
                    "entry_price": trade_row["price_entry"],
                    "exit_price": trade_row["price_exit"],
                    "pnl": trade_row["pnl"],
                    "side": trade_row.get("side", "LONG"),
                })

        # MÃ©tadonnÃ©es enrichies
        metadata = {
            "strategy": strategy,
            "params": params,
            "gpu_enabled": result.meta.get("mode") in ["single_gpu", "multi_gpu"],
            "multi_gpu_enabled": result.meta.get("mode") == "multi_gpu",
            "devices_used": result.meta.get("devices", []),
            "gpu_balance": result.meta.get("gpu_balance", {}),
            "execution_time_sec": time.time() - start_time,
            "engine_meta": result.meta,
        }

        # Ajout stats monitoring si disponible
        if monitor:
            try:
                stats = monitor.get_stats_summary()
                metadata["monitoring_stats"] = stats
                logger.info(f"ðŸ“Š Monitoring stats: CPU_mean={stats.get('cpu_mean', 0):.1f}%, "
                           f"GPU1_mean={stats.get('gpu1_mean', 0):.1f}%, "
                           f"GPU2_mean={stats.get('gpu2_mean', 0):.1f}%")
            except Exception as e:
                logger.warning(f"âš ï¸ Erreur rÃ©cupÃ©ration stats monitoring: {e}")

        duration = time.time() - start_time
        logger.info(f"âœ… Backtest GPU terminÃ© en {duration:.2f}s, "
                   f"{len(trades_list)} trades, "
                   f"Sharpe={sharpe:.2f}")

        return BacktestResult(
            equity=result.equity,
            metrics=metrics,
            trades=trades_list,
            metadata=metadata,
        )

    except Exception as exc:
        duration = time.time() - start_time
        logger.error(f"âŒ Erreur backtest GPU aprÃ¨s {duration:.2f}s: {exc}", exc_info=True)

        # Fallback vers version CPU simple
        logger.warning("âš ï¸ Fallback vers backtest CPU simple")
        return run_backtest(df, strategy, params)




----------------------------------------
Fichier: ui\fast_sweep.py
"""
ThreadX Fast Sweep - Optimisation Ultra-Rapide
==============================================

Sweep optimisÃ© pour l'interface Streamlit avec :
- Batch processing des indicateurs (calcul 1 fois seulement)
- Mise Ã  jour UI espacÃ©e (tous les 50 runs)
- Calculs vectorisÃ©s numpy
- Pas de recalcul inutile

Objectif : 100+ runs/seconde

Author: ThreadX Framework
Version: 1.0
"""

import time
from typing import Dict, List, Any, Callable, Optional
import numpy as np
import pandas as pd
import logging

logger = logging.getLogger(__name__)

# Optional global cancel hook from optimization engine
try:
    from threadx.optimization.engine import is_global_stop_requested  # type: ignore
except Exception:
    def is_global_stop_requested() -> bool:  # type: ignore
        return False


def fast_parameter_sweep(
    data: pd.DataFrame,
    param_name: str,
    param_values: List[Any],
    strategy_func: Callable,
    *,
    capital_initial: float = 10000.0,
    update_callback: Optional[Callable] = None,
    update_frequency: int = 50,
    should_cancel: Optional[Callable[[], bool]] = None,
) -> pd.DataFrame:
    """
    Sweep ultra-rapide avec batch processing.

    Cette fonction optimise le sweep en :
    1. PrÃ©-calculant tous les indicateurs une seule fois
    2. EspaÃ§ant les mises Ã  jour UI (tous les update_frequency runs)
    3. Utilisant des calculs vectorisÃ©s numpy
    4. Ã‰vitant tout recalcul inutile

    Args:
        data: DataFrame OHLCV
        param_name: Nom du paramÃ¨tre Ã  optimiser
        param_values: Liste des valeurs Ã  tester
        strategy_func: Fonction de stratÃ©gie Ã  tester
        capital_initial: Capital de dÃ©part
        update_callback: Fonction appelÃ©e pour mise Ã  jour UI (idx, total, result)
        update_frequency: FrÃ©quence de mise Ã  jour UI (tous les N runs)

    Returns:
        DataFrame avec rÃ©sultats pour chaque valeur

    Example:
        >>> results = fast_parameter_sweep(
        ...     data=df_ohlcv,
        ...     param_name="window",
        ...     param_values=list(range(10, 50)),
        ...     strategy_func=simple_ma_strategy,
        ...     capital_initial=10000,
        ...     update_callback=lambda i, total, r: print(f"{i}/{total}")
        ... )
    """
    start_time = time.time()
    n_params = len(param_values)

    logger.info(f"ðŸš€ Fast Sweep dÃ©marrÃ©: {n_params} paramÃ¨tres, param={param_name}")

    # === Ã‰TAPE 1: PrÃ©-calcul des prix et returns (1 fois seulement) ===
    close = data["close"].values
    returns = np.diff(close) / close[:-1]
    returns = np.concatenate([[0.0], returns])  # Pad pour avoir mÃªme longueur

    results = []

    # === Ã‰TAPE 2: Boucle sur les paramÃ¨tres (vectorisÃ©e autant que possible) ===
    for idx, param_value in enumerate(param_values):
        # Cooperative cancellation: allow UI to request stop
        if (should_cancel and should_cancel()) or is_global_stop_requested():
            logger.info("â¹ï¸  Fast Sweep cancellation requested by user")
            break
        iter_start = time.time()

        # Appliquer stratÃ©gie pour ce paramÃ¨tre
        try:
            # StratÃ©gie retourne positions {-1, 0, 1}
            params = {param_name: param_value}
            positions = strategy_func(data, params)

            # Calculs vectorisÃ©s des mÃ©triques
            strategy_returns = returns * positions[:-1] if len(positions) > len(returns) else returns * positions

            # Equity curve
            cumulative_returns = np.cumprod(1 + strategy_returns)
            total_return = cumulative_returns[-1] - 1.0 if len(cumulative_returns) > 0 else 0.0

            # Sharpe ratio (annualisÃ©)
            if len(strategy_returns) > 1 and strategy_returns.std() > 0:
                sharpe = (strategy_returns.mean() / strategy_returns.std()) * np.sqrt(252)
            else:
                sharpe = 0.0

            # Max drawdown (vectorisÃ©)
            cummax = np.maximum.accumulate(cumulative_returns)
            drawdown = (cumulative_returns - cummax) / cummax
            max_dd = drawdown.min() if len(drawdown) > 0 else 0.0

            # PnL en euros
            pnl_euros = capital_initial * total_return
            equity_final = capital_initial * (1 + total_return)

            # Nombre de trades (approximation: changements de position)
            position_changes = np.diff(positions)
            nb_trades = int(np.sum(np.abs(position_changes) > 0))

            # Taille moyenne des trades (approximation)
            if nb_trades > 0:
                # PnL moyen par trade
                avg_trade_size = abs(pnl_euros / nb_trades)
            else:
                avg_trade_size = 0.0

            # RÃ©sultat
            result = {
                "param": param_value,
                "sharpe": sharpe,
                "return_pct": total_return * 100,
                "pnl_euros": pnl_euros,
                "equity_final": equity_final,
                "max_dd": max_dd * 100,
                "nb_trades": nb_trades,
                "avg_trade_size": avg_trade_size,
                "compute_time_ms": (time.time() - iter_start) * 1000,
            }

            results.append(result)

            # Callback UI (seulement tous les update_frequency runs)
            if update_callback and (idx % update_frequency == 0 or idx == n_params - 1):
                update_callback(idx + 1, n_params, result)

        except Exception as e:
            logger.error(f"Erreur param {param_value}: {e}")
            # RÃ©sultat par dÃ©faut
            results.append({
                "param": param_value,
                "sharpe": 0.0,
                "return_pct": 0.0,
                "pnl_euros": 0.0,
                "equity_final": capital_initial,
                "max_dd": 0.0,
                "nb_trades": 0,
                "avg_trade_size": 0.0,
                "compute_time_ms": 0.0,
                "error": str(e),
            })

    # === Ã‰TAPE 3: Construction DataFrame final ===
    results_df = pd.DataFrame(results)

    total_time = time.time() - start_time
    throughput = n_params / total_time if total_time > 0 else 0

    logger.info(f"âœ… Fast Sweep terminÃ©: {n_params} runs en {total_time:.2f}s "
                f"({throughput:.1f} runs/sec)")

    # Stats de performance
    if not results_df.empty and "compute_time_ms" in results_df.columns:
        avg_time = results_df["compute_time_ms"].mean()
        logger.info(f"   Temps moyen par run: {avg_time:.2f}ms")

    return results_df


def simple_bollinger_strategy(data: pd.DataFrame, params: Dict[str, Any]) -> np.ndarray:
    """
    StratÃ©gie Bollinger simple ultra-rapide (vectorisÃ©e).

    Cette implÃ©mentation Ã©vite toutes les opÃ©rations lentes :
    - Pas de boucles Python
    - Calculs vectorisÃ©s numpy/pandas
    - Pas de conditions if/else dans la boucle

    Args:
        data: DataFrame OHLCV
        params: ParamÃ¨tres {"window": int, "std": float, ...}

    Returns:
        Array numpy de positions {-1: short, 0: flat, 1: long}
    """
    # Extraction paramÃ¨tres
    window = params.get("window", params.get("period", 20))
    std_mult = params.get("std", params.get("bb_std", 2.0))

    # Calculs vectorisÃ©s Bollinger
    close = data["close"]
    rolling = close.rolling(window=window, min_periods=window)

    middle = rolling.mean()
    std = rolling.std()

    upper = middle + std_mult * std
    lower = middle - std_mult * std

    # Signaux vectorisÃ©s
    # Long: prix <= lower (oversold)
    # Short: prix >= upper (overbought)
    # Flat: entre les bandes

    positions = np.zeros(len(close))

    # Mean reversion: long quand prix touche bande basse
    long_signal = (close <= lower).astype(int)

    # Exit/flat quand prix revient au milieu
    flat_signal = (close > lower) & (close < upper)

    # StratÃ©gie simple: long dans oversold, flat sinon
    # (on pourrait amÃ©liorer avec gestion d'Ã©tat mais restons vectorisÃ©)

    # Approche vectorisÃ©e simple: si prix < lower â†’ position = 1
    positions = (close <= lower).astype(float)

    return positions


def bollinger_zscore_strategy(data: pd.DataFrame, params: Dict[str, Any]) -> np.ndarray:
    """
    StratÃ©gie Bollinger avec Z-score (plus sophistiquÃ©e mais toujours vectorisÃ©e).

    Args:
        data: DataFrame OHLCV
        params: {"window": int, "std": float, "entry_z": float}

    Returns:
        Positions array
    """
    window = params.get("window", params.get("period", 20))
    std_mult = params.get("std", params.get("bb_std", 2.0))
    entry_z = params.get("entry_z", 2.0)

    close = data["close"]
    rolling = close.rolling(window=window, min_periods=window)

    middle = rolling.mean()
    std = rolling.std()

    # Z-score
    z_score = (close - middle) / std.replace(0, np.nan)
    z_score = z_score.fillna(0)

    # Positions basÃ©es sur Z-score
    # Long: z < -entry_z
    # Short: z > entry_z
    # Flat: abs(z) < entry_z

    positions = np.zeros(len(close))

    # Mean reversion
    long_mask = z_score <= -entry_z
    short_mask = z_score >= entry_z

    positions[long_mask] = 1.0
    positions[short_mask] = -1.0

    return positions


def adaptive_ma_strategy(data: pd.DataFrame, params: Dict[str, Any]) -> np.ndarray:
    """
    StratÃ©gie moving average adaptative (trend following).

    Args:
        data: DataFrame OHLCV
        params: {"window": int}

    Returns:
        Positions array
    """
    window = params.get("window", params.get("period", 20))

    close = data["close"]
    ma = close.rolling(window=window, min_periods=window).mean()

    # Trend following: long si prix > MA
    positions = (close > ma).astype(float)

    return positions


# Mapping des stratÃ©gies disponibles
STRATEGY_FUNCTIONS = {
    "bollinger_reversion": simple_bollinger_strategy,
    "bollinger_zscore": bollinger_zscore_strategy,
    "ma_trend": adaptive_ma_strategy,
}


def get_strategy_function(strategy_name: str) -> Callable:
    """
    RÃ©cupÃ¨re la fonction de stratÃ©gie optimisÃ©e.

    Args:
        strategy_name: Nom de la stratÃ©gie

    Returns:
        Fonction de stratÃ©gie

    Raises:
        ValueError: Si stratÃ©gie inconnue
    """
    if strategy_name not in STRATEGY_FUNCTIONS:
        # Fallback vers Bollinger par dÃ©faut
        logger.warning(f"StratÃ©gie '{strategy_name}' inconnue, fallback vers bollinger_reversion")
        return simple_bollinger_strategy

    return STRATEGY_FUNCTIONS[strategy_name]

----------------------------------------
Fichier: ui\page_backtest_optimization.py
"""
ThreadX - Page Backtest & Optimisation
=======================================

Page fusionnÃ©e combinant le backtest simple et l'optimisation Sweep.
Interface organisÃ©e en onglets pour une navigation intuitive.

Author: ThreadX Framework
Version: 2.0.0 - UI Redesign
"""

from __future__ import annotations

import time
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st

from .backtest_bridge import BacktestResult, run_backtest, run_backtest_gpu
from .system_monitor import get_global_monitor
from .fast_sweep import fast_parameter_sweep, get_strategy_function
from ..data_access import load_ohlcv
from .strategy_registry import (
    base_params_for,
    list_strategies,
    parameter_specs_for,
    resolve_range,
    tunable_parameters_for,
)
from threadx.indicators.bank import IndicatorBank, IndicatorSettings
from threadx.optimization.engine import SweepRunner
from threadx.optimization.scenarios import ScenarioSpec
from threadx.utils.log import get_logger

logger = get_logger(__name__)


def _require_configuration() -> Dict[str, Any]:
    """VÃ©rifie que la configuration est complÃ¨te."""
    required_keys = ("symbol", "timeframe", "start_date", "end_date", "strategy")
    missing = [key for key in required_keys if key not in st.session_state]

    if missing:
        st.warning(
            f"âš ï¸ Configuration incomplÃ¨te. "
            f"Veuillez d'abord configurer : {', '.join(missing)}"
        )
        st.info("ðŸ‘ˆ Allez sur la page **Configuration & StratÃ©gie** pour commencer.")
        st.stop()

    data_frame = st.session_state.get("data")
    if not isinstance(data_frame, pd.DataFrame) or data_frame.empty:
        st.warning("âš ï¸ Aucune donnÃ©e chargÃ©e.")
        st.info(
            "ðŸ‘ˆ Retournez sur **Configuration & StratÃ©gie** et cliquez sur 'Charger & PrÃ©visualiser'."
        )
        st.stop()

    return {key: st.session_state[key] for key in required_keys}


def _render_config_badge(context: Dict[str, Any]) -> None:
    """Affiche un badge rÃ©capitulatif de la configuration."""
    st.info(
        f"ðŸ“Š **{context['symbol']}** @ {context['timeframe']} | "
        f"ðŸ“… {context['start_date']} â†’ {context['end_date']} | "
        f"âš™ï¸ {context['strategy']}"
    )


def _render_price_chart(
    df: pd.DataFrame, indicators: Dict[str, Dict[str, Any]]
) -> None:
    """Graphique OHLC avec indicateurs."""
    fig = go.Figure()

    # Candlestick
    fig.add_trace(
        go.Candlestick(
            x=df.index,
            open=df["open"],
            high=df["high"],
            low=df["low"],
            close=df["close"],
            name="OHLC",
            increasing_line_color="#26a69a",
            decreasing_line_color="#ef5350",
        )
    )

    # Bollinger Bands si configurÃ©
    bollinger = indicators.get("bollinger", {})
    if {"window", "std"} <= set(bollinger.keys()) and not df["close"].empty:
        window = int(bollinger["window"])
        std_mult = float(bollinger["std"])
        rolling_close = df["close"].rolling(window, min_periods=window)
        mid = rolling_close.mean()
        std = rolling_close.std()

        fig.add_trace(
            go.Scatter(
                x=df.index,
                y=mid,
                name="BB Mid",
                mode="lines",
                line=dict(color="#ffa726", width=1),
            )
        )
        fig.add_trace(
            go.Scatter(
                x=df.index,
                y=mid + std_mult * std,
                name="BB Upper",
                mode="lines",
                line=dict(color="#42a5f5", width=1, dash="dash"),
            )
        )
        fig.add_trace(
            go.Scatter(
                x=df.index,
                y=mid - std_mult * std,
                name="BB Lower",
                mode="lines",
                line=dict(color="#42a5f5", width=1, dash="dash"),
            )
        )

    fig.update_layout(
        height=500,
        margin=dict(l=0, r=0, t=20, b=0),
        template="plotly_dark",
        xaxis_title="",
        yaxis_title="Prix (USD)",
        xaxis=dict(rangeslider=dict(visible=False), gridcolor="rgba(128,128,128,0.2)"),
        yaxis=dict(gridcolor="rgba(128,128,128,0.2)"),
        plot_bgcolor="rgba(0,0,0,0)",
        paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#a8b2d1", size=11),
        hovermode="x unified",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
    )

    st.plotly_chart(fig, use_container_width=True, key="backtest_chart")


def _render_equity_curve(equity: pd.Series) -> None:
    """Courbe d'Ã©quitÃ© moderne."""
    if equity.empty:
        st.warning("âš ï¸ Courbe d'Ã©quitÃ© vide.")
        return

    fig = go.Figure()

    fig.add_trace(
        go.Scatter(
            x=equity.index,
            y=equity.values,
            mode="lines",
            name="Ã‰quitÃ©",
            line=dict(color="#26a69a", width=2),
            fill="tozeroy",
            fillcolor="rgba(38, 166, 154, 0.1)",
        )
    )

    # Ligne initiale
    fig.add_hline(
        y=equity.iloc[0],
        line_dash="dash",
        line_color="gray",
        opacity=0.5,
        annotation_text="Capital initial",
        annotation_position="right",
    )

    fig.update_layout(
        height=300,
        margin=dict(l=0, r=0, t=20, b=0),
        template="plotly_dark",
        xaxis_title="",
        yaxis_title="Ã‰quitÃ© ($)",
        xaxis=dict(gridcolor="rgba(128,128,128,0.2)"),
        yaxis=dict(gridcolor="rgba(128,128,128,0.2)"),
        plot_bgcolor="rgba(0,0,0,0)",
        paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#a8b2d1", size=11),
        hovermode="x unified",
    )

    st.plotly_chart(fig, use_container_width=True, key="equity_curve")


def _render_metrics(metrics: Dict[str, Any]) -> None:
    """MÃ©triques de performance en cartes."""
    if not metrics:
        st.info("â„¹ï¸ Aucune mÃ©trique calculÃ©e.")
        return

    # Organiser mÃ©triques en colonnes
    metrics_list = list(metrics.items())
    n_metrics = len(metrics_list)
    n_cols = min(4, n_metrics)

    # Afficher en grille
    for i in range(0, n_metrics, n_cols):
        cols = st.columns(n_cols)
        for j, col in enumerate(cols):
            if i + j < n_metrics:
                key, value = metrics_list[i + j]
                with col:
                    formatted = f"{value:.4f}" if isinstance(value, float) else value
                    # Couleur delta basÃ©e sur valeur
                    delta_color = "normal"
                    if isinstance(value, (int, float)):
                        delta_color = "normal" if value > 0 else "inverse"

                    st.metric(
                        label=key.replace("_", " ").title(),
                        value=formatted,
                    )

    # Bouton export
    st.markdown("")
    metrics_df = pd.DataFrame(list(metrics.items()), columns=["MÃ©trique", "Valeur"])
    csv = metrics_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "ðŸ“¥ Exporter les mÃ©triques (CSV)",
        csv,
        "metrics.csv",
        mime="text/csv",
        use_container_width=True,
    )


def _build_sweep_grid(
    min_value: float, max_value: float, step: float, value_type: str
) -> np.ndarray:
    """CrÃ©e une grille de valeurs pour le sweep en gÃ©rant int/float proprement."""
    if max_value < min_value:
        min_value, max_value = max_value, min_value

    if value_type == "int":
        min_int = int(round(min_value))
        max_int = int(round(max_value))
        step_int = max(1, int(round(step)))
        return np.arange(min_int, max_int + step_int, step_int, dtype=int)

    step_float = float(step) if step else 0.1
    if step_float <= 0:
        step_float = 0.1

    span = max_value - min_value
    if span <= 0:
        return np.array([min_value], dtype=float)

    count = int(round(span / step_float)) + 1
    values = min_value + np.arange(count) * step_float
    values = values[values <= max_value + step_float * 1e-6]
    if len(values) == 0 or values[-1] < max_value:
        values = np.append(values, max_value)
    return np.round(values, 8)


def _render_monte_carlo_tab() -> None:
    """Onglet d'optimisation Monte-Carlo."""
    st.markdown("### ðŸŽ² Optimisation Monte-Carlo")

    context = _require_configuration()
    data = st.session_state.get("data")

    if not isinstance(data, pd.DataFrame) or data.empty:
        st.warning("âš ï¸ Chargez d'abord des donnÃ©es.")
        return

    strategies = list_strategies()
    if not strategies:
        st.error("âŒ Aucune stratÃ©gie disponible.")
        return

    _render_config_badge(context)

    st.markdown("#### Configuration Monte-Carlo")
    col_strategy, col_gpu, col_multigpu, col_workers = st.columns(4)

    with col_strategy:
        strategy = st.selectbox(
            "StratÃ©gie",
            strategies,
            index=(
                strategies.index(context["strategy"])
                if context["strategy"] in strategies
                else 0
            ),
            key="mc_strategy",
        )

    with col_gpu:
        use_gpu = st.checkbox(
            "Activer GPU",
            value=st.session_state.get("mc_use_gpu", True),
            key="mc_use_gpu",
        )

    with col_multigpu:
        use_multigpu = st.checkbox(
            "Multi-GPU (5090+2060)",
            value=st.session_state.get("mc_use_multigpu", True),
            key="mc_use_multigpu",
        )

    with col_workers:
        # RÃ©cupÃ©rer la sÃ©lection prÃ©cÃ©dente depuis session_state
        current_mode = st.session_state.get("mc_workers_mode", "Auto (Dynamique)")
        mode_index = 1 if current_mode == "Manuel" else 0

        workers_mode = st.selectbox(
            "Workers",
            ["Auto (Dynamique)", "Manuel"],
            index=mode_index,
            key="mc_workers_mode",
        )
        if workers_mode == "Manuel":
            max_workers = st.number_input(
                "Nb Workers",
                min_value=2,
                max_value=32,
                value=st.session_state.get("mc_manual_workers", 8),
                step=1,
                key="mc_manual_workers",
            )
        else:
            max_workers = None

    tunable_specs = tunable_parameters_for(strategy)
    if not tunable_specs:
        st.info("â„¹ï¸ Aucun paramÃ¨tre optimisable pour cette stratÃ©gie.")
        return

    configured_params = st.session_state.get("strategy_params", {}) or {}
    base_strategy_params = base_params_for(strategy)

    range_preferences = st.session_state.get("strategy_param_ranges", {}).copy()
    st.markdown("##### Plages de paramÃ¨tres")
    param_ranges: Dict[str, Tuple[float, float]] = {}
    param_types: Dict[str, str] = {}

    for key, spec in tunable_specs.items():
        label = spec.get("label") or key.replace("_", " ").title()
        param_type = spec.get("type") or (
            "float" if isinstance(spec.get("default"), float) else "int"
        )
        param_types[key] = param_type

        default_val = configured_params.get(key, spec.get("default"))
        if default_val is None:
            default_val = base_strategy_params.get(
                key, 0 if param_type == "int" else 0.0
            )

        min_val = spec.get("min")
        max_val = spec.get("max")
        opt_min, opt_max = resolve_range(spec)
        if min_val is None:
            min_val = opt_min if opt_min is not None else default_val
        if max_val is None:
            max_val = opt_max if opt_max is not None else default_val
        if min_val is None:
            min_val = 0 if param_type == "int" else 0.0
        if max_val is None or max_val <= min_val:
            max_val = min_val + (
                spec.get("step") or (1 if param_type == "int" else 0.1)
            )

        stored_range = range_preferences.get(key)

        # CrÃ©er 2 colonnes: plage + sensibilitÃ© (Monte-Carlo)
        col_range, col_sense = st.columns([3, 1])

        with col_range:
            if param_type == "int":
                min_val = int(round(min_val))
                max_val = int(round(max_val))
                if stored_range:
                    stored_low, stored_high = map(int, stored_range)
                    default_tuple = (
                        max(min_val, stored_low),
                        min(max_val, stored_high),
                    )
                else:
                    default_tuple = (
                        (min(int(default_val), max_val), max(int(default_val), min_val))
                        if isinstance(default_val, (int, float))
                        else (min_val, max_val)
                    )
                selected_range = st.slider(
                    label,
                    min_value=min_val,
                    max_value=max_val,
                    value=(int(default_tuple[0]), int(default_tuple[1])),
                    step=1,
                    key=f"mc_range_{key}",
                )
            else:
                min_val = float(min_val)
                max_val = float(max_val)
                float_step = float(spec.get("step") or 0.1)
                if stored_range:
                    stored_low = float(stored_range[0])
                    stored_high = float(stored_range[1])
                    default_tuple = (
                        max(min_val, stored_low),
                        min(max_val, stored_high),
                    )
                else:
                    if default_val is not None:
                        default_min = float(default_val) - 0.1 * abs(float(default_val))
                        default_max = float(default_val) + 0.1 * abs(float(default_val))
                        default_tuple = (
                            max(min_val, default_min),
                            min(max_val, default_max),
                        )
                    else:
                        default_tuple = (min_val, max_val)
                selected_range = st.slider(
                    label,
                    min_value=min_val,
                    max_value=max_val,
                    value=(float(default_tuple[0]), float(default_tuple[1])),
                    step=float_step,
                    key=f"mc_range_{key}",
                )

        # SensibilitÃ© : Slider de STEP (granularitÃ© d'exploration) - Monte-Carlo
        with col_sense:
            range_span = selected_range[1] - selected_range[0]
            base_step = float(spec.get("step") or 0.1)

            if param_type == "int":
                # Pour entiers : step de 1 Ã  range_span
                step_val_display = st.slider(
                    "ðŸ“Š Step",
                    min_value=1,
                    max_value=max(1, int(range_span)),
                    value=1,
                    step=1,
                    key=f"mc_step_{key}",
                    label_visibility="collapsed",
                )
            else:
                # Pour floats : step de (base_step/10) Ã  range_span
                step_val_display = st.slider(
                    "ðŸ“Š Step",
                    min_value=base_step / 10,
                    max_value=range_span if range_span > 0 else base_step,
                    value=base_step,
                    step=base_step / 100,
                    format="%.4f",
                    key=f"mc_step_{key}",
                    label_visibility="collapsed",
                )

        range_preferences[key] = (selected_range[0], selected_range[1])
        param_ranges[key] = (selected_range[0], selected_range[1])

        # Display combination count for this parameter (Monte-Carlo)
        range_min, range_max = selected_range
        span = range_max - range_min

        if param_type == "int":
            # For integers: count the values in the range with this step
            n_combinations = len(
                range(int(range_min), int(range_max) + 1, max(1, int(step_val_display)))
            )
        else:
            # For floats: (span / step)
            n_combinations = span / step_val_display if step_val_display > 0 else 1

        # Show the combination count
        comb_text = f"ðŸ“Š Plage: {range_min} â†’ {range_max} | SensibilitÃ©: {step_val_display} | Combinaisons: {n_combinations:.1f}"
        st.caption(comb_text)

    st.session_state["strategy_param_ranges"] = range_preferences
    st.markdown("##### ParamÃ¨tres d'Ã©chantillonnage")
    col_count, col_seed = st.columns(2)
    with col_count:
        n_scenarios = st.number_input(
            "Nombre de scÃ©narios",
            min_value=50,
            max_value=10000,
            value=st.session_state.get("mc_n", 500),
            step=50,
            key="mc_n",
        )
    with col_seed:
        seed = st.number_input(
            "Seed",
            min_value=0,
            max_value=999999,
            value=st.session_state.get("mc_seed", 42),
            step=1,
            key="mc_seed",
        )

    if st.button(
        "ðŸŽ² Lancer Monte-Carlo",
        type="primary",
        use_container_width=True,
        key="run_mc_btn",
    ):
        indicator_settings = IndicatorSettings(use_gpu=use_gpu)
        indicator_bank = IndicatorBank(indicator_settings)
        runner = SweepRunner(
            indicator_bank=indicator_bank,
            max_workers=max_workers,
            use_multigpu=use_multigpu,
        )

        scenario_params: Dict[str, Any] = {}
        for key, (min_v, max_v) in param_ranges.items():
            if param_types[key] == "int":
                values = list(range(int(min_v), int(max_v) + 1))
            else:
                values = np.linspace(min_v, max_v, num=50).tolist()
            scenario_params[key] = {"values": values}

        # ðŸ”¥ FIX CRITIQUE: Ajouter TOUS les paramÃ¨tres par dÃ©faut manquants
        # Garantir que min_pnl_pct et autres params sont TOUJOURS prÃ©sents
        all_param_specs = parameter_specs_for(strategy)
        for key, spec in all_param_specs.items():
            if key not in scenario_params:
                # PrioritÃ©: configured_params > base_strategy_params > spec default
                value = configured_params.get(
                    key,
                    base_strategy_params.get(
                        key, spec.get("default") if isinstance(spec, dict) else spec
                    ),
                )
                scenario_params[key] = {"value": value}
                logger.debug(f"[MC] Param par dÃ©faut ajoutÃ©: {key} = {value}")

        spec = ScenarioSpec(
            type="monte_carlo",
            params=scenario_params,
            n_scenarios=int(n_scenarios),
            seed=int(seed),
        )

        # RÃ©cupÃ©rer les donnÃ©es rÃ©elles pour le backtest
        symbol = st.session_state.get("symbol", "BTC")
        timeframe = st.session_state.get("timeframe", "1h")
        start_date = st.session_state.get("start_date")
        end_date = st.session_state.get("end_date")

        # ðŸ”¥ FIX CRITIQUE: Recharger les donnÃ©es avec les dates correctes
        # Les donnÃ©es en session peuvent Ãªtre obsolÃ¨tes si l'utilisateur a changÃ© les dates
        try:
            real_data = load_ohlcv(symbol, timeframe, start=start_date, end=end_date)
            if real_data.empty:
                st.error(
                    f"âš ï¸ Aucune donnÃ©e disponible pour {symbol}/{timeframe} entre {start_date} et {end_date}"
                )
                return
            # Mettre Ã  jour le cache pour cohÃ©rence
            st.session_state.data = real_data
        except Exception as e:
            st.error(f"âŒ Erreur chargement donnÃ©es: {e}")
            return

        try:
            # Lancer le Monte-Carlo avec barre de progression
            st.markdown("### ðŸŽ² ExÃ©cution du Monte-Carlo")
            results = _run_monte_carlo_with_progress(
                runner, spec, real_data, symbol, timeframe, strategy, int(n_scenarios)
            )
            st.session_state["monte_carlo_results"] = results

            # Afficher les informations de configuration
            st.markdown("---")
            st.markdown("### âš™ï¸ Configuration d'exÃ©cution")
            col_info1, col_info2, col_info3 = st.columns(3)
            with col_info1:
                st.metric(
                    "Mode Multi-GPU", "ActivÃ© âœ…" if use_multigpu else "DÃ©sactivÃ© âŠ˜"
                )
            with col_info2:
                actual_workers = runner.max_workers if runner.max_workers else "Auto"
                st.metric("Workers utilisÃ©s", str(actual_workers))
            with col_info3:
                st.metric(
                    "Total des rÃ©sultats",
                    len(results) if isinstance(results, pd.DataFrame) else 0,
                )
        except Exception as exc:
            st.error(f"âŒ Erreur Monte-Carlo: {exc}")
            import traceback

            st.code(traceback.format_exc())
            return

    results_df = st.session_state.get("monte_carlo_results")

    if isinstance(results_df, pd.DataFrame) and not results_df.empty:
        st.markdown("---")
        st.markdown("### ðŸ“ˆ RÃ©sultats Monte-Carlo")

        score_col = None
        for candidate in ["score", "objective", "sharpe", "total_return"]:
            if candidate in results_df.columns:
                score_col = candidate
                break

        if score_col:
            results_sorted = results_df.sort_values(by=score_col, ascending=False)
        else:
            results_sorted = results_df

        st.dataframe(results_sorted.head(100), use_container_width=True, height=400)

        best_row = results_sorted.iloc[0]
        st.markdown("#### ðŸ† Meilleur scÃ©nario")
        st.json(best_row.to_dict())

        csv = results_df.to_csv(index=False).encode("utf-8")
        st.download_button(
            "ðŸ’¾ Exporter les rÃ©sultats Monte-Carlo (CSV)",
            csv,
            "monte_carlo_results.csv",
            "text/csv",
            use_container_width=True,
        )


def _run_sweep_with_progress(
    runner, spec, real_data, symbol, timeframe, strategy, total_combinations
):
    """Lance un sweep avec barre de progression et statistiques de vitesse."""
    import threading

    # CrÃ©er les placeholders pour l'UI
    progress_placeholder = st.empty()
    stats_cols = st.columns(4)

    # Ã‰tat partagÃ© (thread-safe via GIL Python)
    shared_state = {
        "running": False,
        "current": 0,
        "total": 0,
        "start_time": time.time(),
        "should_stop": False,  # Signal d'arrÃªt
    }

    # DÃ©marrer le sweep dans un thread pour ne pas bloquer Streamlit
    def run_sweep_thread():
        """Thread qui exÃ©cute le sweep (pas de Streamlit calls ici!)."""
        try:
            shared_state["running"] = True
            shared_state["start_time"] = time.time()
            results = runner.run_grid(
                spec,
                real_data,
                symbol,
                timeframe,
                strategy_name=strategy,
                reuse_cache=True,
            )
            shared_state["results"] = results
            shared_state["error"] = None
        except Exception as e:
            # Ignorer les erreurs si arrÃªt demandÃ©
            if shared_state["should_stop"]:
                shared_state["error"] = "ArrÃªt demandÃ© par l'utilisateur"
                shared_state["results"] = None
            else:
                shared_state["error"] = str(e)
                shared_state["results"] = None
        finally:
            shared_state["running"] = False

    # DÃ©marrer le sweep
    sweep_thread = threading.Thread(target=run_sweep_thread, daemon=True)
    sweep_thread.start()

    # Boucle de mise Ã  jour UI (thread principal, synchrone avec Streamlit)
    start_time = time.time()
    status_placeholder = stats_cols[0].empty()
    speed_placeholder = stats_cols[1].empty()
    eta_placeholder = stats_cols[2].empty()
    completed_placeholder = stats_cols[3].empty()

    # Progress initial
    progress_placeholder.progress(0, text="ðŸš€ Initialisation du Sweep...")
    status_placeholder.metric("ðŸ“Š Status", "Initialisation...", delta=None)

    # Boucle: mettre Ã  jour l'UI jusqu'Ã  fin du sweep
    while shared_state["running"]:
        try:
            # VÃ©rifier si l'utilisateur a demandÃ© l'arrÃªt
            if st.session_state.get("run_stop_requested", False):
                from threadx.optimization.engine import request_global_stop

                request_global_stop()  # Signal au moteur d'arrÃªter
                shared_state["should_stop"] = True
                st.session_state.run_stop_requested = False  # RÃ©initialiser le flag
                progress_placeholder.progress(0, text="â¹ï¸ ArrÃªt en cours...")
                status_placeholder.metric("ðŸ“Š Status", "ArrÃªt en cours...", delta=None)
                break  # Quitter la boucle d'affichage

            if runner.total_scenarios > 0:
                current = runner.current_scenario
                total = runner.total_scenarios
                progress = min(current / total, 0.99)
                elapsed = time.time() - start_time

                if current > 0 and elapsed > 0:
                    speed = current / elapsed
                    remaining = total - current
                    eta_seconds = remaining / speed if speed > 0 else 0
                    eta_minutes, eta_secs = divmod(eta_seconds, 60)
                    eta_str = f"{int(eta_minutes)}m {int(eta_secs)}s"

                    # Mise Ã  jour UI (thread principal)
                    progress_placeholder.progress(
                        progress, text=f"â³ {current}/{total} ({progress*100:.0f}%)"
                    )
                    status_placeholder.metric("ðŸ“Š Status", "ExÃ©cution...", delta=None)
                    speed_placeholder.metric("ðŸš€ Vitesse", f"{speed:.1f} tests/sec")
                    eta_placeholder.metric("â±ï¸ ETA", eta_str)
                    completed_placeholder.metric("ðŸ“ˆ ComplÃ©tÃ©s", f"{current}")

            time.sleep(
                0.1
            )  # Mettre Ã  jour plus frÃ©quemment (100ms) pour rÃ©activitÃ© d'arrÃªt
        except:
            pass  # Ignorer erreurs de mise Ã  jour

    # Attendre fin du thread
    sweep_thread.join(timeout=5)

    # Afficher rÃ©sultats final
    elapsed_time = time.time() - start_time

    if shared_state["error"]:
        progress_placeholder.progress(0, text=f"âŒ Erreur aprÃ¨s {elapsed_time:.1f}s")
        status_placeholder.metric("ðŸ“Š Status", "Erreur âŒ", delta=None)
        st.error(f"Sweep Ã©chouÃ©: {shared_state['error']}")
        raise Exception(shared_state["error"])

    results = shared_state.get("results")
    if results is None:
        results = pd.DataFrame()

    completed = len(results) if isinstance(results, pd.DataFrame) else 0
    tests_per_second = completed / elapsed_time if elapsed_time > 0 else 0
    minutes, seconds = divmod(elapsed_time, 60)
    time_str = f"{int(minutes)}m {int(seconds)}s"

    # Stats finales
    progress_placeholder.progress(1.0, text=f"âœ… Sweep terminÃ© en {time_str}")
    status_placeholder.metric("ðŸ“Š Status", "ComplÃ©tÃ© âœ…", delta=None)
    speed_placeholder.metric("ðŸš€ Vitesse", f"{tests_per_second:.1f} tests/sec")
    eta_placeholder.metric("â±ï¸ Temps Total", time_str)
    completed_placeholder.metric("ðŸ“ˆ RÃ©sultats", f"{completed}")

    return results


def _run_monte_carlo_with_progress(
    runner, spec, real_data, symbol, timeframe, strategy, n_scenarios
):
    """Lance un Monte-Carlo avec barre de progression et statistiques de vitesse."""
    import threading

    # CrÃ©er les placeholders pour l'UI
    progress_placeholder = st.empty()
    stats_cols = st.columns(4)

    # Ã‰tat partagÃ© (thread-safe via GIL Python)
    shared_state = {
        "running": False,
        "current": 0,
        "total": 0,
        "start_time": time.time(),
        "should_stop": False,  # Signal d'arrÃªt
    }

    # DÃ©marrer le Monte-Carlo dans un thread
    def run_monte_carlo_thread():
        """Thread qui exÃ©cute le Monte-Carlo (pas de Streamlit calls ici!)."""
        try:
            shared_state["running"] = True
            shared_state["start_time"] = time.time()
            results = runner.run_monte_carlo(
                spec,
                real_data,
                symbol,
                timeframe,
                strategy_name=strategy,
                reuse_cache=True,
            )
            shared_state["results"] = results
            shared_state["error"] = None
        except Exception as e:
            # Ignorer les erreurs si arrÃªt demandÃ©
            if shared_state["should_stop"]:
                shared_state["error"] = "ArrÃªt demandÃ© par l'utilisateur"
                shared_state["results"] = None
            else:
                shared_state["error"] = str(e)
                shared_state["results"] = None
        finally:
            shared_state["running"] = False

    # DÃ©marrer le Monte-Carlo
    mc_thread = threading.Thread(target=run_monte_carlo_thread, daemon=True)
    mc_thread.start()

    # Boucle de mise Ã  jour UI (thread principal, synchrone avec Streamlit)
    start_time = time.time()
    status_placeholder = stats_cols[0].empty()
    speed_placeholder = stats_cols[1].empty()
    eta_placeholder = stats_cols[2].empty()
    completed_placeholder = stats_cols[3].empty()

    # Progress initial
    progress_placeholder.progress(0, text="ðŸŽ² Initialisation du Monte-Carlo...")
    status_placeholder.metric("ðŸ“Š Status", "Initialisation...", delta=None)

    # Boucle: mettre Ã  jour l'UI jusqu'Ã  fin du Monte-Carlo
    while shared_state["running"]:
        try:
            # VÃ©rifier si l'utilisateur a demandÃ© l'arrÃªt
            if st.session_state.get("run_stop_requested", False):
                from threadx.optimization.engine import request_global_stop

                request_global_stop()  # Signal au moteur d'arrÃªter
                shared_state["should_stop"] = True
                st.session_state.run_stop_requested = False  # RÃ©initialiser le flag
                progress_placeholder.progress(0, text="â¹ï¸ ArrÃªt en cours...")
                status_placeholder.metric("ðŸ“Š Status", "ArrÃªt en cours...", delta=None)
                break  # Quitter la boucle d'affichage

            if runner.total_scenarios > 0:
                current = runner.current_scenario
                total = runner.total_scenarios
                progress = min(current / total, 0.99)
                elapsed = time.time() - start_time

                if current > 0 and elapsed > 0:
                    speed = current / elapsed
                    remaining = total - current
                    eta_seconds = remaining / speed if speed > 0 else 0
                    eta_minutes, eta_secs = divmod(eta_seconds, 60)
                    eta_str = f"{int(eta_minutes)}m {int(eta_secs)}s"

                    # Mise Ã  jour UI (thread principal)
                    progress_placeholder.progress(
                        progress, text=f"â³ {current}/{total} ({progress*100:.0f}%)"
                    )
                    status_placeholder.metric("ðŸ“Š Status", "ExÃ©cution...", delta=None)
                    speed_placeholder.metric("ðŸš€ Vitesse", f"{speed:.1f} scÃ©n/sec")
                    eta_placeholder.metric("â±ï¸ ETA", eta_str)
                    completed_placeholder.metric("ðŸ“ˆ ComplÃ©tÃ©s", f"{current}")

            time.sleep(
                0.1
            )  # Mettre Ã  jour plus frÃ©quemment (100ms) pour rÃ©activitÃ© d'arrÃªt
        except:
            pass  # Ignorer erreurs de mise Ã  jour

    # Attendre fin du thread
    mc_thread.join(timeout=5)

    # Afficher rÃ©sultats final
    elapsed_time = time.time() - start_time

    if shared_state["error"]:
        progress_placeholder.progress(0, text=f"âŒ Erreur aprÃ¨s {elapsed_time:.1f}s")
        status_placeholder.metric("ðŸ“Š Status", "Erreur âŒ", delta=None)
        st.error(f"Monte-Carlo Ã©chouÃ©: {shared_state['error']}")
        raise Exception(shared_state["error"])

    results = shared_state.get("results")
    if results is None:
        results = pd.DataFrame()

    completed = len(results) if isinstance(results, pd.DataFrame) else 0
    scenarios_per_second = completed / elapsed_time if elapsed_time > 0 else 0
    minutes, seconds = divmod(elapsed_time, 60)
    time_str = f"{int(minutes)}m {int(seconds)}s"

    # Stats finales
    progress_placeholder.progress(1.0, text=f"âœ… Monte-Carlo terminÃ© en {time_str}")
    status_placeholder.metric("ðŸ“Š Status", "ComplÃ©tÃ© âœ…", delta=None)
    speed_placeholder.metric("ðŸš€ Vitesse", f"{scenarios_per_second:.1f} scÃ©n/sec")
    eta_placeholder.metric("â±ï¸ Temps Total", time_str)
    completed_placeholder.metric("ðŸ“ˆ RÃ©sultats", f"{completed}")

    return results


def _format_param_value(value: float, value_type: str, decimals: int = 4) -> str:
    if value_type == "int":
        return str(int(round(value)))
    formatted = f"{value:.{decimals}f}"
    return formatted.rstrip("0").rstrip(".")


def _render_trades_table(trades: List[Dict[str, Any]]) -> None:
    """Table des transactions."""
    if not trades:
        st.info("â„¹ï¸ Aucune transaction enregistrÃ©e.")
        return

    trades_df = pd.DataFrame(trades)

    # Formater si colonnes spÃ©cifiques existent
    if "profit" in trades_df.columns:
        trades_df["profit"] = trades_df["profit"].apply(lambda x: f"${x:.2f}")

    st.dataframe(
        trades_df,
        use_container_width=True,
        height=300,
    )

    # Bouton export
    csv = trades_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "ðŸ“¥ Exporter les trades (CSV)",
        csv,
        "trades.csv",
        "text/csv",
        use_container_width=True,
    )


def _render_monitoring_section(
    metadata: Optional[Dict[str, Any]], history: Optional[pd.DataFrame]
) -> None:
    """Affiche les diagnostics GPU/CPU et les courbes de monitoring."""
    has_metadata = isinstance(metadata, dict) and bool(metadata)
    has_history = isinstance(history, pd.DataFrame) and not history.empty

    if not has_metadata and not has_history:
        return

    st.markdown("#### ðŸ” Diagnostics SystÃ¨me & GPU")

    if has_metadata:
        devices = metadata.get("devices_used", [])
        gpu_enabled = metadata.get("gpu_enabled", False)
        multi_gpu = metadata.get("multi_gpu_enabled", False)
        gpu_balance = metadata.get("gpu_balance", {})
        exec_time = metadata.get("execution_time_sec")
        monitor_stats = metadata.get("monitoring_stats", {})

        col_meta1, col_meta2, col_meta3 = st.columns(3)
        with col_meta1:
            st.metric("GPU activÃ©", "Oui" if gpu_enabled else "Non")
            st.metric("Multi-GPU", "Oui" if multi_gpu else "Non")
        with col_meta2:
            st.metric("DurÃ©e (s)", f"{exec_time:.2f}" if exec_time else "N/A")
            st.metric(
                "GPU 1 moyen (%)",
                f"{monitor_stats.get('gpu1_mean', 0):.1f}" if monitor_stats else "N/A",
            )
        with col_meta3:
            st.metric(
                "GPU 2 moyen (%)",
                f"{monitor_stats.get('gpu2_mean', 0):.1f}" if monitor_stats else "N/A",
            )
            st.metric(
                "CPU moyen (%)",
                f"{monitor_stats.get('cpu_mean', 0):.1f}" if monitor_stats else "N/A",
            )

        with st.expander("DÃ©tails GPU", expanded=False):
            st.write("PÃ©riphÃ©riques :", devices or "Inconnu")
            if gpu_balance:
                st.write("Balance de charge :", gpu_balance)
            if monitor_stats:
                st.json(monitor_stats)

    if has_history:
        df = history.copy()
        fig = go.Figure()
        fig.add_trace(
            go.Scatter(
                x=df["time"], y=df["cpu"], name="CPU (%)", line=dict(color="#26a69a")
            )
        )
        fig.add_trace(
            go.Scatter(
                x=df["time"], y=df["gpu1"], name="GPU 1 (%)", line=dict(color="#42a5f5")
            )
        )
        fig.add_trace(
            go.Scatter(
                x=df["time"], y=df["gpu2"], name="GPU 2 (%)", line=dict(color="#ef5350")
            )
        )
        fig.update_layout(
            height=320,
            template="plotly_dark",
            xaxis_title="Temps (s)",
            yaxis_title="Utilisation (%)",
            legend=dict(
                orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1
            ),
            margin=dict(l=0, r=0, t=30, b=0),
        )
        st.plotly_chart(fig, use_container_width=True, key="monitoring_chart")


def _render_backtest_tab() -> None:
    """Onglet Backtest simple avec option GPU."""
    context = _require_configuration()
    indicators = st.session_state.get("indicators", {})
    params = st.session_state.get("strategy_params", {}) or {}

    _render_config_badge(context)

    st.markdown("### ðŸš€ Lancer le Backtest")
    col_mode, col_monitor = st.columns(2)
    with col_mode:
        use_gpu = st.checkbox(
            "Activer le moteur GPU (BacktestEngine)",
            value=st.session_state.get("backtest_use_gpu", True),
            key="backtest_use_gpu",
        )
    with col_monitor:
        monitoring = st.checkbox(
            "Monitoring CPU/GPU en temps rÃ©el",
            value=st.session_state.get("backtest_monitoring", True),
            key="backtest_monitoring",
        )

    if st.button(
        "ðŸš€ ExÃ©cuter le Backtest",
        type="primary",
        use_container_width=True,
        key="run_backtest_btn",
    ):
        with st.spinner("ðŸ› ï¸ ExÃ©cution du backtest en cours..."):
            monitor_history = None
            try:
                df = load_ohlcv(
                    context["symbol"],
                    context["timeframe"],
                    start=context["start_date"],
                    end=context["end_date"],
                )

                if df.empty:
                    st.error("âš ï¸ Dataset vide pour cette plage.")
                    return

                run_params = dict(params) if isinstance(params, dict) else {}

                if use_gpu:
                    monitor = get_global_monitor() if monitoring else None
                    if monitor:
                        if monitor.is_running():
                            monitor.stop()
                        monitor.clear_history()

                    result = run_backtest_gpu(
                        df=df,
                        strategy=context["strategy"],
                        params=run_params,
                        symbol=context["symbol"],
                        timeframe=context["timeframe"],
                        use_gpu=True,
                        enable_monitoring=monitoring,
                    )

                    if monitoring:
                        monitor = get_global_monitor()
                        if monitor.is_running():
                            monitor.stop()
                        monitor_history = monitor.get_history_df()
                        monitor.clear_history()
                else:
                    result = run_backtest(
                        df=df, strategy=context["strategy"], params=run_params
                    )

                    monitor = get_global_monitor()
                    if monitor.is_running():
                        monitor.stop()
                    monitor.clear_history()

                st.session_state.backtest_results = result
                st.session_state.data = df
                st.session_state["monitor_history"] = monitor_history

                st.success("âœ… Backtest terminÃ© avec succÃ¨s !")

            except FileNotFoundError as exc:
                st.error(f"âš ï¸ {exc}")
                return
            except Exception as exc:
                st.error(f"âŒ Erreur lors du backtest: {exc}")
                return

    result: BacktestResult = st.session_state.get("backtest_results")
    if result:
        st.markdown("---")
        st.markdown("### ðŸ“Š RÃ©sultats du Backtest")

        res_tab1, res_tab2, res_tab3 = st.tabs(
            ["ðŸ” Graphiques", "ðŸ“ˆ MÃ©triques", "ðŸ‘¥ Transactions"]
        )

        with res_tab1:
            st.markdown("#### Prix & Indicateurs")
            data_df = st.session_state.get("data")
            if isinstance(data_df, pd.DataFrame):
                _render_price_chart(data_df, indicators)

            st.markdown("#### Courbe d'Ã©quitÃ©")
            _render_equity_curve(result.equity)

            history_df = st.session_state.get("monitor_history")
            _render_monitoring_section(result.metadata, history_df)

        with res_tab2:
            _render_metrics(result.metrics)

        with res_tab3:
            _render_trades_table(result.trades)


def _render_optimization_tab() -> None:
    """Onglet d'optimisation par balayage exhaustif de paramÃ¨tres (Sweep)."""
    st.markdown("### ðŸ”¬ Optimisation par Sweep (Grille Exhaustive)")

    context = _require_configuration()
    data = st.session_state.get("data")

    if not isinstance(data, pd.DataFrame) or data.empty:
        st.warning(
            "âš ï¸ Chargez d'abord des donnÃ©es sur la page 'Chargement des DonnÃ©es'."
        )
        return

    strategies = list_strategies()
    if not strategies:
        st.error("âŒ Aucune stratÃ©gie disponible.")
        return

    _render_config_badge(context)

    st.markdown("#### Configuration du Sweep")
    col_strategy, col_gpu, col_multigpu, col_workers = st.columns(4)

    with col_strategy:
        strategy = st.selectbox(
            "StratÃ©gie Ã  optimiser",
            strategies,
            index=(
                strategies.index(context["strategy"])
                if context["strategy"] in strategies
                else 0
            ),
            key="sweep_strategy",
        )

    with col_gpu:
        use_gpu = st.checkbox(
            "Activer GPU",
            value=st.session_state.get("sweep_use_gpu", True),
            key="sweep_use_gpu",
        )

    with col_multigpu:
        use_multigpu = st.checkbox(
            "Multi-GPU (5090+2060)",
            value=st.session_state.get("sweep_use_multigpu", True),
            key="sweep_use_multigpu",
        )

    with col_workers:
        # RÃ©cupÃ©rer la sÃ©lection prÃ©cÃ©dente depuis session_state
        current_mode = st.session_state.get("sweep_workers_mode", "Auto (Dynamique)")
        mode_index = 1 if current_mode == "Manuel" else 0

        workers_mode = st.selectbox(
            "Workers",
            ["Auto (Dynamique)", "Manuel"],
            index=mode_index,
            key="sweep_workers_mode",
        )
        if workers_mode == "Manuel":
            max_workers = st.number_input(
                "Nb Workers",
                min_value=2,
                max_value=32,
                value=st.session_state.get("sweep_manual_workers", 8),
                step=1,
                key="sweep_manual_workers",
            )
        else:
            max_workers = None

    try:
        tunable_specs = tunable_parameters_for(strategy)
    except KeyError:
        st.error(f"âŒ StratÃ©gie inconnue: {strategy}")
        return

    if not tunable_specs:
        st.info("â„¹ï¸ Aucun paramÃ¨tre optimisable pour cette stratÃ©gie.")
        return

    configured_params = st.session_state.get("strategy_params", {}) or {}
    base_strategy_params = base_params_for(strategy)

    # Configuration des plages pour TOUS les paramÃ¨tres
    range_preferences = st.session_state.get("strategy_param_ranges", {}).copy()
    st.markdown("##### Plages de paramÃ¨tres Ã  optimiser")

    param_ranges: Dict[str, Tuple[float, float]] = {}
    param_types: Dict[str, str] = {}
    param_steps: Dict[str, float] = {}

    for key, spec in tunable_specs.items():
        label = spec.get("label") or key.replace("_", " ").title()
        param_type = spec.get("type") or (
            "float" if isinstance(spec.get("default"), float) else "int"
        )
        param_types[key] = param_type

        default_val = configured_params.get(key, spec.get("default"))
        if default_val is None:
            default_val = base_strategy_params.get(
                key, 0 if param_type == "int" else 0.0
            )

        min_val = spec.get("min")
        max_val = spec.get("max")
        step_val = spec.get("step") or (1 if param_type == "int" else 0.1)
        opt_min, opt_max = resolve_range(spec)

        if min_val is None:
            min_val = opt_min if opt_min is not None else default_val
        if max_val is None:
            max_val = opt_max if opt_max is not None else default_val
        if min_val is None:
            min_val = 0 if param_type == "int" else 0.0
        if max_val is None or max_val <= min_val:
            max_val = min_val + (step_val * 10)

        stored_range = range_preferences.get(key)

        # CrÃ©er 2 colonnes: plage + sensibilitÃ©
        col_range, col_sense = st.columns([3, 1])

        with col_range:
            if param_type == "int":
                min_val = int(round(min_val))
                max_val = int(round(max_val))
                step_val = max(1, int(round(step_val)))

                if stored_range:
                    stored_low, stored_high = map(int, stored_range)
                    default_tuple = (
                        max(min_val, stored_low),
                        min(max_val, stored_high),
                    )
                else:
                    default_tuple = (min_val, max_val)

                selected_range = st.slider(
                    label,
                    min_value=min_val,
                    max_value=max_val,
                    value=(int(default_tuple[0]), int(default_tuple[1])),
                    step=1,
                    key=f"sweep_range_{key}",
                )
            else:
                min_val = float(min_val)
                max_val = float(max_val)
                step_val = float(step_val)

                if stored_range:
                    stored_low = float(stored_range[0])
                    stored_high = float(stored_range[1])
                    default_tuple = (
                        max(min_val, stored_low),
                        min(max_val, stored_high),
                    )
                else:
                    default_tuple = (min_val, max_val)

                selected_range = st.slider(
                    label,
                    min_value=min_val,
                    max_value=max_val,
                    value=(float(default_tuple[0]), float(default_tuple[1])),
                    step=step_val,
                    key=f"sweep_range_{key}",
                )

        # SensibilitÃ© : Slider de STEP (granularitÃ© d'exploration)
        with col_sense:
            range_span = selected_range[1] - selected_range[0]

            if param_type == "int":
                # Pour entiers : step de 1 Ã  range_span
                step_input = st.slider(
                    "ðŸ“Š Step",
                    min_value=1,
                    max_value=max(1, int(range_span)),
                    value=step_val,
                    step=1,
                    key=f"sweep_step_{key}",
                    label_visibility="collapsed",
                )
            else:
                # Pour floats : step de (step_val/10) Ã  range_span
                step_input = st.slider(
                    "ðŸ“Š Step",
                    min_value=step_val / 10,
                    max_value=range_span if range_span > 0 else step_val,
                    value=step_val,
                    step=step_val / 100,
                    format="%.4f",
                    key=f"sweep_step_{key}",
                    label_visibility="collapsed",
                )

        range_preferences[key] = (selected_range[0], selected_range[1])
        param_ranges[key] = (selected_range[0], selected_range[1])
        param_steps[key] = step_input

        # Display combination count for this parameter
        range_min, range_max = selected_range
        step = step_input
        span = range_max - range_min

        if param_type == "int":
            # For integers: count the values in the range with this step
            n_combinations = len(
                range(int(range_min), int(range_max) + 1, max(1, int(step)))
            )
        else:
            # For floats: (span / step)
            n_combinations = span / step if step > 0 else 1

        # Show the combination count
        comb_text = f"ðŸ“Š Plage: {range_min} â†’ {range_max} | SensibilitÃ©: {step} | Combinaisons: {n_combinations:.1f}"
        st.caption(comb_text)

    st.session_state["strategy_param_ranges"] = range_preferences

    # Calculer le nombre total de combinaisons
    total_combinations = 1
    for key, (min_v, max_v) in param_ranges.items():
        step = param_steps[key]
        if param_types[key] == "int":
            n_values = len(range(int(min_v), int(max_v) + 1, max(1, int(step))))
        else:
            # Utiliser le mÃªme calcul que np.linspace pour cohÃ©rence
            n_values = max(2, int((max_v - min_v) / step) + 1)
        total_combinations *= n_values

    # Affichage du nombre total de combinaisons
    if total_combinations <= 100000:
        st.success(
            f"âœ… **{total_combinations} combinaisons** - Grille optimale (rapide)"
        )
    elif total_combinations <= 1000000:
        st.info(
            f"ðŸ“Š **{total_combinations} combinaisons** - Grille normale (quelques minutes)"
        )
    elif total_combinations <= 3000000:
        st.warning(
            f"âš ï¸ **ATTENTION: {total_combinations} combinaisons** - Peut prendre 30-60 min avec GPU"
        )
        st.info("ðŸ’¡ **Note:** Grille large mais faisable avec multi-GPU et 30 workers")
    else:
        st.error(f"âŒ **BLOKÃ‰: {total_combinations} combinaisons trop nombreuses!**")
        st.error("ðŸ›‘ Cette grille causera un MemoryError (>3M mÃªme avec GPU).")
        st.info(
            "âœ¨ **Solutions:**\n1. Augmentez le step (sensibilitÃ©) pour tous les paramÃ¨tres\n2. RÃ©duisez les plages (min/max)\n3. Utilisez Monte-Carlo Ã  la place"
        )

    # Bouton de lancement (dÃ©sactivÃ© si grille > 3 millions)
    button_disabled = total_combinations > 3000000
    if st.button(
        "ðŸ”¬ Lancer le Sweep",
        type="primary",
        use_container_width=True,
        key="run_sweep_btn",
        disabled=button_disabled,
    ):
        indicator_settings = IndicatorSettings(use_gpu=use_gpu)
        indicator_bank = IndicatorBank(indicator_settings)
        runner = SweepRunner(
            indicator_bank=indicator_bank,
            max_workers=max_workers,
            use_multigpu=use_multigpu,
        )

        # Construire les paramÃ¨tres pour le sweep
        scenario_params: Dict[str, Any] = {}
        for key, (min_v, max_v) in param_ranges.items():
            step = param_steps[key]
            if param_types[key] == "int":
                values = list(range(int(min_v), int(max_v) + 1, max(1, int(step))))
            else:
                values = np.linspace(
                    min_v, max_v, num=max(2, int((max_v - min_v) / step) + 1)
                ).tolist()
            scenario_params[key] = {"values": values}

        # ðŸ”¥ FIX CRITIQUE: Ajouter TOUS les paramÃ¨tres par dÃ©faut manquants
        # Garantir que min_pnl_pct et autres params sont TOUJOURS prÃ©sents
        all_param_specs = parameter_specs_for(strategy)
        for key, spec in all_param_specs.items():
            if key not in scenario_params:
                # PrioritÃ©: configured_params > base_strategy_params > spec default
                value = configured_params.get(
                    key,
                    base_strategy_params.get(
                        key, spec.get("default") if isinstance(spec, dict) else spec
                    ),
                )
                scenario_params[key] = {"value": value}
                logger.debug(f"Param par dÃ©faut ajoutÃ©: {key} = {value}")

        # Utiliser run_grid pour explorer toutes les combinaisons
        spec = ScenarioSpec(type="grid", params=scenario_params)

        # RÃ©cupÃ©rer les donnÃ©es rÃ©elles pour le backtest
        symbol = st.session_state.get("symbol", "BTC")
        timeframe = st.session_state.get("timeframe", "1h")
        start_date = st.session_state.get("start_date")
        end_date = st.session_state.get("end_date")

        # ðŸ”¥ FIX CRITIQUE: Recharger les donnÃ©es avec les dates correctes
        # Les donnÃ©es en session peuvent Ãªtre obsolÃ¨tes ou couvrir une pÃ©riode diffÃ©rente
        try:
            real_data = load_ohlcv(symbol, timeframe, start=start_date, end=end_date)
            if real_data.empty:
                st.error(
                    f"âš ï¸ Aucune donnÃ©e disponible pour {symbol}/{timeframe} entre {start_date} et {end_date}"
                )
                return
            # Mettre Ã  jour le cache pour cohÃ©rence
            st.session_state.data = real_data
            st.info(
                f"ðŸ“Š DonnÃ©es chargÃ©es: {len(real_data)} barres "
                f"({real_data.index[0].date()} â†’ {real_data.index[-1].date()})"
            )
        except Exception as e:
            st.error(f"âŒ Erreur chargement donnÃ©es: {e}")
            return

        try:
            # Lancer le sweep avec barre de progression
            st.markdown("### ðŸš€ ExÃ©cution du Sweep")
            results = _run_sweep_with_progress(
                runner, spec, real_data, symbol, timeframe, strategy, total_combinations
            )
            st.session_state["sweep_results"] = results

            # Afficher les informations de configuration
            st.markdown("---")
            st.markdown("### âš™ï¸ Configuration d'exÃ©cution")
            col_info1, col_info2, col_info3 = st.columns(3)
            with col_info1:
                st.metric(
                    "Mode Multi-GPU", "ActivÃ© âœ…" if use_multigpu else "DÃ©sactivÃ© âŠ˜"
                )
            with col_info2:
                actual_workers = runner.max_workers if runner.max_workers else "Auto"
                st.metric("Workers utilisÃ©s", str(actual_workers))
            with col_info3:
                st.metric(
                    "Total des rÃ©sultats",
                    len(results) if isinstance(results, pd.DataFrame) else 0,
                )
        except Exception as exc:
            st.error(f"âŒ Erreur Sweep: {exc}")
            import traceback

            st.code(traceback.format_exc())
            return

    # Affichage des rÃ©sultats
    results_df = st.session_state.get("sweep_results")

    if isinstance(results_df, pd.DataFrame) and not results_df.empty:
        st.markdown("---")
        st.markdown("### ðŸ“Š RÃ©sultats du Sweep")

        score_col = None
        for candidate in ["score", "objective", "sharpe", "total_return", "pnl"]:
            if candidate in results_df.columns:
                score_col = candidate
                break

        if score_col:
            results_sorted = results_df.sort_values(by=score_col, ascending=False)
        else:
            results_sorted = results_df

        st.dataframe(results_sorted.head(100), use_container_width=True, height=400)

        best_row = results_sorted.iloc[0]
        st.markdown("#### ðŸ† Meilleure configuration")
        st.json(best_row.to_dict())

        csv = results_df.to_csv(index=False).encode("utf-8")
        st.download_button(
            "ðŸ’¾ Exporter les rÃ©sultats Sweep (CSV)",
            csv,
            "sweep_results.csv",
            "text/csv",
            use_container_width=True,
        )


def _build_sweep_grid(
    min_value: float, max_value: float, step: float, value_type: str
) -> np.ndarray:
    """CrÃ©e une grille de valeurs pour le sweep en gÃ©rant int/float proprement."""
    if max_value < min_value:
        min_value, max_value = max_value, min_value

    if value_type == "int":
        min_int = int(round(min_value))
        max_int = int(round(max_value))
        step_int = max(1, int(round(step)))
        return np.arange(min_int, max_int + step_int, step_int, dtype=int)

    step_float = float(step) if step else 0.1
    if step_float <= 0:
        step_float = 0.1

    span = max_value - min_value
    if span <= 0:
        return np.array([min_value], dtype=float)

    count = int(round(span / step_float)) + 1
    values = min_value + np.arange(count) * step_float
    values = values[values <= max_value + step_float * 1e-6]
    if len(values) == 0 or values[-1] < max_value:
        values = np.append(values, max_value)
    return np.round(values, 8)


def _format_param_value(value: float, value_type: str, decimals: int = 4) -> str:
    if value_type == "int":
        return str(int(round(value)))
    formatted = f"{value:.{decimals}f}"
    return formatted.rstrip("0").rstrip(".")


def _render_trades_table(trades: List[Dict[str, Any]]) -> None:
    """Table des transactions."""
    if not trades:
        st.info("â„¹ï¸ Aucune transaction enregistrÃ©e.")
        return

    trades_df = pd.DataFrame(trades)

    # Formater si colonnes spÃ©cifiques existent
    if "profit" in trades_df.columns:
        trades_df["profit"] = trades_df["profit"].apply(lambda x: f"${x:.2f}")

    st.dataframe(
        trades_df,
        use_container_width=True,
        height=300,
    )

    # Bouton export
    csv = trades_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "ðŸ“¥ Exporter les trades (CSV)",
        csv,
        "trades.csv",
        "text/csv",
        use_container_width=True,
    )


def _render_monitoring_section(
    metadata: Optional[Dict[str, Any]], history: Optional[pd.DataFrame]
) -> None:
    """Affiche les diagnostics GPU/CPU et les courbes de monitoring."""
    has_metadata = isinstance(metadata, dict) and bool(metadata)
    has_history = isinstance(history, pd.DataFrame) and not history.empty

    if not has_metadata and not has_history:
        return

    st.markdown("#### ðŸ” Diagnostics SystÃ¨me & GPU")

    if has_metadata:
        devices = metadata.get("devices_used", [])
        gpu_enabled = metadata.get("gpu_enabled", False)
        multi_gpu = metadata.get("multi_gpu_enabled", False)
        gpu_balance = metadata.get("gpu_balance", {})
        exec_time = metadata.get("execution_time_sec")
        monitor_stats = metadata.get("monitoring_stats", {})

        col_meta1, col_meta2, col_meta3 = st.columns(3)
        with col_meta1:
            st.metric("GPU activÃ©", "Oui" if gpu_enabled else "Non")
            st.metric("Multi-GPU", "Oui" if multi_gpu else "Non")
        with col_meta2:
            st.metric("DurÃ©e (s)", f"{exec_time:.2f}" if exec_time else "N/A")
            st.metric(
                "GPU 1 moyen (%)",
                f"{monitor_stats.get('gpu1_mean', 0):.1f}" if monitor_stats else "N/A",
            )
        with col_meta3:
            st.metric(
                "GPU 2 moyen (%)",
                f"{monitor_stats.get('gpu2_mean', 0):.1f}" if monitor_stats else "N/A",
            )
            st.metric(
                "CPU moyen (%)",
                f"{monitor_stats.get('cpu_mean', 0):.1f}" if monitor_stats else "N/A",
            )

        with st.expander("DÃ©tails GPU", expanded=False):
            st.write("PÃ©riphÃ©riques :", devices or "Inconnu")
            if gpu_balance:
                st.write("Balance de charge :", gpu_balance)
            if monitor_stats:
                st.json(monitor_stats)

    if has_history:
        df = history.copy()
        fig = go.Figure()
        fig.add_trace(
            go.Scatter(
                x=df["time"], y=df["cpu"], name="CPU (%)", line=dict(color="#26a69a")
            )
        )
        fig.add_trace(
            go.Scatter(
                x=df["time"], y=df["gpu1"], name="GPU 1 (%)", line=dict(color="#42a5f5")
            )
        )
        fig.add_trace(
            go.Scatter(
                x=df["time"], y=df["gpu2"], name="GPU 2 (%)", line=dict(color="#ef5350")
            )
        )
        fig.update_layout(
            height=320,
            template="plotly_dark",
            xaxis_title="Temps (s)",
            yaxis_title="Utilisation (%)",
            legend=dict(
                orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1
            ),
            margin=dict(l=0, r=0, t=30, b=0),
        )
        st.plotly_chart(fig, use_container_width=True, key="monitoring_chart")


def main() -> None:
    """Point d'entrÃ©e de la page Optimisation."""
    st.title("ðŸ”¬ Optimisation de StratÃ©gies")
    # Unified run-state across UI
    if "run_active" not in st.session_state:
        st.session_state.run_active = False
    if "run_kind" not in st.session_state:
        st.session_state.run_kind = None
    if "run_stop_requested" not in st.session_state:
        st.session_state.run_stop_requested = False
    if "current_runner" not in st.session_state:
        st.session_state.current_runner = None

    # Global Stop control in sidebar
    with st.sidebar:
        if st.button(
            "â¹ ArrÃªter l'exÃ©cution", use_container_width=True, key="global_stop_btn"
        ):
            st.session_state.run_stop_requested = True
            try:
                from threadx.optimization.engine import request_global_stop

                request_global_stop()
            except Exception:
                pass
            st.warning("ArrÃªt demandÃ© â€” tentative d'interruption des tÃ¢ches en cours.")
    st.markdown("*Optimisez vos paramÃ¨tres de trading avec Sweep ou Monte-Carlo*")
    st.markdown("---")

    # Onglets principaux (Backtest Simple supprimÃ©)
    tab1, tab2 = st.tabs(["ðŸ”¬ Sweep", "ðŸŽ² Monte-Carlo"])

    with tab1:
        _render_optimization_tab()

    with tab2:
        _render_monte_carlo_tab()


if __name__ == "__main__":
    main()

----------------------------------------
Fichier: ui\page_config_strategy.py
"""
ThreadX - Page Configuration & StratÃ©gie
=========================================

Page fusionnÃ©e combinant la sÃ©lection des donnÃ©es et la configuration de stratÃ©gie.
Interface moderne et intuitive avec sections collapsibles et organisation optimisÃ©e.

Author: ThreadX Framework
Version: 2.0.0 - UI Redesign
"""

from __future__ import annotations

from datetime import date
from typing import Dict, Any, List, Optional, Tuple

import pandas as pd
import plotly.graph_objects as go
import streamlit as st

from ..data_access import (
    DATA_DIR,
    discover_tokens_and_timeframes,
    get_available_timeframes_for_token,
    load_ohlcv,
)
from ..dataset.validate import validate_dataset
from .strategy_registry import indicator_specs_for, list_strategies, parameter_specs_for

DEFAULT_SYMBOL = "BTCUSDC"
DEFAULT_TIMEFRAME = "15m"
DEFAULT_START_DATE = date(2024, 12, 1)
DEFAULT_END_DATE = date(2025, 1, 31)


def _render_ohlcv_chart(df: pd.DataFrame) -> None:
    """Affiche un graphique en chandelier moderne des donnÃ©es OHLCV."""
    fig = go.Figure()

    fig.add_trace(
        go.Candlestick(
            x=df.index,
            open=df["open"],
            high=df["high"],
            low=df["low"],
            close=df["close"],
            name="OHLC",
            increasing_line_color='#26a69a',
            decreasing_line_color='#ef5350',
        )
    )

    fig.update_layout(
        height=450,
        margin=dict(l=0, r=0, t=10, b=0),
        template="plotly_dark",
        xaxis_title="",
        yaxis_title="Prix (USD)",
        xaxis=dict(
            rangeslider=dict(visible=False),
            gridcolor='rgba(128,128,128,0.2)',
        ),
        yaxis=dict(gridcolor='rgba(128,128,128,0.2)'),
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        font=dict(color='#a8b2d1', size=11),
        hovermode='x unified',
    )

    st.plotly_chart(fig, use_container_width=True, key="chart_preview")


def _render_data_section() -> None:
    """Section de sÃ©lection et prÃ©visualisation des donnÃ©es."""
    st.markdown("### ðŸ“Š SÃ©lection des DonnÃ©es")

    tokens, _ = discover_tokens_and_timeframes()
    if not tokens:
        st.error("âŒ Aucun dataset trouvÃ©. Ajoutez vos fichiers dans le dossier data/.")
        tokens = [DEFAULT_SYMBOL]

    # RÃ©cupÃ©rer valeurs session
    default_symbol = st.session_state.get("symbol", tokens[0] if tokens else DEFAULT_SYMBOL)
    if default_symbol not in tokens:
        default_symbol = tokens[0]

    # Layout en colonnes pour sÃ©lection compacte
    col1, col2, col3, col4 = st.columns([2, 2, 2, 2])

    with col1:
        symbol = st.selectbox(
            "Token",
            options=tokens,
            index=tokens.index(default_symbol) if default_symbol in tokens else 0,
            key="sel_symbol"
        )

    # Timeframes dynamiques
    timeframes = get_available_timeframes_for_token(symbol)
    if not timeframes:
        timeframes = [DEFAULT_TIMEFRAME]

    default_timeframe = st.session_state.get("timeframe", timeframes[0])
    if default_timeframe not in timeframes:
        default_timeframe = timeframes[0]

    with col2:
        timeframe = st.selectbox(
            "Timeframe",
            options=timeframes,
            index=timeframes.index(default_timeframe) if default_timeframe in timeframes else 0,
            key="sel_timeframe"
        )

    default_start = st.session_state.get("start_date", DEFAULT_START_DATE)
    default_end = st.session_state.get("end_date", DEFAULT_END_DATE)

    with col3:
        start_date = st.date_input("DÃ©but", value=default_start, key="sel_start")

    with col4:
        end_date = st.date_input("Fin", value=default_end, key="sel_end")

    # Sauvegarder dans session
    st.session_state.symbol = symbol
    st.session_state.timeframe = timeframe
    st.session_state.start_date = start_date
    st.session_state.end_date = end_date

    # Validation
    if start_date > end_date:
        st.error("âš ï¸ La date de dÃ©but doit Ãªtre antÃ©rieure Ã  la date de fin.")
        return

    # Bouton de chargement/prÃ©visualisation
    st.markdown("")
    if st.button("ðŸ”„ Charger & PrÃ©visualiser", type="primary", use_container_width=True):
        with st.spinner("â³ Chargement des donnÃ©es..."):
            try:
                df = load_ohlcv(symbol, timeframe, start=start_date, end=end_date)

                if df.empty:
                    st.warning("âš ï¸ Aucune donnÃ©e disponible pour cette plage.")
                    return

                # Sauvegarder dans session
                st.session_state.data = df

                # Afficher rÃ©sumÃ©
                st.success(
                    f"âœ… {len(df)} lignes chargÃ©es | "
                    f"{df.index.min().strftime('%Y-%m-%d')} â†’ {df.index.max().strftime('%Y-%m-%d')}"
                )

                # Graphique
                st.markdown("#### ðŸ“ˆ AperÃ§u du MarchÃ©")
                _render_ohlcv_chart(df)

                # Stats rapides en colonnes
                col_s1, col_s2, col_s3, col_s4 = st.columns(4)
                with col_s1:
                    st.metric("Prix moyen", f"${df['close'].mean():.2f}")
                with col_s2:
                    st.metric("Min / Max", f"${df['low'].min():.2f} / ${df['high'].max():.2f}")
                with col_s3:
                    volatility = df['close'].pct_change().std() * 100
                    st.metric("VolatilitÃ©", f"{volatility:.2f}%")
                with col_s4:
                    st.metric("Volume moyen", f"{df['volume'].mean():.0f}")

            except FileNotFoundError as exc:
                st.error(f"âŒ {exc}")
            except Exception as exc:
                st.error(f"âŒ Erreur: {exc}")

    # Afficher info si donnÃ©es dÃ©jÃ  chargÃ©es
    current_df = st.session_state.get("data")
    if isinstance(current_df, pd.DataFrame) and not current_df.empty:
        st.info(
            f"ðŸ’¾ {len(current_df)} lignes en mÃ©moire pour {symbol}/{timeframe} "
            f"({current_df.index.min().strftime('%Y-%m-%d')} â†’ {current_df.index.max().strftime('%Y-%m-%d')})"
        )


def _normalize_spec(spec: Any) -> Dict[str, Any]:
    if isinstance(spec, dict):
        normalized = dict(spec)
        if "type" not in normalized:
            default = normalized.get("default")
            if isinstance(default, bool):
                normalized["type"] = "bool"
            elif isinstance(default, int) and not isinstance(default, bool):
                normalized["type"] = "int"
            elif isinstance(default, float):
                normalized["type"] = "float"
            elif "options" in normalized:
                normalized["type"] = "select"
            else:
                normalized["type"] = "text"
        return normalized

    default = spec
    if isinstance(default, bool):
        inferred_type = "bool"
    elif isinstance(default, int) and not isinstance(default, bool):
        inferred_type = "int"
    elif isinstance(default, float):
        inferred_type = "float"
    else:
        inferred_type = "text"

    return {
        "default": default,
        "type": inferred_type,
    }





def _render_param_control(
    label: str,
    widget_key: str,
    spec: Dict[str, Any],
    prefill: Any,
    range_store: Optional[Dict[str, Tuple[Any, Any]]] = None,
    store_key: Optional[str] = None,
) -> Any:
    normalized = _normalize_spec(spec)
    param_type = normalized.get("type", "text")
    default = normalized.get("default")
    min_value = normalized.get("min")
    max_value = normalized.get("max")
    step = normalized.get("step")
    options = normalized.get("options")
    control = normalized.get("control")
    opt_range = normalized.get("opt_range")

    if prefill is None:
        prefill = default

    # Gestion des sliders de plage pour les paramÃ¨tres numÃ©riques
    if (
        range_store is not None
        and store_key
        and param_type in {"int", "float"}
        and normalized.get("range_slider", True)
    ):
        if opt_range and min_value is None:
            min_value = opt_range[0]
        if opt_range and max_value is None:
            max_value = opt_range[1]

        if param_type == "int":
            step_val = int(step or 1)
            if min_value is None:
                min_value = int(prefill) if prefill is not None else 0
            else:
                min_value = int(min_value)
            if max_value is None:
                max_value = int(prefill + step_val * 10) if prefill is not None else min_value + step_val * 10
            else:
                max_value = int(max_value)

            stored_range = range_store.get(store_key) or opt_range
            if stored_range:
                low, high = map(int, stored_range)
            else:
                center = int(prefill) if prefill is not None else (min_value + max_value) // 2
                low = center - step_val * 5
                high = center + step_val * 5

            low = max(min_value, low)
            high = min(max_value, high)
            if low > high:
                low, high = min_value, max_value

            slider_value = st.slider(
                label,
                min_value=min_value,
                max_value=max_value,
                value=(int(low), int(high)),
                step=step_val,
                key=widget_key,
            )
            slider_value = (int(slider_value[0]), int(slider_value[1]))
            range_store[store_key] = slider_value
            return int(round((slider_value[0] + slider_value[1]) / 2))

        if param_type == "float":
            step_val = float(step or 0.05)
            if min_value is None:
                min_value = float(prefill) - step_val * 10 if prefill is not None else 0.0
            else:
                min_value = float(min_value)
            if max_value is None:
                if prefill is not None:
                    max_value = float(prefill) + step_val * 10
                else:
                    max_value = min_value + step_val * 20
            else:
                max_value = float(max_value)

            stored_range = range_store.get(store_key) or opt_range
            if stored_range:
                low, high = map(float, stored_range)
            else:
                center = float(prefill) if prefill is not None else (min_value + max_value) / 2.0
                span = step_val * 5
                low = center - span
                high = center + span

            low = max(min_value, low)
            high = min(max_value, high)
            if low > high:
                low, high = min_value, max_value

            slider_value = st.slider(
                label,
                min_value=float(min_value),
                max_value=float(max_value),
                value=(float(low), float(high)),
                step=step_val,
                key=widget_key,
            )
            slider_value = (float(slider_value[0]), float(slider_value[1]))
            range_store[store_key] = slider_value
            return float((slider_value[0] + slider_value[1]) / 2.0)

    if min_value is not None and prefill is not None:
        prefill = max(prefill, min_value)
    if max_value is not None and prefill is not None:
        prefill = min(prefill, max_value)

    if param_type == "bool":
        return st.checkbox(label, value=bool(prefill), key=widget_key)

    if options:
        try:
            index = options.index(prefill)
        except ValueError:
            index = 0
        return st.selectbox(label, options=options, index=index, key=widget_key)

    if param_type == "int":
        step_val = int(step or 1)
        if control == "number_input" or min_value is None or max_value is None:
            return st.number_input(
                label,
                value=int(prefill) if prefill is not None else int(default or 0),
                step=step_val,
                key=widget_key,
            )

        min_int = int(min_value)
        max_int = int(max_value)
        value = int(prefill) if prefill is not None else int(default or min_int)
        value = min(max(value, min_int), max_int)
        return st.slider(
            label,
            min_value=min_int,
            max_value=max_int,
            value=value,
            step=step_val,
            key=widget_key,
        )

    if param_type == "float":
        step_val = float(step or 0.1)
        if control == "number_input" or min_value is None or max_value is None:
            return st.number_input(
                label,
                value=float(prefill) if prefill is not None else float(default or 0.0),
                step=step_val,
                key=widget_key,
            )

        min_float = float(min_value)
        max_float = float(max_value)
        value = float(prefill) if prefill is not None else float(default or min_float)
        value = min(max(value, min_float), max_float)
        return st.slider(
            label,
            min_value=min_float,
            max_value=max_float,
            value=value,
            step=step_val,
            key=widget_key,
        )

    return st.text_input(label, value=str(prefill) if prefill is not None else "", key=widget_key)


def _render_indicator_inputs(name: str, specs: Dict[str, Any], range_store: Dict[str, Tuple[Any, Any]]) -> Dict[str, Any]:
    """Rendu des inputs pour un indicateur."""
    prev_indicators = st.session_state.get("indicators", {})
    saved = prev_indicators.get(name, {})
    result: Dict[str, Any] = {}

    for key, spec in specs.items():
        normalized = _normalize_spec(spec)
        prefill = saved.get(key, normalized.get("default"))
        label = normalized.get("label") or f"{key}".replace("_", " ").title()
        col_key = f"{name}_{key}"
        store_key = f"{name}.{key}"
        result[key] = _render_param_control(label, col_key, normalized, prefill, range_store, store_key)

    return result


def _render_strategy_section() -> None:
    """Section de configuration de la stratÃ©gie."""
    st.markdown("### ?? Configuration de la StratÃ©gie")

    strategies = list_strategies()
    if not strategies:
        st.error("? Aucune stratÃ©gie disponible dans le registre.")
        return

    default_strat = st.session_state.get("strategy", strategies[0])
    if default_strat not in strategies:
        default_strat = strategies[0]

    strategy = st.selectbox(
        "SÃ©lectionnez une stratÃ©gie",
        strategies,
        index=strategies.index(default_strat),
        key="sel_strategy"
    )

    try:
        indicator_specs = indicator_specs_for(strategy)
        param_specs = parameter_specs_for(strategy)
    except KeyError:
        st.error(f"? StratÃ©gie inconnue: {strategy}")
        return

    # Initialiser valeurs d'indicateurs avec leurs dÃ©fauts (onglet supprimÃ©)
    if indicator_specs:
        indicator_values = {
            ind_name: {
                key: _normalize_spec(spec).get("default")
                for key, spec in ind_spec.items()
            }
            for ind_name, ind_spec in indicator_specs.items()
        }
    else:
        indicator_values = {}

    range_store = st.session_state.get("strategy_param_ranges", {}).copy()

    st.markdown("#### ?? ParamÃ¨tres de StratÃ©gie")
    prev_params = st.session_state.get("strategy_params", {})
    if not param_specs:
        st.info("?? Cette stratÃ©gie n'a pas de paramÃ¨tres configurables.")
        strategy_params = {}
    else:
        items = list(param_specs.items())
        columns = st.columns(2) if 1 <= len(items) <= 4 else None
        strategy_params = {}
        for idx, (key, spec) in enumerate(items):
            normalized = _normalize_spec(spec)
            label = normalized.get("label") or key.replace("_", " ").title()
            prefill = prev_params.get(key, normalized.get("default"))
            param_key = f"strat_param_{key}"
            if columns:
                container = columns[idx % len(columns)]
                with container:
                    strategy_params[key] = _render_param_control(label, param_key, normalized, prefill, range_store, key)
            else:
                strategy_params[key] = _render_param_control(label, param_key, normalized, prefill, range_store, key)

    st.session_state.strategy = strategy
    st.session_state.indicators = indicator_values
    st.session_state.strategy_params = strategy_params
    st.session_state["strategy_param_ranges"] = range_store

    st.success(f"? Configuration enregistrÃ©e : **{strategy}**")




def main() -> None:
    """Point d'entrÃ©e de la page Chargement & Visualisation."""
    st.title("ðŸ“Š Chargement des DonnÃ©es")
    st.markdown("*SÃ©lectionnez et prÃ©visualisez vos donnÃ©es de marchÃ©*")
    st.markdown("---")

    # Section unique : Chargement et visualisation
    _render_data_section()
    st.markdown("---")
    _render_strategy_section()


if __name__ == "__main__":
    main()

----------------------------------------
Fichier: ui\strategy_registry.py
from __future__ import annotations
from typing import Any, Dict, List, Tuple


def _scalar_default(spec: Any) -> Any:
    if isinstance(spec, dict):
        return spec.get("default")
    return spec


# Mapping Strategie â†’ mÃ©tadonnÃ©es indicateurs + paramÃ¨tres
REGISTRY: Dict[str, Dict[str, Any]] = {
    "Bollinger_Breakout": {
        "indicators": {
            "bollinger": {
                "period": {
                    "default": 20,
                    "min": 5,
                    "max": 120,
                    "step": 1,
                    "type": "int",
                    "label": "PÃ©riode Bollinger",
                },
                "std": {
                    "default": 2.0,
                    "min": 1.0,
                    "max": 4.0,
                    "step": 0.1,
                    "type": "float",
                    "label": "Sigma (Ïƒ)",
                },
            },
            "atr": {
                "period": {
                    "default": 14,
                    "min": 5,
                    "max": 50,
                    "step": 1,
                    "type": "int",
                    "label": "PÃ©riode ATR",
                },
            },
        },
        "params": {
            "bb_period": {
                "default": 20,
                "min": 10,
                "max": 50,
                "step": 10,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "int",
                "label": "PÃ©riode Bollinger (SMA)",
                "opt_range": (10, 50),  # Plage classique du tableau : 10 â†’ 50
            },
            "bb_std": {
                "default": 2.0,
                "min": 1.5,
                "max": 3.0,
                "step": 0.5,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "float",
                "label": "Sigma Bollinger (K)",
                "opt_range": (1.5, 3.0),  # Plage classique du tableau : 1.5 â†’ 3.0
            },
            "entry_z": {
                "default": 1.0,
                "min": 0.5,
                "max": 2.5,
                "step": 0.25,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "float",
                "label": "Seuil Z-score entrÃ©e",
                "opt_range": (0.8, 2.0),
            },
            "entry_logic": {
                "default": "AND",
                "options": ["AND", "OR"],
                "type": "select",
                "label": "Logique d'entrÃ©e (AND/OR)",
                "tunable": False,  # Non-optimisable (enum, pas de plage numÃ©rique)
            },
            "atr_period": {
                "default": 14,
                "min": 7,
                "max": 21,
                "step": 4,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "int",
                "label": "PÃ©riode ATR",
                "opt_range": (7, 21),  # Plage classique du tableau : 7 â†’ 21
            },
            "atr_multiplier": {
                "default": 1.5,
                "min": 1.0,
                "max": 3.0,
                "step": 0.5,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "float",
                "label": "Multiplicateur ATR pour stops",
                "opt_range": (1.0, 3.0),  # Plage classique : 1.0 â†’ 3.0 (pas de 0.25)
            },
            "trailing_stop": {
                "default": True,
                "type": "bool",
                "label": "Activer Trailing Stop ATR",
                "tunable": False,  # Non-optimisable (boolÃ©en, pas de plage)
            },
            "risk_per_trade": {
                "default": 0.02,
                "min": 0.005,
                "max": 0.1,
                "step": 0.005,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "float",
                "label": "Risque par Trade (fraction du capital)",
                "opt_range": (
                    0.015,
                    0.03,
                ),  # Plage d'optimisation : 1.5% â†’ 3% (pas de 0.25%)
            },
            "min_pnl_pct": {
                "default": 0.0,  # FIX: 0.0 = dÃ©sactivÃ© (0.01% filtrait TOUS les trades)
                "min": 0.0,
                "max": 0.5,
                "step": 0.02,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "float",
                "label": "Filtre PnL Minimum (%)",
                "opt_range": (0.0, 0.05),  # Plage d'optimisation : 0% â†’ 5%
            },
            "leverage": {
                "default": 1.0,
                "min": 1.0,
                "max": 150.0,
                "step": 1.0,
                "type": "float",
                "label": "Levier (1.0 = sans levier, OPTIONNEL)",
                "tunable": False,  # Non optimisable par dÃ©faut
            },
            "max_hold_bars": {
                "default": 72,
                "min": 10,
                "max": 500,
                "step": 40,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "int",
                "label": "DurÃ©e Maximale en Position (barres)",
                "opt_range": (30, 200),
            },
            "spacing_bars": {
                "default": 6,
                "min": 1,
                "max": 50,
                "step": 5,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "int",
                "label": "Espacement Minimum entre Trades",
                "opt_range": (3, 20),
            },
            "trend_period": {
                "default": 0,
                "min": 0,
                "max": 100,
                "step": 10,  # AugmentÃ© pour rÃ©duire combinaisons (ajustable via slider sensibilitÃ©)
                "type": "int",
                "label": "PÃ©riode EMA Filtre Tendance (0 = dÃ©sactivÃ©)",
                "opt_range": (0, 50),  # Plage d'optimisation : 0 (dÃ©sactivÃ©) â†’ 50
            },
        },
    },
    "EMA_Cross": {
        "indicators": {
            "ema_fast": {
                "window": {
                    "default": 12,
                    "min": 2,
                    "max": 120,
                    "step": 1,
                    "type": "int",
                    "label": "EMA rapide",
                }
            },
            "ema_slow": {
                "window": {
                    "default": 26,
                    "min": 5,
                    "max": 200,
                    "step": 1,
                    "type": "int",
                    "label": "EMA lente",
                }
            },
        },
        "params": {
            "fast_window": {
                "default": 12,
                "min": 2,
                "max": 120,
                "step": 1,
                "type": "int",
                "label": "FenÃªtre EMA rapide",
            },
            "slow_window": {
                "default": 26,
                "min": 5,
                "max": 200,
                "step": 1,
                "type": "int",
                "label": "FenÃªtre EMA lente",
            },
        },
    },
    "ATR_Channel": {
        "indicators": {
            "atr": {
                "window": {
                    "default": 14,
                    "min": 2,
                    "max": 100,
                    "step": 1,
                    "type": "int",
                    "label": "PÃ©riode ATR",
                },
                "mult": {
                    "default": 2.0,
                    "min": 0.5,
                    "max": 5.0,
                    "step": 0.1,
                    "type": "float",
                    "label": "Multiplicateur",
                },
            }
        },
        "params": {
            "atr_window": {
                "default": 14,
                "min": 2,
                "max": 100,
                "step": 1,
                "type": "int",
                "label": "FenÃªtre ATR",
            },
            "atr_mult": {
                "default": 2.0,
                "min": 0.5,
                "max": 5.0,
                "step": 0.1,
                "type": "float",
                "label": "Multiplicateur ATR",
            },
        },
    },
    "Bollinger_Dual": {
        "indicators": {
            "bollinger": {
                "window": {
                    "default": 20,
                    "min": 5,
                    "max": 120,
                    "step": 1,
                    "type": "int",
                    "label": "PÃ©riode Bollinger",
                },
                "std": {
                    "default": 2.0,
                    "min": 1.0,
                    "max": 4.0,
                    "step": 0.1,
                    "type": "float",
                    "label": "Sigma (Ïƒ)",
                },
            },
            "ma": {
                "window": {
                    "default": 10,
                    "min": 2,
                    "max": 100,
                    "step": 1,
                    "type": "int",
                    "label": "PÃ©riode MA Franchissement",
                },
                "type": {
                    "default": "sma",
                    "options": ["sma", "ema"],
                    "type": "select",
                    "label": "Type MA",
                },
            },
        },
        "params": {
            "bb_window": {
                "default": 20,
                "min": 5,
                "max": 120,
                "step": 1,
                "type": "int",
                "label": "PÃ©riode Bollinger",
                "opt_range": (10, 60),
            },
            "bb_std": {
                "default": 2.0,
                "min": 1.0,
                "max": 4.0,
                "step": 0.1,
                "type": "float",
                "label": "Sigma Bollinger",
                "opt_range": (1.5, 3.5),
            },
            "ma_window": {
                "default": 10,
                "min": 2,
                "max": 100,
                "step": 1,
                "type": "int",
                "label": "PÃ©riode MA",
                "opt_range": (5, 50),
            },
            "ma_type": {
                "default": "sma",
                "options": ["sma", "ema"],
                "type": "select",
                "label": "Type MA",
            },
            "trailing_pct": {
                "default": 0.8,
                "min": 0.1,
                "max": 1.0,
                "step": 0.05,
                "type": "float",
                "label": "% Trailing Stop",
                "opt_range": (0.5, 1.0),
            },
            "short_stop_pct": {
                "default": 0.37,
                "min": 0.0,
                "max": 1.0,
                "step": 0.05,
                "type": "float",
                "label": "% Stop Loss SHORT",
                "opt_range": (0.2, 0.6),
            },
            "risk_per_trade": {
                "default": 0.02,
                "min": 0.001,
                "max": 0.1,
                "step": 0.001,
                "type": "float",
                "label": "Risque par Trade",
                "opt_range": (0.01, 0.05),
            },
            "max_hold_bars": {
                "default": 100,
                "min": 10,
                "max": 500,
                "step": 10,
                "type": "int",
                "label": "DurÃ©e Max (barres)",
                "opt_range": (50, 300),
            },
        },
    },
}


def list_strategies() -> List[str]:
    """Retourne la liste ordonnÃ©e des stratÃ©gies disponibles."""
    return list(REGISTRY.keys())


def indicator_specs_for(strategy: str) -> Dict[str, Any]:
    return REGISTRY[strategy].get("indicators", {})


def parameter_specs_for(strategy: str) -> Dict[str, Any]:
    return REGISTRY[strategy].get("params", {})


def _extract_indicator_defaults(
    specs: Dict[str, Dict[str, Any]],
) -> Dict[str, Dict[str, Any]]:
    defaults: Dict[str, Dict[str, Any]] = {}
    for indicator, params in specs.items():
        defaults[indicator] = {}
        for key, spec in params.items():
            defaults[indicator][key] = _scalar_default(spec)
    return defaults


def _extract_param_defaults(specs: Dict[str, Any]) -> Dict[str, Any]:
    defaults: Dict[str, Any] = {}
    for key, spec in specs.items():
        if isinstance(spec, dict):
            defaults[key] = spec.get("default")
        else:
            defaults[key] = spec
    return defaults


def indicators_for(strategy: str) -> Dict[str, Dict[str, Any]]:
    """Retourne uniquement les valeurs par dÃ©faut des indicateurs."""
    return _extract_indicator_defaults(indicator_specs_for(strategy))


def base_params_for(strategy: str) -> Dict[str, Any]:
    """Retourne les paramÃ¨tres par dÃ©faut pour la stratÃ©gie."""
    return _extract_param_defaults(parameter_specs_for(strategy))


def tunable_parameters_for(strategy: str) -> Dict[str, Dict[str, Any]]:
    """ParamÃ¨tres numÃ©riques pouvant Ãªtre optimisÃ©s (min/max/step)."""
    specs = parameter_specs_for(strategy)
    tunables: Dict[str, Dict[str, Any]] = {}
    for key, raw_spec in specs.items():
        if isinstance(raw_spec, dict):
            param_type = raw_spec.get("type")
            default = raw_spec.get("default")
            # Exclure les paramÃ¨tres non-optimisables (tunable: False)
            if raw_spec.get("tunable") is False:
                continue
        else:
            default = raw_spec
            param_type = (
                "float"
                if isinstance(raw_spec, float)
                else "int" if isinstance(raw_spec, int) else None
            )
            raw_spec = {"default": raw_spec}

        if param_type in {"int", "float"} or isinstance(default, (int, float)):
            spec = dict(raw_spec)
            spec.setdefault(
                "type", param_type or ("float" if isinstance(default, float) else "int")
            )
            tunables[key] = spec

    return tunables


def resolve_range(spec: Dict[str, Any]) -> Tuple[Any, Any]:
    """Renvoie la plage par dÃ©faut (min/max) pour un paramÃ¨tre."""
    opt_range = spec.get("opt_range")
    if isinstance(opt_range, (list, tuple)) and len(opt_range) == 2:
        return opt_range[0], opt_range[1]
    return spec.get("min", spec.get("default")), spec.get("max", spec.get("default"))

----------------------------------------
Fichier: ui\system_monitor.py
"""
ThreadX System Monitor - Monitoring Temps RÃ©el CPU/GPU
======================================================

Module de monitoring systÃ¨me pour visualiser l'utilisation des ressources
pendant les backtests et optimisations.

Features:
- Monitoring CPU (utilisation, mÃ©moire)
- Monitoring GPU 1 (RTX 5090) - utilisation, mÃ©moire
- Monitoring GPU 2 (RTX 2060) - utilisation, mÃ©moire
- Thread-safe data collection
- Streamlit-ready visualizations

Author: ThreadX Framework
Version: 1.0
"""

import time
import threading
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from collections import deque
import logging

import psutil
import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

# Import GPU monitoring
try:
    import cupy as cp

    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False

try:
    from threadx.utils.gpu.device_manager import list_devices, get_device_by_name

    GPU_MANAGER_AVAILABLE = True
except ImportError:
    GPU_MANAGER_AVAILABLE = False

# Import NVIDIA Management Library pour vraies mÃ©triques GPU
try:
    import pynvml

    PYNVML_AVAILABLE = True
except ImportError:
    PYNVML_AVAILABLE = False


@dataclass
class SystemSnapshot:
    """
    Snapshot instantanÃ© des ressources systÃ¨me.

    Attributes:
        timestamp: Temps du snapshot (epoch)
        cpu_percent: Utilisation CPU (0-100)
        memory_percent: Utilisation mÃ©moire RAM (0-100)
        gpu1_percent: Utilisation GPU 1 (0-100)
        gpu1_memory_percent: MÃ©moire GPU 1 (0-100)
        gpu1_temperature: TempÃ©rature GPU 1 (Â°C)
        gpu1_power_usage: Consommation GPU 1 (W)
        gpu2_percent: Utilisation GPU 2 (0-100)
        gpu2_memory_percent: MÃ©moire GPU 2 (0-100)
        gpu2_temperature: TempÃ©rature GPU 2 (Â°C)
        gpu2_power_usage: Consommation GPU 2 (W)
    """

    timestamp: float
    cpu_percent: float = 0.0
    memory_percent: float = 0.0
    gpu1_percent: float = 0.0
    gpu1_memory_percent: float = 0.0
    gpu1_temperature: float = 0.0
    gpu1_power_usage: float = 0.0
    gpu2_percent: float = 0.0
    gpu2_memory_percent: float = 0.0
    gpu2_temperature: float = 0.0
    gpu2_power_usage: float = 0.0

    def to_dict(self) -> Dict[str, float]:
        """Convertit en dictionnaire."""
        return {
            "timestamp": self.timestamp,
            "cpu": self.cpu_percent,
            "memory": self.memory_percent,
            "gpu1": self.gpu1_percent,
            "gpu1_mem": self.gpu1_memory_percent,
            "gpu1_temp": self.gpu1_temperature,
            "gpu1_power": self.gpu1_power_usage,
            "gpu2": self.gpu2_percent,
            "gpu2_mem": self.gpu2_memory_percent,
            "gpu2_temp": self.gpu2_temperature,
            "gpu2_power": self.gpu2_power_usage,
        }


class SystemMonitor:
    """
    Moniteur systÃ¨me temps rÃ©el avec collecte thread-safe.

    Collecte les mÃ©triques CPU/RAM/GPU dans un thread sÃ©parÃ©
    et fournit des mÃ©thodes pour visualisation Streamlit.

    Example:
        >>> monitor = SystemMonitor(interval=0.5, max_history=120)
        >>> monitor.start()
        >>> # ... exÃ©cution de code ...
        >>> snapshot = monitor.get_latest_snapshot()
        >>> monitor.stop()
    """

    def __init__(self, interval: float = 0.5, max_history: int = 120):
        """
        Initialise le moniteur systÃ¨me.

        Args:
            interval: Intervalle de collecte en secondes (dÃ©faut: 0.5s)
            max_history: Nombre max de snapshots gardÃ©s en mÃ©moire (dÃ©faut: 120 = 1min Ã  0.5s)
        """
        self.interval = interval
        self.max_history = max_history

        # Thread de collecte
        self._running = False
        self._thread: Optional[threading.Thread] = None
        self._lock = threading.Lock()

        # Historique des snapshots
        self._history: deque = deque(maxlen=max_history)

        # DÃ©tection GPU via pynvml
        self._gpu1_handle = None
        self._gpu2_handle = None
        self._gpu_available = False

        if PYNVML_AVAILABLE:
            try:
                pynvml.nvmlInit()
                device_count = pynvml.nvmlDeviceGetCount()

                # Identifier GPUs par nom
                for i in range(device_count):
                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                    name = pynvml.nvmlDeviceGetName(handle).decode("utf-8")

                    if "5090" in name or "5080" in name:  # RTX 5090/5080
                        self._gpu1_handle = handle
                        logger.info(f"GPU 1 dÃ©tectÃ©: {name} (index {i})")
                    elif "2060" in name:  # RTX 2060
                        self._gpu2_handle = handle
                        logger.info(f"GPU 2 dÃ©tectÃ©: {name} (index {i})")

                self._gpu_available = (self._gpu1_handle is not None) or (
                    self._gpu2_handle is not None
                )

            except Exception as e:
                logger.warning(f"Erreur initialisation pynvml: {e}")
                self._gpu_available = False
        else:
            logger.info("pynvml non disponible - monitoring GPU dÃ©sactivÃ©")

        logger.info(
            f"SystemMonitor initialisÃ©: interval={interval}s, max_history={max_history}"
        )

    def _collect_cpu_metrics(self) -> Dict[str, float]:
        """Collecte les mÃ©triques CPU/RAM."""
        try:
            cpu_percent = psutil.cpu_percent(interval=None)
            memory = psutil.virtual_memory()
            memory_percent = memory.percent

            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
            }
        except Exception as e:
            logger.warning(f"Erreur collecte CPU: {e}")
            return {"cpu_percent": 0.0, "memory_percent": 0.0}

    def _collect_gpu_metrics(self) -> Dict[str, float]:
        """Collecte les mÃ©triques GPU via pynvml."""
        metrics = {
            "gpu1_percent": 0.0,
            "gpu1_memory_percent": 0.0,
            "gpu1_temperature": 0.0,
            "gpu1_power_usage": 0.0,
            "gpu2_percent": 0.0,
            "gpu2_memory_percent": 0.0,
            "gpu2_temperature": 0.0,
            "gpu2_power_usage": 0.0,
        }

        if not self._gpu_available or not PYNVML_AVAILABLE:
            return metrics

        try:
            # GPU 1 (RTX 5090/5080)
            if self._gpu1_handle:
                # Utilisation GPU (%)
                util = pynvml.nvmlDeviceGetUtilizationRates(self._gpu1_handle)
                metrics["gpu1_percent"] = float(util.gpu)

                # MÃ©moire VRAM (%)
                mem_info = pynvml.nvmlDeviceGetMemoryInfo(self._gpu1_handle)
                metrics["gpu1_memory_percent"] = (
                    (mem_info.used / mem_info.total * 100)
                    if mem_info.total > 0
                    else 0.0
                )

                # TempÃ©rature (Â°C)
                temp = pynvml.nvmlDeviceGetTemperature(
                    self._gpu1_handle, pynvml.NVML_TEMPERATURE_GPU
                )
                metrics["gpu1_temperature"] = float(temp)

                # Consommation (W)
                power = (
                    pynvml.nvmlDeviceGetPowerUsage(self._gpu1_handle) / 1000.0
                )  # mW â†’ W
                metrics["gpu1_power_usage"] = float(power)

            # GPU 2 (RTX 2060)
            if self._gpu2_handle:
                util = pynvml.nvmlDeviceGetUtilizationRates(self._gpu2_handle)
                metrics["gpu2_percent"] = float(util.gpu)

                mem_info = pynvml.nvmlDeviceGetMemoryInfo(self._gpu2_handle)
                metrics["gpu2_memory_percent"] = (
                    (mem_info.used / mem_info.total * 100)
                    if mem_info.total > 0
                    else 0.0
                )

                temp = pynvml.nvmlDeviceGetTemperature(
                    self._gpu2_handle, pynvml.NVML_TEMPERATURE_GPU
                )
                metrics["gpu2_temperature"] = float(temp)

                power = pynvml.nvmlDeviceGetPowerUsage(self._gpu2_handle) / 1000.0
                metrics["gpu2_power_usage"] = float(power)

        except Exception as e:
            logger.warning(f"Erreur collecte GPU: {e}")

        return metrics

    def _collect_snapshot(self) -> SystemSnapshot:
        """Collecte un snapshot complet du systÃ¨me."""
        timestamp = time.time()

        # Collecte CPU/RAM
        cpu_metrics = self._collect_cpu_metrics()

        # Collecte GPU
        gpu_metrics = self._collect_gpu_metrics()

        return SystemSnapshot(
            timestamp=timestamp,
            cpu_percent=cpu_metrics["cpu_percent"],
            memory_percent=cpu_metrics["memory_percent"],
            gpu1_percent=gpu_metrics["gpu1_percent"],
            gpu1_memory_percent=gpu_metrics["gpu1_memory_percent"],
            gpu1_temperature=gpu_metrics["gpu1_temperature"],
            gpu1_power_usage=gpu_metrics["gpu1_power_usage"],
            gpu2_percent=gpu_metrics["gpu2_percent"],
            gpu2_memory_percent=gpu_metrics["gpu2_memory_percent"],
            gpu2_temperature=gpu_metrics["gpu2_temperature"],
            gpu2_power_usage=gpu_metrics["gpu2_power_usage"],
        )

    def _monitoring_loop(self):
        """Boucle de monitoring (exÃ©cutÃ©e dans thread sÃ©parÃ©)."""
        logger.info("Thread monitoring dÃ©marrÃ©")

        while self._running:
            try:
                snapshot = self._collect_snapshot()

                with self._lock:
                    self._history.append(snapshot)

                time.sleep(self.interval)

            except Exception as e:
                logger.error(f"Erreur dans monitoring loop: {e}")
                time.sleep(self.interval)

        logger.info("Thread monitoring arrÃªtÃ©")

    def start(self) -> None:
        """DÃ©marre la collecte de mÃ©triques."""
        if self._running:
            logger.warning("Monitoring dÃ©jÃ  dÃ©marrÃ©")
            return

        self._running = True
        self._thread = threading.Thread(
            target=self._monitoring_loop, daemon=True, name="SystemMonitor"
        )
        self._thread.start()

        logger.info("Monitoring dÃ©marrÃ©")

    def stop(self) -> None:
        """ArrÃªte la collecte de mÃ©triques."""
        if not self._running:
            return

        self._running = False

        if self._thread:
            self._thread.join(timeout=2.0)
            self._thread = None

        logger.info("Monitoring arrÃªtÃ©")

    def get_latest_snapshot(self) -> Optional[SystemSnapshot]:
        """
        RÃ©cupÃ¨re le snapshot le plus rÃ©cent.

        Returns:
            SystemSnapshot ou None si aucune donnÃ©e
        """
        with self._lock:
            if not self._history:
                return None
            return self._history[-1]

    def get_history(self, n_last: Optional[int] = None) -> List[SystemSnapshot]:
        """
        RÃ©cupÃ¨re l'historique des snapshots.

        Args:
            n_last: Nombre de derniers snapshots (None = tous)

        Returns:
            Liste des snapshots
        """
        with self._lock:
            history = list(self._history)

            if n_last is not None and n_last > 0:
                return history[-n_last:]

            return history

    def get_history_df(self, n_last: Optional[int] = None) -> pd.DataFrame:
        """
        RÃ©cupÃ¨re l'historique sous forme de DataFrame.

        Args:
            n_last: Nombre de derniers snapshots

        Returns:
            DataFrame avec colonnes [timestamp, cpu, memory, gpu1, gpu1_mem, gpu1_temp, gpu1_power, gpu2, gpu2_mem, gpu2_temp, gpu2_power]
        """
        history = self.get_history(n_last)

        if not history:
            return pd.DataFrame(
                columns=[
                    "timestamp",
                    "cpu",
                    "memory",
                    "gpu1",
                    "gpu1_mem",
                    "gpu1_temp",
                    "gpu1_power",
                    "gpu2",
                    "gpu2_mem",
                    "gpu2_temp",
                    "gpu2_power",
                ]
            )

        data = [snap.to_dict() for snap in history]
        df = pd.DataFrame(data)

        # Convertir timestamp en datetime relatif (secondes depuis dÃ©but)
        if not df.empty:
            df["time"] = df["timestamp"] - df["timestamp"].iloc[0]

        return df

    def get_history_dataframe(self, n_last: Optional[int] = None) -> pd.DataFrame:
        """Alias de get_history_df() pour compatibilitÃ©."""
        return self.get_history_df(n_last)

    def clear_history(self) -> None:
        """Vide l'historique des snapshots."""
        with self._lock:
            self._history.clear()

        logger.info("Historique monitoring vidÃ©")

    def get_stats_summary(self) -> Dict[str, Any]:
        """
        Calcule les statistiques rÃ©sumÃ©es sur l'historique.

        Returns:
            Dict avec moyennes, max, min pour chaque mÃ©trique
        """
        df = self.get_history_df()

        if df.empty:
            return {}

        summary = {
            "cpu_mean": df["cpu"].mean(),
            "cpu_max": df["cpu"].max(),
            "memory_mean": df["memory"].mean(),
            "memory_max": df["memory"].max(),
            "gpu1_mean": df["gpu1"].mean(),
            "gpu1_max": df["gpu1"].max(),
            "gpu1_mem_mean": df["gpu1_mem"].mean(),
            "gpu1_mem_max": df["gpu1_mem"].max(),
            "gpu2_mean": df["gpu2"].mean(),
            "gpu2_max": df["gpu2"].max(),
            "gpu2_mem_mean": df["gpu2_mem"].mean(),
            "gpu2_mem_max": df["gpu2_mem"].max(),
            "duration_seconds": df["time"].iloc[-1] if len(df) > 0 else 0.0,
            "n_samples": len(df),
        }

        return summary

    def is_running(self) -> bool:
        """VÃ©rifie si le monitoring est actif."""
        return self._running

    def __enter__(self):
        """Context manager: dÃ©marre le monitoring."""
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager: arrÃªte le monitoring."""
        self.stop()


# Singleton global pour partage entre composants
_global_monitor: Optional[SystemMonitor] = None


def get_global_monitor() -> SystemMonitor:
    """
    RÃ©cupÃ¨re le moniteur systÃ¨me global (singleton).

    Returns:
        Instance SystemMonitor partagÃ©e

    Example:
        >>> monitor = get_global_monitor()
        >>> monitor.start()
        >>> # ... utilisation ...
        >>> monitor.stop()
    """
    global _global_monitor

    if _global_monitor is None:
        _global_monitor = SystemMonitor(interval=0.5, max_history=120)

    return _global_monitor

----------------------------------------
Fichier: ui\__init__.py
# package marker for apps.threadx_streamlit




----------------------------------------
Fichier: ui\_legacy_v1\page_backtest_results.py
from __future__ import annotations

from typing import Any, Dict, List

import pandas as pd
import plotly.graph_objects as go
import streamlit as st

from .backtest_bridge import run_backtest, BacktestResult
from ..data_access import load_ohlcv


def _require_selection() -> Dict[str, Any]:
    """Retrieve the selection stored in session_state or stop the page."""
    required_keys = ("symbol", "timeframe", "start_date", "end_date", "strategy")
    missing = [key for key in required_keys if key not in st.session_state]
    if missing:
        st.info(
            "Veuillez completer les etapes precedentes (selection et strategie) "
            "avant de lancer un backtest."
        )
        st.stop()

    data_frame = st.session_state.get("data")
    if not isinstance(data_frame, pd.DataFrame) or data_frame.empty:
        st.warning("Chargez des donnees avant d'executer un backtest.")
        st.stop()

    return {key: st.session_state[key] for key in required_keys}


def _render_selection_badge(context: Dict[str, Any]) -> None:
    st.caption(
        f"{context['symbol']} @ {context['timeframe']} | "
        f"{context['start_date']} -> {context['end_date']} | {context['strategy']}"
    )


def _render_price_chart(
    df: pd.DataFrame,
    indicators: Dict[str, Dict[str, Any]],
    trades: List[Dict[str, Any]] | None = None,
    marker_scale: float = 1.0,
    offset_factor: float = 0.02,
) -> None:
    fig = go.Figure()
    fig.add_trace(
        go.Candlestick(
            x=df.index,
            open=df["open"],
            high=df["high"],
            low=df["low"],
            close=df["close"],
            name="OHLC",
        )
    )

    bollinger = indicators.get("bollinger", {})
    if {"window", "std"} <= set(bollinger.keys()) and not df["close"].empty:
        window = int(bollinger["window"])
        std_mult = float(bollinger["std"])
        rolling_close = df["close"].rolling(window, min_periods=window)
        mid = rolling_close.mean()
        std = rolling_close.std()
        fig.add_trace(go.Scatter(x=df.index, y=mid, name="BB mid", mode="lines"))
        fig.add_trace(go.Scatter(x=df.index, y=mid + std_mult * std, name="BB up", mode="lines"))
        fig.add_trace(go.Scatter(x=df.index, y=mid - std_mult * std, name="BB low", mode="lines"))

    trades = trades or []
    marker_scale = max(marker_scale, 0.1)
    offset_factor = max(offset_factor, 0.001)

    if trades:
        entry_x: List[pd.Timestamp] = []
        entry_y: List[float] = []
        entry_text: List[str] = []
        entry_color: List[str] = []
        entry_size: List[float] = []
        entry_symbols: List[str] = []

        exit_x: List[pd.Timestamp] = []
        exit_y: List[float] = []
        exit_text: List[str] = []
        exit_color: List[str] = []
        exit_size: List[float] = []

        marker_shapes: List[go.layout.Shape] = []

        entry_base_size = 11.0 * marker_scale
        exit_base_size = 13.0 * marker_scale

        index_tz = getattr(df.index, "tz", None)

        def _resolve_timestamp(raw: Any) -> pd.Timestamp | None:
            if raw is None:
                return None
            try:
                ts = pd.to_datetime(raw)
            except (TypeError, ValueError):
                return None
            if isinstance(ts, pd.DatetimeIndex):
                ts = ts[0]
            if pd.isna(ts):
                return None
            if index_tz is not None:
                if ts.tzinfo is None:
                    ts = ts.tz_localize(index_tz)
                else:
                    ts = ts.tz_convert(index_tz)
            else:
                if ts.tzinfo is not None:
                    ts = ts.tz_convert(None)
            return ts

        def _resolve_price(raw: Any) -> float | None:
            if raw is None:
                return None
            try:
                value = float(raw)
            except (TypeError, ValueError):
                return None
            return value

        def _lookup_row(ts: pd.Timestamp | None) -> pd.Series | None:
            if ts is None:
                return None
            try:
                row = df.loc[ts]
            except KeyError:
                try:
                    idx = df.index.get_indexer([ts], method="nearest")[0]
                except Exception:
                    return None
                if idx == -1:
                    return None
                row = df.iloc[idx]
            if isinstance(row, pd.DataFrame):
                row = row.iloc[-1]
            return row

        for trade in trades:
            side_raw = trade.get("side") or trade.get("direction") or trade.get("type")
            side = str(side_raw).upper() if isinstance(side_raw, str) else ""
            is_short = side == "SHORT"
            is_long = not is_short

            entry_time = trade.get("entry_time") or trade.get("entry_ts")
            exit_time = trade.get("exit_time") or trade.get("exit_ts")
            entry_ts = _resolve_timestamp(entry_time)
            exit_ts = _resolve_timestamp(exit_time)

            entry_price = _resolve_price(
                trade.get("price_entry", trade.get("entry_price"))
            )
            exit_price = _resolve_price(
                trade.get("price_exit", trade.get("exit_price"))
            )

            pnl_value = _resolve_price(trade.get("pnl"))

            entry_row = _lookup_row(entry_ts)
            exit_row = _lookup_row(exit_ts)

            def _compute_offset(row: pd.Series | None, reference: float | None) -> tuple[float, float, float]:
                if row is None:
                    if reference is None:
                        return (0.0, 0.0, 0.0)
                    return (reference, reference, abs(reference) * offset_factor)
                try:
                    high_val = _resolve_price(row.get("high"))
                    low_val = _resolve_price(row.get("low"))
                except AttributeError:
                    high_val = None
                    low_val = None
                if high_val is None:
                    high_val = reference if reference is not None else _resolve_price(row.get("close"))
                if low_val is None:
                    low_val = reference if reference is not None else _resolve_price(row.get("close"))
                if high_val is None or low_val is None:
                    high_val = low_val = reference if reference is not None else 0.0
                candle_range = max(high_val - low_val, abs(reference or 0.0) * 0.001, 0.1)
                vertical_offset = candle_range * offset_factor
                return float(high_val), float(low_val), float(vertical_offset)

            entry_high, entry_low, entry_offset = _compute_offset(entry_row, entry_price)
            exit_high, exit_low, exit_offset = _compute_offset(exit_row, exit_price)

            if entry_ts is not None and entry_price is not None:
                marker_y = (
                    (entry_low - entry_offset) if is_long else (entry_high + entry_offset)
                )
                entry_x.append(entry_ts)
                entry_y.append(marker_y)
                label_side = "Long" if is_long else "Short"
                entry_text.append(f"{label_side} â€¢ EntrÃ©e {entry_price:.2f}")
                entry_color.append("#00d26a" if is_long else "#ff4d4f")
                entry_symbols.append("triangle-up" if is_long else "triangle-down")
                entry_size.append(entry_base_size)
                marker_shapes.append(
                    go.layout.Shape(
                        type="line",
                        x0=entry_ts,
                        x1=entry_ts,
                        y0=entry_price,
                        y1=marker_y,
                        xref="x",
                        yref="y",
                        line=dict(
                            color="#00d26a" if is_long else "#ff4d4f",
                            width=1,
                            dash="dot",
                        ),
                    )
                )

            if exit_ts is not None and exit_price is not None:
                marker_y_exit = (
                    (exit_high + exit_offset) if is_long else (exit_low - exit_offset)
                )
                exit_x.append(exit_ts)
                exit_y.append(marker_y_exit)
                direction = "Gain" if pnl_value is not None and pnl_value >= 0 else "Perte"
                exit_label = f"{direction} â€¢ Sortie {exit_price:.2f}"
                if pnl_value is not None:
                    exit_label = f"{exit_label} â€¢ PnL: {pnl_value:.2f}"
                exit_text.append(exit_label)
                exit_color.append("#00d26a" if pnl_value is not None and pnl_value >= 0 else "#ff4d4f")
                exit_size.append(exit_base_size)
                marker_shapes.append(
                    go.layout.Shape(
                        type="line",
                        x0=exit_ts,
                        x1=exit_ts,
                        y0=exit_price,
                        y1=marker_y_exit,
                        xref="x",
                        yref="y",
                        line=dict(
                            color="#00d26a" if pnl_value is not None and pnl_value >= 0 else "#ff4d4f",
                            width=1,
                            dash="dot",
                        ),
                    )
                )

        if entry_x:
            fig.add_trace(
                go.Scatter(
                    x=entry_x,
                    y=entry_y,
                    mode="markers",
                    marker=dict(
                        symbol=entry_symbols,
                        size=entry_size,
                        color=entry_color,
                        line=dict(width=1, color="#1f1f2e"),
                    ),
                    name="EntrÃ©es trade",
                    text=entry_text,
                    hovertemplate="%{text}<br>%{x|%Y-%m-%d %H:%M}<extra></extra>",
                )
            )

        if exit_x:
            fig.add_trace(
                go.Scatter(
                    x=exit_x,
                    y=exit_y,
                    mode="markers",
                    marker=dict(
                        symbol="diamond",
                        size=exit_size,
                        color=exit_color,
                        line=dict(width=1, color="#1f1f2e"),
                    ),
                    name="Sorties trade",
                    text=exit_text,
                    hovertemplate="%{text}<br>%{x|%Y-%m-%d %H:%M}<extra></extra>",
                )
            )

        if marker_shapes:
            existing_shapes = list(fig.layout.shapes) if fig.layout.shapes else []
            fig.update_layout(shapes=existing_shapes + marker_shapes)

    fig.update_layout(height=500, margin=dict(l=10, r=10, t=30, b=10))
    st.plotly_chart(fig, use_container_width=True)


def _render_equity_curve(equity: pd.Series) -> None:
    if equity.empty:
        st.warning("Courbe d'equite vide.")
        return
    st.line_chart(equity, use_container_width=True)


def _render_metrics(metrics: Dict[str, Any]) -> None:
    if not metrics:
        st.info("Aucune metrique calculee.")
        return

    columns = st.columns(len(metrics))
    for column, (key, value) in zip(columns, metrics.items()):
        formatted = f"{value:.4f}" if isinstance(value, float) else value
        column.metric(key, formatted)

    metrics_df = pd.DataFrame(list(metrics.items()), columns=["Metrique", "Valeur"])
    csv = metrics_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "Exporter metriques (CSV)",
        csv,
        "metrics.csv",
        mime="text/csv",
        use_container_width=True,
    )


def _render_trades(trades: List[Dict[str, Any]]) -> None:
    if not trades:
        st.info("Aucune transaction enregistree.")
        return

    st.dataframe(pd.DataFrame(trades), use_container_width=True)


def main() -> None:
    """Page 3 : chargement des donnees, execution et affichage des resultats."""
    st.title("Backtest & Resultats")

    # Unified run-state across pages
    if "run_active" not in st.session_state:
        st.session_state.run_active = False
    if "run_kind" not in st.session_state:
        st.session_state.run_kind = None
    if "run_stop_requested" not in st.session_state:
        st.session_state.run_stop_requested = False

    context = _require_selection()
    indicators = st.session_state.get("indicators", {})
    params = st.session_state.get("strategy_params", {})

    _render_selection_badge(context)

    # Toggle Start/Stop button
    if st.session_state.run_active and st.session_state.run_kind == "backtest":
        stop_clicked = st.button("â¹ ArrÃªter", type="secondary", use_container_width=True, key="stop_backtest_btn")
        if stop_clicked:
            st.session_state.run_stop_requested = True
            st.warning("ArrÃªt demandÃ©. Le backtest va s'interrompre dÃ¨s que possible.")
    else:
        run_clicked = st.button("Lancer le backtest", type="primary", use_container_width=True, key="start_backtest_btn")
        if run_clicked:
            st.session_state.run_active = True
            st.session_state.run_kind = "backtest"
            st.session_state.run_stop_requested = False

    # Start run when requested and not already processing
    if st.session_state.run_active and st.session_state.run_kind == "backtest" and not st.session_state.run_stop_requested:
        try:
            df = load_ohlcv(
                context["symbol"],
                context["timeframe"],
                start=context["start_date"],
                end=context["end_date"],
            )
        except FileNotFoundError as exc:
            st.error(str(exc))
            st.session_state.run_active = False
            st.session_state.run_kind = None
            return
        except Exception as exc:  # pragma: no cover - defensive UI safeguard
            st.error(f"Impossible de charger les donnees: {exc}")
            st.session_state.run_active = False
            st.session_state.run_kind = None
            return

        if df.empty:
            st.warning("Dataset vide pour cette plage.")
            st.session_state.run_active = False
            st.session_state.run_kind = None
            return

        try:
            result = run_backtest(df=df, strategy=context["strategy"], params=params)
        except Exception as exc:
            st.error(f"Backtest interrompu: {exc}")
            st.session_state.run_active = False
            st.session_state.run_kind = None
            return

        st.session_state.backtest_results = result
        st.session_state.data = df
        # Reset run state after completion
        st.session_state.run_active = False
        st.session_state.run_kind = None

    stored_result = st.session_state.get("backtest_results")
    stored_df = st.session_state.get("data")

    marker_scale_default = st.session_state.get("trade_marker_scale", 1.0)
    marker_offset_default = st.session_state.get("trade_marker_offset", 0.02)

    with st.expander("Configuration des marqueurs de trades"):
        marker_scale = st.select_slider(
            "Taille des symboles",
            options=[1.0, 1.3, 1.5, 2.0],
            value=marker_scale_default if marker_scale_default in [1.0, 1.3, 1.5, 2.0] else 1.0,
            format_func=lambda value: f"{value:.1f}Ã—",
        )
        marker_offset_percent = st.slider(
            "DÃ©calage vertical (%)",
            min_value=1,
            max_value=10,
            value=int(round(marker_offset_default * 100)),
            step=1,
        )

    st.session_state.trade_marker_scale = marker_scale
    st.session_state.trade_marker_offset = marker_offset_percent / 100.0

    if (
        isinstance(stored_result, BacktestResult)
        and isinstance(stored_df, pd.DataFrame)
        and not stored_df.empty
    ):
        st.subheader("Graphique OHLC")
        _render_price_chart(
            stored_df,
            indicators,
            stored_result.trades,
            marker_scale=st.session_state.trade_marker_scale,
            offset_factor=st.session_state.trade_marker_offset,
        )

        st.subheader("Courbe d'equite")
        _render_equity_curve(stored_result.equity)

        st.subheader("Metriques")
        _render_metrics(stored_result.metrics)

        st.subheader("Transactions")
        _render_trades(stored_result.trades)
    else:
        st.info("Lancez un backtest pour afficher les resultats.")


if __name__ == "__main__":
    main()




----------------------------------------
Fichier: ui\_legacy_v1\page_strategy_indicators.py
from __future__ import annotations

from typing import Any, Dict, List

import streamlit as st

from .strategy_registry import indicator_specs_for, list_strategies, parameter_specs_for


def _pick_strategy(strategies: List[str]) -> str:
    """Return the UI-selected strategy with a safe default."""
    if not strategies:
        st.error("Aucune strategie disponible dans le registre.")
        st.stop()

    default = st.session_state.get("strategy", strategies[0])
    if default not in strategies:
        default = strategies[0]

    index = strategies.index(default)
    return st.selectbox("Strategie", strategies, index=index)


def _normalize_spec(spec: Any) -> Dict[str, Any]:
    if isinstance(spec, dict):
        normalized = dict(spec)
        if "type" not in normalized:
            default = normalized.get("default")
            if isinstance(default, bool):
                normalized["type"] = "bool"
            elif isinstance(default, int) and not isinstance(default, bool):
                normalized["type"] = "int"
            elif isinstance(default, float):
                normalized["type"] = "float"
            elif "options" in normalized:
                normalized["type"] = "select"
            else:
                normalized["type"] = "text"
        return normalized

    default = spec
    if isinstance(default, bool):
        inferred_type = "bool"
    elif isinstance(default, int) and not isinstance(default, bool):
        inferred_type = "int"
    elif isinstance(default, float):
        inferred_type = "float"
    else:
        inferred_type = "text"

    return {
        "default": default,
        "type": inferred_type,
    }


def _render_param_control(label: str, widget_key: str, spec: Dict[str, Any], prefill: Any) -> Any:
    normalized = _normalize_spec(spec)
    param_type = normalized.get("type", "text")
    default = normalized.get("default")
    min_value = normalized.get("min")
    max_value = normalized.get("max")
    step = normalized.get("step")
    options = normalized.get("options")
    control = normalized.get("control")

    if prefill is None:
        prefill = default

    if min_value is not None and prefill is not None:
        prefill = max(prefill, min_value)
    if max_value is not None and prefill is not None:
        prefill = min(prefill, max_value)

    if param_type == "bool":
        return st.checkbox(label, value=bool(prefill), key=widget_key)

    if options:
        try:
            index = options.index(prefill)
        except ValueError:
            index = 0
        return st.selectbox(label, options=options, index=index, key=widget_key)

    if param_type == "int":
        step_val = int(step or 1)
        if control == "number_input" or min_value is None or max_value is None:
            return st.number_input(
                label,
                value=int(prefill) if prefill is not None else int(default or 0),
                step=step_val,
                key=widget_key,
            )

        min_int = int(min_value)
        max_int = int(max_value)
        value = int(prefill) if prefill is not None else int(default or min_int)
        value = min(max(value, min_int), max_int)
        return st.slider(
            label,
            min_value=min_int,
            max_value=max_int,
            value=value,
            step=step_val,
            key=widget_key,
        )

    if param_type == "float":
        step_val = float(step or 0.1)
        if control == "number_input" or min_value is None or max_value is None:
            return st.number_input(
                label,
                value=float(prefill) if prefill is not None else float(default or 0.0),
                step=step_val,
                key=widget_key,
            )

        min_float = float(min_value)
        max_float = float(max_value)
        value = float(prefill) if prefill is not None else float(default or min_float)
        value = min(max(value, min_float), max_float)
        return st.slider(
            label,
            min_value=min_float,
            max_value=max_float,
            value=value,
            step=step_val,
            key=widget_key,
        )

    return st.text_input(label, value=str(prefill) if prefill is not None else "", key=widget_key)


def _indicator_inputs(name: str, specs: Dict[str, Any]) -> Dict[str, Any]:
    """Render inputs for a given indicator and return updated values."""
    prev_indicators = st.session_state.get("indicators", {})
    saved = prev_indicators.get(name, {})
    result: Dict[str, Any] = {}

    with st.expander(name, expanded=True):
        for key, raw_spec in specs.items():
            spec = _normalize_spec(raw_spec)
            prefill = saved.get(key, spec.get("default"))
            label = spec.get("label") or f"{name}.{key}"
            widget_key = f"{name}_{key}"
            result[key] = _render_param_control(label, widget_key, spec, prefill)

    return result


def _strategy_param_inputs(specs: Dict[str, Any]) -> Dict[str, Any]:
    """Render inputs for strategy-level parameters."""
    saved = st.session_state.get("strategy_params", {})
    result: Dict[str, Any] = {}

    for key, raw_spec in specs.items():
        spec = _normalize_spec(raw_spec)
        label = spec.get("label") or key.replace("_", " ").title()
        prefill = saved.get(key, spec.get("default"))
        widget_key = f"strat_param_{key}"
        result[key] = _render_param_control(label, widget_key, spec, prefill)

    return result


def main() -> None:
    """Configure la strategie et les indicateurs associes."""
    st.title("Strategie & Indicateurs")

    strategies = list_strategies()
    strategy = _pick_strategy(strategies)

    try:
        indicator_specs = indicator_specs_for(strategy)
        param_specs = parameter_specs_for(strategy)
    except KeyError:
        st.error(f"Strategie inconnue: {strategy}.")
        st.stop()

    if not indicator_specs:
        st.warning(f"Aucun indicateur defini pour '{strategy}'.")

    st.subheader("Indicateurs requis")
    indicator_values = {
        name: _indicator_inputs(name, defaults) for name, defaults in indicator_specs.items()
    }

    st.subheader("Parametres de strategie")
    strategy_params = _strategy_param_inputs(param_specs)

    st.session_state.strategy = strategy
    st.session_state.indicators = indicator_values
    st.session_state.strategy_params = strategy_params

    st.success("Strategie et indicateurs enregistres.")


if __name__ == "__main__":
    main()




----------------------------------------
Fichier: utils\batching.py
"""
ThreadX Utils Module - Phase 9
Batching Utilities for Large Dataset Processing.

Provides efficient batch processing for large time series and datasets:
- Memory-efficient batch generation
- Configurable batch sizes with adaptive sizing
- Progress tracking and performance monitoring
- Integration with ThreadX caching and timing utilities

Designed for processing large indicator calculations, backtesting sweeps,
and Monte Carlo simulations without memory overflow.
"""

import logging
from typing import Iterator, Any, Optional, Union, Callable, Tuple, List
from dataclasses import dataclass
import numpy as np

# Import ThreadX logger - fallback to standard logging if not available
try:
    from threadx.utils.log import get_logger
except ImportError:

    def get_logger(name: str) -> logging.Logger:
        return logging.getLogger(name)


# Import ThreadX Settings - fallback if not available
try:
    from threadx.config import load_settings

    SETTINGS_AVAILABLE = True
except Exception:  # pragma: no cover - optional dependency during tests
    SETTINGS_AVAILABLE = False

# Import ThreadX utils if available
try:
    from threadx.utils.timing import Timer, performance_context

    TIMING_AVAILABLE = True
except ImportError:
    TIMING_AVAILABLE = False

try:
    from threadx.utils import xp as xpmod

    xp = xpmod.xp
    get_array_info = xpmod.get_array_info
    XP_AVAILABLE = True
except ImportError:
    XP_AVAILABLE = False


logger = get_logger(__name__)


@dataclass
class BatchInfo:
    """Information about a data batch."""

    batch_id: int
    start_idx: int
    end_idx: int
    size: int
    total_batches: int
    progress_pct: float
    memory_mb: Optional[float] = None


def _get_default_batch_size() -> int:
    """Get default batch size from ThreadX settings."""
    if SETTINGS_AVAILABLE:
        try:
            settings = load_settings()
            return settings.VECTORIZATION_BATCH_SIZE
        except Exception:
            pass
    return 10000  # Fallback default


def batch_generator(
    data: Any,
    batch_size: Optional[int] = None,
    overlap: int = 0,
    *,
    track_memory: bool = True,
    progress_callback: Optional[Callable[[BatchInfo], None]] = None,
) -> Iterator[Tuple[Any, BatchInfo]]:
    """
    Generate batches from input data with optional overlap.

    Memory-efficient batch generation for large datasets. Supports
    overlap for windowed operations (e.g., moving averages).

    Parameters
    ----------
    data : array-like
        Input data to batch. Must support len() and slicing.
    batch_size : int, optional
        Size of each batch. If None, uses ThreadX settings default.
    overlap : int, default 0
        Number of elements to overlap between batches.
    track_memory : bool, default True
        Whether to track memory usage of batches.
    progress_callback : callable, optional
        Callback function called for each batch with BatchInfo.

    Yields
    ------
    tuple[Any, BatchInfo]
        Batch data and batch information.

    Examples
    --------
    >>> import numpy as np
    >>> data = np.random.randn(100000)
    >>>
    >>> for batch, info in batch_generator(data, batch_size=5000):
    ...     result = np.mean(batch)  # Process batch
    ...     print(f"Batch {info.batch_id}: {info.progress_pct:.1f}% complete")

    >>> # With overlap for moving window operations
    >>> window_size = 20
    >>> for batch, info in batch_generator(data, batch_size=1000, overlap=window_size-1):
    ...     moving_avg = np.convolve(batch, np.ones(window_size)/window_size, mode='valid')

    Notes
    -----
    - Overlap allows for consistent windowed operations across batch boundaries
    - Memory tracking helps identify optimal batch sizes
    - Progress callback enables UI updates during long operations
    """
    if batch_size is None:
        batch_size = _get_default_batch_size()

    if batch_size <= 0:
        raise ValueError("Batch size must be positive")

    if overlap < 0:
        raise ValueError("Overlap must be non-negative")

    if overlap >= batch_size:
        raise ValueError("Overlap must be less than batch size")

    try:
        data_length = len(data)
    except TypeError:
        raise TypeError("Data must support len() operation")

    if data_length == 0:
        logger.warning("Empty data provided to batch_generator")
        return

    # Calculate number of batches
    effective_step = batch_size - overlap
    if effective_step <= 0:
        raise ValueError("Effective step size (batch_size - overlap) must be positive")

    total_batches = max(
        1, (data_length - overlap + effective_step - 1) // effective_step
    )

    logger.info(
        f"Starting batch processing: {data_length} items, "
        f"batch_size={batch_size}, overlap={overlap}, "
        f"total_batches={total_batches}"
    )

    batch_id = 0
    start_idx = 0

    while start_idx < data_length:
        # Calculate batch boundaries
        end_idx = min(start_idx + batch_size, data_length)
        actual_size = end_idx - start_idx

        # Extract batch
        try:
            batch_data = data[start_idx:end_idx]
        except Exception as e:
            logger.error(f"Failed to extract batch {batch_id}: {e}")
            break

        # Calculate progress
        progress_pct = (batch_id + 1) / total_batches * 100.0

        # Track memory if requested
        memory_mb = None
        if track_memory and XP_AVAILABLE:
            try:
                array_info = get_array_info(batch_data)
                memory_mb = array_info.get("memory_mb", 0.0)
            except Exception as e:
                logger.debug(f"Memory tracking failed for batch {batch_id}: {e}")

        # Create batch info
        info = BatchInfo(
            batch_id=batch_id,
            start_idx=start_idx,
            end_idx=end_idx,
            size=actual_size,
            total_batches=total_batches,
            progress_pct=progress_pct,
            memory_mb=memory_mb,
        )

        # Call progress callback
        if progress_callback:
            try:
                progress_callback(info)
            except Exception as e:
                logger.warning(f"Progress callback failed for batch {batch_id}: {e}")

        yield batch_data, info

        # Move to next batch
        batch_id += 1
        start_idx += effective_step

        # Break if this was the last possible batch
        if end_idx >= data_length:
            break

    logger.info(f"Batch processing completed: {batch_id} batches processed")


def adaptive_batch_size(
    data_size: int,
    target_memory_mb: float = 256.0,
    element_size_bytes: int = 8,
    min_batch_size: int = 100,
    max_batch_size: Optional[int] = None,
) -> int:
    """
    Calculate adaptive batch size based on memory constraints.

    Automatically determines optimal batch size to stay within memory limits
    while maximizing processing efficiency.

    Parameters
    ----------
    data_size : int
        Total number of elements in dataset.
    target_memory_mb : float, default 256.0
        Target memory usage per batch in MB.
    element_size_bytes : int, default 8
        Approximate size per element in bytes (e.g., 8 for float64).
    min_batch_size : int, default 100
        Minimum allowed batch size.
    max_batch_size : int, optional
        Maximum allowed batch size. If None, uses ThreadX settings.

    Returns
    -------
    int
        Recommended batch size.

    Examples
    --------
    >>> # For 1M float64 elements, targeting 256MB per batch
    >>> batch_size = adaptive_batch_size(1_000_000, element_size_bytes=8)
    >>> print(f"Recommended batch size: {batch_size}")

    >>> # For smaller memory constraint
    >>> batch_size = adaptive_batch_size(1_000_000, target_memory_mb=64.0)
    """
    if max_batch_size is None:
        max_batch_size = _get_default_batch_size()

    # Calculate batch size based on memory target
    target_bytes = target_memory_mb * 1024 * 1024
    calculated_size = int(target_bytes / element_size_bytes)

    # Apply constraints
    batch_size = max(min_batch_size, calculated_size)
    batch_size = min(batch_size, max_batch_size)
    batch_size = min(batch_size, data_size)  # Can't be larger than data

    logger.info(
        f"Adaptive batch size: {batch_size} "
        f"(target: {target_memory_mb}MB, element_size: {element_size_bytes}B)"
    )

    return batch_size


def batch_process(
    data: Any,
    processor_func: Callable[[Any], Any],
    batch_size: Optional[int] = None,
    *,
    combine_func: Optional[Callable[[List[Any]], Any]] = None,
    track_performance: bool = True,
    progress_callback: Optional[Callable[[BatchInfo], None]] = None,
) -> Any:
    """
    Process data in batches and optionally combine results.

    High-level batch processing with automatic result combination.
    Includes performance tracking and progress reporting.

    Parameters
    ----------
    data : array-like
        Input data to process.
    processor_func : callable
        Function to apply to each batch. Should accept batch data and return result.
    batch_size : int, optional
        Batch size. If None, uses adaptive sizing.
    combine_func : callable, optional
        Function to combine batch results. If None, returns list of results.
    track_performance : bool, default True
        Whether to measure and log performance.
    progress_callback : callable, optional
        Progress callback for UI updates.

    Returns
    -------
    Any
        Combined results or list of batch results.

    Examples
    --------
    >>> import numpy as np
    >>> data = np.random.randn(100000)
    >>>
    >>> # Simple batch processing
    >>> results = batch_process(
    ...     data,
    ...     lambda batch: np.mean(batch),
    ...     combine_func=lambda results: np.array(results)
    ... )

    >>> # With custom batch size and progress tracking
    >>> def progress_update(info):
    ...     print(f"Processing: {info.progress_pct:.1f}% complete")
    >>>
    >>> results = batch_process(
    ...     data,
    ...     lambda batch: expensive_calculation(batch),
    ...     batch_size=5000,
    ...     progress_callback=progress_update
    ... )
    """
    if batch_size is None:
        # Use adaptive batch sizing
        try:
            element_size = 8  # Default float64
            if hasattr(data, "dtype"):
                element_size = data.dtype.itemsize
            batch_size = adaptive_batch_size(len(data), element_size_bytes=element_size)
        except Exception:
            batch_size = _get_default_batch_size()

    results = []
    total_items = 0

    # Setup performance tracking
    perf_context = None
    if track_performance and TIMING_AVAILABLE:
        try:
            perf_context = performance_context(
                "batch_process", task_count=len(data), unit_of_work="item"
            )
            perf_context.__enter__()
        except Exception as e:
            logger.debug(f"Performance tracking setup failed: {e}")

    try:
        # Process batches
        for batch_data, batch_info in batch_generator(
            data, batch_size=batch_size, progress_callback=progress_callback
        ):
            try:
                batch_result = processor_func(batch_data)
                results.append(batch_result)
                total_items += batch_info.size

            except Exception as e:
                logger.error(f"Processing failed for batch {batch_info.batch_id}: {e}")
                # Continue with other batches
                continue

        # Combine results
        if combine_func and results:
            try:
                final_result = combine_func(results)
                logger.info(
                    f"Batch processing completed: {total_items} items, combined results"
                )
                return final_result
            except Exception as e:
                logger.error(f"Result combination failed: {e}")
                # Fall back to returning list

        logger.info(
            f"Batch processing completed: {total_items} items, {len(results)} batches"
        )
        return results

    finally:
        # Cleanup performance tracking
        if perf_context:
            try:
                perf_context.__exit__(None, None, None)
            except Exception as e:
                logger.debug(f"Performance tracking cleanup failed: {e}")


# Convenience functions for common patterns
def batch_apply(
    data: Any, func: Callable[[Any], Any], batch_size: Optional[int] = None, **kwargs
) -> List[Any]:
    """Apply function to data in batches, returning list of results."""
    return batch_process(data, func, batch_size, **kwargs)


def batch_reduce(
    data: Any,
    func: Callable[[Any], Any],
    reduce_func: Callable[[List[Any]], Any],
    batch_size: Optional[int] = None,
    **kwargs,
) -> Any:
    """Apply function to data in batches and reduce results."""
    return batch_process(data, func, batch_size, combine_func=reduce_func, **kwargs)


def chunked(iterable: Any, chunk_size: int) -> Iterator[List[Any]]:
    """
    Simple chunking utility for iterables.

    Alternative to batch_generator for simple use cases without overlap
    or advanced features.

    Parameters
    ----------
    iterable : iterable
        Input iterable to chunk.
    chunk_size : int
        Size of each chunk.

    Yields
    ------
    list
        Chunks of the iterable.

    Examples
    --------
    >>> data = range(100)
    >>> for chunk in chunked(data, 10):
    ...     print(f"Processing {len(chunk)} items")
    """
    iterator = iter(iterable)
    while True:
        chunk = list(iterator.__next__() for _ in range(chunk_size))
        if not chunk:
            break
        yield chunk




----------------------------------------
Fichier: utils\cache.py
"""
ThreadX Utils Module - Phase 9
Caching Infrastructure with LRU and TTL Support.

Provides high-performance, thread-safe caching with:
- LRU (Least Recently Used) eviction policy
- TTL (Time To Live) expiration
- Combined LRU+TTL caching via decorator
- Comprehensive statistics and observability
- Stable key generation for reproducible caching
- Thread-safe operations with fine-grained locking

Designed for integration with ThreadX Indicators Bank without API changes.
Windows-first, no environment variables, TOML/Settings configuration.
"""

import time
import threading
import hashlib
import pickle
from typing import (
    Any,
    Optional,
    Callable,
    Dict,
    List,
    Tuple,
    Union,
    Generic,
    TypeVar,
    Hashable,
    NamedTuple,
)
from dataclasses import dataclass, field
from functools import wraps
from collections import OrderedDict
import logging

# Import ThreadX logger - fallback to standard logging if not available
try:
    from threadx.utils.log import get_logger
except ImportError:

    def get_logger(name: str) -> logging.Logger:
        return logging.getLogger(name)


# Import ThreadX Settings - fallback if not available
try:
    from threadx.config import load_settings

    SETTINGS_AVAILABLE = True
except Exception:  # pragma: no cover - optional dependency during tests
    SETTINGS_AVAILABLE = False


logger = get_logger(__name__)

K = TypeVar("K", bound=Hashable)
V = TypeVar("V")


class CacheEvent(NamedTuple):
    """Cache event for observability callbacks."""

    event_type: str  # 'hit', 'miss', 'eviction', 'expiration', 'clear'
    key: str
    namespace: Optional[str] = None
    value_size: Optional[int] = None
    timestamp: float = field(default_factory=time.time)


@dataclass
class CacheStats:
    """Cache statistics container."""

    hits: int = 0
    misses: int = 0
    evictions: int = 0
    expirations: int = 0
    current_size: int = 0
    capacity: int = 0

    @property
    def hit_rate(self) -> float:
        """Calculate hit rate as percentage."""
        total = self.hits + self.misses
        return (self.hits / total * 100.0) if total > 0 else 0.0

    @property
    def total_requests(self) -> int:
        """Total cache requests."""
        return self.hits + self.misses

    def reset(self) -> None:
        """Reset all statistics."""
        self.hits = 0
        self.misses = 0
        self.evictions = 0
        self.expirations = 0


class LRUCache(Generic[K, V]):
    """
    Thread-safe LRU (Least Recently Used) cache implementation.

    Features:
    - O(1) get/set operations using OrderedDict
    - Thread-safe with fine-grained locking
    - Comprehensive statistics tracking
    - Configurable capacity with automatic eviction
    - Optional observability callbacks

    Parameters
    ----------
    capacity : int
        Maximum number of items to store.
    on_cache_event : callable, optional
        Callback function for cache events.

    Examples
    --------
    >>> cache = LRUCache[str, int](capacity=100)
    >>> cache.set("key1", 42)
    >>> value = cache.get("key1")  # Returns 42
    >>> cache.contains("key1")    # Returns True
    >>> stats = cache.stats()
    >>> print(f"Hit rate: {stats.hit_rate:.1f}%")
    """

    def __init__(
        self,
        capacity: int,
        on_cache_event: Optional[Callable[[CacheEvent], None]] = None,
    ):
        if capacity <= 0:
            raise ValueError("Cache capacity must be positive")

        self._capacity = capacity
        self._cache: OrderedDict[K, V] = OrderedDict()
        self._stats = CacheStats(capacity=capacity)
        self._lock = threading.RLock()
        self._on_cache_event = on_cache_event

    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:
        """
        Get value by key, updating LRU order.

        Parameters
        ----------
        key : Hashable
            Cache key.
        default : Any, optional
            Default value if key not found.

        Returns
        -------
        Any
            Cached value or default.
        """
        with self._lock:
            if key in self._cache:
                # Move to end (most recently used)
                value = self._cache[key]
                self._cache.move_to_end(key)
                self._stats.hits += 1

                if self._on_cache_event:
                    self._on_cache_event(CacheEvent("hit", str(key)))

                return value
            else:
                self._stats.misses += 1

                if self._on_cache_event:
                    self._on_cache_event(CacheEvent("miss", str(key)))

                return default

    def set(self, key: K, value: V) -> None:
        """
        Set key-value pair, evicting LRU item if needed.

        Parameters
        ----------
        key : Hashable
            Cache key.
        value : Any
            Value to cache.
        """
        with self._lock:
            if key in self._cache:
                # Update existing key
                self._cache[key] = value
                self._cache.move_to_end(key)
            else:
                # Add new key
                self._cache[key] = value
                self._stats.current_size = len(self._cache)

                # Evict LRU item if over capacity
                if len(self._cache) > self._capacity:
                    evicted_key, evicted_value = self._cache.popitem(last=False)
                    self._stats.evictions += 1
                    self._stats.current_size = len(self._cache)

                    if self._on_cache_event:
                        self._on_cache_event(CacheEvent("eviction", str(evicted_key)))

    def contains(self, key: K) -> bool:
        """Check if key exists in cache without updating LRU order."""
        with self._lock:
            return key in self._cache

    def remove(self, key: K) -> bool:
        """Remove key from cache. Returns True if key existed."""
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                self._stats.current_size = len(self._cache)
                return True
            return False

    def clear(self) -> None:
        """Clear all cached items."""
        with self._lock:
            self._cache.clear()
            self._stats.current_size = 0

            if self._on_cache_event:
                self._on_cache_event(CacheEvent("clear", "all"))

    def stats(self) -> CacheStats:
        """Get cache statistics."""
        with self._lock:
            # Return a copy to avoid concurrent modification
            stats_copy = CacheStats(
                hits=self._stats.hits,
                misses=self._stats.misses,
                evictions=self._stats.evictions,
                expirations=self._stats.expirations,
                current_size=len(self._cache),
                capacity=self._capacity,
            )
            return stats_copy

    @property
    def size(self) -> int:
        """Current cache size."""
        with self._lock:
            return len(self._cache)

    @property
    def capacity(self) -> int:
        """Maximum cache capacity."""
        return self._capacity


class TTLCache(Generic[K, V]):
    """
    Thread-safe TTL (Time To Live) cache implementation.

    Features:
    - Automatic expiration based on TTL
    - Lazy expiration on access + explicit purge methods
    - Thread-safe operations
    - Comprehensive statistics
    - Optional observability callbacks

    Parameters
    ----------
    ttl_seconds : float
        Time to live in seconds.
    on_cache_event : callable, optional
        Callback function for cache events.

    Examples
    --------
    >>> cache = TTLCache[str, int](ttl_seconds=300)  # 5 minute TTL
    >>> cache.set("key1", 42)
    >>> # ... after 6 minutes ...
    >>> cache.get("key1")  # Returns None (expired)
    >>> cache.purge_expired()  # Explicit cleanup
    """

    def __init__(
        self,
        ttl_seconds: float,
        on_cache_event: Optional[Callable[[CacheEvent], None]] = None,
    ):
        if ttl_seconds <= 0:
            raise ValueError("TTL must be positive")

        self._ttl_seconds = ttl_seconds
        self._cache: Dict[K, Tuple[V, float]] = {}  # value, expiry_time
        self._stats = CacheStats()
        self._lock = threading.RLock()
        self._on_cache_event = on_cache_event

    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:
        """
        Get value by key, checking expiration.

        Parameters
        ----------
        key : Hashable
            Cache key.
        default : Any, optional
            Default value if key not found or expired.

        Returns
        -------
        Any
            Cached value or default.
        """
        current_time = time.time()

        with self._lock:
            if key in self._cache:
                value, expiry_time = self._cache[key]

                if current_time <= expiry_time:
                    # Valid, not expired
                    self._stats.hits += 1

                    if self._on_cache_event:
                        self._on_cache_event(CacheEvent("hit", str(key)))

                    return value
                else:
                    # Expired, remove it
                    del self._cache[key]
                    self._stats.expirations += 1
                    self._stats.misses += 1

                    if self._on_cache_event:
                        self._on_cache_event(CacheEvent("expiration", str(key)))

                    return default
            else:
                self._stats.misses += 1

                if self._on_cache_event:
                    self._on_cache_event(CacheEvent("miss", str(key)))

                return default

    def set(self, key: K, value: V) -> None:
        """
        Set key-value pair with TTL expiration.

        Parameters
        ----------
        key : Hashable
            Cache key.
        value : Any
            Value to cache.
        """
        expiry_time = time.time() + self._ttl_seconds

        with self._lock:
            self._cache[key] = (value, expiry_time)

    def contains(self, key: K) -> bool:
        """Check if key exists and is not expired."""
        current_time = time.time()

        with self._lock:
            if key in self._cache:
                _, expiry_time = self._cache[key]
                if current_time <= expiry_time:
                    return True
                else:
                    # Expired, clean up
                    del self._cache[key]
                    self._stats.expirations += 1
                    return False
            return False

    def remove(self, key: K) -> bool:
        """Remove key from cache. Returns True if key existed."""
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                return True
            return False

    def purge_expired(self) -> int:
        """
        Remove all expired entries.

        Returns
        -------
        int
            Number of entries removed.
        """
        current_time = time.time()
        expired_keys = []

        with self._lock:
            for key, (value, expiry_time) in self._cache.items():
                if current_time > expiry_time:
                    expired_keys.append(key)

            for key in expired_keys:
                del self._cache[key]
                self._stats.expirations += 1

                if self._on_cache_event:
                    self._on_cache_event(CacheEvent("expiration", str(key)))

        return len(expired_keys)

    def clear(self) -> None:
        """Clear all cached items."""
        with self._lock:
            self._cache.clear()

            if self._on_cache_event:
                self._on_cache_event(CacheEvent("clear", "all"))

    def stats(self) -> CacheStats:
        """Get cache statistics."""
        with self._lock:
            stats_copy = CacheStats(
                hits=self._stats.hits,
                misses=self._stats.misses,
                evictions=self._stats.evictions,
                expirations=self._stats.expirations,
                current_size=len(self._cache),
                capacity=0,  # TTL cache has no fixed capacity
            )
            return stats_copy

    @property
    def size(self) -> int:
        """Current cache size (including potentially expired items)."""
        with self._lock:
            return len(self._cache)

    @property
    def ttl_seconds(self) -> float:
        """Time to live in seconds."""
        return self._ttl_seconds


def generate_stable_key(
    func: Callable,
    args: Tuple[Any, ...],
    kwargs: Dict[str, Any],
    namespace: Optional[str] = None,
) -> str:
    """
    Generate stable, deterministic cache key.

    Creates a consistent hash from function signature and arguments,
    suitable for cross-session caching with stable keys.

    Parameters
    ----------
    func : callable
        Function being cached.
    args : tuple
        Function positional arguments.
    kwargs : dict
        Function keyword arguments.
    namespace : str, optional
        Optional namespace prefix.

    Returns
    -------
    str
        Stable cache key.

    Notes
    -----
    - Handles nested data structures
    - Warns about float arguments (potential precision issues)
    - Uses pickle + SHA256 for deterministic hashing
    - Includes function name and module for uniqueness
    """
    try:
        # Start with function signature
        func_sig = f"{func.__module__}.{func.__qualname__}"

        # Check for potentially unstable float arguments
        def check_floats(obj, path=""):
            if isinstance(obj, float):
                if abs(obj) > 1e-10:  # Not close to zero
                    logger.debug(
                        f"Float in cache key at {path}: {obj} - ensure precision consistency"
                    )
            elif isinstance(obj, (list, tuple)):
                for i, item in enumerate(obj):
                    check_floats(item, f"{path}[{i}]")
            elif isinstance(obj, dict):
                for key, value in obj.items():
                    check_floats(value, f"{path}.{key}")

        check_floats(args, "args")
        check_floats(kwargs, "kwargs")

        # Create stable representation
        key_data = {
            "func": func_sig,
            "args": args,
            "kwargs": sorted(kwargs.items()) if kwargs else None,
            "namespace": namespace,
        }

        # Serialize and hash
        serialized = pickle.dumps(key_data, protocol=pickle.HIGHEST_PROTOCOL)
        key_hash = hashlib.sha256(serialized).hexdigest()

        # Create readable key with hash
        readable_part = f"{func.__name__}"
        if namespace:
            readable_part = f"{namespace}.{readable_part}"

        return f"{readable_part}_{key_hash[:16]}"

    except Exception as e:
        logger.warning(f"Failed to generate stable cache key for {func.__name__}: {e}")
        # Fallback to simpler key
        return f"{func.__name__}_{hash((args, tuple(sorted(kwargs.items())) if kwargs else None))}"


def cached(
    ttl: Optional[int] = None,
    lru: Optional[int] = None,
    key_fn: Optional[Callable] = None,
    namespace: Optional[str] = None,
    stats_logging: bool = True,
) -> Callable:
    """
    Comprehensive caching decorator with LRU and/or TTL support.

    Provides flexible caching with configurable eviction policies.
    Can use LRU-only, TTL-only, or combined LRU+TTL caching.

    Parameters
    ----------
    ttl : int, optional
        Time to live in seconds. If None, no TTL expiration.
    lru : int, optional
        LRU cache capacity. If None, no LRU eviction.
    key_fn : callable, optional
        Custom key generation function. If None, uses generate_stable_key.
    namespace : str, optional
        Cache namespace for key prefixing.
    stats_logging : bool, default True
        Whether to log cache statistics periodically.

    Returns
    -------
    callable
        Decorated function with caching.

    Examples
    --------
    Basic TTL caching:
    >>> @cached(ttl=300)  # 5 minutes
    ... def compute_indicator(symbol, period):
    ...     return expensive_calculation(symbol, period)

    Combined LRU + TTL:
    >>> @cached(ttl=600, lru=1000)  # 10min TTL, 1000 item LRU
    ... def fetch_market_data(symbol, timeframe):
    ...     return api_call(symbol, timeframe)

    Custom key function:
    >>> def custom_key(func, args, kwargs, namespace):
    ...     return f"custom_{args[0]}_{kwargs.get('period', 'default')}"
    >>> @cached(ttl=300, key_fn=custom_key)
    ... def custom_computation(data, period=10):
    ...     return process(data, period)

    Integration with ThreadX Indicators Bank:
    >>> # Wrap existing indicator functions without changing signatures
    >>> @cached(ttl=3600, lru=500, namespace="indicators")
    ... def compute_bollinger_bands(prices, period, std_dev):
    ...     return calculate_bands(prices, period, std_dev)

    Notes
    -----
    - Thread-safe caching with comprehensive statistics
    - Stable key generation prevents cache invalidation issues
    - Integrates with ThreadX logging system
    - Performance optimized for hot-path indicator calculations
    - No changes needed to existing function signatures
    """

    # Cache instance storage (per decorated function)
    _cache_registry: Dict[str, Union[LRUCache, TTLCache, Tuple[LRUCache, TTLCache]]] = (
        {}
    )
    _stats_last_logged: Dict[str, float] = {}

    def _get_stats_interval() -> float:
        """Get stats logging interval from settings."""
        if SETTINGS_AVAILABLE:
            try:
                settings = load_settings()
                return settings.CACHE_STATS_LOG_INTERVAL_SEC
            except Exception:
                pass
        return 300.0  # 5 minute default

    def _log_cache_stats(func_name: str, cache_obj: Any) -> None:
        """Log cache statistics if interval has passed."""
        if not stats_logging:
            return

        current_time = time.time()
        last_logged = _stats_last_logged.get(func_name, 0)
        interval = _get_stats_interval()

        if current_time - last_logged >= interval:
            try:
                if hasattr(cache_obj, "stats"):
                    stats = cache_obj.stats()
                    logger.info(
                        f"Cache stats for {func_name}: "
                        f"hit_rate={stats.hit_rate:.1f}%, "
                        f"hits={stats.hits}, misses={stats.misses}, "
                        f"size={stats.current_size}"
                    )
                elif isinstance(cache_obj, tuple):
                    # Combined LRU+TTL
                    lru_cache, ttl_cache = cache_obj
                    lru_stats = lru_cache.stats()
                    ttl_stats = ttl_cache.stats()
                    logger.info(
                        f"Combined cache stats for {func_name}: "
                        f"LRU hit_rate={lru_stats.hit_rate:.1f}%, "
                        f"TTL hit_rate={ttl_stats.hit_rate:.1f}%, "
                        f"LRU size={lru_stats.current_size}, TTL size={ttl_stats.current_size}"
                    )

                _stats_last_logged[func_name] = current_time

            except Exception as e:
                logger.debug(f"Failed to log cache stats for {func_name}: {e}")

    def decorator(func: Callable) -> Callable:
        func_key = f"{func.__module__}.{func.__qualname__}"

        # Initialize cache for this function
        if func_key not in _cache_registry:
            if lru and ttl:
                # Combined LRU + TTL
                lru_cache = LRUCache(capacity=lru)
                ttl_cache = TTLCache(ttl_seconds=ttl)
                _cache_registry[func_key] = (lru_cache, ttl_cache)
                logger.info(
                    f"Initialized combined LRU+TTL cache for {func.__name__} (lru={lru}, ttl={ttl}s)"
                )
            elif lru:
                # LRU only
                _cache_registry[func_key] = LRUCache(capacity=lru)
                logger.info(
                    f"Initialized LRU cache for {func.__name__} (capacity={lru})"
                )
            elif ttl:
                # TTL only
                _cache_registry[func_key] = TTLCache(ttl_seconds=ttl)
                logger.info(f"Initialized TTL cache for {func.__name__} (ttl={ttl}s)")
            else:
                raise ValueError("Must specify either 'lru' capacity or 'ttl' seconds")

        @wraps(func)
        def wrapper(*args, **kwargs):
            # Generate cache key
            if key_fn:
                cache_key = key_fn(func, args, kwargs, namespace)
            else:
                cache_key = generate_stable_key(func, args, kwargs, namespace)

            cache_obj = _cache_registry[func_key]

            # Try to get from cache
            cached_result = None

            if isinstance(cache_obj, tuple):
                # Combined LRU + TTL - check both
                lru_cache, ttl_cache = cache_obj

                # Try LRU first (faster access pattern)
                cached_result = lru_cache.get(cache_key)
                if cached_result is None:
                    # Try TTL cache
                    cached_result = ttl_cache.get(cache_key)
                    if cached_result is not None:
                        # Found in TTL, promote to LRU
                        lru_cache.set(cache_key, cached_result)

            else:
                # Single cache (LRU or TTL)
                cached_result = cache_obj.get(cache_key)

            if cached_result is not None:
                # Cache hit - log stats and return
                _log_cache_stats(func.__name__, cache_obj)
                return cached_result

            # Cache miss - compute result
            result = func(*args, **kwargs)

            # Store in cache
            if isinstance(cache_obj, tuple):
                # Store in both caches
                lru_cache, ttl_cache = cache_obj
                lru_cache.set(cache_key, result)
                ttl_cache.set(cache_key, result)
            else:
                cache_obj.set(cache_key, result)

            # Log stats periodically
            _log_cache_stats(func.__name__, cache_obj)

            return result

        # Add cache management methods to wrapper
        def cache_stats():
            cache_obj = _cache_registry[func_key]
            if isinstance(cache_obj, tuple):
                lru_cache, ttl_cache = cache_obj
                return {"lru": lru_cache.stats(), "ttl": ttl_cache.stats()}
            else:
                return cache_obj.stats()

        def cache_clear():
            cache_obj = _cache_registry[func_key]
            if isinstance(cache_obj, tuple):
                lru_cache, ttl_cache = cache_obj
                lru_cache.clear()
                ttl_cache.clear()
            else:
                cache_obj.clear()

        def cache_info():
            cache_obj = _cache_registry[func_key]
            if isinstance(cache_obj, tuple):
                lru_cache, ttl_cache = cache_obj
                return {
                    "type": "combined_lru_ttl",
                    "lru_capacity": lru_cache.capacity,
                    "lru_size": lru_cache.size,
                    "ttl_seconds": ttl_cache.ttl_seconds,
                    "ttl_size": ttl_cache.size,
                }
            elif hasattr(cache_obj, "capacity"):
                return {
                    "type": "lru",
                    "capacity": cache_obj.capacity,
                    "size": cache_obj.size,
                }
            else:
                return {
                    "type": "ttl",
                    "ttl_seconds": cache_obj.ttl_seconds,
                    "size": cache_obj.size,
                }

        # Attach management methods
        wrapper.cache_stats = cache_stats
        wrapper.cache_clear = cache_clear
        wrapper.cache_info = cache_info

        return wrapper

    return decorator


# Convenience functions for common caching patterns
def lru_cache(capacity: int, namespace: Optional[str] = None) -> Callable:
    """LRU-only caching decorator."""
    return cached(lru=capacity, namespace=namespace)


def ttl_cache(ttl_seconds: int, namespace: Optional[str] = None) -> Callable:
    """TTL-only caching decorator."""
    return cached(ttl=ttl_seconds, namespace=namespace)


def indicators_cache(ttl_seconds: int = 3600, lru_capacity: int = 1000) -> Callable:
    """
    Specialized caching decorator for ThreadX indicators.

    Optimized for indicator computation patterns with sensible defaults.
    """
    return cached(ttl=ttl_seconds, lru=lru_capacity, namespace="indicators")




----------------------------------------
Fichier: utils\common_imports.py
"""
Module d'imports communs pour ThreadX.

Ce module centralise les imports frÃ©quemment utilisÃ©s Ã  travers le projet
pour rÃ©duire la duplication (DRY principle).

âš ï¸ IMPORTANT: Ce module ne doit PAS importer de modules ThreadX qui pourraient
crÃ©er des dÃ©pendances circulaires (bridge, optimization, backtest, etc.).
Seuls les imports standards et typing sont autorisÃ©s ici.

Usage:
    from threadx.utils.common_imports import pd, np, create_logger
    # ou:
    from threadx.utils.common_imports import *

Author: ThreadX Framework
Phase: Phase 2 Step 3.1 - DRY Refactoring
"""

# === Data Science Libraries ===
import pandas as pd
import numpy as np

# === Typing ===
from typing import (
    Any,
    Dict,
    List,
    Optional,
    Tuple,
    Union,
    Callable,
    TypeVar,
    Generic,
)

# === Logging (safe - no circular dependency) ===
from threadx.utils.log import get_logger

# === Exports pour from common_imports import * ===
__all__ = [
    # Data science
    "pd",
    "np",
    # Typing
    "Any",
    "Dict",
    "List",
    "Optional",
    "Tuple",
    "Union",
    "Callable",
    "TypeVar",
    "Generic",
    # Logging
    "get_logger",
    "create_logger",
]


def create_logger(name: str):
    """
    Helper pour crÃ©er un logger avec le bon nom.

    Args:
        name: Nom du module (ex: "threadx.dataset.loader")

    Returns:
        Logger configurÃ©

    Examples:
        >>> logger = create_logger(__name__)
        >>> logger.info("Message")
    """
    return get_logger(name)


# === Default logger pour usage simple ===
logger = get_logger("threadx")




----------------------------------------
Fichier: utils\determinism.py
"""
ThreadX Determinism Utilities - Global Seeds & Stable Merges
===========================================================

Utilitaires pour garantir le dÃ©terminisme dans ThreadX :
- Seeds globaux pour tous les gÃ©nÃ©rateurs alÃ©atoires
- Merges dÃ©terministes de DataFrames
- Hachage stable pour checksums reproductibles

Garantit la reproductibilitÃ© des rÃ©sultats entre exÃ©cutions.

Author: ThreadX Framework
Version: Phase 10 - Determinism
"""

import hashlib
import json
import random
import sys
from typing import Any, List, Union

import numpy as np
import pandas as pd

from threadx.utils.log import get_logger

logger = get_logger(__name__)


def set_global_seed(seed: int) -> None:
    """
    Configure le seed global pour tous les gÃ©nÃ©rateurs alÃ©atoires.

    Configure :
    - random (Python standard)
    - numpy.random
    - cupy.random (si disponible)
    - torch (si disponible)

    Args:
        seed: Seed Ã  utiliser (entier)

    Example:
        >>> set_global_seed(42)
        >>> # Tous les gÃ©nÃ©rateurs utilisent maintenant le seed 42
    """
    logger.info(f"Configuration du seed global: {seed}")

    # Python standard random
    random.seed(seed)

    # NumPy
    np.random.seed(seed)

    # CuPy (si disponible)
    try:
        import cupy as cp

        cp.random.seed(seed)
        logger.debug("CuPy seed configurÃ©")
    except ImportError:
        logger.debug("CuPy non disponible - seed ignorÃ©")

    # PyTorch (si disponible)
    try:
        import torch

        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        logger.debug("PyTorch seed configurÃ©")
    except ImportError:
        logger.debug("PyTorch non disponible - seed ignorÃ©")

    # Configuration dÃ©terministe pour NumPy
    if hasattr(np.random, "bit_generator"):
        # NumPy moderne (>=1.17)
        np.random.default_rng(seed)

    logger.info(f"Seed global {seed} configurÃ© pour tous les gÃ©nÃ©rateurs")


def enforce_deterministic_merges(df_list: List[pd.DataFrame]) -> pd.DataFrame:
    """
    Merge dÃ©terministe de DataFrames avec ordre stable.

    Garantit un ordre reproductible indÃ©pendamment de :
    - L'ordre d'arrivÃ©e des DataFrames
    - Le parallÃ©lisme de calcul
    - Les variations d'implÃ©mentation pandas

    Args:
        df_list: Liste de DataFrames Ã  merger

    Returns:
        DataFrame merged avec ordre dÃ©terministe

    Example:
        >>> df1 = pd.DataFrame({'a': [1, 2], 'b': ['x', 'y']})
        >>> df2 = pd.DataFrame({'a': [3, 4], 'b': ['z', 'w']})
        >>> merged = enforce_deterministic_merges([df1, df2])
        >>> # Ordre garanti identique entre exÃ©cutions
    """
    if not df_list:
        return pd.DataFrame()

    if len(df_list) == 1:
        return df_list[0].copy()

    logger.debug(f"Merge dÃ©terministe de {len(df_list)} DataFrames")

    # Concatenation avec rÃ©initialisation d'index
    merged_df = pd.concat(df_list, ignore_index=True)

    # Tri dÃ©terministe basÃ© sur toutes les colonnes
    # Ordre lexicographique pour stabilitÃ© maximale
    if len(merged_df) > 1:
        sort_columns = list(merged_df.columns)

        # Tri multi-colonnes avec gestion des NaN
        merged_df = merged_df.sort_values(
            by=sort_columns,
            na_position="last",  # NaN Ã  la fin
            kind="mergesort",  # Algorithme stable
        ).reset_index(drop=True)

    logger.debug(f"Merge terminÃ©: {len(merged_df)} lignes")

    return merged_df


def stable_hash(payload: Any) -> str:
    """
    GÃ©nÃ¨re un hash SHA-256 stable et reproductible.

    Utilise une sÃ©rialisation JSON canonique pour garantir
    que le mÃªme objet produit toujours le mÃªme hash,
    indÃ©pendamment de l'ordre d'insertion des clÃ©s.

    Args:
        payload: Objet Ã  hasher (doit Ãªtre JSON-sÃ©rialisable)

    Returns:
        Hash SHA-256 en hexadÃ©cimal

    Example:
        >>> hash1 = stable_hash({'b': 2, 'a': 1})
        >>> hash2 = stable_hash({'a': 1, 'b': 2})
        >>> hash1 == hash2  # True - ordre des clÃ©s ignorÃ©
    """
    try:
        # SÃ©rialisation JSON canonique
        json_str = json.dumps(
            payload,
            ensure_ascii=True,
            sort_keys=True,  # ClÃ©s triÃ©es
            separators=(",", ":"),  # Pas d'espaces
            default=str,  # Fallback pour objets non-sÃ©rialisables
        )

        # Hash SHA-256
        hash_bytes = hashlib.sha256(json_str.encode("utf-8")).hexdigest()

        return hash_bytes

    except (TypeError, ValueError) as e:
        logger.warning(f"Impossible de hasher l'objet: {e}")
        # Fallback : hash de la reprÃ©sentation string
        fallback_str = str(payload)
        return hashlib.sha256(fallback_str.encode("utf-8")).hexdigest()


def create_deterministic_splits(
    data: Union[pd.DataFrame, np.ndarray], n_splits: int, seed: int = 42
) -> List[Union[pd.DataFrame, np.ndarray]]:
    """
    CrÃ©e des splits dÃ©terministes de donnÃ©es.

    Garantit que les mÃªmes donnÃ©es avec le mÃªme seed
    produisent toujours les mÃªmes splits.

    Args:
        data: DonnÃ©es Ã  splitter
        n_splits: Nombre de splits Ã  crÃ©er
        seed: Seed pour le gÃ©nÃ©rateur alÃ©atoire

    Returns:
        Liste des splits

    Example:
        >>> df = pd.DataFrame({'a': range(100)})
        >>> splits = create_deterministic_splits(df, 3, seed=42)
        >>> len(splits) == 3  # True
    """
    if n_splits <= 0:
        raise ValueError("n_splits doit Ãªtre > 0")

    # Configuration du seed local
    rng = np.random.RandomState(seed)

    # Taille des donnÃ©es
    n_samples = len(data)

    if n_splits > n_samples:
        raise ValueError(f"n_splits ({n_splits}) > n_samples ({n_samples})")

    # GÃ©nÃ©ration d'indices dÃ©terministes
    indices = np.arange(n_samples)
    rng.shuffle(indices)  # MÃ©lange dÃ©terministe

    # Calcul des tailles de splits
    base_size = n_samples // n_splits
    remainder = n_samples % n_splits

    split_sizes = [base_size] * n_splits
    for i in range(remainder):
        split_sizes[i] += 1

    # CrÃ©ation des splits
    splits = []
    start_idx = 0

    for split_size in split_sizes:
        end_idx = start_idx + split_size
        split_indices = indices[start_idx:end_idx]

        if isinstance(data, pd.DataFrame):
            split_data = data.iloc[split_indices].copy()
        else:
            split_data = data[split_indices].copy()

        splits.append(split_data)
        start_idx = end_idx

    logger.debug(
        f"Splits dÃ©terministes crÃ©Ã©s: {n_splits} splits, "
        f"tailles: {[len(s) for s in splits]}"
    )

    return splits


def hash_df(df: pd.DataFrame, cols: List[str] = None) -> str:
    """
    Calcule un hash stable pour un DataFrame.

    Args:
        df: DataFrame Ã  hasher
        cols: Liste des colonnes Ã  inclure (toutes si None)

    Returns:
        Hash hexadÃ©cimal (SHA256)

    Example:
        >>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
        >>> hash_df(df)
        'e25388fde8290dc286a6164fa2d97e551b53498dcbf7bc378eb1f178'
    """
    # SÃ©lectionner colonnes
    if cols is not None:
        df = df[cols].copy()

    # Convertir en chaÃ®ne JSON stable
    json_str = df.to_json(orient="records", date_format="iso")

    # Calculer hash
    return hashlib.sha256(json_str.encode("utf-8")).hexdigest()


def validate_determinism(func, args, kwargs=None, n_runs: int = 3) -> bool:
    """
    Valide qu'une fonction produit des rÃ©sultats dÃ©terministes.

    ExÃ©cute la fonction plusieurs fois avec les mÃªmes arguments
    et vÃ©rifie que les rÃ©sultats sont identiques.

    Args:
        func: Fonction Ã  tester
        args: Arguments de la fonction
        kwargs: Arguments nommÃ©s de la fonction
        n_runs: Nombre d'exÃ©cutions de test

    Returns:
        True si les rÃ©sultats sont dÃ©terministes

    Example:
        >>> def random_func(seed):
        ...     set_global_seed(seed)
        ...     return np.random.rand(10)
        >>> is_deterministic = validate_determinism(random_func, (42,))
    """
    if kwargs is None:
        kwargs = {}

    logger.debug(f"Validation dÃ©terminisme: {n_runs} exÃ©cutions")

    results = []

    for run in range(n_runs):
        try:
            result = func(*args, **kwargs)

            # Conversion en format hashable
            if isinstance(result, (pd.DataFrame, pd.Series)):
                result_hash = stable_hash(result.to_dict())
            elif isinstance(result, np.ndarray):
                result_hash = stable_hash(result.tolist())
            else:
                result_hash = stable_hash(result)

            results.append(result_hash)

        except Exception as e:
            logger.error(f"Erreur lors du run {run}: {e}")
            return False

    # VÃ©rification de l'identitÃ© des rÃ©sultats
    is_deterministic = len(set(results)) == 1

    if is_deterministic:
        logger.debug("âœ… Fonction dÃ©terministe validÃ©e")
    else:
        logger.warning("âŒ Fonction non-dÃ©terministe dÃ©tectÃ©e")
        logger.debug(f"Hashes distincts: {set(results)}")

    return is_deterministic


# === Contexte de dÃ©terminisme ===


class DeterministicContext:
    """
    Context manager pour garantir le dÃ©terminisme temporaire.

    Sauvegarde l'Ã©tat des gÃ©nÃ©rateurs, applique un seed,
    puis restaure l'Ã©tat original Ã  la sortie.
    """

    def __init__(self, seed: int):
        self.seed = seed
        self.saved_states = {}

    def __enter__(self):
        # Sauvegarde des Ã©tats actuels
        self.saved_states["python_random"] = random.getstate()
        self.saved_states["numpy_random"] = np.random.get_state()

        try:
            import cupy as cp

            self.saved_states["cupy_random"] = cp.random.get_random_state()
        except ImportError:
            pass

        # Application du seed temporaire
        set_global_seed(self.seed)

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Restauration des Ã©tats
        random.setstate(self.saved_states["python_random"])
        np.random.set_state(self.saved_states["numpy_random"])

        if "cupy_random" in self.saved_states:
            try:
                import cupy as cp

                cp.random.set_random_state(self.saved_states["cupy_random"])
            except ImportError:
                pass


# === Utilitaires de debugging ===


def get_random_states() -> dict:
    """RÃ©cupÃ¨re l'Ã©tat actuel de tous les gÃ©nÃ©rateurs alÃ©atoires."""
    states = {
        "python_random": random.getstate(),
        "numpy_random": np.random.get_state(),
    }

    try:
        import cupy as cp

        states["cupy_random"] = cp.random.get_random_state()
    except ImportError:
        pass

    return states


def compare_random_states(state1: dict, state2: dict) -> bool:
    """Compare deux Ã©tats de gÃ©nÃ©rateurs alÃ©atoires."""
    return stable_hash(state1) == stable_hash(state2)


if __name__ == "__main__":
    # Tests rapides
    print("Test dÃ©terminisme ThreadX...")

    # Test seed global
    set_global_seed(42)
    sample1 = np.random.rand(5)

    set_global_seed(42)
    sample2 = np.random.rand(5)

    print(f"Samples identiques: {np.array_equal(sample1, sample2)}")

    # Test merge dÃ©terministe
    df1 = pd.DataFrame({"a": [3, 1, 2], "b": ["c", "a", "b"]})
    df2 = pd.DataFrame({"a": [6, 4, 5], "b": ["f", "d", "e"]})

    merged = enforce_deterministic_merges([df1, df2])
    print(f"Merge rÃ©ussi: {len(merged)} lignes")

    # Test hash stable
    hash1 = stable_hash({"z": 3, "a": 1, "b": 2})
    hash2 = stable_hash({"a": 1, "b": 2, "z": 3})
    print(f"Hashes identiques: {hash1 == hash2}")

    print("âœ… Tests dÃ©terminisme OK")




----------------------------------------
Fichier: utils\log.py
"""
ThreadX Centralized Logging System -
==============================================

Provides logger configuration with rotation, Windows-compatible file handling,
and Settings/TOML integration for production-ready applications.

Author: ThreadX Framework
Version: Phase 7 - Sweep & Logging
"""

import logging
import logging.handlers
import sys
from pathlib import Path
from typing import Optional
import os
from threading import Lock

# Global setup lock to prevent double initialization
_setup_lock = Lock()
_setup_done = False


def configure_logging(
    level: str = "INFO", log_file: Optional[str] = None, console: bool = True
) -> None:
    """
    Configure logging avec niveau et options spÃ©cifiques.

    Args:
        level: Niveau de log ("DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL")
        log_file: Chemin du fichier de log (optionnel)
        console: Activer logs console
    """
    # Convertir string niveau en constante
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f"Niveau de log invalide: {level}")

    # Configuration de base
    logger = logging.getLogger()
    logger.setLevel(numeric_level)

    # Supprimer handlers existants
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Format standard
    formatter = logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )

    # Handler console
    if console:
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    # Handler fichier
    if log_file:
        try:
            log_dir = os.path.dirname(log_file)
            if log_dir:
                os.makedirs(log_dir, exist_ok=True)

            file_handler = logging.FileHandler(log_file)
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        except Exception as e:
            logging.error(f"Impossible de configurer log file {log_file}: {e}")


def setup_logging_once() -> None:
    """
    Configure logging system once globally.

    Idempotent function that sets up console and file handlers with rotation.
    Prevents duplicate handlers during multiple calls.

    Notes
    -----
    Thread-safe initialization using locks.
    Creates logs directory if missing.
    Windows-compatible file handling.
    """
    global _setup_done

    with _setup_lock:
        if _setup_done:
            return

        try:
            # Import Settings here to avoid circular dependencies
            try:
                from threadx.utils.settings import Settings

                log_dir = Settings.LOG_DIR
                log_level = Settings.LOG_LEVEL
            except ImportError:
                # Valeurs par dÃ©faut si Settings n'est pas disponible
                log_dir = Path("logs")
                log_level = "INFO"
        except (ImportError, AttributeError):
            # Fallback if Settings not available
            log_dir = Path("logs")
            log_level = "INFO"

        # Ensure log directory exists
        log_dir = Path(log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)

        # Configure root logger
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))

        # Clear any existing handlers
        root_logger.handlers.clear()

        # Console handler for immediate feedback
        console_handler = logging.StreamHandler(sys.stdout)
        console_formatter = logging.Formatter(
            fmt="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        console_handler.setFormatter(console_formatter)
        console_handler.setLevel(logging.INFO)
        root_logger.addHandler(console_handler)

        # File handler with rotation (Windows-safe)
        log_file = log_dir / "threadx.log"
        file_handler = logging.handlers.RotatingFileHandler(
            filename=str(log_file),
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,
            encoding="utf-8",
        )
        file_formatter = logging.Formatter(
            fmt="%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        file_handler.setFormatter(file_formatter)
        file_handler.setLevel(logging.DEBUG)
        root_logger.addHandler(file_handler)

        _setup_done = True


def get_logger(
    name: str,
    *,
    log_dir: Optional[Path] = None,
    level: Optional[str] = None,
    rotate_max_bytes: int = 10 * 1024 * 1024,
    rotate_backups: int = 5,
) -> logging.Logger:
    """
    CrÃ©e ou rÃ©cupÃ¨re un logger ThreadX.

    Args:
        name: Nom du logger (gÃ©nÃ©ralement __name__)
        level: Niveau de log optionnel ('DEBUG', 'INFO', 'WARNING', 'ERROR')

    Returns:
        Logger configurÃ© pour ThreadX
    """
    logger = logging.getLogger(name)

    # Configuration uniquement si pas dÃ©jÃ  configurÃ©
    if not logger.handlers:
        # Handler console
        handler = logging.StreamHandler(sys.stdout)
        formatter = logging.Formatter(
            "[%(asctime)s] %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)

        # Niveau par dÃ©faut
        log_level = level or "INFO"
        logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))

        # Ã‰viter propagation aux loggers parents
        logger.propagate = False

    return logger


def setup_logging(log_file: Optional[Path] = None, level: str = "INFO") -> None:
    """
    Configuration globale du logging ThreadX.

    Args:
        log_file: Fichier de log optionnel
        level: Niveau de log global
    """
    # Configuration root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, level.upper(), logging.INFO))

    # Nettoyer handlers existants
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_formatter = logging.Formatter(
        "[%(asctime)s] %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)

    # File handler si spÃ©cifiÃ©
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        file_formatter = logging.Formatter(
            "[%(asctime)s] %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)


# Logger par dÃ©faut pour ce module
logger = get_logger(__name__)




----------------------------------------
Fichier: utils\resource_monitor.py
"""
ThreadX - Resource Monitoring Utilities
========================================

Monitoring utilisation CPU/RAM/GPU en temps rÃ©el pour optimisation.

Usage:
    >>> from threadx.utils.resource_monitor import get_resource_usage, log_resource_usage
    >>> stats = get_resource_usage()
    >>> print(f"CPU: {stats['cpu_percent']:.1f}%")
    >>> log_resource_usage(logger)  # Log formatÃ©
"""

import time
from typing import Dict, Optional
from threadx.utils.log import get_logger

logger = get_logger(__name__)

# Imports optionnels
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    logger.warning("psutil non disponible, monitoring CPU/RAM dÃ©sactivÃ©")

try:
    import cupy as cp

    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False
    logger.warning("CuPy non disponible, monitoring GPU dÃ©sactivÃ©")


def get_resource_usage() -> Dict[str, float]:
    """
    RÃ©cupÃ¨re utilisation actuelle CPU/RAM/GPU.

    Returns:
        Dict avec stats ressources:
        {
            'cpu_percent': 45.2,
            'ram_percent': 62.1,
            'ram_used_gb': 15.3,
            'ram_total_gb': 32.0,
            'gpu0_percent': 25.8,
            'gpu0_vram_used_gb': 2.5,
            'gpu0_vram_total_gb': 16.0,
            'gpu1_percent': 8.3,
            'gpu1_vram_used_gb': 0.8,
            'gpu1_vram_total_gb': 8.0
        }

    Example:
        >>> stats = get_resource_usage()
        >>> if stats['gpu0_percent'] < 30:
        >>>     logger.warning("GPU0 sous-utilisÃ©!")
    """
    stats = {}

    # CPU
    if PSUTIL_AVAILABLE:
        stats["cpu_percent"] = psutil.cpu_percent(interval=0.1)
        stats["cpu_count"] = psutil.cpu_count(logical=False)
    else:
        stats["cpu_percent"] = 0.0
        stats["cpu_count"] = 0

    # RAM
    if PSUTIL_AVAILABLE:
        mem = psutil.virtual_memory()
        stats["ram_percent"] = mem.percent
        stats["ram_used_gb"] = mem.used / (1024**3)
        stats["ram_total_gb"] = mem.total / (1024**3)
        stats["ram_available_gb"] = mem.available / (1024**3)
    else:
        stats["ram_percent"] = 0.0
        stats["ram_used_gb"] = 0.0
        stats["ram_total_gb"] = 0.0

    # GPUs
    if CUPY_AVAILABLE:
        try:
            gpu_count = cp.cuda.runtime.getDeviceCount()
            stats["gpu_count"] = gpu_count

            for i in range(gpu_count):
                with cp.cuda.Device(i):
                    try:
                        # MÃ©moire GPU
                        mem_info = cp.cuda.runtime.memGetInfo()
                        free_bytes = mem_info[0]
                        total_bytes = mem_info[1]
                        used_bytes = total_bytes - free_bytes

                        used_gb = used_bytes / (1024**3)
                        total_gb = total_bytes / (1024**3)
                        percent = (used_gb / total_gb) * 100 if total_gb > 0 else 0

                        stats[f"gpu{i}_percent"] = percent
                        stats[f"gpu{i}_vram_used_gb"] = used_gb
                        stats[f"gpu{i}_vram_total_gb"] = total_gb
                        stats[f"gpu{i}_vram_free_gb"] = free_bytes / (1024**3)

                        # PropriÃ©tÃ©s device
                        props = cp.cuda.runtime.getDeviceProperties(i)
                        stats[f"gpu{i}_name"] = props["name"].decode("utf-8")

                    except Exception as e:
                        logger.debug(f"Erreur stats GPU{i}: {e}")
                        stats[f"gpu{i}_percent"] = 0.0
                        stats[f"gpu{i}_vram_used_gb"] = 0.0
        except Exception as e:
            logger.debug(f"Erreur accÃ¨s GPUs: {e}")
            stats["gpu_count"] = 0
    else:
        stats["gpu_count"] = 0

    return stats


def log_resource_usage(custom_logger=None):
    """
    Log formatÃ© de l'utilisation ressources.

    Args:
        custom_logger: Logger optionnel (utilise logger module si None)

    Example:
        >>> from threadx.utils.log import get_logger
        >>> logger = get_logger(__name__)
        >>> log_resource_usage(logger)
        # ðŸ’» CPU: 45.2% (8 cores) | ðŸ§  RAM: 62.1% (15.3 / 32.0 GB) |
        # ðŸŽ® GPU0: 25.8% (2.5 / 16.0 GB) | ðŸŽ® GPU1: 8.3% (0.8 / 8.0 GB)
    """
    log = custom_logger or logger
    stats = get_resource_usage()

    # Format CPU
    cpu_str = f"ðŸ’» CPU: {stats.get('cpu_percent', 0):.1f}%"
    if stats.get("cpu_count", 0) > 0:
        cpu_str += f" ({stats['cpu_count']} cores)"

    # Format RAM
    ram_str = (
        f"ðŸ§  RAM: {stats.get('ram_percent', 0):.1f}% "
        f"({stats.get('ram_used_gb', 0):.1f} / {stats.get('ram_total_gb', 0):.1f} GB)"
    )

    # Format GPUs
    gpu_parts = []
    gpu_count = stats.get("gpu_count", 0)
    for i in range(gpu_count):
        gpu_pct = stats.get(f"gpu{i}_percent", 0)
        gpu_used = stats.get(f"gpu{i}_vram_used_gb", 0)
        gpu_total = stats.get(f"gpu{i}_vram_total_gb", 0)
        gpu_name = stats.get(f"gpu{i}_name", f"GPU{i}")

        gpu_parts.append(
            f"ðŸŽ® {gpu_name}: {gpu_pct:.1f}% ({gpu_used:.1f} / {gpu_total:.1f} GB)"
        )

    # Log complet
    if gpu_parts:
        log.info(f"{cpu_str} | {ram_str} | {' | '.join(gpu_parts)}")
    else:
        log.info(f"{cpu_str} | {ram_str}")


def check_resource_saturation(
    cpu_threshold: float = 85.0,
    ram_threshold: float = 85.0,
    gpu_threshold: float = 85.0,
) -> Dict[str, bool]:
    """
    VÃ©rifie si ressources sont saturÃ©es (proche de la limite).

    Args:
        cpu_threshold: Seuil CPU (%)
        ram_threshold: Seuil RAM (%)
        gpu_threshold: Seuil GPU (%)

    Returns:
        Dict avec flags saturation:
        {
            'cpu_saturated': False,
            'ram_saturated': True,
            'gpu0_saturated': False,
            'gpu1_saturated': False,
            'any_saturated': True
        }
    """
    stats = get_resource_usage()
    result = {}

    # CPU
    result["cpu_saturated"] = stats.get("cpu_percent", 0) >= cpu_threshold

    # RAM
    result["ram_saturated"] = stats.get("ram_percent", 0) >= ram_threshold

    # GPUs
    gpu_count = stats.get("gpu_count", 0)
    for i in range(gpu_count):
        gpu_pct = stats.get(f"gpu{i}_percent", 0)
        result[f"gpu{i}_saturated"] = gpu_pct >= gpu_threshold

    # Any
    result["any_saturated"] = any(v for k, v in result.items() if k != "any_saturated")

    return result


def get_utilization_score() -> float:
    """
    Calcule score d'utilisation global (0-100%).

    Plus le score est Ã©levÃ©, mieux les ressources sont utilisÃ©es.
    Objectif: >80% pour optimisation maximale.

    Returns:
        Score global (moyenne pondÃ©rÃ©e CPU/RAM/GPU)

    Example:
        >>> score = get_utilization_score()
        >>> if score < 50:
        >>>     logger.warning(f"Sous-utilisation ressources: {score:.1f}%")
    """
    stats = get_resource_usage()

    weights = []
    values = []

    # CPU (poids 0.3)
    if stats.get("cpu_percent", 0) > 0:
        weights.append(0.3)
        values.append(stats["cpu_percent"])

    # RAM (poids 0.2)
    if stats.get("ram_percent", 0) > 0:
        weights.append(0.2)
        values.append(stats["ram_percent"])

    # GPUs (poids 0.5 total, rÃ©parti entre GPUs)
    gpu_count = stats.get("gpu_count", 0)
    if gpu_count > 0:
        gpu_weight = 0.5 / gpu_count
        for i in range(gpu_count):
            gpu_pct = stats.get(f"gpu{i}_percent", 0)
            if gpu_pct > 0:
                weights.append(gpu_weight)
                values.append(gpu_pct)

    # Calcul score pondÃ©rÃ©
    if not weights:
        return 0.0

    total_weight = sum(weights)
    weighted_sum = sum(w * v for w, v in zip(weights, values))
    score = weighted_sum / total_weight if total_weight > 0 else 0.0

    return score


__all__ = [
    "get_resource_usage",
    "log_resource_usage",
    "check_resource_saturation",
    "get_utilization_score",
]

----------------------------------------
Fichier: utils\timing.py
"""
ThreadX Utils Module
Timing, Profiling and Performance Measurement Utilities.

Provides standardized decorators and context managers for:
- Throughput measurement (tasks/min) with configurable warning thresholds
- Memory consumption tracking via psutil (with graceful fallback)
- Performance timing and cumulative measurements

Integrates with ThreadX Settings/TOML configuration for thresholds.
No environment variables - Windows-first design.
"""

import time
import functools
import logging
from typing import Optional, Callable, Any, Dict, Union
from contextlib import contextmanager
from dataclasses import dataclass, field

# Import psutil with graceful fallback
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

# Import ThreadX logger - fallback to standard logging if not available
try:
    from threadx.utils.log import get_logger
except ImportError:

    def get_logger(name: str) -> logging.Logger:
        return logging.getLogger(name)


# Import ThreadX Settings - fallback if not available
try:
    from threadx.config import load_settings

    SETTINGS_AVAILABLE = True
except Exception:  # pragma: no cover - optional dependency during tests
    SETTINGS_AVAILABLE = False


logger = get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """Container for performance measurement results."""

    elapsed_sec: float
    tasks_completed: int = 0
    tasks_per_min: float = 0.0
    memory_peak_mb: Optional[float] = None
    memory_avg_mb: Optional[float] = None
    function_name: str = ""
    unit_of_work: str = "task"


class Timer:
    """
    High-precision timer context manager.

    Provides accurate timing measurements with minimal overhead.
    Thread-safe and Windows-compatible.

    Examples
    --------
    >>> with Timer() as timer:
    ...     # Some work
    ...     time.sleep(0.1)
    >>> print(f"Elapsed: {timer.elapsed_sec:.3f}s")
    Elapsed: 0.100s

    >>> timer = Timer()
    >>> timer.start()
    >>> # Some work
    >>> timer.stop()
    >>> print(timer.elapsed_sec)
    """

    def __init__(self):
        self._start_time: Optional[float] = None
        self._end_time: Optional[float] = None
        self._elapsed: float = 0.0

    def start(self) -> None:
        """Start timing measurement."""
        self._start_time = time.perf_counter()
        self._end_time = None

    def stop(self) -> None:
        """Stop timing measurement."""
        if self._start_time is None:
            raise RuntimeError("Timer not started")
        self._end_time = time.perf_counter()
        self._elapsed = self._end_time - self._start_time

    @property
    def elapsed_sec(self) -> float:
        """Get elapsed time in seconds."""
        if self._start_time is None:
            return 0.0
        if self._end_time is None:
            # Timer still running
            return time.perf_counter() - self._start_time
        return self._elapsed

    def __enter__(self) -> "Timer":
        """Context manager entry."""
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Context manager exit."""
        self.stop()


class MemoryTracker:
    """
    Memory usage tracker using psutil.

    Tracks peak and average memory consumption during execution.
    Graceful fallback if psutil is not available.
    """

    def __init__(self):
        self.start_memory_mb: Optional[float] = None
        self.peak_memory_mb: Optional[float] = None
        self.memory_samples: list = []
        self._process = None

        if PSUTIL_AVAILABLE:
            try:
                self._process = psutil.Process()
            except Exception as e:
                logger.warning(f"Failed to initialize memory tracker: {e}")

    def start(self) -> None:
        """Start memory tracking."""
        if not PSUTIL_AVAILABLE or self._process is None:
            return

        try:
            memory_info = self._process.memory_info()
            self.start_memory_mb = memory_info.rss / (1024 * 1024)
            self.peak_memory_mb = self.start_memory_mb
            self.memory_samples = [self.start_memory_mb]
        except Exception as e:
            logger.debug(f"Memory tracking start failed: {e}")

    def sample(self) -> None:
        """Take a memory sample."""
        if not PSUTIL_AVAILABLE or self._process is None:
            return

        try:
            memory_info = self._process.memory_info()
            current_mb = memory_info.rss / (1024 * 1024)
            self.memory_samples.append(current_mb)

            if self.peak_memory_mb is None or current_mb > self.peak_memory_mb:
                self.peak_memory_mb = current_mb
        except Exception as e:
            logger.debug(f"Memory sampling failed: {e}")

    def get_stats(self) -> Dict[str, Optional[float]]:
        """Get memory usage statistics."""
        if not self.memory_samples:
            return {"peak_mb": None, "avg_mb": None, "start_mb": None}

        return {
            "peak_mb": self.peak_memory_mb,
            "avg_mb": sum(self.memory_samples) / len(self.memory_samples),
            "start_mb": self.start_memory_mb,
        }


def _get_throughput_threshold() -> int:
    """Get throughput warning threshold from settings."""
    if not SETTINGS_AVAILABLE:
        return 1000  # Default fallback

    try:
        settings = load_settings()
        return settings.MIN_TASKS_PER_MIN
    except Exception as e:
        logger.debug(f"Failed to load throughput threshold from settings: {e}")
        return 1000


def measure_throughput(
    name: Optional[str] = None, *, unit_of_work: str = "task"
) -> Callable:
    """
    Decorator to measure function throughput (tasks per minute).

    Logs execution time, throughput, and issues WARNING if below threshold.
    Integrates with ThreadX Settings for configurable warning threshold.

    Parameters
    ----------
    name : str, optional
        Custom name for the measurement. If None, uses function name.
    unit_of_work : str, default "task"
        Description of the unit being measured (e.g., "indicator", "trade", "calculation").

    Returns
    -------
    callable
        Decorated function with throughput measurement.

    Examples
    --------
    >>> @measure_throughput()
    ... def process_trades(trades):
    ...     return [t * 2 for t in trades]

    >>> @measure_throughput("custom_indicator", unit_of_work="calculation")
    ... def compute_rsi(prices):
    ...     return prices.rolling(14).mean()

    Notes
    -----
    - Assumes the function's first argument or return value indicates task count
    - WARNING issued if throughput < MIN_TASKS_PER_MIN from Settings
    - Logs at INFO level for normal operation, WARNING for low throughput
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            func_name = name or func.__name__
            threshold = _get_throughput_threshold()

            # Estimate task count - try different strategies
            task_count = 1  # Default fallback

            # Strategy 1: Look for common parameter names
            if args:
                first_arg = args[0]
                if hasattr(first_arg, "__len__"):
                    try:
                        task_count = len(first_arg)
                    except (TypeError, AttributeError):
                        pass

            # Strategy 2: Check for explicit task_count parameter
            if "task_count" in kwargs:
                task_count = kwargs["task_count"]
            elif "n_tasks" in kwargs:
                task_count = kwargs["n_tasks"]

            logger.info(f"Starting {func_name} with {task_count} {unit_of_work}(s)")

            with Timer() as timer:
                result = func(*args, **kwargs)

            # Update task count from result if possible
            if hasattr(result, "__len__"):
                try:
                    result_count = len(result)
                    if result_count > 0:
                        task_count = result_count
                except (TypeError, AttributeError):
                    pass

            elapsed_sec = timer.elapsed_sec
            tasks_per_min = (
                (task_count * 60.0) / elapsed_sec if elapsed_sec > 0 else 0.0
            )

            # Log results
            log_msg = (
                f"{func_name} completed: {task_count} {unit_of_work}(s) "
                f"in {elapsed_sec:.3f}s ({tasks_per_min:.1f} {unit_of_work}s/min)"
            )

            if tasks_per_min < threshold:
                logger.warning(
                    f"{log_msg} - PERFORMANCE WARNING: Below threshold "
                    f"({threshold} {unit_of_work}s/min). Consider vectorization or batching."
                )
            else:
                logger.info(log_msg)

            return result

        return wrapper

    return decorator


def track_memory(name: Optional[str] = None) -> Callable:
    """
    Decorator to track memory consumption during function execution.

    Uses psutil to monitor peak and average memory usage. Provides graceful
    fallback if psutil is unavailable (logs INFO and continues).

    Parameters
    ----------
    name : str, optional
        Custom name for the measurement. If None, uses function name.

    Returns
    -------
    callable
        Decorated function with memory tracking.

    Examples
    --------
    >>> @track_memory()
    ... def process_large_dataset(data):
    ...     return data.copy()

    >>> @track_memory("indicator_computation")
    ... def compute_indicators(prices):
    ...     return expensive_calculation(prices)

    Notes
    -----
    - Requires psutil for actual memory tracking
    - Falls back gracefully if psutil unavailable (logs None values)
    - Memory values in MB for readability
    - Samples memory periodically during execution for peak detection
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            func_name = name or func.__name__

            if not PSUTIL_AVAILABLE:
                logger.info(
                    f"{func_name} memory tracking: psutil unavailable, skipping"
                )
                return func(*args, **kwargs)

            tracker = MemoryTracker()
            tracker.start()

            logger.info(f"Starting memory tracking for {func_name}")

            try:
                # Sample memory during execution (simple approach)
                result = func(*args, **kwargs)
                tracker.sample()

                stats = tracker.get_stats()

                if stats["peak_mb"] is not None:
                    log_msg = (
                        f"{func_name} memory usage: "
                        f"peak={stats['peak_mb']:.1f}MB, "
                        f"avg={stats['avg_mb']:.1f}MB"
                    )
                    logger.info(log_msg)
                else:
                    logger.info(f"{func_name} memory tracking: no data collected")

                return result

            except Exception as e:
                logger.warning(f"Memory tracking failed for {func_name}: {e}")
                return func(*args, **kwargs)

        return wrapper

    return decorator


def combined_measurement(
    name: Optional[str] = None,
    *,
    unit_of_work: str = "task",
    track_memory_usage: bool = True,
) -> Callable:
    """
    Combined decorator for throughput and memory measurement.

    Convenience decorator that applies both @measure_throughput and @track_memory.

    Parameters
    ----------
    name : str, optional
        Custom name for measurements.
    unit_of_work : str, default "task"
        Unit description for throughput measurement.
    track_memory_usage : bool, default True
        Whether to include memory tracking.

    Returns
    -------
    callable
        Decorated function with combined measurements.

    Examples
    --------
    >>> @combined_measurement("bulk_processing", unit_of_work="record")
    ... def process_records(records):
    ...     return [process_record(r) for r in records]
    """

    def decorator(func: Callable) -> Callable:
        # Apply decorators in reverse order (they're applied inside-out)
        decorated = measure_throughput(name, unit_of_work=unit_of_work)(func)
        if track_memory_usage:
            decorated = track_memory(name)(decorated)
        return decorated

    return decorator


@contextmanager
def performance_context(
    name: str,
    *,
    unit_of_work: str = "task",
    task_count: int = 1,
    track_memory_usage: bool = True,
):
    """
    Context manager for performance measurement.

    Alternative to decorators for measuring performance of code blocks.

    Parameters
    ----------
    name : str
        Name for the measurement.
    unit_of_work : str, default "task"
        Unit description.
    task_count : int, default 1
        Number of tasks being processed.
    track_memory_usage : bool, default True
        Whether to track memory usage.

    Yields
    ------
    PerformanceMetrics
        Metrics object that gets populated during execution.

    Examples
    --------
    >>> with performance_context("data_processing", task_count=1000) as perf:
    ...     # Process data
    ...     results = process_data(data)
    >>> print(f"Processed at {perf.tasks_per_min:.1f} tasks/min")
    """
    threshold = _get_throughput_threshold()

    metrics = PerformanceMetrics(
        elapsed_sec=0.0,
        tasks_completed=task_count,
        function_name=name,
        unit_of_work=unit_of_work,
    )

    memory_tracker = None
    if track_memory_usage and PSUTIL_AVAILABLE:
        memory_tracker = MemoryTracker()
        memory_tracker.start()

    logger.info(f"Starting {name} with {task_count} {unit_of_work}(s)")

    with Timer() as timer:
        try:
            yield metrics
        finally:
            metrics.elapsed_sec = timer.elapsed_sec

            if metrics.elapsed_sec > 0:
                metrics.tasks_per_min = (task_count * 60.0) / metrics.elapsed_sec
            else:
                metrics.tasks_per_min = 0.0

            # Collect memory stats
            if memory_tracker:
                memory_tracker.sample()
                stats = memory_tracker.get_stats()
                metrics.memory_peak_mb = stats["peak_mb"]
                metrics.memory_avg_mb = stats["avg_mb"]

            # Log results
            log_msg = (
                f"{name} completed: {task_count} {unit_of_work}(s) "
                f"in {metrics.elapsed_sec:.3f}s ({metrics.tasks_per_min:.1f} {unit_of_work}s/min)"
            )

            if memory_tracker and metrics.memory_peak_mb:
                log_msg += f", peak_memory={metrics.memory_peak_mb:.1f}MB"

            if metrics.tasks_per_min < threshold:
                logger.warning(
                    f"{log_msg} - PERFORMANCE WARNING: Below threshold "
                    f"({threshold} {unit_of_work}s/min). Consider vectorization or batching."
                )
            else:
                logger.info(log_msg)




----------------------------------------
Fichier: utils\xp.py
"""Backend-agnostic array helpers with lazy CuPy support."""

from __future__ import annotations

import importlib
import logging
import time
from contextlib import contextmanager
from typing import (
    Any,
    Callable,
    Iterable,
    Mapping,
    MutableMapping,
    Optional,
    Sequence,
    Tuple,
    Union,
)

import numpy as np

logger = logging.getLogger(__name__)

# Public placeholders updated at runtime.
cp: Optional[Any] = None
CUPY_AVAILABLE: bool = False

# Internal caches.
_CUPY_IMPORT_ERROR: Optional[BaseException] = None
_CUPY_MODULE: Optional[Any] = None
_XP_BACKEND: Any = np
_BACKEND_OVERRIDE: Optional[str] = None  # "numpy", "cupy" or None (auto)


def _get_device_manager() -> Optional[Any]:
    try:
        from threadx.utils.gpu import device_manager  # type: ignore

        return device_manager
    except Exception as exc:  # pragma: no cover - diagnostics only
        logger.debug("device_manager unavailable: %s", exc)
        return None


def _get_cupy() -> Optional[Any]:
    global _CUPY_MODULE, _CUPY_IMPORT_ERROR, CUPY_AVAILABLE, cp

    if _CUPY_MODULE is not None:
        return _CUPY_MODULE
    if _CUPY_IMPORT_ERROR is not None:
        return None

    try:
        module = importlib.import_module("cupy")
        _CUPY_MODULE = module
        cp = module
        CUPY_AVAILABLE = True
        logger.debug("CuPy successfully imported")
        return module
    except Exception as exc:  # pragma: no cover - import failure path is CI common
        _CUPY_IMPORT_ERROR = exc
        CUPY_AVAILABLE = False
        cp = None
        logger.debug("CuPy import failed: %s", exc)
        return None


def refresh_cupy_cache() -> None:
    """Reset cached CuPy module information."""
    global _CUPY_MODULE, _CUPY_IMPORT_ERROR, CUPY_AVAILABLE, cp

    _CUPY_MODULE = None
    _CUPY_IMPORT_ERROR = None
    CUPY_AVAILABLE = False
    cp = None


def _sync_cupy_state() -> Optional[Any]:
    module = _get_cupy()
    return module


def _set_backend(backend: Any) -> Any:
    global _XP_BACKEND, xp

    if backend is not _XP_BACKEND:
        _XP_BACKEND = backend
        logger.debug("Backend switched to %s", "cupy" if backend is cp else "numpy")
    xp = _XP_BACKEND
    return _XP_BACKEND


def gpu_available() -> bool:
    """Return True if GPU execution is possible with the current environment."""
    module = _sync_cupy_state()
    if module is None:
        return False

    manager = _get_device_manager()
    if manager is not None and hasattr(manager, "is_available"):
        try:
            available = bool(manager.is_available())
            logger.debug("device_manager reports GPU available: %s", available)
            return available
        except Exception as exc:  # pragma: no cover - defensive logging
            logger.debug("device_manager.is_available raised: %s", exc)

    try:
        count = module.cuda.runtime.getDeviceCount()
        logger.debug("Detected %s CUDA device(s)", count)
        return count > 0
    except Exception as exc:  # pragma: no cover - CUDA probing failure
        logger.debug("CuPy runtime detection failed: %s", exc)
        return False


def _select_backend(prefer_gpu: bool) -> Any:
    if _BACKEND_OVERRIDE == "numpy":
        return np
    if _BACKEND_OVERRIDE == "cupy":
        module = _sync_cupy_state()
        if module is not None and gpu_available():
            return module
        logger.debug("Requested CuPy backend but unavailable; falling back to NumPy")
        return np

    if prefer_gpu and gpu_available():
        module = _sync_cupy_state()
        if module is not None:
            return module

    return np


def get_xp(prefer_gpu: bool = True) -> Any:
    """Return the numerical backend module (NumPy or CuPy)."""
    backend = _select_backend(prefer_gpu)
    return _set_backend(backend)


# Public shorthand mirroring NumPy/CuPy API.
xp: Any = get_xp(prefer_gpu=False)


def is_gpu_backend() -> bool:
    return _XP_BACKEND is not np


def get_backend_name() -> str:
    backend = "cupy" if is_gpu_backend() and _sync_cupy_state() is not None else "numpy"
    return backend


def configure_backend(preferred: str) -> None:
    """Force a backend ("numpy", "cupy", or "auto")."""
    valid = {"numpy", "cupy", "auto"}
    if preferred not in valid:
        raise ValueError(
            f"Invalid backend '{preferred}'. Expected one of {sorted(valid)}"
        )

    global _BACKEND_OVERRIDE
    _BACKEND_OVERRIDE = None if preferred == "auto" else preferred
    get_xp(prefer_gpu=True)


def to_device(array: Any, dtype: Optional[Any] = None) -> Any:
    backend = get_xp()
    if backend is np:
        return np.array(array, dtype=dtype, copy=False)

    module = _sync_cupy_state()
    if module is None:
        raise RuntimeError("CuPy backend requested but unavailable")
    if hasattr(module, "asarray"):
        return module.asarray(array, dtype=dtype)
    return module.array(array, dtype=dtype)


def to_host(array: Any) -> np.ndarray:
    module = _sync_cupy_state()
    if module is None or isinstance(array, np.ndarray):
        return np.asarray(array)
    return module.asnumpy(array)


def asnumpy(array: Any) -> np.ndarray:
    return to_host(array)


def ascupy(array: Any, dtype: Optional[Any] = None) -> Any:
    module = _sync_cupy_state()
    if module is None:
        raise RuntimeError("CuPy is not available on this system")
    if isinstance(array, module.ndarray):
        return array if dtype is None else array.astype(dtype)
    return module.asarray(array, dtype=dtype)


def ensure_array_type(
    array: Any, dtype: Optional[Any] = None, *, xp_module: Any = None
) -> Any:
    backend = xp_module or get_xp()
    if backend is np:
        return np.asarray(array, dtype=dtype)

    module = _sync_cupy_state()
    if module is None:
        return np.asarray(array, dtype=dtype)
    return module.asarray(array, dtype=dtype)


def memory_pool_info() -> Mapping[str, Union[int, float, str]]:
    module = _sync_cupy_state()
    if module is None:
        return {"backend": "numpy", "used_bytes": 0, "total_bytes": 0}

    pool = module.get_default_memory_pool()
    pinned = module.cuda.get_default_pinned_memory_pool()
    info: MutableMapping[str, Union[int, float, str]] = {
        "backend": "cupy",
        "used_bytes": pool.used_bytes(),
        "total_bytes": pool.total_bytes(),
        "pinned_used": pinned.used_bytes(),
        "pinned_total": pinned.total_bytes(),
    }
    return info


def clear_memory_pool() -> None:
    module = _sync_cupy_state()
    if module is not None:
        module.get_default_memory_pool().free_all_blocks()
        module.cuda.get_default_pinned_memory_pool().free_all_blocks()


@contextmanager
def _gpu_timer(module: Any) -> Iterable[Tuple[Callable[[], float], Any]]:
    start = module.cuda.Event()
    end = module.cuda.Event()
    stream = module.cuda.get_current_stream()
    start.record(stream)
    yield (lambda: _elapsed_gpu_time(module, start, end, stream), stream)


def _elapsed_gpu_time(module: Any, start: Any, end: Any, stream: Any) -> float:
    end.record(stream)
    end.synchronize()
    return module.cuda.get_elapsed_time(start, end) / 1000.0


def benchmark_operation(
    func: Callable[..., Any], *args: Any, **kwargs: Any
) -> Tuple[Any, float]:
    """Execute *func* and return (result, elapsed_seconds)."""
    backend = get_xp()
    if backend is np:
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = time.perf_counter() - start
        return result, elapsed

    module = _sync_cupy_state()
    if module is None:
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = time.perf_counter() - start
        return result, elapsed

    with _gpu_timer(module) as (timer, stream):
        result = func(*args, **kwargs)
        elapsed = timer()
    stream.synchronize()
    return result, elapsed


def device_synchronize() -> None:
    module = _sync_cupy_state()
    if module is not None:
        module.cuda.get_current_stream().synchronize()


def get_array_info(array: Any) -> Mapping[str, Any]:
    module = _sync_cupy_state()
    backend = "numpy"
    device: Union[str, int] = "cpu"

    if module is not None and isinstance(array, getattr(module, "ndarray", ())):
        backend = "cupy"
        try:
            device = module.cuda.get_current_device().id
        except Exception:  # pragma: no cover - device query failure
            device = "gpu"
        shape = getattr(array, "shape", ())
        dtype = getattr(array, "dtype", "unknown")
        size = int(getattr(array, "size", module.asnumpy(array).size))
        memory_mb = float(getattr(array, "nbytes", module.asnumpy(array).nbytes)) / (
            1024**2
        )
        return {
            "backend": backend,
            "device": device,
            "shape": shape,
            "dtype": dtype,
            "size": size,
            "memory_mb": memory_mb,
        }

    arr = np.asarray(array)
    return {
        "backend": backend,
        "device": device,
        "shape": arr.shape,
        "dtype": arr.dtype,
        "size": int(arr.size),
        "memory_mb": float(arr.nbytes) / (1024**2),
    }


__all__ = [
    "np",
    "cp",
    "CUPY_AVAILABLE",
    "gpu_available",
    "get_xp",
    "xp",
    "is_gpu_backend",
    "get_backend_name",
    "configure_backend",
    "to_device",
    "to_host",
    "asnumpy",
    "ascupy",
    "ensure_array_type",
    "memory_pool_info",
    "clear_memory_pool",
    "benchmark_operation",
    "device_synchronize",
    "get_array_info",
    "refresh_cupy_cache",
]




----------------------------------------
Fichier: utils\__init__.py
#!/usr/bin/env python3
"""
ThreadX Utils Package - Enhanced with Phase 9
==============================================

Utilities and helper modules for ThreadX framework.

Available modules:
- log: Centralized logging utilities
- gpu: GPU utilities and device management (Phase 5)
- timing: Performance measurement and throughput tracking (Phase 9)
- cache: High-performance LRU/TTL caching infrastructure (Phase 9)
- xp: Device-agnostic computing (NumPy/CuPy abstraction) (Phase 9)

Usage:
    # Legacy utilities
    from threadx.utils.log import get_logger
    from threadx.utils.gpu import is_available, MultiGPUManager

    # Phase 9 utilities
    from threadx.utils.timing import measure_throughput, Timer
    from threadx.utils.cache import cached, LRUCache, TTLCache
    from threadx.utils import xp as xpmod
    xp = xpmod.xp
    gpu_available = xpmod.gpu_available
    to_device = xpmod.to_device
    to_host = xpmod.to_host
"""

from threadx.utils.log import get_logger

# Import common imports module (Phase 2 DRY refactoring)
from . import common_imports

# Import Phase 9 utilities with graceful fallback
try:
    # Core timing utilities
    from .timing import (
        Timer,
        PerformanceMetrics,
        measure_throughput,
        track_memory,
        combined_measurement,
        performance_context,
    )

    # Caching infrastructure
    from .cache import (
        LRUCache,
        TTLCache,
        CacheStats,
        CacheEvent,
        cached,
        lru_cache,
        ttl_cache,
        indicators_cache,
        generate_stable_key,
    )

    # Device-agnostic computing
    from .xp import (
        xp,
        gpu_available,

        to_device,
        to_host,
        device_synchronize,
        get_array_info,
        ensure_array_type,
        memory_pool_info,
        clear_memory_pool,
        asnumpy,
        ascupy,
        benchmark_operation,
    )

    PHASE_9_AVAILABLE = True

except ImportError as e:
    import logging

    logging.getLogger(__name__).warning(f"Phase 9 utilities not fully available: {e}")
    PHASE_9_AVAILABLE = False

__all__ = [
    "get_logger",
    "common_imports",  # Phase 2 DRY refactoring module
    # Phase 9 exports (if available)
    "Timer",
    "PerformanceMetrics",
    "measure_throughput",
    "track_memory",
    "combined_measurement",
    "performance_context",
    "LRUCache",
    "TTLCache",
    "CacheStats",
    "CacheEvent",
    "cached",
    "lru_cache",
    "ttl_cache",
    "indicators_cache",
    "generate_stable_key",
    "xp",
    "gpu_available",
    "get_gpu_devices",
    "to_device",
    "to_host",
    "device_synchronize",
    "get_array_info",
    "ensure_array_type",
    "memory_pool_info",
    "clear_memory_pool",
    "asnumpy",
    "ascupy",
    "benchmark_operation",
    "PHASE_9_AVAILABLE",
]

__version__ = "1.0.0"
__author__ = "ThreadX Team"




----------------------------------------
Fichier: visualization\backtest_charts.py
"""
ThreadX - Graphiques de Backtest
==================================

GÃ©nÃ©ration graphiques interactifs avec Plotly pour visualiser rÃ©sultats.

Features:
- Bougies japonaises (OHLCV)
- Marqueurs entrÃ©es/sorties (â–² vert / â–¼ rouge)
- Bandes de Bollinger overlay
- Courbe d'Ã©quitÃ©
- Indicateurs techniques (sous-graphique)

Usage:
    >>> from threadx.visualization.backtest_charts import generate_backtest_chart
    >>> generate_backtest_chart(
    ...     results_df=best_results,
    ...     ohlcv_data=ohlcv,
    ...     best_combo={'bb_window': 20, 'bb_num_std': 2.0},
    ...     symbol='BTCUSDC',
    ...     timeframe='1h',
    ...     output_path='backtest_BTCUSDC_1h.html'
    ... )
"""

from pathlib import Path
from typing import Dict, Optional, Union
import pandas as pd

from threadx.utils.log import get_logger

logger = get_logger(__name__)

# Imports optionnels
try:
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots

    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    logger.warning("Plotly non disponible. Installez avec: pip install plotly kaleido")


def generate_backtest_chart(
    results_df: pd.DataFrame,
    ohlcv_data: pd.DataFrame,
    best_combo: Dict[str, float],
    symbol: str,
    timeframe: str,
    output_path: Union[str, Path],
    show_browser: bool = False,
) -> Optional[Path]:
    """
    GÃ©nÃ¨re graphique interactif Plotly avec rÃ©sultats backtest.

    Args:
        results_df: DataFrame avec colonnes:
            - timestamp: Index temporel
            - position: 1 (long), -1 (short), 0 (flat)
            - equity: Valeur du portefeuille
            - entry_price: Prix d'entrÃ©e (NaN si pas d'entrÃ©e)
            - exit_price: Prix de sortie (NaN si pas de sortie)
        ohlcv_data: DataFrame OHLCV avec colonnes:
            - timestamp: Index
            - open, high, low, close, volume
        best_combo: ParamÃ¨tres optimaux (ex: {'bb_window': 20, 'bb_num_std': 2.0})
        symbol: Symbole (ex: 'BTCUSDC')
        timeframe: Intervalle temporel (ex: '1h', '4h')
        output_path: Chemin fichier HTML (ex: 'backtest_BTCUSDC_1h.html')
        show_browser: Si True, ouvre dans navigateur

    Returns:
        Path du fichier HTML gÃ©nÃ©rÃ© (ou None si erreur)

    Example:
        >>> chart_path = generate_backtest_chart(
        ...     results_df=best_results,
        ...     ohlcv_data=ohlcv,
        ...     best_combo={'bb_window': 20, 'bb_num_std': 2.0},
        ...     symbol='BTCUSDC',
        ...     timeframe='1h',
        ...     output_path='charts/backtest_BTCUSDC_1h.html'
        ... )
        >>> logger.info(f"Graphique gÃ©nÃ©rÃ©: {chart_path}")
    """
    if not PLOTLY_AVAILABLE:
        logger.error("Plotly non disponible. Impossible de gÃ©nÃ©rer graphique.")
        return None

    try:
        # Conversion path
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Merge OHLCV + results
        merged = pd.merge(
            ohlcv_data,
            results_df[["position", "equity", "entry_price", "exit_price"]],
            left_index=True,
            right_index=True,
            how="left",
        )

        # Calcul Bollinger Bands si paramÃ¨tres fournis
        bb_window = best_combo.get("bb_window", 20)
        bb_std = best_combo.get("bb_num_std", 2.0)

        if "close" in merged.columns:
            merged["bb_middle"] = merged["close"].rolling(window=int(bb_window)).mean()
            std = merged["close"].rolling(window=int(bb_window)).std()
            merged["bb_upper"] = merged["bb_middle"] + (bb_std * std)
            merged["bb_lower"] = merged["bb_middle"] - (bb_std * std)

        # CrÃ©ation figure avec 3 sous-graphiques
        fig = make_subplots(
            rows=3,
            cols=1,
            shared_xaxes=True,
            vertical_spacing=0.03,
            row_heights=[0.6, 0.2, 0.2],
            subplot_titles=(
                f"{symbol} {timeframe} - Prix & Signaux",
                "Courbe d'Ã‰quitÃ©",
                "Indicateurs Techniques",
            ),
        )

        # === ROW 1: CANDLESTICKS + BOLLINGER BANDS + SIGNAUX ===

        # Bougies japonaises
        fig.add_trace(
            go.Candlestick(
                x=merged.index,
                open=merged["open"],
                high=merged["high"],
                low=merged["low"],
                close=merged["close"],
                name="OHLC",
                increasing_line_color="#26A69A",
                decreasing_line_color="#EF5350",
            ),
            row=1,
            col=1,
        )

        # Bollinger Bands
        if "bb_upper" in merged.columns:
            # Bande supÃ©rieure
            fig.add_trace(
                go.Scatter(
                    x=merged.index,
                    y=merged["bb_upper"],
                    name="BB Sup",
                    line=dict(color="rgba(250, 128, 114, 0.5)", width=1, dash="dash"),
                    showlegend=True,
                ),
                row=1,
                col=1,
            )

            # Bande moyenne
            fig.add_trace(
                go.Scatter(
                    x=merged.index,
                    y=merged["bb_middle"],
                    name="BB Mid",
                    line=dict(color="rgba(135, 206, 250, 0.7)", width=1.5),
                    showlegend=True,
                ),
                row=1,
                col=1,
            )

            # Bande infÃ©rieure
            fig.add_trace(
                go.Scatter(
                    x=merged.index,
                    y=merged["bb_lower"],
                    name="BB Inf",
                    line=dict(color="rgba(250, 128, 114, 0.5)", width=1, dash="dash"),
                    fill="tonexty",
                    fillcolor="rgba(135, 206, 250, 0.1)",
                    showlegend=True,
                ),
                row=1,
                col=1,
            )

        # Marqueurs d'entrÃ©e (â–² vert)
        entries = merged[merged["entry_price"].notna()]
        if not entries.empty:
            fig.add_trace(
                go.Scatter(
                    x=entries.index,
                    y=entries["entry_price"],
                    mode="markers",
                    name="EntrÃ©e",
                    marker=dict(
                        symbol="triangle-up",
                        size=12,
                        color="#4CAF50",
                        line=dict(color="white", width=1.5),
                    ),
                    showlegend=True,
                ),
                row=1,
                col=1,
            )

        # Marqueurs de sortie (â–¼ rouge)
        exits = merged[merged["exit_price"].notna()]
        if not exits.empty:
            fig.add_trace(
                go.Scatter(
                    x=exits.index,
                    y=exits["exit_price"],
                    mode="markers",
                    name="Sortie",
                    marker=dict(
                        symbol="triangle-down",
                        size=12,
                        color="#F44336",
                        line=dict(color="white", width=1.5),
                    ),
                    showlegend=True,
                ),
                row=1,
                col=1,
            )

        # === ROW 2: COURBE D'Ã‰QUITÃ‰ ===

        fig.add_trace(
            go.Scatter(
                x=merged.index,
                y=merged["equity"],
                name="Ã‰quitÃ©",
                line=dict(color="#2196F3", width=2),
                fill="tozeroy",
                fillcolor="rgba(33, 150, 243, 0.1)",
                showlegend=True,
            ),
            row=2,
            col=1,
        )

        # Ligne de base (capital initial = premiÃ¨re valeur Ã©quitÃ©)
        if "equity" in merged.columns and not merged["equity"].isna().all():
            initial_equity = merged["equity"].iloc[0]
            fig.add_hline(
                y=initial_equity,
                line_dash="dot",
                line_color="gray",
                opacity=0.5,
                row=2,
                col=1,
            )

        # === ROW 3: POSITION (LONG/SHORT/FLAT) ===

        # Convertir position en zone colorÃ©e
        position_colors = {
            1: "#4CAF50",  # Long = vert
            -1: "#F44336",  # Short = rouge
            0: "#9E9E9E",  # Flat = gris
        }

        merged["position_color"] = (
            merged["position"].map(position_colors).fillna("#9E9E9E")
        )

        fig.add_trace(
            go.Bar(
                x=merged.index,
                y=merged["position"].abs(),
                name="Position",
                marker=dict(color=merged["position_color"]),
                showlegend=True,
            ),
            row=3,
            col=1,
        )

        # === MISE EN FORME ===

        # Titre global
        params_str = ", ".join([f"{k}={v}" for k, v in best_combo.items()])
        title = (
            f"<b>{symbol} {timeframe}</b> - Backtest OptimisÃ©<br>"
            f"<sub>ParamÃ¨tres: {params_str}</sub>"
        )

        fig.update_layout(
            title=dict(text=title, x=0.5, xanchor="center"),
            xaxis_rangeslider_visible=False,
            height=900,
            template="plotly_dark",
            hovermode="x unified",
            showlegend=True,
            legend=dict(
                orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1
            ),
        )

        # Axes
        fig.update_xaxes(title_text="Date", row=3, col=1)
        fig.update_yaxes(title_text="Prix", row=1, col=1)
        fig.update_yaxes(title_text="Ã‰quitÃ© ($)", row=2, col=1)
        fig.update_yaxes(title_text="Position", row=3, col=1)

        # Sauvegarde HTML
        fig.write_html(
            str(output_path), config={"displayModeBar": True, "displaylogo": False}
        )

        logger.info(f"âœ… Graphique gÃ©nÃ©rÃ©: {output_path}")

        # Ouvrir dans navigateur si demandÃ©
        if show_browser:
            import webbrowser

            webbrowser.open(str(output_path.absolute()))

        return output_path

    except Exception as e:
        logger.error(f"âŒ Erreur gÃ©nÃ©ration graphique: {e}")
        return None


def generate_multi_timeframe_chart(
    results_dict: Dict[str, pd.DataFrame],
    ohlcv_dict: Dict[str, pd.DataFrame],
    best_combos: Dict[str, Dict[str, float]],
    symbol: str,
    output_path: Union[str, Path],
    show_browser: bool = False,
) -> Optional[Path]:
    """
    GÃ©nÃ¨re graphique multi-timeframes (1h + 4h + 1d par exemple).

    Args:
        results_dict: Dict {timeframe: results_df}
        ohlcv_dict: Dict {timeframe: ohlcv_df}
        best_combos: Dict {timeframe: best_params}
        symbol: Symbole (ex: 'BTCUSDC')
        output_path: Chemin fichier HTML
        show_browser: Ouvrir dans navigateur

    Returns:
        Path du fichier HTML

    Example:
        >>> generate_multi_timeframe_chart(
        ...     results_dict={'1h': results_1h, '4h': results_4h},
        ...     ohlcv_dict={'1h': ohlcv_1h, '4h': ohlcv_4h},
        ...     best_combos={'1h': {...}, '4h': {...}},
        ...     symbol='BTCUSDC',
        ...     output_path='multi_tf_BTCUSDC.html'
        ... )
    """
    if not PLOTLY_AVAILABLE:
        logger.error("Plotly non disponible.")
        return None

    try:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        n_timeframes = len(results_dict)

        # CrÃ©ation figure avec N sous-graphiques (un par timeframe)
        fig = make_subplots(
            rows=n_timeframes,
            cols=1,
            shared_xaxes=False,
            vertical_spacing=0.05,
            subplot_titles=[f"{symbol} {tf}" for tf in results_dict.keys()],
        )

        # Pour chaque timeframe
        for i, (tf, results_df) in enumerate(results_dict.items(), start=1):
            ohlcv = ohlcv_dict[tf]
            best_combo = best_combos[tf]

            # Merge donnÃ©es
            merged = pd.merge(
                ohlcv,
                results_df[["equity", "entry_price", "exit_price"]],
                left_index=True,
                right_index=True,
                how="left",
            )

            # Candlesticks
            fig.add_trace(
                go.Candlestick(
                    x=merged.index,
                    open=merged["open"],
                    high=merged["high"],
                    low=merged["low"],
                    close=merged["close"],
                    name=f"OHLC {tf}",
                    increasing_line_color="#26A69A",
                    decreasing_line_color="#EF5350",
                ),
                row=i,
                col=1,
            )

            # EntrÃ©es
            entries = merged[merged["entry_price"].notna()]
            if not entries.empty:
                fig.add_trace(
                    go.Scatter(
                        x=entries.index,
                        y=entries["entry_price"],
                        mode="markers",
                        name=f"EntrÃ©e {tf}",
                        marker=dict(symbol="triangle-up", size=10, color="#4CAF50"),
                        showlegend=False,
                    ),
                    row=i,
                    col=1,
                )

            # Sorties
            exits = merged[merged["exit_price"].notna()]
            if not exits.empty:
                fig.add_trace(
                    go.Scatter(
                        x=exits.index,
                        y=exits["exit_price"],
                        mode="markers",
                        name=f"Sortie {tf}",
                        marker=dict(symbol="triangle-down", size=10, color="#F44336"),
                        showlegend=False,
                    ),
                    row=i,
                    col=1,
                )

        # Mise en forme
        fig.update_layout(
            title=f"<b>{symbol}</b> - Multi-Timeframes Backtest",
            height=400 * n_timeframes,
            template="plotly_dark",
            xaxis_rangeslider_visible=False,
            showlegend=False,
        )

        # Sauvegarde
        fig.write_html(str(output_path))
        logger.info(f"âœ… Graphique multi-TF gÃ©nÃ©rÃ©: {output_path}")

        if show_browser:
            import webbrowser

            webbrowser.open(str(output_path.absolute()))

        return output_path

    except Exception as e:
        logger.error(f"âŒ Erreur graphique multi-TF: {e}")
        return None


__all__ = [
    "generate_backtest_chart",
    "generate_multi_timeframe_chart",
]

----------------------------------------
Fichier: visualization\__init__.py
"""
ThreadX Visualization Module
=============================

GÃ©nÃ©ration de graphiques interactifs pour visualiser rÃ©sultats backtests.
"""

from .backtest_charts import (
    generate_backtest_chart,
    generate_multi_timeframe_chart,
)

__all__ = [
    "generate_backtest_chart",
    "generate_multi_timeframe_chart",
]




========================================
FICHIER: LE_CODE_analyse_tierce.txt
========================================
Voici les axes dâ€™optimisation les plus rentables pour ton systÃ¨me dâ€™optimisation et backtest (sweep + Monte-Carlo). Les leviers sont classÃ©s du plus critique au plus secondaire. Les rÃ©fÃ©rences pointent vers ce que jâ€™ai observÃ© dans tes fichiers.  

---

## 1. Boucle critique Numba (bottleneck 35.4 %)

Contexte

* `_backtest_loop_numba()` dans `bb_atr.py` est identifiÃ© comme le point chaud principal Ã  35.4 % du temps CPU. Il est dÃ©jÃ  dÃ©corÃ© avec `@njit(fastmath=True, cache=True, boundscheck=False, nogil=True)`. 
* Cette boucle est appelÃ©e pour chaque scÃ©nario dans les workers Sweep et Monte-Carlo. Donc tout gain ici se rÃ©percute partout.

Optimisations Ã  appliquer en prioritÃ©

1. Retirer toute reconstruction dâ€™objets Python aprÃ¨s coup dans la section "reconstruction des trades" (lignes ~843-900). Cette phase est listÃ©e comme "rÃ©gression" donc elle a probablement rÃ©introduit du Python dans le hot path. 

   * Objectif: ne pas construire des listes de trades dans la fonction numba.
   * Faire deux passes:

     * Passe 1 (Numba pur): calculer uniquement les tableaux numpy (entry_idx, exit_idx, pnl, side, etc.).
     * Passe 2 (post-process Python / pandas): reconstruire le DataFrame `trades_df` Ã  partir de ces tableaux.
   * BÃ©nÃ©fice: Numba reste 100 % nopython. Ã‰vite allocations Python dans la zone chaude.

2. Forcer les structures dâ€™entrÃ©e de `_backtest_loop_numba()` Ã  Ãªtre des `np.ndarray` contigus en `float64` / `int64`, dÃ©jÃ  alignÃ©s, sans Series pandas.

   * Dans `BacktestEngine._simulate_trades()` tu fais beaucoup de logique Python par boucle (for i, timestamp, close_price...). 
   * Cette logique doit migrer dans la boucle Numba (stop loss, slippage, fees, PnL). Actuellement tu simules les trades cÃ´tÃ© Python pur. 
   * Objectif: une seule boucle Numba gÃ¨re lâ€™ouverture/fermeture de position et renvoie des arrays prÃªts Ã  transformer en DataFrame.

3. Ajouter `parallel=True` au jit si la logique est indÃ©pendante par barre. Exemple: calcul du PnL incrÃ©mental ou du mark-to-market peut se vectoriser indÃ©pendamment des Ã©tats de position.

   * Attention: le suivi dâ€™une position (stateful) nâ€™est pas parallÃ©lisable naÃ¯vement.
   * StratÃ©gie mixte:

     * 1 boucle sÃ©quentielle pour Ã©tats (position, stop, etc.).
     * 1 post-traitement vectorisÃ© Numba parallel pour dÃ©rivÃ©s simples (equity curve cumulÃ©, pct_change).

4. Supprimer toute allocation dynamique dans la boucle (append, list +=).

   * PrÃ©-allouer la taille max (len(prix)) puis tronquer aprÃ¨s exÃ©cution.
   * Numba dÃ©teste lâ€™append de listes Python dans nopython mode.

Impact attendu

* -20 % Ã  -30 % de temps total par scÃ©nario si tu retires le Python post-trade de la zone jit et que tu restes 100 % nopython sans objet dynamique.
* Ã‡a scale directement avec le nombre de combinaisons.

---

## 2. RÃ©duction du coÃ»t Python par scÃ©nario

Contexte

* `OptimizationEngine.run_sweep()` / `run_monte_carlo()` gÃ©nÃ¨rent des combinaisons puis dÃ©lÃ¨guent Ã  `_execute_combinations()` qui lance les backtests en parallÃ¨le via `ProcessPoolExecutor`. 
* Dans chaque worker, `BacktestEngine.run()` orchestre: validation, gÃ©nÃ©ration signaux, simulation trades, calcul equity, construction metadata. 

ProblÃ¨me

* Chaque scÃ©nario refait tout: validation, logs dÃ©taillÃ©s, construction dâ€™un gros dict meta, DataFrame trades complet, etc. MÃªme si on ne garde Ã  la fin que quelques mÃ©triques dâ€™intÃ©rÃªt.

Optimisations

1. Mode "fast_eval" pour lâ€™optimisation.

   * Nouvelle voie interne genre `BacktestEngine.run_fast(...)`.
   * Cette voie saute:

     * les logs debug par barre
     * la validation lourde (`_validate_inputs`) aprÃ¨s la premiÃ¨re fois (les donnÃ©es ne changent pas entre scÃ©narios)
     * la construction complÃ¨te de `RunResult.meta` (garde uniquement ce quâ€™il faut pour ranking: sharpe, CAGR, max_drawdown, maxDD duration). 
   * BÃ©nÃ©fice: beaucoup moins de sÃ©rialisation inter-processus et moins de RAM.

2. Ne renvoyer au parent que des scalaires ou des petits arrays.

   * Actuellement tu retournes `RunResult` avec `equity`, `returns`, `trades` complets. 
   * Pour lâ€™Ã©tape sweep/mc il suffit de sharpe, max_drawdown, total_return.
   * Donc: dans le worker, compresser tout de suite en un dict light `{combo_id, sharpe, return_pct, dd, winrate}` et renvoyer uniquement Ã§a.
   * Tu peux garder lâ€™option "keep_full_result=True" seulement pour les N meilleurs scÃ©narios Ã  la fin.

3. Mutualiser les indicateurs.

   * `IndicatorBank.ensure(...)` est dÃ©jÃ  optimisÃ© et a un cache avec 100 % hit rate. 
   * Point critique: aujourdâ€™hui chaque process enfant peut avoir son propre cache.
   * AmÃ©lioration:

     * Calculer tous les indicateurs une seule fois dans le parent (Bollinger, ATR, etc.).
     * Les sÃ©rialiser en mÃ©moire partagÃ©e (multiprocessing shared memory / numpy memmap en read-only) et ne pas recalculer par worker.
   * Objectif: aucun recalcul indicateur par combinaison.
   * RÃ©sultat: baisse CPU avant mÃªme de lancer la stratÃ©gie.

---

## 3. Scheduling parallÃ¨le et workers

Contexte

* Le moteur dâ€™optimisation calcule automatiquement le nombre optimal de workers via `_calculate_optimal_workers()` et utilise `ProcessPoolExecutor`. 
* Les tests Grid montent Ã  `max_workers=30`. 
* Chaque task Ã©crit des checkpoints Parquet en mode append avec un verrou fichier thread-safe Windows/Linux. 

AmÃ©liorations

1. SÃ©parer "compute pool" et "I/O writer".

   * ProblÃ¨me classique: si chaque worker fait un `append` Parquet + lock disque, tu crÃ©es un goulot I/O, surtout sur SSD NVMe partagÃ©.
   * Solution:

     * Les workers renvoient leurs rÃ©sultats en mÃ©moire vers un seul thread/worker I/O dÃ©diÃ© qui fait lâ€™append Parquet en batch toutes les N combinaisons.
   * Gain: moins de contention lock, moins de flush disque.

2. Chunking adaptatif des combinaisons.

   * Actuellement `_execute_combinations()` semble traiter chaque combinaison individuellement.
   * Optimisation: grouper par paquets de K scÃ©narios (par ex 32).
   * Chaque process Ã©value un bloc de K scÃ©narios en sÃ©quence interne sans repasser par le pool. RÃ©duction du coÃ»t ProcessPoolExecutor.submit()/future/result.
   * Objectif: rÃ©duire lâ€™overhead de scheduling Python quand le temps par scÃ©nario devient court aprÃ¨s optimisation Numba.

3. AffinitÃ© CPU / NUMA.

   * Pour Windows 11 Pro sur une machine avec beaucoup de cÅ“urs logiques, fixer lâ€™affinitÃ© des workers lourds CPU rÃ©duit la pollution de cache L3.
   * PowerShell: `Start-Process -NoNewWindow -FilePath python.exe -ArgumentList ... -ProcessorAffinity ...`
   * IntÃ©grable via `psutil.Process().cpu_affinity([...])` cÃ´tÃ© worker init.

4. Warmup Numba centralisÃ©.

   * Tu as `_warmup_numba_jit()` dans `OptimizationEngine`. 
   * Action: exÃ©cuter le warmup une seule fois par worker au dÃ©marrage (initializer du ProcessPoolExecutor), pas Ã  chaque scÃ©nario.
   * Sinon tu perds le bÃ©nÃ©fice du cache JIT au dÃ©but.

---

## 4. GPU et multi-GPU

Contexte

* `BacktestEngine` gÃ¨re auto: dÃ©tection GPU, multi-GPU, rÃ©partition 75 % / 25 % via `MultiGPUManager`. 
* Le moteur bascule sur CPU si pas dispo. Il logge le backend choisi. 

ProblÃ¨mes potentiels

* Beaucoup de logique de trading reste en Python pur (`_simulate_trades`) et pandas. 
* Tant que Ã§a reste du Python sÃ©quentiel, le GPU nâ€™apporte rien.

Optimisations

1. Porter le cÅ“ur de `_simulate_trades()` sur GPU via CuPy ou via Numba CUDA si tu restes maison.

   * Les inputs nÃ©cessaires sont dÃ©jÃ  simples: arrays close, timestamps indexÃ©s, params (leverage, stop, slippage). 
   * Le calcul de PnL et des sorties de position est quasiment mÃ©canique.
   * Un kernel CUDA dÃ©diÃ© peut simuler toutes les barres dâ€™un scÃ©nario en mÃ©moire GPU.
   * Attention: logique stateful. Tu peux traiter chaque scÃ©nario sur un GPU diffÃ©rent plutÃ´t que vectoriser intra-scenario.

2. Vraie distribution multi-GPU au niveau scÃ©nario.

   * Aujourdâ€™hui tu annonces "balance 75/25" mais la simulation semble rester mono-thread Python par scÃ©nario. 
   * AmÃ©lioration:

     * Associer chaque worker process Ã  un GPU dÃ©diÃ© (env CUDA_VISIBLE_DEVICES ou device_id passÃ© Ã  Numba/CuPy).
     * Le scheduler `_execute_combinations()` choisit le pool GPU0 vs GPU1 en fonction de la balance.
   * Tu Ã©vites la surcharge dâ€™un seul GPU pendant que lâ€™autre attend.

3. Ã‰viter les aller-retour CPUâ†”GPU pour des petites sÃ©ries.

   * Tant que tu redescends les arrays GPU vers pandas pour chaque bar, tu perds le gain.
   * Le pattern correct:

     * tout calcul scÃ©nario reste GPU du dÃ©but Ã  la fin
     * tu ne recopies sur CPU que les 5 mÃ©triques finales (Sharpe, max DD, etc.).

---

## 5. Calcul des mÃ©triques de performance

Contexte

* Tu construis `RunResult`, puis tu calcules equity, returns, puis plus tard tu calcules Sharpe, etc., y compris dans la fonction `backtest_func` de `run_backtest_with_validation()`. 

AmÃ©liorations

1. Vectoriser equity et drawdown.

   * `_calculate_equity_returns()` gÃ©nÃ¨re lâ€™equity curve en rÃ©Ã©crivant toute la sÃ©rie Ã  chaque sortie de trade (boucle Python). 
   * Optimisation:

     * Construire un array `equity` en float64 taille N, initialisÃ© au capital initial.
     * Pour chaque trade, appliquer `cumulative_pnl[t_exit_index:] += pnl`.
     * Cette opÃ©ration est un prefix-sum inversÃ© et peut Ãªtre faite en pur NumPy avec slicing. Plus de boucle Python sur chaque tick.

2. Calcul Sharpe, MaxDD, WinRate directement dans le worker avant de renvoyer le rÃ©sultat.

   * DÃ©jÃ  prÃ©sent dans `backtest_func` pour la validation walk-forward. 
   * RÃ©utiliser la mÃªme logique pour le sweep principal.
   * Avantage: pas besoin de conserver toute la courbe dans les checkpoints Parquet. Ã‰crire juste les mÃ©triques agrÃ©gÃ©es.

---

## 6. I/O donnÃ©es marchÃ©

Contexte

* Les donnÃ©es OHLCV sont chargÃ©es depuis des fichiers `.parquet`, `.feather`, `.csv`, `.json`. Recherche auto dans plusieurs dossiers. `load_ohlcv()` applique normalisation, parse datetime, tri, etc. 

Optimisations

1. Normalisation en amont et stockage en format colonne binaire optimal une seule fois.

   * PrÃ©-normaliser et enregistrer les sÃ©ries OHLCV dÃ©jÃ  propres en Parquet columnar avec index datetime UTC strict.
   * Objectif: ne plus faire la normalisation conditionnelle Ã  chaque chargement (`normalize_ohlcv`, fallback datetime parsing). 
   * Effet: tu gagnes CPU et Ã©vites la crÃ©ation dâ€™objets intermÃ©diaires pandas Ã  chaque run.

2. MÃ©moire partagÃ©e en read-only.

   * Charger une fois `df_1m` en mÃ©moire partagÃ©e (pyarrow plasma style ou multiprocessing shared memory pour les colonnes numpy).
   * Tous les workers lisent la mÃªme vue.
   * Plus de re-load disque dans chaque process.

3. PrÃ©calcul des indicateurs et cache disque.

   * Calculer Bollinger, ATR, etc. une fois pour la paire/timeframe.
   * Sauvegarder ces colonnes sur disque dans un Parquet `features_<symbol>_<timeframe>.parquet`.
   * Le sweep lit dÃ©jÃ  les features et ne fait plus tourner `IndicatorBank.ensure(...)` sauf pour vÃ©rif. 

---

## 7. SurcoÃ»t logging

Contexte

* `BacktestEngine.run()` logge beaucoup (debug par Ã©tape, dÃ©tails trade par trade, PnL unitaire, etc.). 

Optimisation

* Pendant un sweep massif tu nâ€™as pas besoin de logs verbeux pour chaque scÃ©nario.
* Ajouter un flag interne `silent=True` qui:

  * dÃ©sactive `self.logger.debug(...)` dans `_generate_trading_signals()`, `_simulate_trades()`, `_calculate_equity_returns()`. 
  * garde seulement un rÃ©sumÃ© pÃ©riodique cÃ´tÃ© parent toutes les X combinaisons.
* Le logging Python sous forte contention multiprocess peut coÃ»ter plusieurs % CPU, et surtout il ralentit sÃ©vÃ¨rement sur Windows en I/O disque/console.

---

## 8. Validation walk-forward

Contexte

* `run_backtest_with_validation()` dÃ©coupe les donnÃ©es en fenÃªtres dâ€™entraÃ®nement / test, relance `self.run()` pour chaque split, et calcule un ratio dâ€™overfitting. 

Optimisation

1. Mutualiser les splits

   * Les splits temporels rÃ©utilisent la mÃªme sÃ©rie complÃ¨te.
   * PlutÃ´t que recalculer indicateurs pour chaque split, dÃ©couper les indicateurs dÃ©jÃ  calculÃ©s globalement.
   * Tu notes toi-mÃªme un TODO "recalculer indicateurs par split pour robustesse totale". 
   * Solution pratique:

     * Calculer indicateurs une fois sur tout lâ€™horizon.
     * Pour chaque split train/test, tu fournis une vue slicÃ©e de ces arrays.
   * Tu gagnes beaucoup pour les validations walk-forward sur plusieurs fenÃªtres.

2. Paralleliser les fenÃªtres de validation sur plusieurs workers GPU/CPU.

   * Chaque fenÃªtre est indÃ©pendante.
   * Identique au sweep: appliquer la logique de chunking + scheduler GPU mentionnÃ©e plus haut.

---

## 9. RÃ©sumÃ© actionnable

Ordre dâ€™attaque recommandÃ©

1. Refactor du noyau Numba pour que toute la simulation de trade soit dans un jit nopython pur sans objets Python.
2. Mode "fast_eval" dans le moteur dâ€™optimisation pour renvoyer uniquement les mÃ©triques utiles et pas les DataFrames complets.
3. PrÃ©-calcul et partage immuable des donnÃ©es dâ€™entrÃ©e (OHLCV normalisÃ© + indicateurs) pour tous les workers afin dâ€™Ã©viter recompute et re-load.
4. Scheduler amÃ©liorÃ©: chunker les combinaisons, un seul writer Parquet, warmup Numba par worker.
5. Distribution multi-GPU par scÃ©nario au lieu de juste "GPU on/off" dans le mÃªme process.
6. RÃ©duction du logging en masse.

Ce pipeline rÃ©duit trois choses critiques:

* CPU Python inutile par scÃ©nario
* Aller-retour disque inutile
* Overhead dâ€™ordonnancement multiprocess

Ces trois points sont les vrais goulots aujourdâ€™hui vu la structure actuelle.




========================================
FICHIER: LE_CODE_analyse_tierce_2.txt
========================================
AperÃ§us ClÃ©s sur Votre Projet ThreadX
Votre framework ThreadX semble Ãªtre un outil complet basÃ© sur Python pour le backtesting de stratÃ©gies de trading, avec un fort accent sur l'optimisation des performances, la validation des donnÃ©es et la modularitÃ©. D'aprÃ¨s la structure du dÃ©pÃ´t et le code, il est bien organisÃ© pour un projet dÃ©veloppÃ© en solo, montrant une conception rÃ©flÃ©chie dans des domaines comme l'intÃ©gration GPU et les mesures anti-overfitting. Cependant, certaines zones comme les implÃ©mentations GPU incomplÃ¨tes et une gestion limitÃ©e des donnÃ©es rÃ©elles pourraient limiter sa prÃ©paration pour une utilisation en production.
Points ClÃ©s Principaux :

Forces : Architecture solide avec une sÃ©paration claire des prÃ©occupations, des tests Ã©tendus (plus de 30 fichiers de tests), et des benchmarks de performance qui priorisent la reproductibilitÃ© et l'efficacitÃ© â€“ des recherches suggÃ¨rent que de telles pratiques rÃ©duisent significativement les bugs dans les outils de finance quantitative.
Faiblesses : Des parties du code semblent simulÃ©es ou placeholders (par exemple, benchmarks GPU), et il y a une dÃ©pendance aux donnÃ©es synthÃ©tiques, ce qui pourrait ne pas capturer les complexitÃ©s rÃ©elles du marchÃ© comme le slippage ou les changements de rÃ©gime.
Potentiel d'AmÃ©lioration : AmÃ©liorer l'intÃ©gration de donnÃ©es rÃ©elles et complÃ©ter les fonctionnalitÃ©s GPU pourrait le rendre plus robuste ; il semble probable que cela pourrait Ã©voluer en un atout prÃ©cieux pour le trading algorithmique si poli davantage.
Chemin vers la MonÃ©tisation : Bien que pas immÃ©diatement "prÃªt pour la production", le framework a du potentiel pour un usage personnel ou open-source, mais une rentabilitÃ© rÃ©elle en trading dÃ©pend de validations en live â€“ les preuves penchent vers un optimisme prudent plutÃ´t que des richesses garanties.

Ce Qui Fonctionne Bien
La structure du projet se distingue, avec des rÃ©pertoires logiques (par exemple, src/threadx pour les modules principaux, tests pour les tests unitaires, benchmarks pour l'Ã©valuation des performances). Cette modularitÃ© facilite la maintenance et la scalabilitÃ©. Des fonctionnalitÃ©s clÃ©s comme la gÃ©nÃ©ration de scÃ©narios Monte Carlo et le pruning Pareto dans l'optimisation sont implÃ©mentÃ©es avec des simulations rÃ©alistes, incorporant du dÃ©terminisme via des graines pour des rÃ©sultats fiables. L'interface CLI (par exemple, cli/main.py) ajoute de l'utilisabilitÃ©, la rendant accessible pour des workflows en ligne de commande. Une logique de validation Ã©tendue, telle que les vÃ©rifications d'intÃ©gritÃ© temporelle et la dÃ©tection de biais look-ahead, s'aligne sur les meilleures pratiques en backtesting pour Ã©viter les piÃ¨ges courants.
Zones d'AmÃ©lioration
Bien que la base soit solide, certaines implÃ©mentations sont incomplÃ¨tes â€“ par exemple, les fichiers liÃ©s au GPU comme gpu/device_manager.py suggÃ¨rent des capacitÃ©s avancÃ©es mais manquent d'exÃ©cution complÃ¨te dans les benchmarks. La dÃ©pendance aux donnÃ©es simulÃ©es (par exemple, dans simulate_backtest_results) est pratique pour les tests mais risque l'overfitting lorsqu'appliquÃ©e aux marchÃ©s rÃ©els. La documentation pourrait Ãªtre Ã©tendue au-delÃ  des commentaires au niveau des fichiers, peut-Ãªtre avec un README complet expliquant la configuration et l'utilisation. La gestion des erreurs est prÃ©sente mais pourrait Ãªtre plus complÃ¨te pour des scÃ©narios de production, tels que les Ã©checs d'API lors de la rÃ©cupÃ©ration de donnÃ©es.
Suggestions d'AmÃ©lioration
Priorisez l'intÃ©gration de sources de donnÃ©es historiques rÃ©elles (par exemple, via des bibliothÃ¨ques comme yfinance ou ccxt) pour tester les stratÃ©gies de maniÃ¨re authentique. ComplÃ©tez l'accÃ©lÃ©ration GPU en utilisant des frameworks comme CuPy ou PyTorch pour les calculs d'indicateurs, et exÃ©cutez de vÃ©ritables benchmarks pour valider les revendications de speedup. Envisagez d'ajouter des pipelines CI/CD pour des tests automatisÃ©s. Pour la monÃ©tisation, ouvrez le dÃ©pÃ´t en open-source pour attirer des contributeurs, ou packagez-le comme un module PyPI â€“ cela pourrait le positionner comme un outil de niche dans la communautÃ© quant.
Perspectives sur la ViabilitÃ© Commerciale
Il semble probable qu'avec des raffinements, ThreadX pourrait supporter des stratÃ©gies de trading rentables, particuliÃ¨rement dans des marchÃ©s volatils comme la crypto. Des outils open-source similaires (par exemple, Backtrader ou Zipline) ont permis Ã  des utilisateurs de dÃ©velopper des bots profitables, mais les taux de succÃ¨s sont bas â€“ des Ã©tudes indiquent que seulement 10-20 % des traders algo retail profitent de maniÃ¨re consistente en raison de l'overfitting et des coÃ»ts de transaction. Vos fonctionnalitÃ©s de validation (tests walk-forward dans backtest/validation.py) aident Ã  attÃ©nuer cela, mais un dÃ©ploiement en live nÃ©cessite d'abord du paper trading. Si raffinÃ©, vous pourriez monÃ©tiser via des sponsorings GitHub, une version payante avec des stratÃ©gies premium, ou des services de freelancing en quant. Cependant, ce n'est pas un outil "devenir riche rapidement" ; des preuves de forums de trading suggÃ¨rent qu'un effort soutenu sur 1-2 ans est nÃ©cessaire pour la viabilitÃ©, avec des risques de perte de capital. Priorisez une utilisation Ã©thique et backtestez sur des datasets diversifiÃ©s pour bÃ¢tir la confiance.
En rÃ©sumÃ©, c'est un effort solo impressionnant avec une vision claire pour un framework de trading performant. Avec des amÃ©liorations ciblÃ©es, il pourrait devenir un projet remarquable dans l'espace quant.

ThreadX_big est un dÃ©pÃ´t centrÃ© sur Python axÃ© sur la construction d'un framework de backtesting et d'optimisation pour des stratÃ©gies de trading, nommÃ© "ThreadX". D'aprÃ¨s l'arborescence des fichiers, il englobe une large gamme de composants, incluant des benchmarks, des scripts pour la gestion des donnÃ©es, le code source principal sous src/threadx, des tests et des outils. Le projet semble conÃ§u pour les passionnÃ©s de finance quantitative, en mettant l'accent sur les performances via une accÃ©lÃ©ration GPU potentielle, une validation rigoureuse des donnÃ©es et des techniques d'optimisation comme les simulations Monte Carlo et le pruning Pareto. Aucun contenu README explicite n'est dÃ©taillÃ©, mais la structure implique une configuration impliquant Python 3.x avec des dÃ©pendances comme numpy, pandas et Ã©ventuellement torch pour des Ã©lÃ©ments ML/GPU â€“ l'installation impliquerait probablement de cloner le dÃ©pÃ´t et d'exÃ©cuter pip install -r requirements.txt si prÃ©sent, bien que non spÃ©cifiÃ©.
La racine du dÃ©pÃ´t contient des rÃ©pertoires comme benchmarks, examples, scripts, src, tests et tools, totalisant 141 fichiers comme listÃ©s. Cela indique un projet mature avec une sÃ©paration des prÃ©occupations : des scripts opÃ©rationnels pour l'import/validation des donnÃ©es, une logique principale dans des sous-modules (par exemple, backtest/, indicators/, optimization/), et une couverture de tests Ã©tendue. Par exemple, benchmarks/run_backtests.py dÃ©montre des tests de performance avec des rÃ©sultats simulÃ©s, ciblant des KPI comme un speedup de 3x et un taux de hit cache de 80 %, ce qui s'aligne avec des objectifs de calcul haute performance dans les simulations de trading.
En creusant plus profondÃ©ment dans les positifs, le code exhibe des pratiques professionnelles. Dans benchmarks/run_backtests.py, des fonctions comme simulate_backtest_results utilisent numpy pour une gÃ©nÃ©ration alÃ©atoire efficace avec des corrÃ©lations (par exemple, ratio Sharpe dÃ©rivÃ© du rendement/volatilitÃ©), assurant des mÃ©triques rÃ©alistes. Le dÃ©terminisme est imposÃ© via set_global_seed, crucial pour des backtests reproductibles. Le module d'optimisation (optimization/scenarios.py, pruning.py) gÃ¨re intelligemment de grands espaces de paramÃ¨tres, avec generate_monte_carlo utilisant un Ã©chantillonnage Sobol pour Ã©viter les problÃ¨mes de mÃ©moire â€“ c'est un choix intelligent pour la scalabilitÃ©. Les tests sont approfondis, comme vu dans tests/test_validation.py, couvrant des cas limites comme des dataframes vides, des timestamps dupliquÃ©s et des ratios d'overfitting avec des fixtures pytest. Ce niveau de couverture (par exemple, vÃ©rifications d'intÃ©gritÃ© temporelle levant des ValueErrors pour des donnÃ©es futures ou des indices non chronologiques) rÃ©duit les risques dans les applications financiÃ¨res oÃ¹ les erreurs de donnÃ©es peuvent mener Ã  des dÃ©cisions erronÃ©es.
D'un autre cÃ´tÃ©, plusieurs aspects tombent en deÃ§Ã  d'une maturitÃ© complÃ¨te. L'intÃ©gration GPU, promise dans des fichiers comme gpu/multi_gpu.py et indicators/gpu_integration.py, recourt souvent Ã  des avertissements ou des fallbacks (par exemple, speedup simulÃ© Ã  2.5x au lieu de mesurÃ© via nvidia-smi). Cela pourrait miner les revendications d'utilisation Ã  70 %. La gestion des donnÃ©es repose fortement sur une gÃ©nÃ©ration synthÃ©tique, comme dans les fixtures sample_data utilisant pd.date_range et des cumsums alÃ©atoires â€“ bien qu'utile pour les tests unitaires, cela manque d'intÃ©gration avec des API rÃ©elles, potentiellement manquant des nuances comme des timestamps irrÃ©guliers ou des frais spÃ©cifiques aux Ã©changes. Des scripts comme scripts/repair_and_redownload.py suggÃ¨rent un focus sur la maintenance des donnÃ©es, mais sans implÃ©mentations concrÃ¨tes montrÃ©es, la robustesse en production (par exemple, gestion de fichiers Parquet corrompus) reste incertaine. De plus, la CLI (cli/main.py) est structurÃ©e mais pourrait bÃ©nÃ©ficier de plus de sous-commandes pour une convivialitÃ© accrue.
Pour les amÃ©liorations, commencez par flesher les placeholders : implÃ©mentez de vÃ©ritables kernels GPU pour les indicateurs (par exemple, Bandes de Bollinger dans indicators/bollinger.py utilisant des arrays CuPy) et benchmarkez-les contre des baselines CPU. Ã‰tendez les sources de donnÃ©es en ajoutant un module pour rÃ©cupÃ©rer depuis des API fiables â€“ priorisez CCXT pour la crypto ou Alpha Vantage pour les actions, avec du caching pour atteindre votre cible de 80 % de hit rate. AmÃ©liorez la documentation : ajoutez un README avec des Ã©tapes d'installation, des workflows d'exemple (par exemple, exÃ©cuter un backtest via CLI) et des listes de dÃ©pendances. Incorporez des outils de linting comme flake8 et des hints de type avec mypy pour la qualitÃ© du code. Pour l'optimisation, envisagez des approches hybrides combinant la recherche en grille avec des mÃ©thodes bayÃ©siennes (par exemple, via scikit-optimize) pour affiner le tuning de paramÃ¨tres au-delÃ  de Monte Carlo. Dans les tests, visez une couverture de 80-90 % et ajoutez des tests d'intÃ©gration simulant des pipelines de bout en bout (import de donnÃ©es â†’ enrichissement d'indicateurs â†’ backtest â†’ reporting).
Concernant le potentiel commercial et "devenir riche bientÃ´t", ThreadX a des Ã©lÃ©ments intrigants pour le trading algorithmique, tels que des stratÃ©gies comme strategy/bb_atr.py (Bandes de Bollinger avec stops ATR) qui pourraient exploiter la volatilitÃ© du marchÃ©. Des outils open-source similaires (par exemple, Backtrader ou Zipline) ont permis Ã  des utilisateurs de dÃ©velopper des bots profitables, mais les taux de succÃ¨s sont bas â€“ des Ã©tudes indiquent que seulement 10-20 % des traders algo retail profitent de maniÃ¨re consistente en raison de l'overfitting et des coÃ»ts de transaction. Vos fonctionnalitÃ©s de validation (tests walk-forward dans backtest/validation.py) aident Ã  attÃ©nuer cela, mais un dÃ©ploiement en live nÃ©cessite d'abord du paper trading. Si raffinÃ©, vous pourriez monÃ©tiser via des sponsorings GitHub, une version payante avec des stratÃ©gies premium, ou des services de freelancing en quant. Cependant, ce n'est pas un outil "devenir riche rapidement" ; des preuves de forums de trading suggÃ¨rent qu'un effort soutenu sur 1-2 ans est nÃ©cessaire pour la viabilitÃ©, avec des risques de perte de capital. Priorisez une utilisation Ã©thique et backtestez sur des datasets diversifiÃ©s pour bÃ¢tir la confiance.
En rÃ©sumÃ©, c'est un effort solo impressionnant avec une vision claire pour un framework de trading performant. Avec des amÃ©liorations ciblÃ©es, il pourrait devenir un projet remarquable dans l'espace quant.



========================================
FICHIER: LE_CODE_analyse_tierce_3.txt
========================================
1. Optimisation de la boucle Numba (Goulot d'Ã©tranglement principal)
Le point le plus critique est la fonction _backtest_loop_numba() dans bb_atr.py, qui consomme 35.4 % du temps CPU. Comme cette boucle est appelÃ©e pour chaque scÃ©nario, tout gain ici aura un impact massif.



Supprimer la reconstruction d'objets Python : L'analyse indique une "rÃ©gression" oÃ¹ la reconstruction des trades (lignes ~843-900) rÃ©introduit du Python dans la boucle Numba.

Action : Divise le processus en deux passes.


Passe 1 (Numba pure) : La fonction Numba ne doit que calculer les tableaux NumPy bruts (indices d'entrÃ©e/sortie, PnL, etc.). N'utilise aucune liste Python ni append. PrÃ©-alloue les tableaux Ã  la taille maximale (longueur des prix) et tronque-les aprÃ¨s.





Passe 2 (Python/Pandas) : AprÃ¨s le retour de la fonction Numba, utilise ces tableaux NumPy pour construire le DataFrame trades_df final.


Migrer la logique de simulation dans Numba : Actuellement, la logique de simulation (stop loss, frais, PnL) est gÃ©rÃ©e en Python pur dans BacktestEngine._simulate_trades() , ce qui est confirmÃ© par le code (voir backtest/engine.py ).




Action : DÃ©place toute cette logique de simulation de trade Ã  l'intÃ©rieur de la boucle Numba. L'objectif est d'avoir une seule boucle Numba qui gÃ¨re tout, de l'ouverture Ã  la fermeture.



Forcer les types d'entrÃ©e : Assure-toi que la fonction Numba reÃ§oit des np.ndarray contigus (pas des SÃ©ries pandas) pour Ã©viter toute conversion implicite.

2. RÃ©duction du coÃ»t Python par scÃ©nario
Chaque scÃ©nario de backtest refait beaucoup de travail inutile (validation, logs, construction de gros objets) alors que seules quelques mÃ©triques clÃ©s sont nÃ©cessaires pour l'optimisation.


CrÃ©er un mode "fast_eval" : ImplÃ©mente une voie d'exÃ©cution rapide (ex: BacktestEngine.run_fast()).


Action : Ce mode doit sauter la validation lourde (_validate_inputs) aprÃ¨s le premier run, dÃ©sactiver les logs de dÃ©bogage, et ne construire qu'un dictionnaire de mÃ©tadonnÃ©es minimal (Sharpe, MaxDD, etc.).


RÃ©duire les donnÃ©es de retour : Les workers retournent actuellement des objets RunResult complets, incluant les DataFrames equity et trades.


Action : Modifie les workers pour qu'ils ne retournent qu'un dictionnaire lÃ©ger contenant les mÃ©triques scalaires finales (ex: {combo_id, sharpe, return_pct, dd}). Cela rÃ©duira drastiquement le coÃ»t de sÃ©rialisation entre processus.


3. Mutualisation des calculs et I/O

Mutualiser les indicateurs : L'IndicatorBank a un bon cache , mais chaque processus worker a probablement son propre cache.



Action : Calcule tous les indicateurs (Bollinger, ATR, etc.) une seule fois dans le processus parent avant de lancer les workers. Place ces indicateurs en mÃ©moire partagÃ©e (shared memory / memmap) pour que tous les workers y accÃ¨dent en lecture seule sans recalculer.



Optimiser l'Ã©criture des checkpoints : L'analyse suggÃ¨re que chaque worker Ã©crit dans le fichier Parquet, crÃ©ant un goulot d'Ã©tranglement I/O Ã  cause des verrous de fichier.

Action : SÃ©pare le calcul et l'Ã©criture. Les workers de calcul doivent renvoyer leurs rÃ©sultats (les dictionnaires lÃ©gers de l'Axe 2) Ã  un seul thread/worker I/O dÃ©diÃ©. Ce worker I/O se chargera de grouper les rÃ©sultats et de les Ã©crire en batch dans le fichier Parquet.


Centraliser le "Warmup" Numba : La fonction _warmup_numba_jit() doit Ãªtre exÃ©cutÃ©e une seule fois lors de l'initialisation de chaque worker (ProcessPoolExecutor(initializer=...)) , et non au dÃ©but de chaque scÃ©nario.



4. Optimisation de la parallÃ©lisation et du GPU
Grouper les scÃ©narios (Chunking) : L'overhead de ProcessPoolExecutor.submit() devient significatif si chaque scÃ©nario est trÃ¨s rapide.

Action : Ne soumets pas les scÃ©narios un par un. Groupe-les par paquets (ex: 32 ou 64 scÃ©narios) et envoie un "chunk" entier Ã  chaque worker. Le worker traitera ce chunk dans une boucle interne.


Utiliser le GPU pour la simulation : Le GPU est dÃ©tectÃ© (MultiGPUManager ) mais n'est pas utilisÃ© pour la simulation de trade (_simulate_trades ).



Action 1 : Porte la logique de _simulate_trades (qui est en Python ) vers le GPU en utilisant CuPy ou Numba CUDA. Le calcul de PnL et la gestion des stops sont mÃ©caniques et parallÃ©lisables.



Action 2 : Distribue le travail par scÃ©nario sur les GPU. Au lieu de la balance "75/25" , assigne chaque worker Ã  un GPU spÃ©cifique (ex: Worker 1-15 â†’ GPU0, Worker 16-30 â†’ GPU1).


Action 3 : Ã‰vite les allers-retours CPUâ†”GPU. L'ensemble du calcul d'un scÃ©nario doit rester sur le GPU. Ne recopie vers le CPU que les 5 mÃ©triques finales (Sharpe, MaxDD, etc.).

5. Optimisations secondaires

Vectoriser le calcul de l'Ã©quitÃ© : La fonction _calculate_equity_returns recalcule l'Ã©quitÃ© en bouclant en Python Ã  chaque trade.



Action : Remplace cela par une opÃ©ration NumPy vectorisÃ©e (type "prefix-sum" inversÃ©) en utilisant des slices : cumulative_pnl[t_exit_index:] += pnl.


Normaliser les donnÃ©es en amont : Le chargement des donnÃ©es (load_ohlcv ) effectue la normalisation et le parsing datetime Ã  chaque fois.



Action : Fais ce nettoyage une seule fois et sauvegarde les donnÃ©es OHLCV propres dans un format binaire (Parquet). Charge ce fichier propre directement.


RÃ©duire le logging : Les logs verbeux pendant un sweep coÃ»tent cher.


Action : Ajoute un flag silent=True qui dÃ©sactive tous les logger.debug() dans les boucles de simulation.


Mutualiser la validation Walk-Forward : Ne recalcule pas les indicateurs pour chaque split de validation.


Action : Calcule les indicateurs une fois sur tout l'horizon, puis fournis une vue (slice) de ces arrays Ã  chaque split train/test.

RÃ©sumÃ© de l'analyse
L'analyse  conclut que les goulots d'Ã©tranglement actuels sont :

Le CPU Python inutile par scÃ©nario (validation, logs, objets lourds).

Les allers-retours disque inutiles (chargement OHLCV, Ã©criture checkpoints).

L'overhead de l'ordonnancement multiprocessus (un task par scÃ©nario).

L'ordre d'attaque recommandÃ© est:

Refactor du noyau Numba pour Ãªtre 100 % nopython, sans objets Python.

ImplÃ©mentation du mode "fast_eval" pour ne retourner que les mÃ©triques.

PrÃ©-calcul et partage des donnÃ©es (OHLCV + Indicateurs) pour tous les workers.

AmÃ©lioration du scheduler (chunking, I/O writer unique, warmup Numba par worker).

Mise en place d'une vraie distribution multi-GPU (un GPU par worker/scÃ©nario).



========================================
FICHIER: LE_CODE_analyse_tierce_4.txt
========================================
ðŸŽ¯ Analyse de l'Architecture et OpportunitÃ©s
Votre systÃ¨me a une architecture bien structurÃ©e avec une forte sÃ©paration Grid/Monte-Carlo, mais je vois plusieurs axes d'optimisation au-delÃ  de ceux dÃ©jÃ  identifiÃ©s :
ðŸ“Š Pistes d'Optimisation Prioritaires
1. Optimisation Numba - Impact ImmÃ©diat (35.4% du temps)
A. Ã‰liminer complÃ¨tement Python de la boucle Numba
python# âŒ AVANT (avec reconstruction Python)
@njit(fastmath=True, cache=True)
def _backtest_loop_numba(prices, signals, ...):
    # ... logique de calcul ...
    # Lignes 843-900: reconstruction trades (RÃ‰GRESSION)
    trades_list = []  # âŒ Liste Python
    for i in range(n_trades):
        trades_list.append({...})  # âŒ Dict Python
    
# âœ… APRÃˆS (Numba pur avec 2 passes)
@njit(fastmath=True, cache=True, parallel=True)
def _backtest_loop_numba_pure(prices, signals, ...):
    # PrÃ©-allocation des arrays
    max_trades = len(prices) // 2
    entry_idx = np.empty(max_trades, dtype=np.int32)
    exit_idx = np.empty(max_trades, dtype=np.int32)
    pnl = np.empty(max_trades, dtype=np.float64)
    
    trade_count = 0
    for i in prange(len(prices)):  # ParallÃ©lisation automatique
        # Logique de trade
        if entry_condition:
            entry_idx[trade_count] = i
            # ...
            trade_count += 1
    
    # Tronquer aux trades rÃ©els
    return entry_idx[:trade_count], exit_idx[:trade_count], pnl[:trade_count]

# Puis en Python:
def reconstruct_trades_dataframe(entry_idx, exit_idx, pnl, timestamps):
    return pd.DataFrame({...})  # Construction une seule fois
B. Vectoriser les stops loss/take profit
python@njit(fastmath=True)
def apply_stops_vectorized(prices, entry_prices, stop_pct, tp_pct):
    """Calcul vectorisÃ© des stops sur GPU si disponible"""
    stop_prices = entry_prices * (1 - stop_pct)
    tp_prices = entry_prices * (1 + tp_pct)
    
    # Numba compilera ceci efficacement
    exit_signals = np.where(
        (prices <= stop_prices) | (prices >= tp_prices),
        1, 0
    )
    return exit_signals
2. MÃ©moire PartagÃ©e pour les Indicateurs (7.2% â†’ ~1%)
pythonfrom multiprocessing import shared_memory
import numpy as np

class SharedIndicatorBank:
    """Calcul unique + partage entre tous les workers"""
    
    def __init__(self):
        self.shm_dict = {}
    
    def precompute_all_indicators(self, ohlcv_df):
        """AppelÃ© UNE FOIS dans le processus parent"""
        indicators = {
            'rsi': self._compute_rsi(ohlcv_df),
            'bb_upper': self._compute_bb(ohlcv_df)[0],
            'bb_lower': self._compute_bb(ohlcv_df)[1],
            'atr': self._compute_atr(ohlcv_df),
        }
        
        # CrÃ©er shared memory pour chaque indicateur
        for name, array in indicators.items():
            shm = shared_memory.SharedMemory(
                create=True, 
                size=array.nbytes,
                name=f"indicator_{name}"
            )
            # Copier les donnÃ©es
            shm_array = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf)
            shm_array[:] = array[:]
            self.shm_dict[name] = (shm, array.shape, array.dtype)
        
        return self.shm_dict
    
    @staticmethod
    def worker_get_indicators(shm_metadata):
        """AppelÃ© dans chaque worker pour accÃ©der aux indicateurs"""
        indicators = {}
        for name, (shm_name, shape, dtype) in shm_metadata.items():
            shm = shared_memory.SharedMemory(name=shm_name)
            indicators[name] = np.ndarray(shape, dtype=dtype, buffer=shm.buf)
        return indicators

# Dans engine.py
def run_sweep():
    # PrÃ©-calcul parent
    shared_bank = SharedIndicatorBank()
    shm_meta = shared_bank.precompute_all_indicators(ohlcv_df)
    
    # Passer aux workers
    with ProcessPoolExecutor(
        max_workers=30,
        initializer=init_worker,
        initargs=(shm_meta,)  # âœ… MÃ©tadonnÃ©es uniquement
    ) as executor:
        # ...
3. Batch Processing et I/O CentralisÃ©
pythonfrom queue import Queue
from threading import Thread

class CheckpointWriter:
    """Thread dÃ©diÃ© pour Ã©criture batch des rÃ©sultats"""
    
    def __init__(self, output_path, batch_size=100):
        self.queue = Queue()
        self.batch_size = batch_size
        self.writer_thread = Thread(target=self._write_loop, daemon=True)
        self.writer_thread.start()
    
    def _write_loop(self):
        batch = []
        while True:
            result = self.queue.get()
            if result is None:  # Signal de fin
                if batch:
                    self._write_batch(batch)
                break
            
            batch.append(result)
            if len(batch) >= self.batch_size:
                self._write_batch(batch)
                batch = []
    
    def _write_batch(self, batch):
        """Ã‰criture groupÃ©e optimisÃ©e"""
        df = pd.DataFrame(batch)
        # Ã‰criture append sans lock (un seul thread)
        df.to_parquet(
            self.output_path,
            engine='pyarrow',
            append=True,
            compression='snappy'  # Plus rapide que gzip
        )

# Modification du worker
def _evaluate_single_task_optimized(combo, shared_data):
    """Worker retourne dict lÃ©ger uniquement"""
    result = backtest_engine.run_fast(combo)  # Mode rapide
    
    # âœ… Retour minimal (< 1KB par scÃ©nario vs plusieurs MB avant)
    return {
        'combo_id': combo['id'],
        'params': combo,
        'sharpe': result.sharpe,
        'total_return': result.total_return,
        'max_dd': result.max_dd,
        'win_rate': result.win_rate,
        # PAS de DataFrame equity/trades
    }
4. Chunking Intelligent des ScÃ©narios
pythondef chunk_scenarios(scenarios, chunk_size=64):
    """Groupe les scÃ©narios pour rÃ©duire l'overhead"""
    for i in range(0, len(scenarios), chunk_size):
        yield scenarios[i:i+chunk_size]

def process_chunk(chunk, shared_indicators):
    """Un worker traite N scÃ©narios d'un coup"""
    results = []
    for scenario in chunk:
        result = backtest_fast(scenario, shared_indicators)
        results.append(result)
    return results  # Retour groupÃ©

# Dans run_sweep()
with ProcessPoolExecutor(max_workers=30) as executor:
    futures = []
    for chunk in chunk_scenarios(all_scenarios, chunk_size=64):
        future = executor.submit(process_chunk, chunk, shm_meta)
        futures.append(future)
    
    # RÃ©duction drastique du nombre de tasks soumis
    # 10,000 scÃ©narios â†’ 156 tasks (au lieu de 10,000)
5. GPU: Distribution et Pipeline OptimisÃ©s
pythonimport cupy as cp

class GPUBacktestEngine:
    """Backtest complet sur GPU"""
    
    def __init__(self, gpu_id):
        cp.cuda.Device(gpu_id).use()
        self.gpu_id = gpu_id
    
    @staticmethod
    def backtest_batch_gpu(scenarios_batch, prices_gpu, indicators_gpu):
        """Traite un batch de scÃ©narios en parallÃ¨le sur GPU"""
        n_scenarios = len(scenarios_batch)
        n_bars = len(prices_gpu)
        
        # PrÃ©-allocation GPU
        signals_gpu = cp.zeros((n_scenarios, n_bars), dtype=cp.int8)
        pnls_gpu = cp.zeros(n_scenarios, dtype=cp.float32)
        
        # Kernel CUDA personnalisÃ© pour simulation vectorisÃ©e
        kernel = cp.RawKernel(r'''
        extern "C" __global__
        void backtest_kernel(
            const float* prices,
            const float* indicators,
            const float* params,
            int* signals,
            float* pnls,
            int n_bars,
            int n_scenarios
        ) {
            int scenario_id = blockIdx.x;
            int bar_idx = threadIdx.x;
            
            // Logique de backtest parallÃ©lisÃ©e
            // Chaque bloc = un scÃ©nario
            // Chaque thread = une barre
        }
        ''', 'backtest_kernel')
        
        # ExÃ©cution
        kernel(
            (n_scenarios,),  # Grille de blocs
            (256,),  # Threads par bloc
            (prices_gpu, indicators_gpu, params_gpu, signals_gpu, pnls_gpu, n_bars, n_scenarios)
        )
        
        # Retour CPU: uniquement mÃ©triques finales
        return cp.asnumpy(pnls_gpu)  # Transfer minimal

# Distribution multi-GPU
def run_sweep_multi_gpu(scenarios):
    n_gpus = cp.cuda.runtime.getDeviceCount()
    
    # Split scÃ©narios par GPU
    scenarios_per_gpu = np.array_split(scenarios, n_gpus)
    
    with ThreadPoolExecutor(max_workers=n_gpus) as executor:
        futures = []
        for gpu_id, gpu_scenarios in enumerate(scenarios_per_gpu):
            future = executor.submit(
                process_on_gpu,
                gpu_id,
                gpu_scenarios
            )
            futures.append(future)
        
        results = [f.result() for f in futures]
    
    return np.concatenate(results)
6. Optimisations SpÃ©cifiques Monte-Carlo
pythonclass AdaptiveMonteCarloOptimizer:
    """Monte-Carlo avec Ã©chantillonnage adaptatif"""
    
    def __init__(self, param_ranges, n_initial=1000):
        self.param_ranges = param_ranges
        self.n_initial = n_initial
        self.results_history = []
    
    def run_adaptive(self, n_iterations=5):
        """StratÃ©gie d'Ã©chantillonnage intelligent"""
        
        # Phase 1: Exploration large
        scenarios = self._sample_uniform(self.n_initial)
        results = self._evaluate_batch(scenarios)
        self.results_history.extend(results)
        
        for iteration in range(n_iterations):
            # Phase 2: Exploitation des zones prometteuses
            top_20_percent = self._get_top_percentile(results, 0.2)
            
            # Ã‰chantillonnage gaussien autour des meilleurs
            new_scenarios = self._sample_gaussian_around_best(
                top_20_percent,
                n_samples=500,
                sigma=0.1  # RÃ©duction progressive
            )
            
            results = self._evaluate_batch(new_scenarios)
            self.results_history.extend(results)
            
            # RÃ©duction adaptative de sigma
            sigma *= 0.8
        
        return self.get_best_params()
    
    def _sample_gaussian_around_best(self, best_params, n_samples, sigma):
        """Ã‰chantillonnage autour des meilleurs rÃ©sultats"""
        scenarios = []
        for _ in range(n_samples):
            # SÃ©lectionner un des meilleurs alÃ©atoirement
            template = random.choice(best_params)
            
            # Perturbation gaussienne
            new_scenario = {}
            for param, value in template.items():
                param_range = self.param_ranges[param]
                std = (param_range[1] - param_range[0]) * sigma
                new_value = np.clip(
                    np.random.normal(value, std),
                    param_range[0],
                    param_range[1]
                )
                new_scenario[param] = new_value
            
            scenarios.append(new_scenario)
        
        return scenarios
ðŸŽ¯ Plan d'Action PriorisÃ©
Phase 1 - Gains Rapides (1-2 jours)

âœ… ImplÃ©menter mode fast_eval (retour dict lÃ©ger)
âœ… Chunking des scÃ©narios (rÃ©duction overhead 90%)
âœ… Writer I/O centralisÃ© (Ã©limination verrous)

Gain estimÃ©: 40-50% de performance
Phase 2 - Optimisations Structurelles (3-5 jours)

âœ… Refactor Numba pur (2 passes, Ã©limination Python)
âœ… Shared memory indicateurs (calcul unique)
âœ… Warmup Numba par worker (initializer)

Gain estimÃ©: +30-40% supplÃ©mentaire
Phase 3 - GPU et AvancÃ© (1-2 semaines)

âœ… Pipeline GPU complet (CuPy/Numba CUDA)
âœ… Distribution multi-GPU (1 GPU par groupe de workers)
âœ… Monte-Carlo adaptatif (Ã©chantillonnage intelligent)

Gain estimÃ©: +2-5x avec GPU


