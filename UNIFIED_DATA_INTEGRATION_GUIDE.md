# IntÃ©gration ThreadX - unified_data_historique
## Guide d'IntÃ©gration pour Ã‰viter les TÃ©lÃ©chargements en Doublon

---

## 1. Vue d'ensemble

Le systÃ¨me **UnifiedDataAdapter** permet d'intÃ©grer le programme de tÃ©lÃ©chargement `unified_data_historique_with_indicators.py` avec les donnÃ©es existantes de ThreadX.

### Objectifs
- âœ… **Lecture prÃ©-tÃ©lÃ©chargement** : VÃ©rifier les donnÃ©es existantes avant de tÃ©lÃ©charger
- âœ… **DÃ©tection des gaps** : Identifier uniquement les pÃ©riodes manquantes
- âœ… **Fusion sans doublon** : Combiner nouvelles et existantes via dÃ©duplication par timestamp
- âœ… **Format cohÃ©rent** : Assurer nomenclature, structure, type identiques
- âœ… **Environment alignment** : Utiliser variables d'environnement pour rÃ©pertoires

### Architecture

```
unified_data_historique_with_indicators.py
    â†“ (importe)
UnifiedDataAdapter (src/threadx/data/unified_data_adapter.py)
    â†“ (utilise)
DataCompatibilityManager (src/threadx/data/compatibility.py)
    â†“ (gÃ¨re)
D:\TradXPro\best_token_DataFrame\
    â”œâ”€â”€ BTCUSDC_1h.json / BTCUSDC_1h.parquet
    â”œâ”€â”€ ETHUSDC_1h.json / ETHUSDC_1h.parquet
    â””â”€â”€ ...
```

---

## 2. Installation et Setup

### 2.1 Variables d'Environnement

Les deux systÃ¨mes utilisent la mÃªme variable d'environnement :

```bash
# DÃ©finir la variable d'environnement (Windows)
set DATA_FOLDER=D:\TradXPro\best_token_DataFrame

# Ou en Python (avant l'importation)
import os
os.environ["DATA_FOLDER"] = r"D:\TradXPro\best_token_DataFrame"
```

### 2.2 Importer l'Adaptateur

```python
from threadx.data import UnifiedDataAdapter, create_adapter

# Option 1: CrÃ©er directement
adapter = UnifiedDataAdapter()

# Option 2: Via factory function
adapter = create_adapter()

# Option 3: Avec chemin personnalisÃ©
adapter = UnifiedDataAdapter(data_folder="/custom/path/best_token_DataFrame")
```

---

## 3. Workflow IntÃ©gration - Ã‰tape par Ã‰tape

### 3.1 Avant le TÃ©lÃ©chargement : DÃ©tecter les Gaps

```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()
symbol = "BTCUSDC"
interval = "1h"

# Ã‰tape 1: VÃ©rifier ce qui est manquant
gaps = adapter.get_gaps_to_download(symbol, interval, history_days=365)

print(f"Gaps dÃ©tectÃ©s: {len(gaps)} pÃ©riode(s)")

if not gaps:
    print(f"âœ… {symbol}/{interval} est complÃ¨tement Ã  jour")
else:
    print(f"ğŸ“¥ Ã€ tÃ©lÃ©charger: {len(gaps)} gap(s)")
    for i, (start_ms, end_ms) in enumerate(gaps, 1):
        print(f"  {i}. {start_ms} â†’ {end_ms}")
```

### 3.2 TÃ©lÃ©charger Seulement les Gaps

**Au lieu de tÃ©lÃ©charger la plage complÃ¨te, tÃ©lÃ©charger uniquement les gaps :**

```python
from threadx.data import UnifiedDataAdapter
from your_download_module import download_candles_binance

adapter = UnifiedDataAdapter()
symbol = "BTCUSDC"
interval = "1h"

# RÃ©cupÃ©rer les gaps
gaps = adapter.get_gaps_to_download(symbol, interval)

if gaps:
    print(f"TÃ©lÃ©chargement des {len(gaps)} gap(s)...")

    all_new_candles = []

    for start_ms, end_ms in gaps:
        print(f"  TÃ©lÃ©chargement: {start_ms} â†’ {end_ms}")

        # TÃ©lÃ©charger du API (ex: Binance)
        candles = download_candles_binance(
            symbol=symbol,
            interval=interval,
            start_time=start_ms,
            end_time=end_ms
        )

        if candles:
            all_new_candles.extend(candles)
            print(f"    âœ“ {len(candles)} candles tÃ©lÃ©chargÃ©es")

    if all_new_candles:
        print(f"\nFusion de {len(all_new_candles)} candles avec existantes...")
        result = adapter.merge_and_save(symbol, interval, all_new_candles)
        print(f"âœ… Fusion complÃ©tÃ©e: {len(result)} total")
else:
    print("Pas de gaps, donnÃ©es Ã  jour")
```

### 3.3 Charger les DonnÃ©es Existantes

```python
adapter = UnifiedDataAdapter()

# Format liste de dict (compatible unified_data)
candles_list = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=False)
if candles_list:
    print(f"ChargÃ© {len(candles_list)} candles")
    print(f"Premier: {candles_list[0]}")
    print(f"Dernier: {candles_list[-1]}")

# OU Format DataFrame (pour calculs internes)
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)
if df is not None:
    print(f"DataFrame shape: {df.shape}")
    print(f"Index: {df.index[0]} â†’ {df.index[-1]}")
```

### 3.4 Valider les DonnÃ©es

```python
adapter = UnifiedDataAdapter()

# Valider les donnÃ©es existantes
is_valid, message = adapter.validate_candles("BTCUSDC", "1h")

if is_valid:
    print(f"âœ… {message}")
else:
    print(f"âŒ Validation Ã©chouÃ©e: {message}")
```

### 3.5 Obtenir un RÃ©sumÃ© Complet

```python
adapter = UnifiedDataAdapter()

status = adapter.get_data_status("BTCUSDC", "1h")

print(f"Statut pour {status['symbol']}/{status['interval']}:")
print(f"  Existe: {status['exists']}")
print(f"  Nombre de candles: {status['candle_count']}")

if status['exists']:
    print(f"  Plage: {status['date_range'][0]} â†’ {status['date_range'][1]}")
    print(f"  Premier timestamp: {status['first_timestamp']}")
    print(f"  Dernier timestamp: {status['last_timestamp']}")
    print(f"  Gaps manquants: {len(status['gaps'])}")
    print(f"  ValidÃ©: {status['is_valid']} - {status['validation_msg']}")
```

---

## 4. IntÃ©gration dans unified_data_historique_with_indicators.py

### 4.1 Modification de `download_ohlcv()`

**Avant (tÃ©lÃ©charge tout) :**
```python
def download_ohlcv(symbol: str, interval: str, start_date: str, end_date: str):
    # ... convertir dates en timestamps
    # TÃ©lÃ©charger TOUTE la plage du API
    candles = binance_client.get_historical_klines(symbol, interval, start_time=start_ms, end_time=end_ms)
    return candles
```

**AprÃ¨s (tÃ©lÃ©charge uniquement les gaps) :**
```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

def download_ohlcv(symbol: str, interval: str, start_date: str = None, end_date: str = None):
    """
    TÃ©lÃ©charge OHLCV mais seulement pour les gaps dÃ©tectÃ©s.

    Si start_date/end_date sont fournis, utiliser ces limites.
    Sinon, utiliser la plage par dÃ©faut (last 365 days).
    """

    # DÃ©terminer history_days basÃ© sur start_date
    if start_date:
        history_days = calculate_days_between(start_date, end_date)
    else:
        history_days = 365  # dÃ©faut

    # Ã‰tape 1: RÃ©cupÃ©rer les gaps existants
    gaps = adapter.get_gaps_to_download(symbol, interval, history_days)

    logger.info(f"Pour {symbol}/{interval}: {len(gaps)} gap(s) Ã  tÃ©lÃ©charger")

    if not gaps:
        logger.info(f"âœ… {symbol}/{interval} est Ã  jour, pas de tÃ©lÃ©chargement")
        return None

    # Ã‰tape 2: TÃ©lÃ©charger SEULEMENT les gaps
    all_candles = []

    for start_ms, end_ms in gaps:
        logger.info(f"TÃ©lÃ©chargement gap: {start_ms} â†’ {end_ms}")

        try:
            # TÃ©lÃ©charger cette pÃ©riode spÃ©cifique du API
            candles = binance_client.get_historical_klines(
                symbol=symbol,
                interval=interval,
                start_time=start_ms,
                end_time=end_ms
            )

            if candles:
                all_candles.extend(candles)
                logger.info(f"  âœ“ {len(candles)} candles tÃ©lÃ©chargÃ©es")

        except Exception as e:
            logger.error(f"Erreur tÃ©lÃ©chargement gap {start_ms}â†’{end_ms}: {e}")
            # Continuer avec le prochain gap
            continue

    return all_candles if all_candles else None
```

### 4.2 Modification de `verify_and_complete()`

**Avant (utilise logique locale) :**
```python
def verify_and_complete():
    # ... logique manuelle de vÃ©rification
    for symbol in symbols:
        existing = load_from_local_cache(symbol)
        if not existing:
            # TÃ©lÃ©charger tout
            pass
```

**AprÃ¨s (utilise l'adaptateur) :**
```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

def verify_and_complete():
    """
    VÃ©rifie et complÃ¨te les donnÃ©es pour tous les symboles.
    Utilise UnifiedDataAdapter pour Ã©viter les doublons.
    """

    symbols = ["BTCUSDC", "ETHUSDC", "ADAUSDC"]  # Vos symboles
    interval = "1h"

    for symbol in symbols:
        logger.info(f"VÃ©rification: {symbol}/{interval}")

        # Ã‰tape 1: VÃ©rifier l'Ã©tat des donnÃ©es
        status = adapter.get_data_status(symbol, interval)

        if status['exists']:
            logger.info(f"  DonnÃ©es existantes: {status['candle_count']} candles")
            logger.info(f"  Plage: {status['date_range'][0]} â†’ {status['date_range'][1]}")

            if status['gaps']:
                logger.info(f"  Gaps dÃ©tectÃ©s: {len(status['gaps'])}")
            else:
                logger.info(f"  Aucun gap: donnÃ©es complÃ¨tes")
                continue
        else:
            logger.info(f"  Aucune donnÃ©e existante: tÃ©lÃ©chargement complet requis")

        # Ã‰tape 2: TÃ©lÃ©charger et fusionner
        new_candles = download_ohlcv(symbol, interval)

        if new_candles:
            logger.info(f"  Fusion en cours...")
            result = adapter.merge_and_save(symbol, interval, new_candles)

            if result:
                logger.info(f"  âœ… Total: {len(result)} candles")
            else:
                logger.error(f"  âŒ Erreur fusion")
```

### 4.3 Modification de la Conversion JSONâ†’Parquet

**Avant :**
```python
def convert_to_parquet(symbol: str, interval: str):
    # Charger JSON
    df = pd.read_json(f"{symbol}_{interval}.json")
    # Convertir et sauvegarder Parquet
    df.to_parquet(f"{symbol}_{interval}.parquet")
```

**AprÃ¨s (utilise le format unifiÃ©) :**
```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

def convert_to_parquet(symbol: str, interval: str):
    """
    Convertit JSONâ†’Parquet pour un symbole/intervalle.
    Utilise le systÃ¨me de normalisation ThreadX.
    """

    # Charger les donnÃ©es actuelles
    df = adapter.load_existing_candles(symbol, interval, as_dataframe=True)

    if df is None:
        logger.warning(f"Aucune donnÃ©e pour {symbol}/{interval}")
        return

    # Sauvegarder au format Parquet (automatiquement normalisÃ©)
    saved_path = adapter.manager.save_data(symbol, interval, df, format="parquet")

    if saved_path:
        logger.info(f"Converti â†’ Parquet: {saved_path}")

        # Optionnel: Supprimer l'ancien JSON
        json_path = adapter.manager.data_folder / f"{symbol}_{interval}.json"
        if json_path.exists():
            json_path.unlink()
            logger.info(f"JSON supprimÃ©: {json_path}")
```

### 4.4 Ajout de Flags CLI

**Modification du main :**
```python
import argparse
from threadx.data import UnifiedDataAdapter

def main():
    parser = argparse.ArgumentParser()

    # Flags existants
    parser.add_argument("--symbols", default="BTCUSDC,ETHUSDC")
    parser.add_argument("--interval", default="1h")

    # NOUVEAUX flags pour la compatibilitÃ© ThreadX
    parser.add_argument(
        "--compatible-mode",
        action="store_true",
        help="Utiliser le mode compatible (Ã©vite doublons, lit avant de tÃ©lÃ©charger)"
    )
    parser.add_argument(
        "--validate-after-download",
        action="store_true",
        help="Valider les donnÃ©es aprÃ¨s tÃ©lÃ©chargement"
    )
    parser.add_argument(
        "--convert-to-parquet",
        action="store_true",
        help="Convertir JSONâ†’Parquet aprÃ¨s fusion"
    )
    parser.add_argument(
        "--data-folder",
        type=str,
        help="Chemin personnalisÃ© au dossier de donnÃ©es (dÃ©faut: DATA_FOLDER env)"
    )

    args = parser.parse_args()

    # Initialiser l'adaptateur
    adapter = UnifiedDataAdapter(data_folder=args.data_folder)

    symbols = args.symbols.split(",")
    interval = args.interval

    for symbol in symbols:
        symbol = symbol.strip()

        if args.compatible_mode:
            # Mode compatib : utiliser l'adaptateur
            logger.info(f"\nğŸ“¥ Mode compatible: {symbol}/{interval}")

            gaps = adapter.get_gaps_to_download(symbol, interval)

            if gaps:
                new_candles = download_ohlcv(symbol, interval)
                if new_candles:
                    result = adapter.merge_and_save(symbol, interval, new_candles)
                    logger.info(f"âœ… FusionnÃ©: {len(result)} total")

            # Validation optionnelle
            if args.validate_after_download:
                is_valid, msg = adapter.validate_candles(symbol, interval)
                logger.info(f"Validation: {msg}")

            # Conversion optionnelle
            if args.convert_to_parquet:
                convert_to_parquet(symbol, interval)
        else:
            # Mode standard (ancien comportement)
            download_all_and_save(symbol, interval)

if __name__ == "__main__":
    main()
```

**Utilisation :**
```bash
# Mode compatible avec validation et conversion
python unified_data_historique_with_indicators.py \
    --symbols BTCUSDC,ETHUSDC \
    --interval 1h \
    --compatible-mode \
    --validate-after-download \
    --convert-to-parquet

# Avec chemin personnalisÃ©
python unified_data_historique_with_indicators.py \
    --compatible-mode \
    --data-folder /custom/path/best_token_DataFrame
```

---

## 5. Formats de DonnÃ©es

### 5.1 Format Liste de Dict (unified_data)

```python
# Format retournÃ© par load_existing_candles(as_dataframe=False)
# et attendu par merge_and_save()

candles = [
    {
        "timestamp": 1697289600000,  # ms Unix timestamp
        "open": "27500.00",          # string (peut aussi Ãªtre float)
        "high": "27650.50",
        "low": "27450.00",
        "close": "27600.25",
        "volume": "125.5",           # volume en token/satoshi
    },
    {
        "timestamp": 1697293200000,
        "open": "27600.25",
        # ...
    }
]
```

### 5.2 Format DataFrame (interne ThreadX)

```python
# Format utilisÃ© en interne par load_existing_candles(as_dataframe=True)

# Index: DatetimeIndex en UTC
# Colonnes: open, high, low, close, volume (tous float)

#                     open      high       low     close    volume
# timestamp
# 2023-10-14 16:00 27500.00 27650.500 27450.00 27600.25  125.5000
# 2023-10-14 17:00 27600.25 27700.000 27550.00 27650.75  135.2500
# 2023-10-14 18:00 27650.75 27800.000 27600.00 27750.00  140.0000
```

### 5.3 Nomenclature des Fichiers

```
D:\TradXPro\best_token_DataFrame\
â”œâ”€â”€ BTCUSDC_1h.json          # ou .parquet
â”œâ”€â”€ BTCUSDC_5m.json
â”œâ”€â”€ BTCUSDC_1d.json
â”œâ”€â”€ ETHUSDC_1h.json
â”œâ”€â”€ ETHUSDC_5m.json
â””â”€â”€ ...
```

Patterns supportÃ©s :
- `{SYMBOL}_{INTERVAL}.json`
- `{SYMBOL}_{INTERVAL}.parquet`

Exemple : `BTCUSDC_1h`, `ETHUSDC_5m`, `ADAUSDC_1d`

---

## 6. Cas d'Usage Courants

### 6.1 Mise Ã  Jour Quotidienne (Cron Job)

```python
import schedule
import time
from threadx.data import UnifiedDataAdapter

def daily_update():
    """TÃ©lÃ©charge et fusionne les donnÃ©es manquantes chaque jour."""

    adapter = UnifiedDataAdapter()
    symbols = ["BTCUSDC", "ETHUSDC", "ADAUSDC"]
    interval = "1h"

    for symbol in symbols:
        try:
            gaps = adapter.get_gaps_to_download(symbol, interval)

            if gaps:
                print(f"Mise Ã  jour {symbol}: {len(gaps)} gap(s)")
                new_candles = download_ohlcv(symbol, interval)

                if new_candles:
                    result = adapter.merge_and_save(symbol, interval, new_candles)
                    print(f"âœ… {len(result)} candles total")
            else:
                print(f"âœ… {symbol} Ã  jour")

        except Exception as e:
            print(f"âŒ Erreur {symbol}: {e}")
            # Continuer avec le suivant

# Schedule tous les jours Ã  00:00
schedule.every().day.at("00:00").do(daily_update)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### 6.2 Chargement de DonnÃ©es pour Backtesting

```python
from threadx.data import UnifiedDataAdapter
from threadx.strategy import AmplitudeHunterStrategy

adapter = UnifiedDataAdapter()

# Charger les donnÃ©es
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)

if df is not None:
    # Valider
    is_valid, msg = adapter.validate_candles("BTCUSDC", "1h")
    print(f"Validation: {msg}")

    if is_valid:
        # Utiliser avec une stratÃ©gie
        strategy = AmplitudeHunterStrategy("BTCUSDC", "1h")
        signals, stats = strategy.backtest(df, params_dict, initial_capital=10000)
        print(f"Sharpe: {stats.sharpe_ratio:.3f}")
```

### 6.3 Migration depuis JSON vers Parquet

```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

# Lister tous les fichiers JSON
json_files = [f for f in adapter.list_available_files() if f.endswith(".json")]

print(f"Migration de {len(json_files)} fichiers JSON vers Parquet...")

for filename in json_files:
    # Extraire symbol et interval
    symbol, interval = filename.replace(".json", "").rsplit("_", 1)

    # Charger et convertir
    df = adapter.load_existing_candles(symbol, interval, as_dataframe=True)

    if df is not None:
        saved = adapter.manager.save_data(symbol, interval, df, format="parquet")
        print(f"âœ… {symbol}/{interval}: {saved}")
```

---

## 7. Troubleshooting

### 7.1 "Fichier de donnÃ©es non trouvÃ©"

```
âš ï¸ DataCompatibilityManager initialisÃ©: D:\TradXPro\best_token_DataFrame
âŒ Aucune donnÃ©e existante pour BTCUSDC/1h
```

**Solution :**
- VÃ©rifier que `DATA_FOLDER` pointe vers le bon rÃ©pertoire
- VÃ©rifier que les fichiers existent avec la bonne nomenclature
- VÃ©rifier les permissions d'accÃ¨s

```python
import os
from pathlib import Path

folder = os.environ.get("DATA_FOLDER")
print(f"DATA_FOLDER: {folder}")

path = Path(folder)
if path.exists():
    files = list(path.glob("*.json")) + list(path.glob("*.parquet"))
    print(f"Fichiers trouvÃ©s: {len(files)}")
    for f in files[:5]:
        print(f"  - {f.name}")
```

### 7.2 "Validation Ã©chouÃ©e: NaN dans colonnes critiques"

Les donnÃ©es contiennent des valeurs manquantes.

**Solution :**
```python
adapter = UnifiedDataAdapter()
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)

# VÃ©rifier les NaN
print(df.isna().sum())

# Supprimer les NaN
df = df.dropna()

# Resauvegarder
adapter.manager.save_data("BTCUSDC", "1h", df, format="json")
```

### 7.3 "Gaps dÃ©tectÃ©s vides"

```
get_gaps_to_download("BTCUSDC", "1h") retourne []
```

**Cause :**
- Les donnÃ©es existantes couvrent toute la plage requise
- C'est normal ! Pas besoin de tÃ©lÃ©charger

**VÃ©rifier :**
```python
status = adapter.get_data_status("BTCUSDC", "1h")
print(f"Plage: {status['date_range']}")
print(f"Gaps: {status['gaps']}")
```

### 7.4 "Deduplication issue: doublon timestamps"

Si les donnÃ©es fusionnÃ©es ont des doublons aprÃ¨s merge_and_save():

```python
adapter = UnifiedDataAdapter()
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)

# VÃ©rifier les doublons
duplicates = df.index.duplicated()
if duplicates.any():
    print(f"Doublons trouvÃ©s: {duplicates.sum()}")

    # Nettoyer (garder le premier)
    df = df[~df.index.duplicated(keep='first')]

    # Resauvegarder
    adapter.manager.save_data("BTCUSDC", "1h", df, format="json")
    print("âœ… Nettoyage complÃ©tÃ©")
```

---

## 8. Performance et Optimisations

### Gap Detection Performance
- **Chargement DataFrame** : ~50-200ms (selon taille)
- **DÃ©tection gaps** : ~10-50ms
- **Total avant tÃ©lÃ©chargement** : ~100-300ms

### Merge and Save Performance
- **Concat + deduplicate** : ~50-150ms
- **Validation** : ~20-50ms
- **Sauvegarde JSON** : ~100-500ms (selon taille)
- **Sauvegarde Parquet** : ~50-200ms
- **Total** : ~300-900ms pour ~1000 candles

### Optimisations

1. **Batch Downloads** : TÃ©lÃ©charger plusieurs symbols en parallÃ¨le
```python
from concurrent.futures import ThreadPoolExecutor

def download_batch(symbols):
    adapter = UnifiedDataAdapter()

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {}
        for symbol in symbols:
            gaps = adapter.get_gaps_to_download(symbol, "1h")
            if gaps:
                future = executor.submit(download_ohlcv, symbol, "1h")
                futures[symbol] = future

        for symbol, future in futures.items():
            candles = future.result()
            if candles:
                adapter.merge_and_save(symbol, "1h", candles)
```

2. **Cache DataFrame** : Ã‰viter de recharger si dÃ©jÃ  chargÃ©
```python
cache = {}

def load_with_cache(symbol, interval):
    key = f"{symbol}/{interval}"
    if key not in cache:
        cache[key] = adapter.load_existing_candles(symbol, interval, as_dataframe=True)
    return cache[key]
```

---

## 9. Architecture RÃ©sumÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  unified_data_historique_with_...py     â”‚
â”‚  (Programme de tÃ©lÃ©chargement)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“ importe
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UnifiedDataAdapter                     â”‚
â”‚  - get_gaps_to_download()               â”‚
â”‚  - load_existing_candles()              â”‚
â”‚  - merge_and_save()                     â”‚
â”‚  - validate_candles()                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“ utilise
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataCompatibilityManager               â”‚
â”‚  - get_missing_time_ranges()            â”‚
â”‚  - load_existing_data()                 â”‚
â”‚  - merge_with_existing()                â”‚
â”‚  - validate_data()                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“ gÃ¨re
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  D:\TradXPro\best_token_DataFrame\      â”‚
â”‚  â”œâ”€â”€ BTCUSDC_1h.json/.parquet           â”‚
â”‚  â”œâ”€â”€ ETHUSDC_1h.json/.parquet           â”‚
â”‚  â””â”€â”€ ...                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Summary

L'intÃ©gration fournit une solution complÃ¨te pour :

âœ… **Ã‰viter les doublons** : DÃ©tection gaps + dÃ©duplication timestamp
âœ… **TÃ©lÃ©chargement efficace** : Seulement les pÃ©riodes manquantes
âœ… **Format unifiÃ©** : JSON et Parquet, nomenclature cohÃ©rente
âœ… **Variables d'environnement** : DATA_FOLDER compatible
âœ… **Validation automatique** : VÃ©rification intÃ©gritÃ© post-fusion
âœ… **API simple** : 4 mÃ©thodes clÃ©s (gaps, load, merge, validate)

Le systÃ¨me est **produit et maintenable**, avec logs dÃ©taillÃ©s pour le debugging.

---

**Documentation Version**: 1.0.0
**Date**: Octobre 2025
**Framework**: ThreadX Data Compatibility Layer
