# Int√©gration ThreadX - unified_data_historique
## Guide d'Int√©gration pour √âviter les T√©l√©chargements en Doublon

---

## 1. Vue d'ensemble

Le syst√®me **UnifiedDataAdapter** permet d'int√©grer le programme de t√©l√©chargement `unified_data_historique_with_indicators.py` avec les donn√©es existantes de ThreadX.

### Objectifs
- ‚úÖ **Lecture pr√©-t√©l√©chargement** : V√©rifier les donn√©es existantes avant de t√©l√©charger
- ‚úÖ **D√©tection des gaps** : Identifier uniquement les p√©riodes manquantes
- ‚úÖ **Fusion sans doublon** : Combiner nouvelles et existantes via d√©duplication par timestamp
- ‚úÖ **Format coh√©rent** : Assurer nomenclature, structure, type identiques
- ‚úÖ **Environment alignment** : Utiliser variables d'environnement pour r√©pertoires

### Architecture

```
unified_data_historique_with_indicators.py
    ‚Üì (importe)
UnifiedDataAdapter (src/threadx/data/unified_data_adapter.py)
    ‚Üì (utilise)
DataCompatibilityManager (src/threadx/data/compatibility.py)
    ‚Üì (g√®re)
D:\TradXPro\best_token_DataFrame\
    ‚îú‚îÄ‚îÄ BTCUSDC_1h.json / BTCUSDC_1h.parquet
    ‚îú‚îÄ‚îÄ ETHUSDC_1h.json / ETHUSDC_1h.parquet
    ‚îî‚îÄ‚îÄ ...
```

---

## 2. Installation et Setup

### 2.1 Variables d'Environnement

Les deux syst√®mes utilisent la m√™me variable d'environnement :

```bash
# D√©finir la variable d'environnement (Windows)
set DATA_FOLDER=D:\TradXPro\best_token_DataFrame

# Ou en Python (avant l'importation)
import os
os.environ["DATA_FOLDER"] = r"D:\TradXPro\best_token_DataFrame"
```

### 2.2 Importer l'Adaptateur

```python
from threadx.data import UnifiedDataAdapter, create_adapter

# Option 1: Cr√©er directement
adapter = UnifiedDataAdapter()

# Option 2: Via factory function
adapter = create_adapter()

# Option 3: Avec chemin personnalis√©
adapter = UnifiedDataAdapter(data_folder="/custom/path/best_token_DataFrame")
```

---

## 3. Workflow Int√©gration - √âtape par √âtape

### 3.1 Avant le T√©l√©chargement : D√©tecter les Gaps

```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()
symbol = "BTCUSDC"
interval = "1h"

# √âtape 1: V√©rifier ce qui est manquant
gaps = adapter.get_gaps_to_download(symbol, interval, history_days=365)

print(f"Gaps d√©tect√©s: {len(gaps)} p√©riode(s)")

if not gaps:
    print(f"‚úÖ {symbol}/{interval} est compl√®tement √† jour")
else:
    print(f"üì• √Ä t√©l√©charger: {len(gaps)} gap(s)")
    for i, (start_ms, end_ms) in enumerate(gaps, 1):
        print(f"  {i}. {start_ms} ‚Üí {end_ms}")
```

### 3.2 T√©l√©charger Seulement les Gaps

**Au lieu de t√©l√©charger la plage compl√®te, t√©l√©charger uniquement les gaps :**

```python
from threadx.data import UnifiedDataAdapter
from your_download_module import download_candles_binance

adapter = UnifiedDataAdapter()
symbol = "BTCUSDC"
interval = "1h"

# R√©cup√©rer les gaps
gaps = adapter.get_gaps_to_download(symbol, interval)

if gaps:
    print(f"T√©l√©chargement des {len(gaps)} gap(s)...")

    all_new_candles = []

    for start_ms, end_ms in gaps:
        print(f"  T√©l√©chargement: {start_ms} ‚Üí {end_ms}")

        # T√©l√©charger du API (ex: Binance)
        candles = download_candles_binance(
            symbol=symbol,
            interval=interval,
            start_time=start_ms,
            end_time=end_ms
        )

        if candles:
            all_new_candles.extend(candles)
            print(f"    ‚úì {len(candles)} candles t√©l√©charg√©es")

    if all_new_candles:
        print(f"\nFusion de {len(all_new_candles)} candles avec existantes...")
        result = adapter.merge_and_save(symbol, interval, all_new_candles)
        print(f"‚úÖ Fusion compl√©t√©e: {len(result)} total")
else:
    print("Pas de gaps, donn√©es √† jour")
```

### 3.3 Charger les Donn√©es Existantes

```python
adapter = UnifiedDataAdapter()

# Format liste de dict (compatible unified_data)
candles_list = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=False)
if candles_list:
    print(f"Charg√© {len(candles_list)} candles")
    print(f"Premier: {candles_list[0]}")
    print(f"Dernier: {candles_list[-1]}")

# OU Format DataFrame (pour calculs internes)
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)
if df is not None:
    print(f"DataFrame shape: {df.shape}")
    print(f"Index: {df.index[0]} ‚Üí {df.index[-1]}")
```

### 3.4 Valider les Donn√©es

```python
adapter = UnifiedDataAdapter()

# Valider les donn√©es existantes
is_valid, message = adapter.validate_candles("BTCUSDC", "1h")

if is_valid:
    print(f"‚úÖ {message}")
else:
    print(f"‚ùå Validation √©chou√©e: {message}")
```

### 3.5 Obtenir un R√©sum√© Complet

```python
adapter = UnifiedDataAdapter()

status = adapter.get_data_status("BTCUSDC", "1h")

print(f"Statut pour {status['symbol']}/{status['interval']}:")
print(f"  Existe: {status['exists']}")
print(f"  Nombre de candles: {status['candle_count']}")

if status['exists']:
    print(f"  Plage: {status['date_range'][0]} ‚Üí {status['date_range'][1]}")
    print(f"  Premier timestamp: {status['first_timestamp']}")
    print(f"  Dernier timestamp: {status['last_timestamp']}")
    print(f"  Gaps manquants: {len(status['gaps'])}")
    print(f"  Valid√©: {status['is_valid']} - {status['validation_msg']}")
```

---

## 4. Int√©gration dans unified_data_historique_with_indicators.py

### 4.1 Modification de `download_ohlcv()`

**Avant (t√©l√©charge tout) :**
```python
def download_ohlcv(symbol: str, interval: str, start_date: str, end_date: str):
    # ... convertir dates en timestamps
    # T√©l√©charger TOUTE la plage du API
    candles = binance_client.get_historical_klines(symbol, interval, start_time=start_ms, end_time=end_ms)
    return candles
```

**Apr√®s (t√©l√©charge uniquement les gaps) :**
```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

def download_ohlcv(symbol: str, interval: str, start_date: str = None, end_date: str = None):
    """
    T√©l√©charge OHLCV mais seulement pour les gaps d√©tect√©s.

    Si start_date/end_date sont fournis, utiliser ces limites.
    Sinon, utiliser la plage par d√©faut (last 365 days).
    """

    # D√©terminer history_days bas√© sur start_date
    if start_date:
        history_days = calculate_days_between(start_date, end_date)
    else:
        history_days = 365  # d√©faut

    # √âtape 1: R√©cup√©rer les gaps existants
    gaps = adapter.get_gaps_to_download(symbol, interval, history_days)

    logger.info(f"Pour {symbol}/{interval}: {len(gaps)} gap(s) √† t√©l√©charger")

    if not gaps:
        logger.info(f"‚úÖ {symbol}/{interval} est √† jour, pas de t√©l√©chargement")
        return None

    # √âtape 2: T√©l√©charger SEULEMENT les gaps
    all_candles = []

    for start_ms, end_ms in gaps:
        logger.info(f"T√©l√©chargement gap: {start_ms} ‚Üí {end_ms}")

        try:
            # T√©l√©charger cette p√©riode sp√©cifique du API
            candles = binance_client.get_historical_klines(
                symbol=symbol,
                interval=interval,
                start_time=start_ms,
                end_time=end_ms
            )

            if candles:
                all_candles.extend(candles)
                logger.info(f"  ‚úì {len(candles)} candles t√©l√©charg√©es")

        except Exception as e:
            logger.error(f"Erreur t√©l√©chargement gap {start_ms}‚Üí{end_ms}: {e}")
            # Continuer avec le prochain gap
            continue

    return all_candles if all_candles else None
```

### 4.2 Modification de `verify_and_complete()`

**Avant (utilise logique locale) :**
```python
def verify_and_complete():
    # ... logique manuelle de v√©rification
    for symbol in symbols:
        existing = load_from_local_cache(symbol)
        if not existing:
            # T√©l√©charger tout
            pass
```

**Apr√®s (utilise l'adaptateur) :**
```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

def verify_and_complete():
    """
    V√©rifie et compl√®te les donn√©es pour tous les symboles.
    Utilise UnifiedDataAdapter pour √©viter les doublons.
    """

    symbols = ["BTCUSDC", "ETHUSDC", "ADAUSDC"]  # Vos symboles
    interval = "1h"

    for symbol in symbols:
        logger.info(f"V√©rification: {symbol}/{interval}")

        # √âtape 1: V√©rifier l'√©tat des donn√©es
        status = adapter.get_data_status(symbol, interval)

        if status['exists']:
            logger.info(f"  Donn√©es existantes: {status['candle_count']} candles")
            logger.info(f"  Plage: {status['date_range'][0]} ‚Üí {status['date_range'][1]}")

            if status['gaps']:
                logger.info(f"  Gaps d√©tect√©s: {len(status['gaps'])}")
            else:
                logger.info(f"  Aucun gap: donn√©es compl√®tes")
                continue
        else:
            logger.info(f"  Aucune donn√©e existante: t√©l√©chargement complet requis")

        # √âtape 2: T√©l√©charger et fusionner
        new_candles = download_ohlcv(symbol, interval)

        if new_candles:
            logger.info(f"  Fusion en cours...")
            result = adapter.merge_and_save(symbol, interval, new_candles)

            if result:
                logger.info(f"  ‚úÖ Total: {len(result)} candles")
            else:
                logger.error(f"  ‚ùå Erreur fusion")
```

### 4.3 Modification de la Conversion JSON‚ÜíParquet

**Avant :**
```python
def convert_to_parquet(symbol: str, interval: str):
    # Charger JSON
    df = pd.read_json(f"{symbol}_{interval}.json")
    # Convertir et sauvegarder Parquet
    df.to_parquet(f"{symbol}_{interval}.parquet")
```

**Apr√®s (utilise le format unifi√©) :**
```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

def convert_to_parquet(symbol: str, interval: str):
    """
    Convertit JSON‚ÜíParquet pour un symbole/intervalle.
    Utilise le syst√®me de normalisation ThreadX.
    """

    # Charger les donn√©es actuelles
    df = adapter.load_existing_candles(symbol, interval, as_dataframe=True)

    if df is None:
        logger.warning(f"Aucune donn√©e pour {symbol}/{interval}")
        return

    # Sauvegarder au format Parquet (automatiquement normalis√©)
    saved_path = adapter.manager.save_data(symbol, interval, df, format="parquet")

    if saved_path:
        logger.info(f"Converti ‚Üí Parquet: {saved_path}")

        # Optionnel: Supprimer l'ancien JSON
        json_path = adapter.manager.data_folder / f"{symbol}_{interval}.json"
        if json_path.exists():
            json_path.unlink()
            logger.info(f"JSON supprim√©: {json_path}")
```

### 4.4 Ajout de Flags CLI

**Modification du main :**
```python
import argparse
from threadx.data import UnifiedDataAdapter

def main():
    parser = argparse.ArgumentParser()

    # Flags existants
    parser.add_argument("--symbols", default="BTCUSDC,ETHUSDC")
    parser.add_argument("--interval", default="1h")

    # NOUVEAUX flags pour la compatibilit√© ThreadX
    parser.add_argument(
        "--compatible-mode",
        action="store_true",
        help="Utiliser le mode compatible (√©vite doublons, lit avant de t√©l√©charger)"
    )
    parser.add_argument(
        "--validate-after-download",
        action="store_true",
        help="Valider les donn√©es apr√®s t√©l√©chargement"
    )
    parser.add_argument(
        "--convert-to-parquet",
        action="store_true",
        help="Convertir JSON‚ÜíParquet apr√®s fusion"
    )
    parser.add_argument(
        "--data-folder",
        type=str,
        help="Chemin personnalis√© au dossier de donn√©es (d√©faut: DATA_FOLDER env)"
    )

    args = parser.parse_args()

    # Initialiser l'adaptateur
    adapter = UnifiedDataAdapter(data_folder=args.data_folder)

    symbols = args.symbols.split(",")
    interval = args.interval

    for symbol in symbols:
        symbol = symbol.strip()

        if args.compatible_mode:
            # Mode compatib : utiliser l'adaptateur
            logger.info(f"\nüì• Mode compatible: {symbol}/{interval}")

            gaps = adapter.get_gaps_to_download(symbol, interval)

            if gaps:
                new_candles = download_ohlcv(symbol, interval)
                if new_candles:
                    result = adapter.merge_and_save(symbol, interval, new_candles)
                    logger.info(f"‚úÖ Fusionn√©: {len(result)} total")

            # Validation optionnelle
            if args.validate_after_download:
                is_valid, msg = adapter.validate_candles(symbol, interval)
                logger.info(f"Validation: {msg}")

            # Conversion optionnelle
            if args.convert_to_parquet:
                convert_to_parquet(symbol, interval)
        else:
            # Mode standard (ancien comportement)
            download_all_and_save(symbol, interval)

if __name__ == "__main__":
    main()
```

**Utilisation :**
```bash
# Mode compatible avec validation et conversion
python unified_data_historique_with_indicators.py \
    --symbols BTCUSDC,ETHUSDC \
    --interval 1h \
    --compatible-mode \
    --validate-after-download \
    --convert-to-parquet

# Avec chemin personnalis√©
python unified_data_historique_with_indicators.py \
    --compatible-mode \
    --data-folder /custom/path/best_token_DataFrame
```

---

## 5. Formats de Donn√©es

### 5.1 Format Liste de Dict (unified_data)

```python
# Format retourn√© par load_existing_candles(as_dataframe=False)
# et attendu par merge_and_save()

candles = [
    {
        "timestamp": 1697289600000,  # ms Unix timestamp
        "open": "27500.00",          # string (peut aussi √™tre float)
        "high": "27650.50",
        "low": "27450.00",
        "close": "27600.25",
        "volume": "125.5",           # volume en token/satoshi
    },
    {
        "timestamp": 1697293200000,
        "open": "27600.25",
        # ...
    }
]
```

### 5.2 Format DataFrame (interne ThreadX)

```python
# Format utilis√© en interne par load_existing_candles(as_dataframe=True)

# Index: DatetimeIndex en UTC
# Colonnes: open, high, low, close, volume (tous float)

#                     open      high       low     close    volume
# timestamp
# 2023-10-14 16:00 27500.00 27650.500 27450.00 27600.25  125.5000
# 2023-10-14 17:00 27600.25 27700.000 27550.00 27650.75  135.2500
# 2023-10-14 18:00 27650.75 27800.000 27600.00 27750.00  140.0000
```

### 5.3 Nomenclature des Fichiers

```
D:\TradXPro\best_token_DataFrame\
‚îú‚îÄ‚îÄ BTCUSDC_1h.json          # ou .parquet
‚îú‚îÄ‚îÄ BTCUSDC_5m.json
‚îú‚îÄ‚îÄ BTCUSDC_1d.json
‚îú‚îÄ‚îÄ ETHUSDC_1h.json
‚îú‚îÄ‚îÄ ETHUSDC_5m.json
‚îî‚îÄ‚îÄ ...
```

Patterns support√©s :
- `{SYMBOL}_{INTERVAL}.json`
- `{SYMBOL}_{INTERVAL}.parquet`

Exemple : `BTCUSDC_1h`, `ETHUSDC_5m`, `ADAUSDC_1d`

---

## 6. Cas d'Usage Courants

### 6.1 Mise √† Jour Quotidienne (Cron Job)

```python
import schedule
import time
from threadx.data import UnifiedDataAdapter

def daily_update():
    """T√©l√©charge et fusionne les donn√©es manquantes chaque jour."""

    adapter = UnifiedDataAdapter()
    symbols = ["BTCUSDC", "ETHUSDC", "ADAUSDC"]
    interval = "1h"

    for symbol in symbols:
        try:
            gaps = adapter.get_gaps_to_download(symbol, interval)

            if gaps:
                print(f"Mise √† jour {symbol}: {len(gaps)} gap(s)")
                new_candles = download_ohlcv(symbol, interval)

                if new_candles:
                    result = adapter.merge_and_save(symbol, interval, new_candles)
                    print(f"‚úÖ {len(result)} candles total")
            else:
                print(f"‚úÖ {symbol} √† jour")

        except Exception as e:
            print(f"‚ùå Erreur {symbol}: {e}")
            # Continuer avec le suivant

# Schedule tous les jours √† 00:00
schedule.every().day.at("00:00").do(daily_update)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### 6.2 Chargement de Donn√©es pour Backtesting

```python
from threadx.data import UnifiedDataAdapter
from threadx.strategy import AmplitudeHunterStrategy

adapter = UnifiedDataAdapter()

# Charger les donn√©es
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)

if df is not None:
    # Valider
    is_valid, msg = adapter.validate_candles("BTCUSDC", "1h")
    print(f"Validation: {msg}")

    if is_valid:
        # Utiliser avec une strat√©gie
        strategy = AmplitudeHunterStrategy("BTCUSDC", "1h")
        signals, stats = strategy.backtest(df, params_dict, initial_capital=10000)
        print(f"Sharpe: {stats.sharpe_ratio:.3f}")
```

### 6.3 Migration depuis JSON vers Parquet

```python
from threadx.data import UnifiedDataAdapter

adapter = UnifiedDataAdapter()

# Lister tous les fichiers JSON
json_files = [f for f in adapter.list_available_files() if f.endswith(".json")]

print(f"Migration de {len(json_files)} fichiers JSON vers Parquet...")

for filename in json_files:
    # Extraire symbol et interval
    symbol, interval = filename.replace(".json", "").rsplit("_", 1)

    # Charger et convertir
    df = adapter.load_existing_candles(symbol, interval, as_dataframe=True)

    if df is not None:
        saved = adapter.manager.save_data(symbol, interval, df, format="parquet")
        print(f"‚úÖ {symbol}/{interval}: {saved}")
```

---

## 7. Troubleshooting

### 7.1 "Fichier de donn√©es non trouv√©"

```
‚ö†Ô∏è DataCompatibilityManager initialis√©: D:\TradXPro\best_token_DataFrame
‚ùå Aucune donn√©e existante pour BTCUSDC/1h
```

**Solution :**
- V√©rifier que `DATA_FOLDER` pointe vers le bon r√©pertoire
- V√©rifier que les fichiers existent avec la bonne nomenclature
- V√©rifier les permissions d'acc√®s

```python
import os
from pathlib import Path

folder = os.environ.get("DATA_FOLDER")
print(f"DATA_FOLDER: {folder}")

path = Path(folder)
if path.exists():
    files = list(path.glob("*.json")) + list(path.glob("*.parquet"))
    print(f"Fichiers trouv√©s: {len(files)}")
    for f in files[:5]:
        print(f"  - {f.name}")
```

### 7.2 "Validation √©chou√©e: NaN dans colonnes critiques"

Les donn√©es contiennent des valeurs manquantes.

**Solution :**
```python
adapter = UnifiedDataAdapter()
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)

# V√©rifier les NaN
print(df.isna().sum())

# Supprimer les NaN
df = df.dropna()

# Resauvegarder
adapter.manager.save_data("BTCUSDC", "1h", df, format="json")
```

### 7.3 "Gaps d√©tect√©s vides"

```
get_gaps_to_download("BTCUSDC", "1h") retourne []
```

**Cause :**
- Les donn√©es existantes couvrent toute la plage requise
- C'est normal ! Pas besoin de t√©l√©charger

**V√©rifier :**
```python
status = adapter.get_data_status("BTCUSDC", "1h")
print(f"Plage: {status['date_range']}")
print(f"Gaps: {status['gaps']}")
```

### 7.4 "Deduplication issue: doublon timestamps"

Si les donn√©es fusionn√©es ont des doublons apr√®s merge_and_save():

```python
adapter = UnifiedDataAdapter()
df = adapter.load_existing_candles("BTCUSDC", "1h", as_dataframe=True)

# V√©rifier les doublons
duplicates = df.index.duplicated()
if duplicates.any():
    print(f"Doublons trouv√©s: {duplicates.sum()}")

    # Nettoyer (garder le premier)
    df = df[~df.index.duplicated(keep='first')]

    # Resauvegarder
    adapter.manager.save_data("BTCUSDC", "1h", df, format="json")
    print("‚úÖ Nettoyage compl√©t√©")
```

---

## 8. Performance et Optimisations

### Gap Detection Performance
- **Chargement DataFrame** : ~50-200ms (selon taille)
- **D√©tection gaps** : ~10-50ms
- **Total avant t√©l√©chargement** : ~100-300ms

### Merge and Save Performance
- **Concat + deduplicate** : ~50-150ms
- **Validation** : ~20-50ms
- **Sauvegarde JSON** : ~100-500ms (selon taille)
- **Sauvegarde Parquet** : ~50-200ms
- **Total** : ~300-900ms pour ~1000 candles

### Optimisations

1. **Batch Downloads** : T√©l√©charger plusieurs symbols en parall√®le
```python
from concurrent.futures import ThreadPoolExecutor

def download_batch(symbols):
    adapter = UnifiedDataAdapter()

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {}
        for symbol in symbols:
            gaps = adapter.get_gaps_to_download(symbol, "1h")
            if gaps:
                future = executor.submit(download_ohlcv, symbol, "1h")
                futures[symbol] = future

        for symbol, future in futures.items():
            candles = future.result()
            if candles:
                adapter.merge_and_save(symbol, "1h", candles)
```

2. **Cache DataFrame** : √âviter de recharger si d√©j√† charg√©
```python
cache = {}

def load_with_cache(symbol, interval):
    key = f"{symbol}/{interval}"
    if key not in cache:
        cache[key] = adapter.load_existing_candles(symbol, interval, as_dataframe=True)
    return cache[key]
```

---

## 9. Architecture R√©sum√©e

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  unified_data_historique_with_...py     ‚îÇ
‚îÇ  (Programme de t√©l√©chargement)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì importe
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  UnifiedDataAdapter                     ‚îÇ
‚îÇ  - get_gaps_to_download()               ‚îÇ
‚îÇ  - load_existing_candles()              ‚îÇ
‚îÇ  - merge_and_save()                     ‚îÇ
‚îÇ  - validate_candles()                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì utilise
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DataCompatibilityManager               ‚îÇ
‚îÇ  - get_missing_time_ranges()            ‚îÇ
‚îÇ  - load_existing_data()                 ‚îÇ
‚îÇ  - merge_with_existing()                ‚îÇ
‚îÇ  - validate_data()                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì g√®re
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  D:\TradXPro\best_token_DataFrame\      ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ BTCUSDC_1h.json/.parquet           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ ETHUSDC_1h.json/.parquet           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ ...                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 10. Summary

L'int√©gration fournit une solution compl√®te pour :

‚úÖ **√âviter les doublons** : D√©tection gaps + d√©duplication timestamp
‚úÖ **T√©l√©chargement efficace** : Seulement les p√©riodes manquantes
‚úÖ **Format unifi√©** : JSON et Parquet, nomenclature coh√©rente
‚úÖ **Variables d'environnement** : DATA_FOLDER compatible
‚úÖ **Validation automatique** : V√©rification int√©grit√© post-fusion
‚úÖ **API simple** : 4 m√©thodes cl√©s (gaps, load, merge, validate)

Le syst√®me est **produit et maintenable**, avec logs d√©taill√©s pour le debugging.

---

**Documentation Version**: 1.0.0
**Date**: Octobre 2025
**Framework**: ThreadX Data Compatibility Layer
