"""
ThreadX Backtest Validation Module
===================================

Module de validation anti-overfitting pour backtests robustes.
Impl√©mente walk-forward validation, train/test split, et d√©tection de biais temporels.

Features:
- Walk-forward optimization avec fen√™tres glissantes
- Train/test split avec purge et embargo
- D√©tection automatique de look-ahead bias
- Calcul de ratio d'overfitting
- V√©rification d'int√©grit√© temporelle des donn√©es
- Support pour validation k-fold sur s√©ries temporelles

Author: ThreadX Framework - Quality Initiative Phase 2
Version: 1.0.0 - Anti-Overfitting Validation
"""

from dataclasses import dataclass
from datetime import datetime, timedelta
from threadx.utils.common_imports import (
    pd,
    np,
    List,
    Tuple,
    Optional,
    Callable,
    Dict,
    Any,
    create_logger,
)

logger = create_logger(__name__)


@dataclass
class ValidationConfig:
    """
    Configuration pour validation de backtest.

    Attributes:
        method: M√©thode de validation ('walk_forward', 'train_test', 'k_fold')
        train_ratio: Ratio de donn√©es pour training (0.0 √† 1.0)
        test_ratio: Ratio de donn√©es pour testing (0.0 √† 1.0)
        walk_forward_windows: Nombre de fen√™tres pour walk-forward
        purge_days: Nombre de jours √† purger entre train et test
        embargo_days: Nombre de jours d'embargo apr√®s test
        min_train_samples: Nombre minimum de samples pour training
        min_test_samples: Nombre minimum de samples pour testing

    Notes:
        - train_ratio + test_ratio doit √™tre <= 1.0
        - purge_days pr√©vient le data leakage entre train/test
        - embargo_days simule le d√©lai de traitement r√©el
    """

    method: str = "walk_forward"
    train_ratio: float = 0.7
    test_ratio: float = 0.3
    walk_forward_windows: int = 5
    purge_days: int = 0
    embargo_days: int = 0
    min_train_samples: int = 100
    min_test_samples: int = 50

    def __post_init__(self):
        """Validation de la configuration."""
        if self.method not in ["walk_forward", "train_test", "k_fold"]:
            raise ValueError(
                f"method doit √™tre 'walk_forward', 'train_test' ou 'k_fold', "
                f"re√ßu: {self.method}"
            )

        if not 0 < self.train_ratio <= 1.0:
            raise ValueError(
                f"train_ratio doit √™tre entre 0 et 1, re√ßu: {self.train_ratio}"
            )

        if not 0 < self.test_ratio <= 1.0:
            raise ValueError(
                f"test_ratio doit √™tre entre 0 et 1, re√ßu: {self.test_ratio}"
            )

        if self.train_ratio + self.test_ratio > 1.0:
            raise ValueError(
                f"train_ratio + test_ratio > 1.0: "
                f"{self.train_ratio} + {self.test_ratio} = "
                f"{self.train_ratio + self.test_ratio}"
            )

        if self.purge_days < 0 or self.embargo_days < 0:
            raise ValueError("purge_days et embargo_days doivent √™tre >= 0")


class BacktestValidator:
    """
    Validateur pour backtests avec protection contre overfitting.

    Impl√©mente diff√©rentes strat√©gies de validation pour s√©ries temporelles:
    - Walk-forward: Fen√™tres glissantes train/test
    - Train/test split: Split simple avec purge
    - K-fold: Cross-validation adapt√©e aux s√©ries temporelles

    Examples:
        >>> config = ValidationConfig(method="walk_forward", walk_forward_windows=5)
        >>> validator = BacktestValidator(config)
        >>> windows = validator.walk_forward_split(data)
        >>> for train, test in windows:
        ...     # Entra√Æner sur train, valider sur test
        ...     pass
    """

    def __init__(self, config: ValidationConfig):
        """
        Initialise le validateur.

        Parameters:
            config: Configuration de validation
        """
        self.config = config
        self.validation_results: List[Dict[str, Any]] = []
        logger.info(
            f"BacktestValidator initialis√©: method={config.method}, "
            f"windows={config.walk_forward_windows}, "
            f"train_ratio={config.train_ratio}, "
            f"purge={config.purge_days}d, embargo={config.embargo_days}d"
        )

    def walk_forward_split(
        self, data: pd.DataFrame, n_windows: Optional[int] = None
    ) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
        """
        G√©n√®re des fen√™tres walk-forward pour validation.

        Walk-forward optimization divise les donn√©es en fen√™tres successives
        o√π chaque fen√™tre contient:
        - Train: Donn√©es historiques jusqu'au point de split
        - Test: Donn√©es futures apr√®s le point de split (+ purge)

        Cette m√©thode pr√©vient le look-ahead bias et simule le trading r√©el
        o√π on ne conna√Æt que le pass√©.

        Parameters:
            data: DataFrame avec index DatetimeIndex
            n_windows: Nombre de fen√™tres (None = utilise config)

        Returns:
            Liste de tuples (train_data, test_data)

        Raises:
            ValueError: Si donn√©es insuffisantes ou index non temporel

        Examples:
            >>> windows = validator.walk_forward_split(df, n_windows=5)
            >>> print(f"G√©n√©r√© {len(windows)} fen√™tres")
            >>> train, test = windows[0]
            >>> print(f"Train: {len(train)} rows, Test: {len(test)} rows")
        """
        # Validation des entr√©es
        check_temporal_integrity(data)

        n_windows = n_windows or self.config.walk_forward_windows
        total_len = len(data)

        if total_len < n_windows * (
            self.config.min_train_samples + self.config.min_test_samples
        ):
            raise ValueError(
                f"Donn√©es insuffisantes ({total_len} rows) pour {n_windows} fen√™tres. "
                f"Minimum requis: {n_windows * (self.config.min_train_samples + self.config.min_test_samples)}"
            )

        windows = []
        # Taille de base pour chaque fen√™tre
        window_size = total_len // (n_windows + 1)

        logger.info(
            f"G√©n√©ration de {n_windows} fen√™tres walk-forward "
            f"(window_size={window_size}, total={total_len})"
        )

        for i in range(n_windows):
            # Point de split pour cette fen√™tre
            train_end_idx = (i + 1) * window_size

            # Appliquer purge (skip jours apr√®s train)
            test_start_idx = train_end_idx
            if self.config.purge_days > 0:
                purge_date = data.index[train_end_idx] + pd.Timedelta(
                    days=self.config.purge_days
                )
                test_start_idx = data.index.get_indexer([purge_date], method="nearest")[
                    0
                ]

            # Calculer fin du test avec embargo
            test_end_idx = train_end_idx + window_size
            if self.config.embargo_days > 0:
                test_end_idx = max(
                    test_start_idx + self.config.min_test_samples,
                    test_end_idx - self.config.embargo_days,
                )

            # V√©rifier qu'on a assez de donn√©es
            if test_end_idx > total_len:
                logger.warning(
                    f"Fen√™tre {i+1} d√©passe les donn√©es disponibles, "
                    f"ajustement √† {total_len}"
                )
                test_end_idx = total_len

            if test_end_idx <= test_start_idx + self.config.min_test_samples:
                logger.warning(f"Fen√™tre {i+1} test set trop petit, skip")
                continue

            # Extraire les donn√©es
            train_data = data.iloc[:train_end_idx].copy()
            test_data = data.iloc[test_start_idx:test_end_idx].copy()

            # V√©rification anti-lookahead CRITIQUE
            if not train_data.index.max() < test_data.index.min():
                raise ValueError(
                    f"‚ùå LOOK-AHEAD BIAS D√âTECT√â dans fen√™tre {i+1}!\n"
                    f"Train max: {train_data.index.max()}\n"
                    f"Test min: {test_data.index.min()}\n"
                    f"Les dates train/test se chevauchent!"
                )

            # V√©rifier tailles minimales
            if len(train_data) < self.config.min_train_samples:
                logger.warning(
                    f"Fen√™tre {i+1} train set trop petit "
                    f"({len(train_data)} < {self.config.min_train_samples}), skip"
                )
                continue

            if len(test_data) < self.config.min_test_samples:
                logger.warning(
                    f"Fen√™tre {i+1} test set trop petit "
                    f"({len(test_data)} < {self.config.min_test_samples}), skip"
                )
                continue

            windows.append((train_data, test_data))

            logger.debug(
                f"Fen√™tre {i+1}/{n_windows}: "
                f"train={len(train_data)} rows "
                f"[{train_data.index.min()} ‚Üí {train_data.index.max()}], "
                f"test={len(test_data)} rows "
                f"[{test_data.index.min()} ‚Üí {test_data.index.max()}]"
            )

        if not windows:
            raise ValueError(
                "Aucune fen√™tre valide g√©n√©r√©e. "
                "V√©rifier config.min_train_samples et config.min_test_samples"
            )

        logger.info(f"‚úÖ {len(windows)} fen√™tres walk-forward g√©n√©r√©es avec succ√®s")
        return windows

    def train_test_split(
        self, data: pd.DataFrame, train_ratio: Optional[float] = None
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Split simple train/test avec purge et embargo.

        Divise les donn√©es en deux ensembles chronologiques:
        - Train: Premi√®res train_ratio% des donn√©es
        - Test: Derni√®res donn√©es apr√®s purge

        Parameters:
            data: DataFrame avec index DatetimeIndex
            train_ratio: Ratio train (None = utilise config)

        Returns:
            Tuple (train_data, test_data)

        Raises:
            ValueError: Si donn√©es insuffisantes ou look-ahead d√©tect√©

        Examples:
            >>> train, test = validator.train_test_split(df, train_ratio=0.7)
            >>> print(f"Train: {len(train)}, Test: {len(test)}")
        """
        # Validation des entr√©es
        check_temporal_integrity(data)

        train_ratio = train_ratio or self.config.train_ratio

        # Calculer l'index de split
        split_idx = int(len(data) * train_ratio)

        # Appliquer purge
        purge_idx = split_idx
        if self.config.purge_days > 0:
            purge_date = data.index[split_idx] + pd.Timedelta(
                days=self.config.purge_days
            )
            purge_idx = data.index.get_indexer([purge_date], method="nearest")[0]

        # Appliquer embargo
        end_idx = len(data)
        if self.config.embargo_days > 0:
            embargo_date = data.index[-1] - pd.Timedelta(days=self.config.embargo_days)
            end_idx = data.index.get_indexer([embargo_date], method="nearest")[0]

        # Extraire les donn√©es
        train_data = data.iloc[:split_idx].copy()
        test_data = data.iloc[purge_idx:end_idx].copy()

        # V√©rification anti-lookahead CRITIQUE
        if not train_data.index.max() < test_data.index.min():
            raise ValueError(
                f"‚ùå LOOK-AHEAD BIAS D√âTECT√â!\n"
                f"Train max: {train_data.index.max()}\n"
                f"Test min: {test_data.index.min()}"
            )

        # V√©rifier tailles minimales
        if len(train_data) < self.config.min_train_samples:
            raise ValueError(
                f"Train set trop petit: {len(train_data)} < {self.config.min_train_samples}"
            )

        if len(test_data) < self.config.min_test_samples:
            raise ValueError(
                f"Test set trop petit: {len(test_data)} < {self.config.min_test_samples}"
            )

        logger.info(
            f"‚úÖ Train/test split: train={len(train_data)} rows "
            f"[{train_data.index.min()} ‚Üí {train_data.index.max()}], "
            f"test={len(test_data)} rows "
            f"[{test_data.index.min()} ‚Üí {test_data.index.max()}], "
            f"purge={self.config.purge_days}d, embargo={self.config.embargo_days}d"
        )

        return train_data, test_data

    def validate_backtest(
        self, backtest_func: Callable, data: pd.DataFrame, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute backtest avec validation anti-overfitting.

        Applique la m√©thode de validation configur√©e et retourne les r√©sultats
        in-sample et out-of-sample avec calcul du ratio d'overfitting.

        Parameters:
            backtest_func: Fonction de backtest √† valider
                          Signature: func(data, params) -> dict avec 'sharpe_ratio', etc.
            data: Donn√©es compl√®tes
            params: Param√®tres du backtest

        Returns:
            Dict avec:
                - method: M√©thode utilis√©e
                - n_windows: Nombre de fen√™tres (si walk_forward)
                - in_sample: R√©sultats in-sample agr√©g√©s
                - out_sample: R√©sultats out-of-sample agr√©g√©s
                - overfitting_ratio: Ratio d'overfitting
                - recommendation: Recommandation bas√©e sur le ratio

        Raises:
            ValueError: Si m√©thode inconnue

        Examples:
            >>> def my_backtest(data, params):
            ...     # Votre logique de backtest
            ...     return {'sharpe_ratio': 1.5, 'total_return': 0.25}
            >>>
            >>> results = validator.validate_backtest(my_backtest, df, {'sma': 20})
            >>> print(f"Overfitting ratio: {results['overfitting_ratio']:.2f}")
        """
        logger.info(f"üîç Validation backtest avec m√©thode: {self.config.method}")

        if self.config.method == "walk_forward":
            return self._validate_walk_forward(backtest_func, data, params)
        elif self.config.method == "train_test":
            return self._validate_train_test(backtest_func, data, params)
        else:
            raise ValueError(f"M√©thode non impl√©ment√©e: {self.config.method}")

    def _validate_walk_forward(
        self, backtest_func: Callable, data: pd.DataFrame, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validation walk-forward interne."""
        windows = self.walk_forward_split(data)

        in_sample_results = []
        out_sample_results = []

        logger.info(f"Ex√©cution de {len(windows)} fen√™tres walk-forward...")

        for i, (train, test) in enumerate(windows, 1):
            logger.debug(
                f"Fen√™tre {i}/{len(windows)}: train={len(train)}, test={len(test)}"
            )

            try:
                # Backtest in-sample (pour optimisation)
                train_result = backtest_func(train, params)
                in_sample_results.append(train_result)

                # Backtest out-of-sample (validation)
                test_result = backtest_func(test, params)
                out_sample_results.append(test_result)

                logger.debug(
                    f"Fen√™tre {i}: "
                    f"IS Sharpe={train_result.get('sharpe_ratio', 'N/A'):.2f}, "
                    f"OOS Sharpe={test_result.get('sharpe_ratio', 'N/A'):.2f}"
                )

            except Exception as e:
                logger.error(f"Erreur fen√™tre {i}: {e}")
                continue

        # Agr√©ger r√©sultats
        in_sample_agg = self._aggregate_results(in_sample_results)
        out_sample_agg = self._aggregate_results(out_sample_results)

        # Calculer ratio d'overfitting
        overfitting_ratio = self._calculate_overfitting_ratio(
            in_sample_results, out_sample_results
        )

        # Recommandation
        recommendation = self._get_recommendation(overfitting_ratio)

        result = {
            "method": "walk_forward",
            "n_windows": len(windows),
            "in_sample": in_sample_agg,
            "out_sample": out_sample_agg,
            "overfitting_ratio": overfitting_ratio,
            "recommendation": recommendation,
        }

        logger.info(
            f"‚úÖ Validation walk-forward termin√©e: "
            f"IS Sharpe={in_sample_agg.get('mean_sharpe_ratio', 0):.2f}, "
            f"OOS Sharpe={out_sample_agg.get('mean_sharpe_ratio', 0):.2f}, "
            f"Overfitting Ratio={overfitting_ratio:.2f}"
        )

        self.validation_results.append(result)
        return result

    def _validate_train_test(
        self, backtest_func: Callable, data: pd.DataFrame, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validation train/test simple interne."""
        train, test = self.train_test_split(data)

        logger.info("Ex√©cution train/test split...")

        # Backtest sur train
        train_result = backtest_func(train, params)

        # Backtest sur test
        test_result = backtest_func(test, params)

        # Calculer ratio d'overfitting
        overfitting_ratio = self._calculate_overfitting_ratio(
            [train_result], [test_result]
        )

        # Recommandation
        recommendation = self._get_recommendation(overfitting_ratio)

        result = {
            "method": "train_test",
            "in_sample": train_result,
            "out_sample": test_result,
            "overfitting_ratio": overfitting_ratio,
            "recommendation": recommendation,
        }

        logger.info(
            f"‚úÖ Validation train/test termin√©e: "
            f"IS Sharpe={train_result.get('sharpe_ratio', 0):.2f}, "
            f"OOS Sharpe={test_result.get('sharpe_ratio', 0):.2f}, "
            f"Overfitting Ratio={overfitting_ratio:.2f}"
        )

        self.validation_results.append(result)
        return result

    def _aggregate_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Agr√®ge r√©sultats multiples en moyennes et √©carts-types.

        Parameters:
            results: Liste de dictionnaires de r√©sultats

        Returns:
            Dict avec mean_* et std_* pour chaque m√©trique
        """
        if not results:
            return {}

        # M√©triques √† agr√©ger
        metrics = [
            "sharpe_ratio",
            "total_return",
            "max_drawdown",
            "win_rate",
            "profit_factor",
        ]
        aggregated = {}

        for metric in metrics:
            values = [r.get(metric, np.nan) for r in results]
            # Filtrer NaN
            values = [v for v in values if not np.isnan(v)]

            if values:
                aggregated[f"mean_{metric}"] = np.mean(values)
                aggregated[f"std_{metric}"] = np.std(values)
                aggregated[f"min_{metric}"] = np.min(values)
                aggregated[f"max_{metric}"] = np.max(values)

        aggregated["n_results"] = len(results)

        return aggregated

    def _calculate_overfitting_ratio(
        self, in_sample: List[Dict[str, Any]], out_sample: List[Dict[str, Any]]
    ) -> float:
        """
        Calcule ratio d'overfitting bas√© sur Sharpe ratios.

        Ratio proche de 1.0 = performances similaires IS/OOS (bon)
        Ratio >> 1.0 = overfitting probable (mauvais)
        Ratio < 1.0 = OOS meilleur que IS (excellent mais rare)

        Parameters:
            in_sample: R√©sultats in-sample
            out_sample: R√©sultats out-of-sample

        Returns:
            Ratio d'overfitting (IS_sharpe / OOS_sharpe)
        """
        in_agg = self._aggregate_results(in_sample)
        out_agg = self._aggregate_results(out_sample)

        in_sharpe = in_agg.get("mean_sharpe_ratio", 0)
        out_sharpe = out_agg.get("mean_sharpe_ratio", 0)

        if out_sharpe == 0 or np.isnan(out_sharpe):
            logger.warning("OOS Sharpe = 0 ou NaN, ratio d'overfitting = inf")
            return float("inf")

        ratio = abs(in_sharpe / out_sharpe)

        return ratio

    def _get_recommendation(self, overfitting_ratio: float) -> str:
        """
        G√©n√®re recommandation bas√©e sur ratio d'overfitting.

        Parameters:
            overfitting_ratio: Ratio calcul√©

        Returns:
            String de recommandation
        """
        if overfitting_ratio < 1.2:
            return (
                "‚úÖ EXCELLENT: Performances robustes, pas d'overfitting d√©tect√©. "
                "Strat√©gie valid√©e pour production."
            )
        elif overfitting_ratio < 1.5:
            return (
                "‚ö†Ô∏è ACCEPTABLE: L√©ger overfitting d√©tect√©. "
                "Consid√©rer simplification des param√®tres ou augmentation des donn√©es."
            )
        elif overfitting_ratio < 2.0:
            return (
                "üü° ATTENTION: Overfitting mod√©r√©. "
                "R√©duire nombre de param√®tres optimis√©s, utiliser r√©gularisation, "
                "ou augmenter p√©riode out-of-sample."
            )
        else:
            return (
                "üî¥ CRITIQUE: Overfitting s√©v√®re d√©tect√©! "
                "Strat√©gie non fiable pour production. "
                "Actions requises: r√©duire drastiquement les param√®tres, "
                "utiliser walk-forward plus long, revoir la logique de strat√©gie."
            )


def check_temporal_integrity(data: pd.DataFrame) -> bool:
    """
    V√©rifie l'int√©grit√© temporelle des donn√©es de backtest.

    Effectue les v√©rifications suivantes:
    - Index est DatetimeIndex
    - Pas de donn√©es futures (> maintenant)
    - Pas de timestamps dupliqu√©s
    - Ordre chronologique strict
    - Pas de trous temporels excessifs (optionnel)

    Parameters:
        data: DataFrame avec index temporel

    Returns:
        True si toutes les v√©rifications passent

    Raises:
        ValueError: Si une v√©rification √©choue

    Examples:
        >>> check_temporal_integrity(df)
        True
    """
    # V√©rifier type d'index
    if not isinstance(data.index, pd.DatetimeIndex):
        raise ValueError(
            f"Index doit √™tre DatetimeIndex, re√ßu: {type(data.index).__name__}"
        )

    # V√©rifier pas de donn√©es futures
    now = pd.Timestamp.now(tz="UTC")
    if data.index.max() > now:
        raise ValueError(
            f"‚ùå DONN√âES FUTURES D√âTECT√âES - Look-ahead bias!\n"
            f"Date max dans donn√©es: {data.index.max()}\n"
            f"Date actuelle: {now}\n"
            f"Ceci indique un probl√®me de donn√©es ou de timestamps."
        )

    # V√©rifier duplicates
    if data.index.duplicated().any():
        duplicates = data.index[data.index.duplicated()].unique()
        raise ValueError(
            f"‚ùå TIMESTAMPS DUPLIQU√âS D√âTECT√âS: {len(duplicates)} dates\n"
            f"Exemples: {duplicates[:5].tolist()}\n"
            f"Les timestamps doivent √™tre uniques pour un backtest valide."
        )

    # V√©rifier ordre chronologique
    if not data.index.is_monotonic_increasing:
        raise ValueError(
            f"‚ùå INDEX NON CHRONOLOGIQUE!\n"
            f"L'index doit √™tre strictement croissant.\n"
            f"Premi√®re date: {data.index[0]}\n"
            f"Derni√®re date: {data.index[-1]}"
        )

    # V√©rifications optionnelles
    # D√©tecter trous temporels excessifs (> 7 jours pour donn√©es daily)
    if len(data) > 1:
        time_diffs = data.index.to_series().diff()
        max_gap = time_diffs.max()

        # Alerte si gap > 30 jours (peut √™tre normal pour crypto weekends)
        if max_gap > pd.Timedelta(days=30):
            logger.warning(
                f"‚ö†Ô∏è Gap temporel important d√©tect√©: {max_gap}\n"
                f"V√©rifier si normal pour votre asset (ex: market holidays)"
            )

    logger.debug(
        f"‚úÖ Int√©grit√© temporelle v√©rifi√©e: "
        f"{len(data)} rows, "
        f"[{data.index.min()} ‚Üí {data.index.max()}]"
    )

    return True


def detect_lookahead_bias(
    train_data: pd.DataFrame, test_data: pd.DataFrame, raise_on_detect: bool = True
) -> bool:
    """
    D√©tecte le look-ahead bias entre train et test sets.

    V√©rifie que:
    - Toutes les dates train < toutes les dates test
    - Pas de chevauchement temporel
    - Gap temporel suffisant (si purge configur√©)

    Parameters:
        train_data: Donn√©es d'entra√Ænement
        test_data: Donn√©es de test
        raise_on_detect: Si True, raise ValueError si bias d√©tect√©

    Returns:
        False si bias d√©tect√©, True sinon

    Raises:
        ValueError: Si bias d√©tect√© et raise_on_detect=True

    Examples:
        >>> detect_lookahead_bias(train, test)
        True
    """
    train_max = train_data.index.max()
    test_min = test_data.index.min()

    has_bias = train_max >= test_min

    if has_bias:
        msg = (
            f"‚ùå LOOK-AHEAD BIAS D√âTECT√â!\n"
            f"Train max: {train_max}\n"
            f"Test min: {test_min}\n"
            f"Gap: {test_min - train_max}\n"
            f"Les donn√©es train et test se chevauchent temporellement.\n"
            f"Ceci invalide compl√®tement le backtest!"
        )

        if raise_on_detect:
            raise ValueError(msg)
        else:
            logger.error(msg)
            return False

    logger.debug(
        f"‚úÖ Pas de look-ahead bias: "
        f"gap de {test_min - train_max} entre train et test"
    )

    return True


# Export public API
__all__ = [
    "ValidationConfig",
    "BacktestValidator",
    "check_temporal_integrity",
    "detect_lookahead_bias",
]
