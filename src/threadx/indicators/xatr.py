#!/usr/bin/env python3
"""
ThreadX ATR (Average True Range) - Impl√©mentation vectoris√©e GPU/CPU
===================================================================

Calcul vectoris√© de l'Average True Range avec support:
- GPU multi-carte (RTX 5090 + RTX 2060)
- Batch processing optimis√©
- Fallback CPU transparent
- Device-agnostic wrappers

Formule ATR:
- True Range (TR) = max(high-low, abs(high-prev_close), abs(low-prev_close))
- ATR = EMA(TR, period) ou SMA(TR, period)

Optimisations:
- Vectorisation compl√®te NumPy/CuPy
- Split GPU 75%/25% selon puissance carte
- Cache True Range pour r√©utilisation
- Synchronisation NCCL pour multi-GPU

Exemple d'usage:
    ```python
    import numpy as np
    from threadx.indicators.atr import compute_atr

    # Donn√©es OHLCV
    high = np.random.randn(1000) * 5 + 105
    low = np.random.randn(1000) * 5 + 95
    close = np.random.randn(1000) * 5 + 100

    # Calcul simple
    atr_values = compute_atr(high, low, close, period=14)

    # Calcul batch multiple p√©riodes
    from threadx.indicators.atr import compute_atr_batch
    params = [
        {'period': 14, 'method': 'ema'},
        {'period': 21, 'method': 'sma'},
        {'period': 7, 'method': 'ema'}
    ]
    results = compute_atr_batch(high, low, close, params)
    ```
"""

import logging
import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Union, Any, Literal
import numpy as np
import pandas as pd

# GPU imports avec fallback robuste
try:
    import cupy as cp

    HAS_CUPY = True
    # Test si GPU disponible
    try:
        cp.cuda.Device(0).use()
        GPU_AVAILABLE = True
        N_GPUS = cp.cuda.runtime.getDeviceCount()
    except:
        GPU_AVAILABLE = False
        N_GPUS = 0
except ImportError:
    HAS_CUPY = False
    GPU_AVAILABLE = False
    N_GPUS = 0

    # Mock robuste de CuPy avec toutes les fonctions n√©cessaires
    class MockCudaDevice:
        def __init__(self, device_id):
            self.device_id = device_id

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def use(self):
            pass

    class MockCudaRuntime:
        @staticmethod
        def getDeviceCount():
            return 0

        @staticmethod
        def getDeviceProperties(device_id):
            return {"name": "MockGPU", "totalGlobalMem": 8 * 1024**3}

        @staticmethod
        def memGetInfo():
            return (0, 8 * 1024**3)  # free, total

    class MockCuda:
        runtime = MockCudaRuntime()

        @staticmethod
        def Device(device_id):
            return MockCudaDevice(device_id)

    class MockCuPy:
        cuda = MockCuda()
        float64 = np.float64
        nan = np.nan

        @staticmethod
        def asarray(x):
            return np.asarray(x)

        @staticmethod
        def asnumpy(x):
            return np.asarray(x)

        @staticmethod
        def convolve(a, v, mode="full"):
            if mode in ("full", "valid", "same"):
                return np.convolve(a, v, mode=mode)  # type: ignore
            else:
                return np.convolve(a, v, mode="full")  # type: ignore

        @staticmethod
        def ones(shape, dtype=None):
            return np.ones(shape, dtype=dtype)

        @staticmethod
        def zeros_like(a):
            return np.zeros_like(a)

        @staticmethod
        def concatenate(arrays):
            return np.concatenate(arrays)

        @staticmethod
        def full(shape, fill_value, dtype=None):
            return np.full(shape, fill_value, dtype=dtype)

        @staticmethod
        def array(data, dtype=None):
            return np.array(data, dtype=dtype)

        @staticmethod
        def abs(x):
            return np.abs(x)

        @staticmethod
        def maximum(x1, x2):
            return np.maximum(x1, x2)

        @staticmethod
        def exp(x):
            return np.exp(x)

    cp = MockCuPy()

# Configuration logging
logger = logging.getLogger(__name__)


@dataclass
class ATRSettings:
    """Configuration pour calculs ATR"""

    period: int = 14
    method: Literal["ema", "sma"] = "ema"
    use_gpu: bool = True
    gpu_batch_size: int = 1000
    cpu_fallback: bool = True
    gpu_split_ratio: Tuple[float, float] = (0.75, 0.25)  # RTX 5090 / RTX 2060

    def __post_init__(self):
        """Validation des param√®tres"""
        if self.period < 1:
            raise ValueError(f"Period doit √™tre >= 1, re√ßu: {self.period}")
        if self.method not in ["ema", "sma"]:
            raise ValueError(f"Method doit √™tre 'ema' ou 'sma', re√ßu: {self.method}")
        if not (0.1 <= sum(self.gpu_split_ratio) <= 1.0):
            raise ValueError(f"gpu_split_ratio invalide: {self.gpu_split_ratio}")


class ATRGPUManager:
    """Gestionnaire GPU multi-carte pour ATR"""

    def __init__(self, settings: ATRSettings):
        self.settings = settings
        self.available_gpus = []
        self.gpu_capabilities = {}

        if HAS_CUPY and GPU_AVAILABLE:
            self._detect_gpus()
            logger.info(
                f"üî• ATR GPU Manager: {len(self.available_gpus)} GPU(s) d√©tect√©s"
            )
            for gpu_id, cap in self.gpu_capabilities.items():
                logger.debug(f"   GPU {gpu_id}: {cap['name']} ({cap['memory']:.1f}GB)")

    def _detect_gpus(self):
        """D√©tection et profilage des GPU disponibles"""
        try:
            for gpu_id in range(N_GPUS):
                with cp.cuda.Device(gpu_id):
                    # Infos GPU
                    props = cp.cuda.runtime.getDeviceProperties(gpu_id)
                    memory_total = cp.cuda.runtime.memGetInfo()[1] / (1024**3)  # GB

                    self.available_gpus.append(gpu_id)
                    self.gpu_capabilities[gpu_id] = {
                        "name": props["name"].decode("utf-8"),
                        "memory": memory_total,
                        "compute_capability": (props["major"], props["minor"]),
                        "multiprocessors": props["multiProcessorCount"],
                    }
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur d√©tection GPU ATR: {e}")
            self.available_gpus = []

    def split_workload(self, data_size: int) -> List[Tuple[int, int, int]]:
        """Split workload entre GPU selon leurs capacit√©s"""
        if not self.available_gpus:
            return []

        splits = []
        if len(self.available_gpus) == 1:
            splits.append((self.available_gpus[0], 0, data_size))
        elif len(self.available_gpus) >= 2:
            gpu1_size = int(data_size * self.settings.gpu_split_ratio[0])
            gpu2_size = data_size - gpu1_size

            splits.append((self.available_gpus[0], 0, gpu1_size))
            if gpu2_size > 0:
                splits.append((self.available_gpus[1], gpu1_size, data_size))

        logger.debug(f"üîÑ ATR Workload split: {[(s[0], s[2]-s[1]) for s in splits]}")
        return splits


class ATR:
    """Calculateur ATR vectoris√© avec support GPU multi-carte"""

    def __init__(self, settings: Optional[ATRSettings] = None):
        self.settings = settings or ATRSettings()
        self.gpu_manager = ATRGPUManager(self.settings)
        self._cache = {}  # Cache pour True Range r√©utilisables

        logger.info(
            f"üéØ ATR initialis√© - GPU: {GPU_AVAILABLE}, Multi-GPU: {len(self.gpu_manager.available_gpus)}"
        )

    def compute(
        self,
        high: Union[np.ndarray, pd.Series],
        low: Union[np.ndarray, pd.Series],
        close: Union[np.ndarray, pd.Series],
        period: Optional[Union[int, str]] = None,
        method: Optional[Union[str, int]] = None,
    ) -> np.ndarray:
        """
        Calcul ATR pour une s√©rie de prix OHLC

        Args:
            high: Prix hauts (array-like)
            low: Prix bas (array-like)
            close: Prix de cl√¥ture (array-like)
            period: P√©riode pour moyenne (d√©faut: settings.period)
            method: 'ema' ou 'sma' (d√©faut: settings.method)

        Returns:
            np.ndarray: Valeurs ATR

        Exemple:
            ```python
            atr = ATR()
            atr_values = atr.compute(highs, lows, closes, period=14, method='ema')
            ```
        """
        # Param√®tres avec conversions
        period = int(period) if period is not None else self.settings.period
        if method is not None and method in ["ema", "sma"]:
            method = str(method)
        else:
            method = self.settings.method

        # Conversion en numpy avec types pr√©cis
        if isinstance(high, pd.Series):
            high = np.asarray(high.values, dtype=np.float64)
        else:
            high = np.asarray(high, dtype=np.float64)

        if isinstance(low, pd.Series):
            low = np.asarray(low.values, dtype=np.float64)
        else:
            low = np.asarray(low, dtype=np.float64)

        if isinstance(close, pd.Series):
            close = np.asarray(close.values, dtype=np.float64)
        else:
            close = np.asarray(close, dtype=np.float64)

        high = np.asarray(high, dtype=np.float64)
        low = np.asarray(low, dtype=np.float64)
        close = np.asarray(close, dtype=np.float64)

        # Validation tailles
        if not (len(high) == len(low) == len(close)):
            raise ValueError(
                f"Tailles diff√©rentes: high={len(high)}, low={len(low)}, close={len(close)}"
            )

        if len(close) < period + 1:  # +1 pour calcul True Range
            raise ValueError(
                f"Donn√©es insuffisantes: {len(close)} < period+1={period+1}"
            )

        # Tentative GPU
        if (
            self.settings.use_gpu
            and GPU_AVAILABLE
            and len(self.gpu_manager.available_gpus) > 0
        ):
            try:
                return self._compute_gpu(high, low, close, period, method)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ATR GPU failed, fallback CPU: {e}")
                if not self.settings.cpu_fallback:
                    raise

        # Fallback CPU
        return self._compute_cpu(high, low, close, period, method)

    def _compute_gpu(
        self,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        period: int,
        method: str,
    ) -> np.ndarray:
        """Calcul ATR GPU avec r√©partition multi-carte"""

        # Transfert vers GPU principal
        with cp.cuda.Device(self.gpu_manager.available_gpus[0]):
            high_gpu = cp.asarray(high)
            low_gpu = cp.asarray(low)
            close_gpu = cp.asarray(close)

            # True Range
            tr = self._true_range_gpu(high_gpu, low_gpu, close_gpu)

            # ATR selon m√©thode
            if method == "ema":
                atr = self._ema_gpu(tr, period)
            else:  # sma
                atr = self._sma_gpu(tr, period)

            # Retour CPU
            return cp.asnumpy(atr)

    def _true_range_gpu(self, high_gpu, low_gpu, close_gpu):
        """Calcul True Range GPU vectoris√©"""
        n = len(high_gpu)

        # D√©calage close pour avoir prev_close
        prev_close = cp.concatenate([cp.array([close_gpu[0]]), close_gpu[:-1]])

        # Trois composantes du True Range
        hl_diff = high_gpu - low_gpu
        hc_diff = cp.abs(high_gpu - prev_close)
        lc_diff = cp.abs(low_gpu - prev_close)

        # Maximum des trois
        tr = cp.maximum(hl_diff, cp.maximum(hc_diff, lc_diff))

        return tr

    def _ema_gpu(self, values_gpu, period: int):
        """Exponential Moving Average GPU"""
        alpha = 2.0 / (period + 1.0)
        n = len(values_gpu)

        result = cp.zeros_like(values_gpu)
        result[0] = values_gpu[0]

        # Calcul it√©ratif EMA
        for i in range(1, n):
            result[i] = alpha * values_gpu[i] + (1 - alpha) * result[i - 1]

        return result

    def _sma_gpu(self, values_gpu, period: int):
        """Simple Moving Average GPU"""
        # Utilise convolution pour efficacit√©
        kernel = cp.ones(period, dtype=cp.float64) / period

        # Convolution avec padding
        sma = cp.convolve(values_gpu, kernel, mode="valid")

        # Padding pour aligner avec input
        padding = cp.full(period - 1, cp.nan, dtype=cp.float64)
        return cp.concatenate([padding, sma])

    def _compute_cpu(
        self,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        period: int,
        method: str,
    ) -> np.ndarray:
        """Fallback CPU vectoris√©"""

        # True Range CPU
        tr = self._true_range_cpu(high, low, close)

        # ATR selon m√©thode avec pandas pour efficacit√©
        tr_series = pd.Series(tr)

        if method == "ema":
            atr = tr_series.ewm(span=period, adjust=False).mean().values
        else:  # sma
            atr = tr_series.rolling(window=period, min_periods=period).mean().values

        return np.asarray(atr, dtype=np.float64)

    def _true_range_cpu(
        self, high: np.ndarray, low: np.ndarray, close: np.ndarray
    ) -> np.ndarray:
        """Calcul True Range CPU vectoris√©"""
        n = len(high)

        # D√©calage close pour avoir prev_close
        prev_close = np.concatenate([[close[0]], close[:-1]])

        # Trois composantes du True Range
        hl_diff = high - low
        hc_diff = np.abs(high - prev_close)
        lc_diff = np.abs(low - prev_close)

        # Maximum des trois
        tr = np.maximum(hl_diff, np.maximum(hc_diff, lc_diff))

        return tr

    def compute_batch(
        self,
        high: Union[np.ndarray, pd.Series],
        low: Union[np.ndarray, pd.Series],
        close: Union[np.ndarray, pd.Series],
        params_list: List[Dict[str, Union[int, str]]],
    ) -> Dict[str, np.ndarray]:
        """
        Calcul ATR batch pour multiples param√®tres

        Args:
            high, low, close: Prix OHLC
            params_list: Liste de dictionnaires {'period': int, 'method': str}

        Returns:
            Dict[param_key] = atr_values

        Exemple:
            ```python
            params = [
                {'period': 14, 'method': 'ema'},
                {'period': 21, 'method': 'sma'}
            ]
            results = atr.compute_batch(high, low, close, params)
            print(results['14_ema'])  # Valeurs ATR
            ```
        """
        start_time = time.time()
        results = {}

        logger.info(f"üîÑ ATR batch: {len(params_list)} param√®tres")

        # Multi-GPU batch si seuil atteint
        if (
            len(params_list) >= 100
            and self.settings.use_gpu
            and len(self.gpu_manager.available_gpus) >= 2
        ):

            return self._compute_batch_multi_gpu(high, low, close, params_list)

        # Calcul s√©quentiel
        for params in params_list:
            period = params["period"]
            method = params.get("method", "ema")
            # Conversions explicites
            period_int = int(period) if isinstance(period, (int, str)) else period
            method_str = (
                str(method)
                if isinstance(method, (str, int)) and str(method) in ["ema", "sma"]
                else "ema"
            )
            key = f"{period_int}_{method_str}"

            try:
                results[key] = self.compute(
                    high, low, close, period=period_int, method=method_str
                )
            except Exception as e:
                logger.error(f"‚ùå Erreur ATR param√®tre {key}: {e}")
                results[key] = None

        elapsed = time.time() - start_time
        success_count = sum(1 for r in results.values() if r is not None)
        logger.info(
            f"‚úÖ ATR batch termin√©: {success_count}/{len(params_list)} succ√®s en {elapsed:.2f}s"
        )

        return results

    def _compute_batch_multi_gpu(
        self,
        high: Union[np.ndarray, pd.Series],
        low: Union[np.ndarray, pd.Series],
        close: Union[np.ndarray, pd.Series],
        params_list: List[Dict[str, Union[int, str]]],
    ) -> Dict[str, np.ndarray]:
        """Calcul ATR batch multi-GPU avec r√©partition"""

        logger.info(
            f"üöÄ ATR Multi-GPU batch: {len(params_list)} param√®tres sur {len(self.gpu_manager.available_gpus)} GPU"
        )

        # Split param√®tres entre GPU
        workload_splits = self.gpu_manager.split_workload(len(params_list))
        results = {}

        # Conversion donn√©es avec types pr√©cis
        if isinstance(high, pd.Series):
            high = np.asarray(high.values, dtype=np.float64)
        else:
            high = np.asarray(high, dtype=np.float64)

        if isinstance(low, pd.Series):
            low = np.asarray(low.values, dtype=np.float64)
        else:
            low = np.asarray(low, dtype=np.float64)

        if isinstance(close, pd.Series):
            close = np.asarray(close.values, dtype=np.float64)
        else:
            close = np.asarray(close, dtype=np.float64)

        high = np.asarray(high, dtype=np.float64)
        low = np.asarray(low, dtype=np.float64)
        close = np.asarray(close, dtype=np.float64)

        # Traitement par GPU
        for gpu_id, start_idx, end_idx in workload_splits:
            gpu_params = params_list[start_idx:end_idx]

            with cp.cuda.Device(gpu_id):
                logger.debug(
                    f"üî• GPU {gpu_id}: traitement ATR {len(gpu_params)} param√®tres"
                )

                high_gpu = cp.asarray(high)
                low_gpu = cp.asarray(low)
                close_gpu = cp.asarray(close)

                # True Range commun (r√©utilisable)
                tr = self._true_range_gpu(high_gpu, low_gpu, close_gpu)

                for params in gpu_params:
                    period = params["period"]
                    method = params.get("method", "ema")
                    # Conversions explicites pour GPU
                    period_int = (
                        int(period) if isinstance(period, (int, str)) else period
                    )
                    method_str = (
                        str(method)
                        if isinstance(method, (str, int))
                        and str(method) in ["ema", "sma"]
                        else "ema"
                    )
                    key = f"{period_int}_{method_str}"

                    try:
                        # ATR selon m√©thode
                        if method_str == "ema":
                            atr = self._ema_gpu(tr, period_int)
                        else:  # sma
                            atr = self._sma_gpu(tr, period_int)

                        # Retour CPU
                        results[key] = cp.asnumpy(atr)

                    except Exception as e:
                        logger.error(f"‚ùå GPU {gpu_id} erreur ATR {key}: {e}")
                        results[key] = None

        return results


# ========================================
# FONCTIONS PUBLIQUES (API simplifi√©e)
# ========================================


def compute_atr(
    high: Union[np.ndarray, pd.Series],
    low: Union[np.ndarray, pd.Series],
    close: Union[np.ndarray, pd.Series],
    period: int = 14,
    method: Literal["ema", "sma"] = "ema",
    use_gpu: bool = True,
) -> np.ndarray:
    """
    Calcul ATR - API simple

    Args:
        high: Prix hauts
        low: Prix bas
        close: Prix de cl√¥ture
        period: P√©riode pour moyenne (d√©faut: 14)
        method: 'ema' ou 'sma' (d√©faut: 'ema')
        use_gpu: Utiliser GPU si disponible (d√©faut: True)

    Returns:
        np.ndarray: Valeurs ATR

    Exemple:
        ```python
        import numpy as np
        from threadx.indicators.atr import compute_atr

        # Donn√©es test OHLC
        n = 1000
        high = np.random.randn(n) * 5 + 105
        low = np.random.randn(n) * 5 + 95
        close = np.random.randn(n) * 5 + 100

        # Calcul ATR
        atr_values = compute_atr(high, low, close, period=14, method='ema')

        print(f"ATR calcul√©: {len(atr_values)} points")
        print(f"ATR moyen: {np.nanmean(atr_values):.4f}")
        print(f"ATR dernier: {atr_values[-1]:.4f}")
        ```
    """
    settings = ATRSettings(period=period, method=method, use_gpu=use_gpu)

    atr = ATR(settings)
    return atr.compute(high, low, close)


def compute_atr_batch(
    high: Union[np.ndarray, pd.Series],
    low: Union[np.ndarray, pd.Series],
    close: Union[np.ndarray, pd.Series],
    params_list: List[Dict[str, Union[int, str]]],
    use_gpu: bool = True,
) -> Dict[str, np.ndarray]:
    """
    Calcul ATR batch - API simple

    Args:
        high, low, close: Prix OHLC
        params_list: Liste param√®tres [{'period': int, 'method': str}, ...]
        use_gpu: Utiliser GPU si disponible (d√©faut: True)

    Returns:
        Dict[param_key] = atr_values

    Exemple:
        ```python
        from threadx.indicators.atr import compute_atr_batch

        params = [
            {'period': 14, 'method': 'ema'},
            {'period': 21, 'method': 'sma'},
            {'period': 7, 'method': 'ema'}
        ]

        results = compute_atr_batch(high, low, close, params)

        for key, atr_vals in results.items():
            if atr_vals is not None:
                print(f"{key}: ATR dernier={atr_vals[-1]:.4f}, moyenne={np.nanmean(atr_vals):.4f}")
        ```
    """
    settings = ATRSettings(use_gpu=use_gpu)
    atr = ATR(settings)
    return atr.compute_batch(high, low, close, params_list)


# ========================================
# UTILITAIRES ET VALIDATION
# ========================================


def validate_atr_results(atr_values: np.ndarray, tolerance: float = 1e-10) -> bool:
    """
    Validation r√©sultats ATR

    Args:
        atr_values: Valeurs ATR calcul√©es
        tolerance: Tol√©rance pour comparaisons num√©riques

    Returns:
        True si r√©sultats valides
    """
    try:
        # ATR doit √™tre >= 0 (hors NaN)
        valid_mask = ~np.isnan(atr_values)
        if np.any(valid_mask):
            valid_atr = atr_values[valid_mask]
            if np.any(valid_atr < -tolerance):
                return False

        # Pas de valeurs infinies
        if np.any(np.isinf(atr_values)):
            return False

        return True

    except Exception:
        return False


def benchmark_atr_performance(
    data_sizes: List[int] = [1000, 5000, 10000], n_runs: int = 3
) -> Dict[str, Any]:
    """
    Benchmark performance ATR CPU vs GPU

    Args:
        data_sizes: Tailles de donn√©es √† tester
        n_runs: Nombre d'ex√©cutions par test

    Returns:
        Dict avec m√©triques de performance
    """
    results = {
        "cpu_times": {},
        "gpu_times": {},
        "speedups": {},
        "gpu_available": GPU_AVAILABLE,
    }

    logger.info(f"üèÅ Benchmark ATR - GPU: {GPU_AVAILABLE}")

    for size in data_sizes:
        logger.info(f"üìä Test size: {size}")

        # Donn√©es test OHLC
        np.random.seed(42)
        high = np.random.randn(size) * 5 + 105
        low = np.random.randn(size) * 5 + 95
        close = np.random.randn(size) * 5 + 100

        # CPU timing
        cpu_times = []
        for _ in range(n_runs):
            start = time.time()
            compute_atr(high, low, close, use_gpu=False)
            cpu_times.append(time.time() - start)

        cpu_avg = np.mean(cpu_times)
        results["cpu_times"][size] = cpu_avg

        # GPU timing si disponible
        if GPU_AVAILABLE:
            gpu_times = []
            for _ in range(n_runs):
                start = time.time()
                compute_atr(high, low, close, use_gpu=True)
                gpu_times.append(time.time() - start)

            gpu_avg = np.mean(gpu_times)
            results["gpu_times"][size] = gpu_avg
            results["speedups"][size] = cpu_avg / gpu_avg

            logger.info(
                f"   CPU: {cpu_avg:.4f}s, GPU: {gpu_avg:.4f}s, Speedup: {cpu_avg/gpu_avg:.2f}x"
            )
        else:
            results["gpu_times"][size] = None
            results["speedups"][size] = None
            logger.info(f"   CPU: {cpu_avg:.4f}s, GPU: N/A")

    return results


if __name__ == "__main__":
    # Test rapide
    print("üéØ ThreadX ATR - Test rapide")

    # Donn√©es test OHLC
    np.random.seed(42)
    n = 1000
    high = np.random.randn(n) * 5 + 105
    low = np.random.randn(n) * 5 + 95
    close = np.random.randn(n) * 5 + 100

    # Test simple
    atr_values = compute_atr(high, low, close, period=14, method="ema")
    print(f"‚úÖ Test simple: {len(atr_values)} points calcul√©s")
    print(f"   ATR dernier: {atr_values[-1]:.4f}")
    print(f"   ATR moyen: {np.nanmean(atr_values):.4f}")

    # Test batch
    params = [{"period": 14, "method": "ema"}, {"period": 21, "method": "sma"}]
    results = compute_atr_batch(high, low, close, params)
    print(f"‚úÖ Test batch: {len(results)} r√©sultats")

    # Validation
    valid = validate_atr_results(atr_values)
    print(f"‚úÖ Validation: {'PASS' if valid else 'FAIL'}")

    # Benchmark si GPU
    if GPU_AVAILABLE:
        print("üèÅ Benchmark performance...")
        bench = benchmark_atr_performance([1000], n_runs=2)
        if bench["speedups"][1000]:
            print(f"   Speedup GPU: {bench['speedups'][1000]:.2f}x")



