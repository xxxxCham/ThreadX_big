"""
Base Agent Class for ThreadX LLM System
========================================

Classe abstraite fournissant les fonctionnalitÃ©s communes Ã  tous les agents LLM.

Features:
- Gestion du timeout et retries automatiques
- Logging structurÃ© avec contexte agent
- Validation des rÃ©ponses LLM
- MÃ©triques de performance (latence, token usage)
"""

from __future__ import annotations

import json
import logging
import time
from abc import ABC, abstractmethod
from typing import Any

from threadx.llm.client import LLMClient


class BaseAgent(ABC):
    """
    Classe abstraite pour agents LLM spÃ©cialisÃ©s.

    Attributes:
        name: Nom de l'agent (utilisÃ© pour logging)
        model: ModÃ¨le Ollama Ã  utiliser
        client: Instance LLMClient configurÃ©e
        timeout: Timeout par dÃ©faut pour requÃªtes LLM
        logger: Logger avec prÃ©fixe [AgentName]
    """

    def __init__(
        self,
        name: str,
        model: str,
        timeout: float = 60.0,
        max_retries: int = 2,
        debug: bool = False,
    ):
        """
        Initialise un agent LLM.

        Args:
            name: Nom de l'agent (ex: "Analyst", "Strategist")
            model: ModÃ¨le Ollama (ex: "deepseek-r1:70b", "gpt-oss:20b")
            timeout: Timeout en secondes pour requÃªtes LLM
            max_retries: Nombre de tentatives en cas d'Ã©chec
            debug: Active logging dÃ©taillÃ©
        """
        self.name = name
        self.model = model
        self.timeout = timeout
        self.max_retries = max_retries
        self.debug = debug

        # Logger avec prÃ©fixe agent
        self.logger = logging.getLogger(f"threadx.llm.agents.{name.lower()}")
        if debug:
            self.logger.setLevel(logging.DEBUG)

        # Client LLM partagÃ©
        self.client = LLMClient(
            model=model, timeout=timeout, max_retries=max_retries, debug=debug
        )

        # MÃ©triques de performance
        self._metrics = {"total_calls": 0, "total_time": 0.0, "errors": 0}

        self.logger.info(
            f"ðŸ¤– Agent {name} initialisÃ© (model={model}, timeout={timeout}s)"
        )

    def _call_llm(
        self,
        prompt: str,
        system: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> str:
        """
        Appel LLM avec tracking des mÃ©triques.

        Args:
            prompt: Prompt utilisateur
            system: Message systÃ¨me optionnel
            temperature: TempÃ©rature de gÃ©nÃ©ration (0.0 = dÃ©terministe)
            max_tokens: Nombre max de tokens gÃ©nÃ©rÃ©s

        Returns:
            RÃ©ponse texte du LLM

        Raises:
            RuntimeError: Si LLM Ã©choue aprÃ¨s max_retries
        """
        start_time = time.time()
        self._metrics["total_calls"] += 1

        try:
            if self.debug:
                self.logger.debug(f"ðŸ“¤ LLM Call - Prompt preview: {prompt[:200]}...")

            response = self.client.complete(
                prompt=prompt,
                system=system,
                temperature=temperature,
                max_tokens=max_tokens,
            )

            elapsed = time.time() - start_time
            self._metrics["total_time"] += elapsed

            if self.debug:
                self.logger.debug(
                    f"ðŸ“¥ LLM Response ({elapsed:.2f}s): {response[:150]}..."
                )

            return response

        except Exception as e:
            self._metrics["errors"] += 1
            self.logger.error(f"âŒ LLM call failed: {e}")
            raise

    def _call_llm_structured(
        self,
        prompt: str,
        expected_schema: dict[str, Any] | None = None,
        system: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> dict[str, Any]:
        """
        Appel LLM avec parsing JSON et validation de schÃ©ma.

        Args:
            prompt: Prompt utilisateur (doit demander JSON en output)
            expected_schema: SchÃ©ma attendu {key: type} pour validation (optionnel)
            system: Message systÃ¨me optionnel
            temperature: TempÃ©rature de gÃ©nÃ©ration
            max_tokens: Nombre max de tokens

        Returns:
            Dict parsÃ© depuis la rÃ©ponse JSON du LLM

        Raises:
            ValueError: Si rÃ©ponse non-JSON ou schÃ©ma invalide
            RuntimeError: Si LLM Ã©choue aprÃ¨s retries
        """
        start_time = time.time()
        self._metrics["total_calls"] += 1

        try:
            if self.debug:
                self.logger.debug(
                    f"ðŸ“¤ LLM Structured Call - Expected schema: {expected_schema}"
                )

            # Note: LLMClient.complete_structured() n'a pas de param expected_schema
            # On appelle directement et valide manuellement si besoin
            response = self.client.complete_structured(
                prompt=prompt,
                system=system,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            # Validation optionnelle du schÃ©ma (basique)
            if expected_schema:
                missing_keys = set(expected_schema.keys()) - set(response.keys())
                if missing_keys:
                    self.logger.warning(
                        "Schema validation: missing keys %s", missing_keys
                    )

            elapsed = time.time() - start_time
            self._metrics["total_time"] += elapsed

            if self.debug:
                self.logger.debug(
                    "Structured Response (%.2fs): %s...",
                    elapsed,
                    json.dumps(response, indent=2)[:300],
                )

            return response

        except Exception as e:
            self._metrics["errors"] += 1
            self.logger.error("Structured LLM call failed: %s", e)
            raise

    def get_metrics(self) -> dict[str, Any]:
        """
        RÃ©cupÃ¨re les mÃ©triques de performance de l'agent.

        Returns:
            {
                "total_calls": int,
                "total_time": float,
                "avg_time_per_call": float,
                "errors": int,
                "success_rate": float
            }
        """
        total_calls = self._metrics["total_calls"]
        avg_time = (
            self._metrics["total_time"] / total_calls if total_calls > 0 else 0.0
        )
        success_rate = (
            (total_calls - self._metrics["errors"]) / total_calls
            if total_calls > 0
            else 0.0
        )

        return {
            "agent_name": self.name,
            "model": self.model,
            "total_calls": total_calls,
            "total_time": self._metrics["total_time"],
            "avg_time_per_call": avg_time,
            "errors": self._metrics["errors"],
            "success_rate": success_rate,
        }

    def reset_metrics(self):
        """Reset les mÃ©triques de performance."""
        self._metrics = {"total_calls": 0, "total_time": 0.0, "errors": 0}
        self.logger.debug("ðŸ“Š MÃ©triques reset")

    @abstractmethod
    def analyze(self, *args, **kwargs) -> dict[str, Any]:
        """
        MÃ©thode abstraite pour analyse principale de l'agent.

        Chaque agent spÃ©cialisÃ© doit implÃ©menter cette mÃ©thode
        avec sa logique spÃ©cifique (ex: analyze_sweep_results pour Analyst).
        """
        raise NotImplementedError("Subclasses must implement analyze()")


    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(name={self.name}, model={self.model})"
